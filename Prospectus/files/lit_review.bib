
@article{abrams_meta-analysis_2005,
  title = {Meta-Analysis of Heterogeneously Reported Trials Assessing Change from Baseline},
  author = {Abrams, Keith R. and Gillies, Clare L. and Lambert, Paul C.},
  year = {2005},
  volume = {24},
  pages = {3823--3844},
  issn = {1097-0258},
  doi = {10.1002/sim.2423},
  abstract = {This paper considers the quantitative synthesis of published comparative study results when the outcome measures used in the individual studies and the way in which they are reported varies between studies. Whilst the former difficulty may be overcome, at least to a limited extent, by the use of standardized effects, the latter is often more problematic. Two potential solutions to this problem are; sensitivity analyses and a fully Bayesian approach, in which pertinent external information is included. Both approaches are illustrated using the results of two systematic reviews and meta-analyses which consider the difference in mean change in systolic blood pressure and the difference in physical functioning between an intervention and control group. The two examples illustrate that by adopting a fully Bayesian approach, as opposed to undertaking sensitivity analyses assuming fixed values for unknown parameters, the overall intervention effect can be estimated with greater uncertainty, but that assessing the sensitivity of results to choice of prior distributions in such analyses is crucial. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/MY3IB3NU/Abrams et al. - 2005 - Meta-analysis of heterogeneously reported trials a.pdf;/Users/rritaz/Zotero/storage/W477HC4J/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,effect size estimation (series)},
  language = {en},
  number = {24}
}

@article{achana_extending_2013,
  title = {Extending Methods for Investigating the Relationship between Treatment Effect and Baseline Risk from Pairwise Meta-Analysis to Network Meta-Analysis},
  author = {Achana, Felix A. and Cooper, Nicola J. and Dias, Sofia and Lu, Guobing and Rice, Stephen J. C. and Kendrick, Denise and Sutton, Alex J.},
  year = {2013},
  volume = {32},
  pages = {752--771},
  issn = {1097-0258},
  doi = {10.1002/sim.5539},
  abstract = {Baseline risk is a proxy for unmeasured but important patient-level characteristics, which may be modifiers of treatment effect, and is a potential source of heterogeneity in meta-analysis. Models adjusting for baseline risk have been developed for pairwise meta-analysis using the observed event rate in the placebo arm and taking into account the measurement error in the covariate to ensure that an unbiased estimate of the relationship is obtained. Our objective is to extend these methods to network meta-analysis where it is of interest to adjust for baseline imbalances in the non-intervention group event rate to reduce both heterogeneity and possibly inconsistency. This objective is complicated in network meta-analysis by this covariate being sometimes missing, because of the fact that not all studies in a network may have a non-active intervention arm. A random-effects meta-regression model allowing for inclusion of multi-arm trials and trials without a `non-intervention' arm is developed. Analyses are conducted within a Bayesian framework using the WinBUGS software. The method is illustrated using two examples: (i) interventions to promote functional smoke alarm ownership by households with children and (ii) analgesics to reduce post-operative morphine consumption following a major surgery. The results showed no evidence of baseline effect in the smoke alarm example, but the analgesics example shows that the adjustment can greatly reduce heterogeneity and improve overall model fit. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/XMLQ7NWS/Achana et al. - 2013 - Extending methods for investigating the relationsh.pdf;/Users/rritaz/Zotero/storage/4P9F473B/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),network meta-analysis,random-effects},
  language = {en},
  number = {5}
}

@article{achana_network_2014,
  title = {Network Meta-Analysis of Multiple Outcome Measures Accounting for Borrowing of Information across Outcomes},
  author = {Achana, Felix A and Cooper, Nicola J and Bujkiewicz, Sylwia and Hubbard, Stephanie J and Kendrick, Denise and Jones, David R and Sutton, Alex J},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {92},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-92},
  abstract = {Background: Network meta-analysis (NMA) enables simultaneous comparison of multiple treatments while preserving randomisation. When summarising evidence to inform an economic evaluation, it is important that the analysis accurately reflects the dependency structure within the data, as correlations between outcomes may have implication for estimating the net benefit associated with treatment. A multivariate NMA offers a framework for evaluating multiple treatments across multiple outcome measures while accounting for the correlation structure between outcomes. Methods: The standard NMA model is extended to multiple outcome settings in two stages. In the first stage, information is borrowed across outcomes as well across studies through modelling the within-study and between-study correlation structure. In the second stage, we make use of the additional assumption that intervention effects are exchangeable between outcomes to predict effect estimates for all outcomes, including effect estimates on outcomes where evidence is either sparse or the treatment had not been considered by any one of the studies included in the analysis. We apply the methods to binary outcome data from a systematic review evaluating the effectiveness of nine home safety interventions on uptake of three poisoning prevention practices (safe storage of medicines, safe storage of other household products, and possession of poison centre control telephone number) in households with children. Analyses are conducted in WinBUGS using Markov Chain Monte Carlo (MCMC) simulations. Results: Univariate and the first stage multivariate models produced broadly similar point estimates of intervention effects but the uncertainty around the multivariate estimates varied depending on the prior distribution specified for the between-study covariance structure. The second stage multivariate analyses produced more precise effect estimates while enabling intervention effects to be predicted for all outcomes, including intervention effects on outcomes not directly considered by the studies included in the analysis. Conclusions: Accounting for the dependency between outcomes in a multivariate meta-analysis may or may not improve the precision of effect estimates from a network meta-analysis compared to analysing each outcome separately.},
  file = {/Users/rritaz/Zotero/storage/FSGB4Q9F/Achana et al. - 2014 - Network meta-analysis of multiple outcome measures.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {1}
}

@article{ades_simultaneous_2015,
  title = {Simultaneous Synthesis of Treatment Effects and Mapping to a Common Scale: An Alternative to Standardisation},
  shorttitle = {Simultaneous Synthesis of Treatment Effects and Mapping to a Common Scale},
  author = {Ades, A. E. and Lu, Guobing and Dias, Sofia and Mayo-Wilson, Evan and Kounali, Daphne},
  year = {2015},
  volume = {6},
  pages = {96--107},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1130},
  abstract = {Objective Trials often may report several similar outcomes measured on different test instruments. We explored a method for synthesising treatment effect information both within and between trials and for reporting treatment effects on a common scale as an alternative to standardisation Study design We applied a procedure that simultaneously estimates a pooled treatment effect and the ``mapping'' ratios between the treatment effects on test instruments in a connected network. Standardised and non-standardised treatment effects were compared. The methods were illustrated in a dataset of 22 trials of selective serotonin reuptake inhibitors against placebo for social anxiety disorder, each reporting treatment effects on between one and six of a total nine test instruments. Results Ratios of treatment effects on different test instruments varied from trial to trial, with a coefficient of variation of 18\% (95\% credible interval 11\textendash 29\%). Standardised effect models fitted the data less well, and standardised treatment effects were estimated with less relative precision than non-standardised effects and with greater relative heterogeneity. Conclusion Simultaneous synthesis of treatment effects and mapping to a common scale make fewer assumptions than standardising by dividing effects by the sample standard deviation, allow results to be reported on a common scale, and deliver estimates with superior relative precision. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZXDF2WQC/Ades et al. - 2015 - Simultaneous synthesis of treatment effects and ma.pdf;/Users/rritaz/Zotero/storage/NQGBV84X/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {effect size estimation (series)},
  language = {en},
  number = {1}
}

@article{aert_multistep_2018,
  title = {Multistep Estimators of the Between-Study Variance: {{The}} Relationship with the {{Paule}}-{{Mandel}} Estimator},
  shorttitle = {Multistep Estimators of the Between-Study Variance},
  author = {van Aert, Robbie C. M. and Jackson, Dan},
  year = {2018},
  volume = {37},
  pages = {2616--2629},
  issn = {1097-0258},
  doi = {10.1002/sim.7665},
  abstract = {A wide variety of estimators of the between-study variance are available in random-effects meta-analysis. Many, but not all, of these estimators are based on the method of moments. The DerSimonian-Laird estimator is widely used in applications, but the Paule-Mandel estimator is an alternative that is now recommended. Recently, DerSimonian and Kacker have developed two-step moment-based estimators of the between-study variance. We extend these two-step estimators so that multiple (more than two) steps are used. We establish the surprising result that the multistep estimator tends towards the Paule-Mandel estimator as the number of steps becomes large. Hence, the iterative scheme underlying our new multistep estimator provides a hitherto unknown relationship between two-step estimators and Paule-Mandel estimator. Our analysis suggests that two-step estimators are not necessarily distinct estimators in their own right; instead, they are quantities that are closely related to the usual iterative scheme that is used to calculate the Paule-Mandel estimate. The relationship that we establish between the multistep and Paule-Mandel estimator is another justification for the use of the latter estimator. Two-step and multistep estimators are perhaps best conceptualized as approximate Paule-Mandel estimators.},
  file = {/Users/rritaz/Zotero/storage/KBGSNYPW/Aert and Jackson - 2018 - Multistep estimators of the between-study variance.pdf;/Users/rritaz/Zotero/storage/VC3BVTHC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity estimators,random-effects},
  language = {en},
  number = {17}
}

@article{aert_new_2019,
  title = {A New Justification of the {{Hartung}}-{{Knapp}} Method for Random-Effects Meta-Analysis Based on Weighted Least Squares Regression},
  author = {van Aert, Robbie C. M. and Jackson, Dan},
  year = {2019},
  volume = {10},
  pages = {515--527},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1356},
  abstract = {The Hartung-Knapp method for random-effects meta-analysis, that was also independently proposed by Sidik and Jonkman, is becoming advocated for general use. This method has previously been justified by taking all estimated variances as known and using a different pivotal quantity to the more conventional one when making inferences about the average effect. We provide a new conceptual framework for, and justification of, the Hartung-Knapp method. Specifically, we show that inferences from fitted random-effects models, using both the conventional and the Hartung-Knapp method, are equivalent to those from closely related intercept only weighted least squares regression models. This observation provides a new link between Hartung and Knapp's methodology for meta-analysis and standard linear models, where it can be seen that the Hartung-Knapp method can be justified by a linear model that makes a slightly weaker assumption than taking all variances as known. This provides intuition for why the Hartung-Knapp method has been found to perform better than the conventional one in simulation studies. Furthermore, our new findings give more credence to ad hoc adjustments of confidence intervals from the Hartung-Knapp method that ensure these are at least as wide as more conventional confidence intervals. The conceptual basis for the Hartung-Knapp method that we present here should replace the established one because it more clearly illustrates the potential benefit of using it.},
  file = {/Users/rritaz/Zotero/storage/2997DLF6/Aert and Jackson - 2019 - A new justification of the Hartung-Knapp method fo.pdf;/Users/rritaz/Zotero/storage/EP55QPBX/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,random effects models},
  language = {en},
  number = {4}
}

@article{aert_statistical_2019,
  title = {Statistical Properties of Methods Based on the {{Q}}-Statistic for Constructing a Confidence Interval for the between-Study Variance in Meta-Analysis},
  author = {van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and Viechtbauer, Wolfgang},
  year = {2019},
  volume = {10},
  pages = {225--239},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1336},
  abstract = {The effect sizes of studies included in a meta-analysis do often not share a common true effect size due to differences in for instance the design of the studies. Estimates of this so-called between-study variance are usually imprecise. Hence, reporting a confidence interval together with a point estimate of the amount of between-study variance facilitates interpretation of the meta-analytic results. Two methods that are recommended to be used for creating such a confidence interval are the Q-profile and generalized Q-statistic method that both make use of the Q-statistic. These methods are exact if the assumptions underlying the random-effects model hold, but these assumptions are usually violated in practice such that confidence intervals of the methods are approximate rather than exact confidence intervals. We illustrate by means of two Monte-Carlo simulation studies with odds ratio as effect size measure that coverage probabilities of both methods can be substantially below the nominal coverage rate in situations that are representative for meta-analyses in practice. We also show that these too low coverage probabilities are caused by violations of the assumptions of the random-effects model (ie, normal sampling distributions of the effect size measure and known sampling variances) and are especially prevalent if the sample sizes in the primary studies are small.},
  file = {/Users/rritaz/Zotero/storage/E5CRRNSR/Aert et al. - 2019 - Statistical properties of methods based on the Q-s.pdf;/Users/rritaz/Zotero/storage/AAGVNU5Q/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {heterogeneity estimators,random-effects},
  language = {en},
  number = {2}
}

@article{aguirreurreta_performance_2012,
  title = {Performance of a Proportion-Based Approach to Meta-Analytic Moderator Estimation: Results from {{Monte Carlo}} Simulations},
  shorttitle = {Performance of a Proportion-Based Approach to Meta-Analytic Moderator Estimation},
  author = {Aguirre-Urreta, Miguel I. and Ellis, Michael E. and Sun, Wenying},
  year = {2012},
  volume = {3},
  pages = {11--29},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1038},
  abstract = {This research investigates the performance of a proportion-based approach to meta-analytic moderator estimation through a series of Monte Carlo simulations. This approach is most useful when the moderating potential of a categorical variable has not been recognized in primary research and thus heterogeneous groups have been pooled together as a single sample. Alternative scenarios representing different distributions of group proportions are examined along with varying numbers of studies, subjects per study, and correlation combinations. Our results suggest that the approach is largely unbiased in its estimation of the magnitude of between-group differences and performs well with regard to statistical power and type I error. In particular, the average percentage bias of the estimated correlation for the reference group is positive and largely negligible, in the 0.5\textendash 1.8\% range; the average percentage bias of the difference between correlations is also minimal, in the -0.1\textendash 1.2\% range. Further analysis also suggests both biases decrease as the magnitude of the underlying difference increases, as the number of subjects in each simulated primary study increases, and as the number of simulated studies in each meta-analysis increases. The bias was most evident when the number of subjects and the number of studies were the smallest (80 and 36, respectively). A sensitivity analysis that examines its performance in scenarios down to 12 studies and 40 primary subjects is also included. This research is the first that thoroughly examines the adequacy of the proportion-based approach. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EMJGDNY5/Aguirre‐Urreta et al. - 2012 - Performance of a proportion-based approach to meta.pdf;/Users/rritaz/Zotero/storage/K6JNLHA7/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{ahn_incorporating_2011,
  title = {Incorporating {{Quality Scores}} in {{Meta}}-{{Analysis}}},
  author = {Ahn, Soyeon and Becker, Betsy Jane},
  year = {2011},
  month = oct,
  volume = {36},
  pages = {555--585},
  issn = {1076-9986},
  doi = {10.3102/1076998610393968},
  abstract = {This paper examines the impact of quality-score weights in meta-analysis. A simulation examines the roles of study characteristics such as population effect size (ES) and its variance on the bias and mean square errors (MSEs) of the estimators for several patterns of relationship between quality and ES, and for specific patterns of systematic deviations related to quality differences. The bias and MSEs of the estimators are large when ESs from low-quality studies deviate from the population ES in specific ways, and bias does not approach zero in these cases. Because meta-analysts can never know whether biases due to quality exist, and because quality weights lead to bias in almost every condition studied, we recommend against the use of quality weights.},
  file = {/Users/rritaz/Zotero/storage/XBMSC9DJ/Ahn and Becker - 2011 - Incorporating Quality Scores in Meta-Analysis.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {effect size estimation (series)},
  number = {5}
}

@article{aiello_assessing_2011,
  title = {Assessing Covariate Imbalance in Meta-Analysis Studies},
  author = {Aiello, Fabio and Attanasio, Massimo and Tin{\`e}, Fabio},
  year = {2011},
  volume = {30},
  pages = {2671--2682},
  issn = {1097-0258},
  doi = {10.1002/sim.4311},
  abstract = {The main goal of meta-analysis is to combine data across studies or data sets to obtain summary estimates. In this paper, the novelty is to propose a statistical tool to assess a possible covariate imbalance in baseline variables to investigate similarity of trials. We conducted the detection of the covariate imbalance, first, through some graphical comparison of the empirical cumulative distribution functions or ECDFs, which are built by putting together arms or trials according to some risk factor, and second, through some non-parametric tests such as the Kolmogorov\textendash Smirnov and the Anderson\textendash Darling tests. To overcome the huge presence of ties, we conducted the statistical tests on perturbed versions of the original data sets. The applications concern two real meta-analyses of RCTs: the first one, on interferon-alpha treatment of chronic hepatitis C, with 107 studies involved, and the second one, on cholesterol-lowering treatment with statins, with 14 studies involved. The applications allow for analysis of both when risk factors reflecting demographic or clinical differences in experimental and control arms are balanced or not and when there are structural differences between the levels of some study variables, in order to proceed eventually with the pooling of the studies. We developed our suggestion, which is a quantitative way to assess combinability in meta-analysis, only with respect to RCTs, but it could be applied to a minor extent to non-RCTs. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZXHXMW2W/Aiello et al. - 2011 - Assessing covariate imbalance in meta-analysis stu.pdf;/Users/rritaz/Zotero/storage/YC8JRYS2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,diagnostic techniques},
  language = {en},
  number = {22}
}

@article{alinaghi_meta-analysis_2018,
  title = {Meta-Analysis and Publication Bias: {{How}} Well Does the {{FAT}}-{{PET}}-{{PEESE}} Procedure Work?},
  shorttitle = {Meta-Analysis and Publication Bias},
  author = {Alinaghi, Nazila and Reed, W. Robert},
  year = {2018},
  volume = {9},
  pages = {285--311},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1298},
  abstract = {This paper studies the performance of the FAT-PET-PEESE (FPP) procedure, a commonly employed approach for addressing publication bias in the economics and business meta-analysis literature. The FPP procedure is generally used for 3 purposes: (1) to test whether a sample of estimates suffers from publication bias, (2) to test whether the estimates indicate that the effect of interest is statistically different from zero, and (3) to obtain an estimate of the mean true effect. Our findings indicate that the FPP procedure performs well in the basic but unrealistic environment of fixed effects, where all estimates are assumed to derive from a single population value and sampling error is the only reason for why studies produce different estimates. However, when we study its performance in more realistic data environments, where there is heterogeneity in the population effects across and within studies, the FPP procedure becomes unreliable for the first 2 purposes and is less efficient than other estimators when estimating overall mean effect. Further, hypothesis tests about the mean true effect are frequently unreliable. We corroborate our findings by recreating the simulation framework of Stanley and Doucouliagos (2017) and repeat our tests using their framework.},
  file = {/Users/rritaz/Zotero/storage/TLUTSCUV/Alinaghi and Reed - 2018 - Meta-analysis and publication bias How well does .pdf;/Users/rritaz/Zotero/storage/4XQ88QQI/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {publication bias},
  language = {en},
  number = {2}
}

@article{aloe_alternative_2010,
  title = {{An alternative to R2 for assessing linear models of effect size}},
  author = {Aloe, Ariel M. and Becker, Betsy Jane and Pigott, Therese D.},
  year = {2010},
  volume = {1},
  pages = {272--283},
  issn = {1759-2887},
  doi = {10.1002/jrsm.23},
  abstract = {Reviewers often use regression models in meta-analysis (`meta-regressions') to examine the relationships between effect sizes and study characteristics. In this paper, we propose and illustrate the use of an index (R) that expresses the amount of variance in the outcome that is explained by the meta-regression model. The values of R2 obtained from the standard computer output for linear models of effect size in the meta-analysis context are typically too small, because the typical R2 considers sampling variance to be unexplained whereas in meta-analysis it can be quantified. Although the idea of removing the unexplainable variance from the estimator of variance accounted for in meta-analysis is not new (Cook et al., 1992; Raudenbush, 1994) we explicitly define four estimators of variance explained, and illustrate via two examples that the typical R2 obtained in a linear model of effect size is always lower than our indices. Thus, the typical R2 underestimates the explanatory power of linear models of effect sizes. Our four estimators improve upon typical weighted R2 values. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PBU5H2M4/Aloe et al. - 2010 - An alternative to R2 for assessing linear models o.pdf;/Users/rritaz/Zotero/storage/LQ7UI9KA/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {modeling effect size variation (covariates)},
  language = {fr},
  number = {3-4}
}

@article{aloe_effect_2012,
  title = {An {{Effect Size}} for {{Regression Predictors}} in {{Meta}}-{{Analysis}}},
  author = {Aloe, Ariel M. and Becker, Betsy Jane},
  year = {2012},
  month = apr,
  volume = {37},
  pages = {278--297},
  issn = {1076-9986},
  doi = {10.3102/1076998610396901},
  abstract = {A new effect size representing the predictive power of an independent variable from a multiple regression model is presented. The index, denoted as rsp, is the semipartial correlation of the predictor with the outcome of interest. This effect size can be computed when multiple predictor variables are included in the regression model and represents a partial effect size in the correlation family. The derivations presented in this article provide the effect size and its variance. Standard errors and confidence intervals can be computed for individual rsp values. Also, meta-analysis of the semipartial correlations can proceed in a similar fashion to typical meta-analyses, where weighted analyses can be used to explore heterogeneity and to estimate central tendency and variation in the effects. The authors provide an example from a meta-analysis of studies of the relationship of teacher verbal ability to school outcomes.},
  file = {/Users/rritaz/Zotero/storage/GJMUAWN4/Aloe and Becker - 2012 - An Effect Size for Regression Predictors in Meta-A.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {continuous effect sizes,correlation coefficients},
  number = {2}
}

@article{aloe_inaccuracy_2015,
  title = {Inaccuracy of Regression Results in Replacing Bivariate Correlations},
  author = {Aloe, Ariel M.},
  year = {2015},
  volume = {6},
  pages = {21--27},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1126},
  abstract = {This manuscript considers discrepancies between the bivariate correlation and several indices of association estimated from regression results. These indices can be estimated from results typically reported in primary studies. In recent years, many researchers conducting meta-analyses have used these indices in place of, or together with, the bivariate correlation. I illustrate the differences among these indices and the bivariate correlation. I demonstrate the inaccuracy of these indices as replacements for bivariate effects. Thus, I recommend discontinuing the use of these indices and partial effect sizes as replacement for the bivariate correlation. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/B34A3MQK/Aloe - 2015 - Inaccuracy of regression results in replacing biva.pdf;/Users/rritaz/Zotero/storage/UV34X3EE/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {GLM MA models},
  language = {en},
  number = {1}
}

@article{anderson_theres_2016,
  title = {There's More than One Way to Conduct a Replication Study: {{Beyond}} Statistical Significance.},
  shorttitle = {There's More than One Way to Conduct a Replication Study},
  author = {Anderson, Samantha F. and Maxwell, Scott},
  year = {2016},
  volume = {21},
  pages = {1--12},
  doi = {10.1037/met0000051},
  abstract = {As the field of psychology struggles to trust published findings, replication research has begun to become more of a priority to both scientists and journals. With this increasing emphasis placed on reproducibility, it is essential that replication studies be capable of advancing the field. However, we argue that many researchers have been only narrowly interpreting the meaning of replication, with studies being designed with a simple statistically significant or nonsignificant results framework in mind. Although this interpretation may be desirable in some cases, we develop a variety of additional "replication goals" that researchers could consider when planning studies. Even if researchers are aware of these goals, we show that they are rarely used in practice-as results are typically analyzed in a manner only appropriate to a simple significance test. We discuss each goal conceptually, explain appropriate analysis procedures, and provide 1 or more examples to illustrate these analyses in practice. We hope that these various goals will allow researchers to develop a more nuanced understanding of replication that can be flexible enough to answer the various questions that researchers might seek to understand.},
  file = {/Users/rritaz/Zotero/storage/W8WTUY6W/Anderson and Maxwell - 2016 - There's more than one way to conduct a replication.pdf},
  journal = {Psychological methods},
  keywords = {continuous effect sizes,effect size estimation (series),publication bias},
  number = {1}
}

@article{anzurescabrera_expressing_2011,
  title = {Expressing Findings from Meta-Analyses of Continuous Outcomes in Terms of Risks},
  author = {Anzures-Cabrera, Judith and Sarpatwari, Ameet and Higgins, Julian PT},
  year = {2011},
  volume = {30},
  pages = {2967--2985},
  issn = {1097-0258},
  doi = {10.1002/sim.4298},
  abstract = {Meta-analyses of clinical trials with continuous outcome data typically report the effect of an intervention as either a mean difference or a standardized mean difference. These results can be difficult to interpret, and re-expressing the effect size in terms of risk may facilitate understanding and applicability. We describe three methods for obtaining risks in such situations. Two of these methods involve direct transformation of a standardized mean difference to an odds ratio. The third entails estimation of risks in the two groups for a specific cut point. We extend this third approach to a completed meta-analysis by expressing the finding in the format of a single `meta-study'. We compare the methods in two examples of meta-analyses and in a series of simulation studies that examine their properties in individual studies and in meta-analyses. These simulations show that the methods for expressing meta-analysis results from continuous outcomes are sensitive to underlying distributions, sample sizes and cut points but are remarkably robust to the presence of heterogeneity across studies. We offer suggestions of situations in which the various methods may safely be applied. In particular, if the underlying distribution is approximately normal, then estimation of risks for a specific cut point may be used for large sample sizes; direct transformations may be preferable otherwise. However, if the standard deviations in the two groups are notably different, then none of the methods have good properties. Furthermore, absolute risks are safely estimated after direct transformation only if they are in the region of 20\% to 80\%. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/KSAB9S69/Anzures‐Cabrera et al. - 2011 - Expressing findings from meta-analyses of continuo.pdf;/Users/rritaz/Zotero/storage/FIK4DLAM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size estimation (series),transform cont ES to discrete ES},
  language = {en},
  number = {25}
}

@article{arends_baseline_2000,
  title = {Baseline Risk as Predictor of Treatment Benefit: Three Clinical Meta-Re-Analyses},
  shorttitle = {Baseline Risk as Predictor of Treatment Benefit},
  author = {Arends, Lidia R. and Hoes, Arno W. and Lubsen, Jacobus and Grobbee, Diederik E. and Stijnen, Theo},
  year = {2000},
  volume = {19},
  pages = {3497--3518},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20001230)19:24<3497::AID-SIM830>3.0.CO;2-H},
  abstract = {A relationship between baseline risk and treatment effect is increasingly investigated as a possible explanation of between-study heterogeneity in clinical trial meta-analysis. An approach that is still often applied in the medical literature is to plot the estimated treatment effects against the estimated measures of risk in the control groups (as a measure of baseline risk), and to compute the ordinary weighted least squares regression line. However, it has been pointed out by several authors that this approach can be seriously flawed. The main problem is that the observed treatment effect and baseline risk measures should be viewed as estimates rather than the true values. In recent years several methods have been proposed in the statistical literature to potentially deal with the measurement errors in the estimates. In this article we propose a vague priors Bayesian solution to the problem which can be carried out using the `Bayesian inference using Gibbs sampling' (BUGS) implementation of Markov chain Monte Carlo numerical integration techniques. Different from other proposed methods, it uses the exact rather than an approximate likelihood, while it can handle many different treatment effect measures and baseline risk measures. The method differs from a recently proposed Bayesian method in that it explicitly models the distribution of the underlying baseline risks. We apply the method to three meta-analyses published in the medical literature and compare the results with the outcomes of the other recently proposed methods. In particular we compare our approach to McIntosh's method, for which we show how it can be carried out using standard statistical software. We conclude that our proposed method offers a very general and flexible solution to the problem, which can be carried out relatively easily with existing Bayesian analysis software. A confidence band for the underlying relationship between true effect measure and baseline risk and a confidence interval for the value of the baseline risk measure for which there is no treatment effect are easily obtained by-products of our approach. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/253SQAU3/Arends et al. - 2000 - Baseline risk as predictor of treatment benefit t.pdf;/Users/rritaz/Zotero/storage/WKYC2CPS/1097-0258(20001230)19243497AID-SIM8303.0.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {24}
}

@article{arends_meta-analysis_2008,
  title = {Meta-Analysis of Summary Survival Curve Data},
  author = {Arends, Lidia R. and Hunink, M. G. Myriam and Stijnen, Theo},
  year = {2008},
  volume = {27},
  pages = {4381--4396},
  issn = {1097-0258},
  doi = {10.1002/sim.3311},
  abstract = {The use of standard univariate fixed- and random-effects models in meta-analysis has become well known in the last 20 years. However, these models are unsuitable for meta-analysis of clinical trials that present multiple survival estimates (usually illustrated by a survival curve) during a follow-up period. Therefore, special methods are needed to combine the survival curve data from different trials in a meta-analysis. For this purpose, only fixed-effects models have been suggested in the literature. In this paper, we propose a multivariate random-effects model for joint analysis of survival proportions reported at multiple time points and in different studies, to be combined in a meta-analysis. The model could be seen as a generalization of the fixed-effects model of Dear (Biometrics 1994; 50:989\textendash 1002). We illustrate the method by using a simulated data example as well as using a clinical data example of meta-analysis with aggregated survival curve data. All analyses can be carried out with standard general linear MIXED model software. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/QQVTGQDY/Arends et al. - 2008 - Meta-analysis of summary survival curve data.pdf;/Users/rritaz/Zotero/storage/SXQSY5WT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,random-effects},
  language = {en},
  number = {22}
}

@article{augusteijn_effect_2019,
  title = {The Effect of Publication Bias on the {{Q}} Test and Assessment of Heterogeneity},
  author = {Augusteijn, Hilde E. M. and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2019},
  month = feb,
  volume = {24},
  pages = {116--134},
  issn = {1082-989X},
  doi = {10.1037/met0000197},
  abstract = {One of the main goals of meta-analysis is to test for and estimate the heterogeneity of effect sizes. We examined the effect of publication bias on the Q test and assessments of heterogeneity as a function of true heterogeneity, publication bias, true effect size, number of studies, and variation of sample sizes. The present study has two main contributions and is relevant to all researchers conducting meta-analysis. First, we show when and how publication bias affects the assessment of heterogeneity. The expected values of heterogeneity measures H{$^2$} and I{$^2$} were analytically derived, and the power and Type I error rate of the Q test were examined in a Monte Carlo simulation study. Our results show that the effect of publication bias on the Q test and assessment of heterogeneity is large, complex, and nonlinear. Publication bias can both dramatically decrease and increase heterogeneity in true effect size, particularly if the number of studies is large and population effect size is small. We therefore conclude that the Q test of homogeneity and heterogeneity measures H{$^2$} and I{$^2$} are generally not valid when publication bias is present. Our second contribution is that we introduce a web application, Q-sense, which can be used to determine the impact of publication bias on the assessment of heterogeneity within a certain meta-analysis and to assess the robustness of the meta-analytic estimate to publication bias. Furthermore, we apply Q-sense to 2 published meta-analyses, showing how publication bias can result in invalid estimates of effect size and heterogeneity. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,power,publication bias,random-effects},
  number = {1}
}

@article{bagos_covariance_2012,
  title = {On the Covariance of Two Correlated Log-Odds Ratios},
  author = {Bagos, Pantelis G.},
  year = {2012},
  volume = {31},
  pages = {1418--1431},
  issn = {1097-0258},
  doi = {10.1002/sim.4474},
  abstract = {In many applications two correlated estimates of an effect size need to be considered simultaneously to be combined or compared. Apparently, there is a need for calculating their covariance, which however requires access to the individual data that may not be available to a researcher performing the analysis. We present a simple and efficient method for calculating the covariance of two correlated log-odds ratios. The method is very simple, is based on the well-known large sample approximations, can be applied using only data that are available in the published reports and more importantly, is very general, because it is shown to encompass several previously derived estimates (multiple outcomes, multiple treatments, dose\textendash response models, mutually exclusive outcomes, genetic association studies) as special cases. By encompassing the previous approaches in a unified framework, the method allows easily deriving estimates for the covariance concerning problems that were not easy to be obtained otherwise. We show that the method can be used to derive the covariance of log-odds ratios from matched and unmatched case-control studies that use the same cases, a situation that has been addressed in the past only using individual data. Future applications of the method are discussed. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NHKQGVMK/Bagos - 2012 - On the covariance of two correlated log-odds ratio.pdf;/Users/rritaz/Zotero/storage/Y7Q2IPUB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,discrete effect sizes,random-effects},
  language = {en},
  number = {14}
}

@article{bakbergenuly_beta-binomial_2017,
  title = {Beta-Binomial Model for Meta-Analysis of Odds Ratios},
  author = {Bakbergenuly, Ilyas and Kulinskaya, Elena},
  year = {2017},
  volume = {36},
  pages = {1715--1734},
  issn = {1097-0258},
  doi = {10.1002/sim.7233},
  abstract = {In meta-analysis of odds ratios (ORs), heterogeneity between the studies is usually modelled via the additive random effects model (REM). An alternative, multiplicative REM for ORs uses overdispersion. The multiplicative factor in this overdispersion model (ODM) can be interpreted as an intra-class correlation (ICC) parameter. This model naturally arises when the probabilities of an event in one or both arms of a comparative study are themselves beta-distributed, resulting in beta-binomial distributions. We propose two new estimators of the ICC for meta-analysis in this setting. One is based on the inverted Breslow-Day test, and the other on the improved gamma approximation by Kulinskaya and Dollinger (2015, p. 26) to the distribution of Cochran's Q. The performance of these and several other estimators of ICC on bias and coverage is studied by simulation. Additionally, the Mantel-Haenszel approach to estimation of ORs is extended to the beta-binomial model, and we study performance of various ICC estimators when used in the Mantel-Haenszel or the inverse-variance method to combine ORs in meta-analysis. The results of the simulations show that the improved gamma-based estimator of ICC is superior for small sample sizes, and the Breslow-Day-based estimator is the best for . The Mantel-Haenszel-based estimator of OR is very biased and is not recommended. The inverse-variance approach is also somewhat biased for ORs{$\neq$}1, but this bias is not very large in practical settings. Developed methods and R programs, provided in the Web Appendix, make the beta-binomial model a feasible alternative to the standard REM for meta-analysis of ORs. \textcopyright{} 2017 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/5R5QVCTS/Bakbergenuly and Kulinskaya - 2017 - Beta-binomial model for meta-analysis of odds rati.pdf;/Users/rritaz/Zotero/storage/U76RUIUE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {discrete effect sizes,odds ratio,random-effects,small samples},
  language = {en},
  number = {11}
}

@article{bakbergenuly_estimation_2020,
  title = {Estimation in Meta-Analyses of Mean Difference and Standardized Mean Difference},
  author = {Bakbergenuly, Ilyas and Hoaglin, David C. and Kulinskaya, Elena},
  year = {2020},
  volume = {39},
  pages = {171--191},
  issn = {1097-0258},
  doi = {10.1002/sim.8422},
  abstract = {Methods for random-effects meta-analysis require an estimate of the between-study variance, {$\tau$}2. The performance of estimators of {$\tau$}2 (measured by bias and coverage) affects their usefulness in assessing heterogeneity of study-level effects and also the performance of related estimators of the overall effect. However, as we show, the performance of the methods varies widely among effect measures. For the effect measures mean difference (MD) and standardized MD (SMD), we use improved effect-measure-specific approximations to the expected value of Q for both MD and SMD to introduce two new methods of point estimation of {$\tau$}2 for MD (Welch-type and corrected DerSimonian-Laird) and one WT interval method. We also introduce one point estimator and one interval estimator for {$\tau$}2 in SMD. Extensive simulations compare our methods with four point estimators of {$\tau$}2 (the popular methods of DerSimonian-Laird, restricted maximum likelihood, and Mandel and Paule, and the less-familiar method of Jackson) and four interval estimators for {$\tau$}2 (profile likelihood, Q-profile, Biggerstaff and Jackson, and Jackson). We also study related point and interval estimators of the overall effect, including an estimator whose weights use only study-level sample sizes. We provide measure-specific recommendations from our comprehensive simulation study and discuss an example.},
  file = {/Users/rritaz/Zotero/storage/EB2DZEDU/Bakbergenuly et al. - 2020 - Estimation in meta-analyses of mean difference and.pdf;/Users/rritaz/Zotero/storage/J7ZLHMW6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity estimators,random-effects},
  language = {en},
  number = {2}
}

@article{bakbergenuly_meta-analysis_2018,
  title = {Meta-Analysis of Binary Outcomes via Generalized Linear Mixed Models: A Simulation Study},
  shorttitle = {Meta-Analysis of Binary Outcomes via Generalized Linear Mixed Models},
  author = {Bakbergenuly, Ilyas and Kulinskaya, Elena},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {70},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0531-9},
  abstract = {Background: Systematic reviews and meta-analyses of binary outcomes are widespread in all areas of application. The odds ratio, in particular, is by far the most popular effect measure. However, the standard meta-analysis of odds ratios using a random-effects model has a number of potential problems. An attractive alternative approach for the meta-analysis of binary outcomes uses a class of generalized linear mixed models (GLMMs). GLMMs are believed to overcome the problems of the standard random-effects model because they use a correct binomial-normal likelihood. However, this belief is based on theoretical considerations, and no sufficient simulations have assessed the performance of GLMMs in meta-analysis. This gap may be due to the computational complexity of these models and the resulting considerable time requirements. Methods: The present study is the first to provide extensive simulations on the performance of four GLMM methods (models with fixed and random study effects and two conditional methods) for meta-analysis of odds ratios in comparison to the standard random effects model. Results: In our simulations, the hypergeometric-normal model provided less biased estimation of the heterogeneity variance than the standard random-effects meta-analysis using the restricted maximum likelihood (REML) estimation when the data were sparse, but the REML method performed similarly for the point estimation of the odds ratio, and better for the interval estimation. Conclusions: It is difficult to recommend the use of GLMMs in the practice of meta-analysis. The problem of finding uniformly good methods of the meta-analysis for binary outcomes is still open.},
  file = {/Users/rritaz/Zotero/storage/KJCAYXUC/Bakbergenuly and Kulinskaya - 2018 - Meta-analysis of binary outcomes via generalized l.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {GLM MA models,odds ratio,random-effects},
  language = {en},
  number = {1}
}

@article{baker_new_2016,
  title = {New Models for Describing Outliers in Meta-Analysis},
  author = {Baker, Rose and Jackson, Dan},
  year = {2016},
  volume = {7},
  pages = {314--328},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1191},
  abstract = {An unobserved random effect is often used to describe the between-study variation that is apparent in meta-analysis datasets. A normally distributed random effect is conventionally used for this purpose. When outliers or other unusual estimates are included in the analysis, the use of alternative random effect distributions has previously been proposed. Instead of adopting the usual hierarchical approach to modelling between-study variation, and so directly modelling the study specific true underling effects, we propose two new marginal distributions for modelling heterogeneous datasets. These two distributions are suggested because numerical integration is not needed to evaluate the likelihood. This makes the computation required when fitting our models much more robust. The properties of the new distributions are described, and the methodology is exemplified by fitting models to four datasets. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DHMDIGEL/Baker and Jackson - 2016 - New models for describing outliers in meta-analysi.pdf;/Users/rritaz/Zotero/storage/7ZKI4ZH8/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {3}
}

@article{baker_randomized_2003,
  title = {Randomized Trials, Generalizability, and Meta-Analysis: {{Graphical}} Insights for Binary Outcomes},
  shorttitle = {Randomized Trials, Generalizability, and Meta-Analysis},
  author = {Baker, Stuart G and Kramer, Barnett S},
  year = {2003},
  month = dec,
  volume = {3},
  pages = {10},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-3-10},
  abstract = {Background: Randomized trials stochastically answer the question. "What would be the effect of treatment on outcome if one turned back the clock and switched treatments in the given population?" Generalizations to other subjects are reliable only if the particular trial is performed on a random sample of the target population. By considering an unobserved binary variable, we graphically investigate how randomized trials can also stochastically answer the question, "What would be the effect of treatment on outcome in a population with a possibly different distribution of an unobserved binary baseline variable that does not interact with treatment in its effect on outcome?" Method: For three different outcome measures, absolute difference (DIF), relative risk (RR), and odds ratio (OR), we constructed a modified BK-Plot under the assumption that treatment has the same effect on outcome if either all or no subjects had a given level of the unobserved binary variable. (A BK-Plot shows the effect of an unobserved binary covariate on a binary outcome in two treatment groups; it was originally developed to explain Simpsons's paradox.) Results: For DIF and RR, but not OR, the BK-Plot shows that the estimated treatment effect is invariant to the fraction of subjects with an unobserved binary variable at a given level. Conclusion: The BK-Plot provides a simple method to understand generalizability in randomized trials. Meta-analyses of randomized trials with a binary outcome that are based on DIF or RR, but not OR, will avoid bias from an unobserved covariate that does not interact with treatment in its effect on outcome.},
  file = {/Users/rritaz/Zotero/storage/GPSTZB5H/Baker and Kramer - 2003 - Randomized trials, generalizability, and meta-anal.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,discrete effect sizes},
  language = {en},
  number = {1}
}

@article{barrett_two-stage_2012,
  title = {Two-Stage Meta-Analysis of Survival Data from Individual Participants Using Percentile Ratios},
  author = {Barrett, Jessica K. and Farewell, Vern T. and Siannis, Fotios and Tierney, Jayne and Higgins, Julian P. T.},
  year = {2012},
  volume = {31},
  pages = {4296--4308},
  issn = {1097-0258},
  doi = {10.1002/sim.5516},
  abstract = {Methods for individual participant data meta-analysis of survival outcomes commonly focus on the hazard ratio as a measure of treatment effect. Recently, Siannis et al. (2010, Statistics in Medicine 29:3030\textendash 3045) proposed the use of percentile ratios as an alternative to hazard ratios. We describe a novel two-stage method for the meta-analysis of percentile ratios that avoids distributional assumptions at the study level. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/F8HMH3Y7/Barrett et al. - 2012 - Two-stage meta-analysis of survival data from indi.pdf;/Users/rritaz/Zotero/storage/RAYYBJG2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),Individual Patient Data IPD},
  language = {en},
  number = {30}
}

@article{baujat_graphical_2002,
  title = {A Graphical Method for Exploring Heterogeneity in Meta-Analyses: Application to a Meta-Analysis of 65 Trials},
  shorttitle = {A Graphical Method for Exploring Heterogeneity in Meta-Analyses},
  author = {Baujat, Bertrand and Mah{\'e}, C{\'e}dric and Pignon, Jean-Pierre and Hill, Catherine},
  year = {2002},
  volume = {21},
  pages = {2641--2652},
  issn = {1097-0258},
  doi = {10.1002/sim.1221},
  abstract = {Heterogeneity can be a major component of meta-analyses and by virtue of that fact warrants investigation. Classic analysis methods, such as meta-regression, are used to explore the sources of heterogeneity. However, it may be difficult to apply such a method in complex cases or in the absence of an a priori hypothesis. This paper presents a graphical method to identify trials, groups of trials or groups of patients that are sources of heterogeneity. The contribution of these trials to the overall result can also be evaluated with this method. Each trial is represented by a dot on a 2D graph. The X-axis represents the contribution of the trial to the overall Cochran Q-test for heterogeneity. The Y-axis represents the influence of the trial, defined as the standardized squared difference between the treatment effects estimated with and without the trial. This approach has been applied to data from the Meta-Analysis of Chemotherapy in Head and Neck Cancer (MACH-NC) comprising 10850 patients in 65 randomized trials. The graphical method allowed us to identify trials that contributed considerably to the overall heterogeneity and had a strong influence on the overall result. It also provided useful information for the interpretation of heterogeneity in this meta-analysis. The proposed graphical method identifies trials that account for most of the heterogeneity without having to explore all possible sources of heterogeneity by subgroup analyses. This method can also be applied to identify types of patients that explain heterogeneity in the treatment effect. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UV8GJPYF/Baujat et al. - 2002 - A graphical method for exploring heterogeneity in .pdf;/Users/rritaz/Zotero/storage/DXJM9K9B/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random-effects},
  language = {en},
  number = {18}
}

@article{beach_choosing_1989,
  title = {Choosing Covariates in the Analysis of Clinical Trials},
  author = {Beach, Michael L. and Meier, Paul},
  year = {1989},
  month = dec,
  volume = {10},
  pages = {161--175},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90055-X},
  abstract = {Much of the literature on clinical trials emphasizes the importance of adjusting the results for any covariates (baseline variables) for which randomization fails to produce nearly exact balance, but the literature is very nearly devoid of recipes for assessing the consequences of such adjustments. Several years ago, Paul Canner presented an approximate expression for the effect of a covariate adjustment, and he considered its use in the selection of covariates. With the aid of Canner's equation, using both formal analysis and simulation, the impact of covariate adjustment is further explored. Unless tight control over the analysis plans is established in advance, covariate adjustment can lead to seriously misleading inferences. Illustrations from the clinical trials literature are provided.},
  file = {/Users/rritaz/Zotero/storage/HVQZL3FT/019724568990055X.html},
  journal = {Controlled Clinical Trials},
  keywords = {clinical trials,Covariate adjustment,Monte Carlo study,selection procedures},
  language = {en},
  number = {4, Supplement 1}
}

@article{beath_finite_2014,
  title = {A Finite Mixture Method for Outlier Detection and Robustness in Meta-Analysis},
  author = {Beath, Ken J.},
  year = {2014},
  volume = {5},
  pages = {285--293},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1114},
  abstract = {When performing a meta-analysis unexplained variation above that predicted by within study variation is usually modeled by a random effect. However, in some cases, this is not sufficient to explain all the variation because of outlier or unusual studies. A previously described method is to define an outlier as a study requiring a higher random effects variance and testing each study sequentially. An extension is described where the studies are considered to be a finite mixture of outliers and non-outliers, allowing any number of outlier studies and the use of standard mixture model techniques. The bootstrap likelihood ratio test is used to determine if there are any outliers present by comparing models with and without outliers, and the outlier studies are identified using posterior predicted probabilities. The estimation of the overall treatment effect is then determined including all observations but with the outliers down-weighted. This has the advantage that studies that are marginal outliers are still included in the meta-analysis but with an appropriate weighting. The method is applied to examples from meta-analysis and meta-regression. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JVLTAHQE/Beath - 2014 - A finite mixture method for outlier detection and .pdf;/Users/rritaz/Zotero/storage/VVKLK5EH/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,random-effects},
  language = {en},
  number = {4}
}

@article{belias_statistical_2019,
  title = {Statistical Approaches to Identify Subgroups in Meta-Analysis of Individual Participant Data: A Simulation Study},
  shorttitle = {Statistical Approaches to Identify Subgroups in Meta-Analysis of Individual Participant Data},
  author = {Belias, Michail and Rovers, Maroeska M. and Reitsma, Johannes B. and Debray, Thomas P. A. and IntHout, Joanna},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {183},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0817-6},
  abstract = {Background: Individual participant data meta-analysis (IPD-MA) is considered the gold standard for investigating subgroup effects. Frequently used regression-based approaches to detect subgroups in IPD-MA are: meta-regression, per-subgroup meta-analysis (PS-MA), meta-analysis of interaction terms (MA-IT), naive one-stage IPD-MA (ignoring potential study-level confounding), and centred one-stage IPD-MA (accounting for potential study-level confounding). Clear guidance on the analyses is lacking and clinical researchers may use approaches with suboptimal efficiency to investigate subgroup effects in an IPD setting. Therefore, our aim is to overview and compare the aforementioned methods, and provide recommendations over which should be preferred. Methods: We conducted a simulation study where we generated IPD of randomised trials and varied the magnitude of subgroup effect (0, 25, 50\% relative reduction), between-study treatment effect heterogeneity (none, medium, large), ecological bias (none, quantitative, qualitative), sample size (50,100,200), and number of trials (5,10) for binary, continuous and time-to-event outcomes. For each scenario, we assessed the power, false positive rate (FPR) and bias of aforementioned five approaches. Results: Naive and centred IPD-MA yielded the highest power, whilst preserving acceptable FPR around the nominal 5\% in all scenarios. Centred IPD-MA showed slightly less biased estimates than na\"ive IPD-MA. Similar results were obtained for MA-IT, except when analysing binary outcomes (where it yielded less power and FPR {$<$} 5\%). PS-MA showed similar power as MA-IT in non-heterogeneous scenarios, but power collapsed as heterogeneity increased, and decreased even more in the presence of ecological bias. PS-MA suffered from too high FPRs in non-heterogeneous settings and showed biased estimates in all scenarios. Meta-regression showed poor power ({$<$} 20\%) in all scenarios and completely biased results in settings with qualitative ecological bias. Conclusions: Our results indicate that subgroup detection in IPD-MA requires careful modelling. Naive and centred IPD-MA performed equally well, but due to less bias of the estimates in the presence of ecological bias, we recommend the latter.},
  file = {/Users/rritaz/Zotero/storage/38KU7XY4/Belias et al. - 2019 - Statistical approaches to identify subgroups in me.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{benjamini_adaptive_2000,
  title = {On the {{Adaptive Control}} of the {{False Discovery Rate}} in {{Multiple Testing With Independent Statistics}}},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {2000},
  month = mar,
  volume = {25},
  pages = {60--83},
  issn = {1076-9986},
  doi = {10.3102/10769986025001060},
  abstract = {A new approach to problems of multiple significance testing was presented in Benjamini and Hochberg (1995), which calls for controlling the expected ratio of the number of erroneous rejections to the number of rejections?the False Discovery Rate (FDR). The procedure given there was shown to control the FDR for independent test statistics. When some of the hypotheses are in fact false, that procedure is too conservative. We present here an adaptive procedure, where the number of true null hypotheses is estimated first as in Hochberg and Benjamini (1990), and this estimate is used in the procedure of Benjamini and Hochberg (1995). The result is still a simple stepwise procedure, to which we also give a graphical companion. The new procedure is used in several examples drawn from educational and behavioral studies, addressing problems in multi-center studies, subset analysis and meta-analysis. The examples vary in the number of hypotheses tested, and the implication of the new procedure on the conclusions. In a large simulation study of independent test statistics the adaptive procedure is shown to control the FDR and have substantially better power than the previously suggested FDR controlling method, which by itself is more powerful than the traditional family wise error-rate controlling methods. In cases where most of the tested hypotheses are far from being true there is hardly any penalty due to the simultaneous testing of many hypotheses.},
  file = {/Users/rritaz/Zotero/storage/FRAPNK79/Benjamini and Hochberg - 2000 - On the Adaptive Control of the False Discovery Rat.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {combined significance},
  number = {1}
}

@article{biggerstaff_exact_2008,
  title = {The Exact Distribution of {{Cochran}}'s Heterogeneity Statistic in One-Way Random Effects Meta-Analysis},
  author = {Biggerstaff, Brad J. and Jackson, Dan},
  year = {2008},
  volume = {27},
  pages = {6093--6110},
  issn = {1097-0258},
  doi = {10.1002/sim.3428},
  abstract = {The presence and impact of heterogeneity in the standard one-way random effects model in meta-analysis are often assessed using the Q statistic due to Cochran. We derive the exact distribution of this statistic under the assumptions of the random effects model, and also suggest two moment-based approximations and a saddlepoint approximation for Q. The exact and approximate distributions are then applied to obtain the corresponding distributions of the recently proposed heterogeneity measures I2 and H, the power of the standard test for the presence of heterogeneity and confidence intervals for the between-study variance parameter when the DerSimonian\textendash Laird or the Hartung\textendash Makambi estimator is used. The methodology is illustrated by revisiting a recent simulation study concerning the heterogeneity measures and applying all the proposed methods to four published meta-analyses. Published in 2008 by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5AGKCZ4U/Biggerstaff and Jackson - 2008 - The exact distribution of Cochran's heterogeneity .pdf;/Users/rritaz/Zotero/storage/P4E4LJ89/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Q distribution,random-effects},
  language = {en},
  number = {29}
}

@article{biggerstaff_incorporating_1997,
  title = {Incorporating {{Variability}} in {{Estimates}} of {{Heterogeneity}} in the {{Random Effects Model}} in {{Meta}}-{{Analysis}}},
  author = {Biggerstaff, B. J. and Tweedie, R. L.},
  year = {1997},
  volume = {16},
  pages = {753--768},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19970415)16:7<753::AID-SIM494>3.0.CO;2-G},
  abstract = {When combining results from separate investigations in a meta-analysis, random effects methods enable the modelling of differences between studies by incorporating a heterogeneity parameter {$\tau$}2 that accounts explicitly for across-study variation. We develop a simple form for the variance of Cochran's homogeneity statistic Q, leading to interval estimation of {$\tau$}2 utilizing an approximating distribution for Q; this enables us to extend the point estimation of DerSimonian and Laird. We also develop asymptotic likelihood methods and compared them with this method. We then use these approximating distributions to give a new method of calculating the weight given to the individual studies' results when estimating the overall mean which takes into account variation in these point estimates of {$\tau$}2. Two examples illustrate the methods presented, where we show that the new weighting scheme is between the standard fixed and random effects models in down-weighting the results of large studies and up-weighting those of small studies. \textcopyright{} 1997 by John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819970415\%2916\%3A7\%3C753\%3A\%3AAID-SIM494\%3E3.0.CO\%3B2-G},
  copyright = {Copyright \textcopyright{} 1997 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/MRDIWYUZ/Biggerstaff and Tweedie - 1997 - Incorporating Variability in Estimates of Heteroge.pdf;/Users/rritaz/Zotero/storage/6PUBER77/(SICI)1097-0258(19970415)167753AID-SIM4943.0.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {7}
}

@article{bipat_multivariate_2010,
  title = {Multivariate Fixed- and Random-Effects Models for Summarizing Ordinal Data in Meta-Analysis of Diagnostic Staging Studies},
  author = {Bipat, Shandra and Zwinderman, Aeilko H.},
  year = {2010},
  volume = {1},
  pages = {136--148},
  issn = {1759-2887},
  doi = {10.1002/jrsm.10},
  abstract = {For many diseases (e.g. rectal cancer and the Crohn disease), more than two stages exist and as treatment mostly depends on disease stages, correctly determining this by a diagnostic test is very important. To determine their role in clinical practice, the value of these tests should be carefully evaluated, and summarizing results in meta-analysis should also be done appropriately. A multinomial model for meta-analyzing data with more than two categories has previously been developed; these data were considered as nominal categories. However, there is an ordinal character within staging data. In this study we extended this multinomial model to three ordinal models (models for the logits of adjacent-categories, for continuation-ratio logits and for proportional odds logits) to summarize the ordinal character of staging data. Both fixed- and random-effects approaches were developed and compared. The principles of the multinomial model as well as three ordinal models are shown by fitting these models using the data on staging of rectal cancer by endoluminal ultrasonography and magnetic resonance imaging. The proportions of patients correctly staged, understaged, and overstaged per stage are obtained by these models. Because of the increased interest in meta-analyses for evidence-based guidelines, these models can be helpful in summarizing staging data. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5S42J3JV/Bipat and Zwinderman - 2010 - Multivariate fixed- and random-effects models for .pdf;/Users/rritaz/Zotero/storage/F8VBQWQA/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,discrete effect sizes,multivariate,random-effects},
  language = {en},
  number = {2}
}

@article{bodnar_bayesian_2017,
  title = {Bayesian Estimation in Random Effects Meta-Analysis Using a Non-Informative Prior},
  author = {Bodnar, Olha and Link, Alfred and Arendack{\'a}, Barbora and Possolo, Antonio and Elster, Clemens},
  year = {2017},
  volume = {36},
  pages = {378--399},
  issn = {1097-0258},
  doi = {10.1002/sim.7156},
  abstract = {Pooling information from multiple, independent studies (meta-analysis) adds great value to medical research. Random effects models are widely used for this purpose. However, there are many different ways of estimating model parameters, and the choice of estimation procedure may be influential upon the conclusions of the meta-analysis. In this paper, we describe a recently proposed Bayesian estimation procedure and compare it with a profile likelihood method and with the DerSimonian\textendash Laird and Mandel\textendash Paule estimators including the Knapp\textendash Hartung correction. The Bayesian procedure uses a non-informative prior for the overall mean and the between-study standard deviation that is determined by the Berger and Bernardo reference prior principle. The comparison of these procedures focuses on the frequentist properties of interval estimates for the overall mean. The results of our simulation study reveal that the Bayesian approach is a promising alternative producing more accurate interval estimates than those three conventional procedures for meta-analysis. The Bayesian procedure is also illustrated using three examples of meta-analysis involving real data. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/FUR8F3NU/Bodnar et al. - 2017 - Bayesian estimation in random effects meta-analysi.pdf;/Users/rritaz/Zotero/storage/QZP3Q33D/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {2}
}

@article{bohning_general_2002,
  title = {Some General Points in Estimating Heterogeneity Variance with the {{DerSimonian}}-{{Laird}} Estimator},
  author = {Bohning, D.},
  year = {2002},
  month = dec,
  volume = {3},
  pages = {445--457},
  issn = {14654644, 14684357},
  doi = {10.1093/biostatistics/3.4.445},
  abstract = {In this paper we consider estimating heterogeneity variance with the DerSimonian\textendash Laird (DSL) estimator as typically used in meta-analysis. In its general form the DSL estimator requires inverse population-averaged study-specific variances as weights, in which case the estimator is unbiased. It has become common practice, however, to use estimates of the study-specific variances instead of their population-averaged versions. This can lead to considerable bias. Simulations illustrate these findings.},
  file = {/Users/rritaz/Zotero/storage/6QWYV989/Bohning - 2002 - Some general points in estimating heterogeneity va.pdf},
  journal = {Biostatistics},
  language = {en},
  number = {4}
}

@article{bom_kinked_2019,
  title = {A Kinked Meta-Regression Model for Publication Bias Correction},
  author = {Bom, Pedro R. D. and Rachinger, Heiko},
  year = {2019},
  volume = {10},
  pages = {497--514},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1352},
  abstract = {Publication bias distorts the available empirical evidence and misinforms policymaking. Evidence of publication bias is mounting in virtually all fields of empirical research. This paper proposes the endogenous kink (EK) meta-regression model as a novel method of publication bias correction. The EK method fits a piecewise linear meta-regression of the primary estimates on their standard errors, with a kink at the cutoff value of the standard error below which publication selection is unlikely. We provide a simple method of endogenously determining this cutoff value as a function of a first-stage estimate of the true effect and an assumed threshold of statistical significance. Our Monte Carlo simulations show that EK is less biased and more efficient than other related regression-based methods of publication bias correction in a variety of research conditions.},
  file = {/Users/rritaz/Zotero/storage/RIEIIUFC/Bom and Rachinger - 2019 - A kinked meta-regression model for publication bia.pdf;/Users/rritaz/Zotero/storage/2RUA4GD2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {publication bias},
  language = {en},
  number = {4}
}

@article{bond_meta-analysis_2003,
  title = {Meta-{{Analysis}} of {{Raw Mean Differences}}},
  author = {Bond, Charles F. Jr. and Wiitala, Wyndy L. and Richard, F. Dan},
  year = {2003},
  month = dec,
  volume = {8},
  pages = {406--418},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.8.4.406},
  abstract = {This article discusses the meta-analysis of raw mean differences. It presents a rationale for cumulating psychological effects in a raw metric and compares raw mean differences to standardized mean differences. Some limitations of standardization are noted, and statistical techniques for raw meta-analysis are described. These include a graphical device for decomposing effect sizes. Several illustrative data sets are analyzed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/PLXSJZ3V/Bond et al. - 2003 - Meta-Analysis of Raw Mean Differences.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,diagnostic techniques,random-effects},
  number = {4},
  series = {Metric in {{Meta}}-{{Analysis}}}
}

@article{bonett_meta-analytic_2008,
  title = {Meta-Analytic Interval Estimation for Bivariate Correlations.},
  author = {Bonett, Douglas G.},
  year = {2008},
  volume = {13},
  pages = {173--181},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0012868},
  file = {/Users/rritaz/Zotero/storage/MRX3R88F/Bonett - 2008 - Meta-analytic interval estimation for bivariate co.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlated effects,correlation coefficients},
  language = {en},
  number = {3}
}

@article{bonett_meta-analytic_2009,
  title = {Meta-Analytic Interval Estimation for Standardized and Unstandardized Mean Differences.},
  author = {Bonett, Douglas G.},
  year = {2009},
  volume = {14},
  pages = {225--238},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0016619},
  abstract = {The fixed-effects (FE) meta-analytic confidence intervals for unstandardized and standardized mean differences are based on an unrealistic assumption of effect-size homogeneity and perform poorly when this assumption is violated. The random-effects (RE) meta-analytic confidence intervals are based on an unrealistic assumption that the selected studies represent a random sample from a large superpopulation of studies. The RE approach cannot be justified in typical meta-analysis applications in which studies are nonrandomly selected. New FE meta-analytic confidence intervals for unstandardized and standardized mean differences are proposed that are easy to compute and perform properly under effect-size heterogeneity and nonrandomly selected studies. The proposed meta-analytic confidence intervals may be used to combine unstandardized or standardized mean differences from studies having either independent samples or dependent samples and may also be used to integrate results from previous studies into a new study. An alternative approach to assessing effect-size heterogeneity is presented.},
  file = {/Users/rritaz/Zotero/storage/CHETIIJR/Bonett - 2009 - Meta-analytic interval estimation for standardized.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,effect size estimation (series),GLM MA models,random effects models,random-effects},
  language = {en},
  number = {3}
}

@article{bonett_varying_2010,
  title = {Varying Coefficient Meta-Analytic Methods for Alpha Reliability.},
  author = {Bonett, Douglas G.},
  year = {2010},
  month = dec,
  volume = {15},
  pages = {368--385},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0020142},
  file = {/Users/rritaz/Zotero/storage/ZMMPWY6J/Bonett - 2010 - Varying coefficient meta-analytic methods for alph.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlation coefficients,random-effects},
  language = {en},
  number = {4}
}

@article{bonett_varying_2015,
  title = {Varying Coefficient Meta-Analysis Methods for Odds Ratios and Risk Ratios.},
  author = {Bonett, Douglas G. and Price, Robert M.},
  year = {2015},
  volume = {20},
  pages = {394--406},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000032},
  file = {/Users/rritaz/Zotero/storage/X24T6WUV/Bonett and Price - 2015 - Varying coefficient meta-analysis methods for odds.pdf},
  journal = {Psychological Methods},
  keywords = {discrete effect sizes,effect size estimation (series),GLM MA models},
  language = {en},
  number = {3}
}

@article{bonofiglio_meta-analysis_2016,
  title = {Meta-Analysis for Aggregated Survival Data with Competing Risks: A Parametric Approach Using Cumulative Incidence Functions},
  shorttitle = {Meta-Analysis for Aggregated Survival Data with Competing Risks},
  author = {Bonofiglio, Federico and Beyersmann, Jan and Schumacher, Martin and Koller, Michael and Schwarzer, Guido},
  year = {2016},
  volume = {7},
  pages = {282--293},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1165},
  abstract = {Meta-analysis of a survival endpoint is typically based on the pooling of hazard ratios (HRs). If competing risks occur, the HRs may lose translation into changes of survival probability. The cumulative incidence functions (CIFs), the expected proportion of cause-specific events over time, re-connect the cause-specific hazards (CSHs) to the probability of each event type. We use CIF ratios to measure treatment effect on each event type. To retrieve information on aggregated, typically poorly reported, competing risks data, we assume constant CSHs. Next, we develop methods to pool CIF ratios across studies. The procedure computes pooled HRs alongside and checks the influence of follow-up time on the analysis. We apply the method to a medical example, showing that follow-up duration is relevant both for pooled cause-specific HRs and CIF ratios. Moreover, if all-cause hazard and follow-up time are large enough, CIF ratios may reveal additional information about the effect of treatment on the cumulative probability of each event type. Finally, to improve the usefulness of such analysis, better reporting of competing risks data is needed. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SULDUNSK/Bonofiglio et al. - 2016 - Meta-analysis for aggregated survival data with co.pdf;/Users/rritaz/Zotero/storage/QC6TVQL8/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {effect size combination (small sample \& discrete)},
  language = {en},
  number = {3}
}

@article{botella_meta-analysis_2015,
  title = {Meta-Analysis of the Accuracy of Tools Used for Binary Classification When the Primary Studies Employ Different References},
  author = {Botella, Juan and Huang, Huiling and Suero, Manuel},
  year = {2015},
  month = sep,
  volume = {20},
  pages = {331--341},
  issn = {1082-989X},
  doi = {10.1037/met0000012},
  abstract = {The quality of tools used in binary classification is evaluated by studies that assess the accuracy of the classification. The empirical evidence is summarized in 2 \texttimes{} 2 contingency tables. These provide the joint frequencies between the true status of a sample and the classification made by the test. The accuracy of the test is better estimated in a meta-analysis that synthesizes the results of a set of primary studies. The true status is determined by a reference that ideally is a gold standard, which means that it is error free. However, in psychology, it is rare that all the primary studies have employed the same reference, and often they have used an imperfect reference with suboptimal accuracy instead of an actual gold standard. An imperfect reference biases both the estimates of the accuracy of the test and the empirical prevalence of the target status in the primary studies. We discuss several strategies for meta-analysis when different references are employed. Special attention is paid to the simplest case, where the meta-analyst has 1 group of primary studies using a reference that can be considered a gold standard and a 2nd group of primary studies using an imperfect reference. A procedure is recommended in which the frequencies from the primary studies with the imperfect reference are corrected prior to the meta-analysis itself. Then, a hierarchical meta-analytic model is fitted. An example with actual data from SCOFF (Sick-Control-One-Fat-Food; Hill, Reid, Morgan, \& Lacey, 2010; Morgan, Reid, \& Lacey, 1999) a simple but efficient test for detecting eating disorders, is described. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {correlated effects,discrete effect sizes},
  number = {3},
  series = {Meta-{{Analysis Topics}}}
}

@article{botella_psychometric_2010,
  title = {Psychometric Inferences from a Meta-Analysis of Reliability and Internal Consistency Coefficients.},
  author = {Botella, Juan and Suero, Manuel and Gambara, Hilda},
  year = {2010},
  volume = {15},
  pages = {386--397},
  doi = {10.1037/a0019626},
  abstract = {A meta-analysis of the reliability of the scores from a specific test, also called reliability generalization, allows the quantitative synthesis of its properties from a set of studies. It is usually assumed that part of the variation in the reliability coefficients is due to some unknown and implicit mechanism that restricts and biases the selection of participants in the studies' samples. Sometimes this variation has been reduced by adjusting the coefficients by a formula associated with range restrictions. We propose a framework in which that variation is included (instead of adjusted) in the models intended to explain the variability and in which parallel analyses of the studies' means and variances are performed. Furthermore, the analysis of the residuals enables inferences to be made about the nature of the variability accounted for by moderator variables. The meta-analysis of the 3 studies' statistics-reliability coefficient, mean, and variance--allows psychometric inferences about the test scores. A numerical example illustrates the proposed framework.},
  file = {/Users/rritaz/Zotero/storage/E72DFENN/Botella et al. - 2010 - Psychometric inferences from a meta-analysis of re.pdf},
  journal = {Psychological methods},
  keywords = {continuous effect sizes,correlation coefficients,effect size estimation (series)},
  number = {4}
}

@article{bowden_framework_2017,
  title = {A Framework for the Investigation of Pleiotropy in Two-Sample Summary Data {{Mendelian}} Randomization},
  author = {Bowden, Jack and Del Greco M, Fabiola and Minelli, Cosetta and Smith, George Davey and Sheehan, Nuala and Thompson, John},
  year = {2017},
  volume = {36},
  pages = {1783--1802},
  issn = {1097-0258},
  doi = {10.1002/sim.7221},
  abstract = {Mendelian randomization (MR) uses genetic data to probe questions of causality in epidemiological research, by invoking the Instrumental Variable (IV) assumptions. In recent years, it has become commonplace to attempt MR analyses by synthesising summary data estimates of genetic association gleaned from large and independent study populations. This is referred to as two-sample summary data MR. Unfortunately, due to the sheer number of variants that can be easily included into summary data MR analyses, it is increasingly likely that some do not meet the IV assumptions due to pleiotropy. There is a pressing need to develop methods that can both detect and correct for pleiotropy, in order to preserve the validity of the MR approach in this context. In this paper, we aim to clarify how established methods of meta-regression and random effects modelling from mainstream meta-analysis are being adapted to perform this task. Specifically, we focus on two contrastin g approaches: the Inverse Variance Weighted (IVW) method which assumes in its simplest form that all genetic variants are valid IVs, and the method of MR-Egger regression that allows all variants to violate the IV assumptions, albeit in a specific way. We investigate the ability of two popular random effects models to provide robustness to pleiotropy under the IVW approach, and propose statistics to quantify the relative goodness-of-fit of the IVW approach over MR-Egger regression. \textcopyright{} 2017 The Authors. Statistics in Medicine Published by JohnWiley \& Sons Ltd},
  file = {/Users/rritaz/Zotero/storage/4KGKKJQ4/Bowden et al. - 2017 - A framework for the investigation of pleiotropy in.pdf;/Users/rritaz/Zotero/storage/6M2QQHH7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,modeling effect size variation (covariates),physical/biological fields,random-effects},
  language = {en},
  number = {11}
}

@article{bowden_individual_2011,
  title = {Individual Patient Data Meta-Analysis of Time-to-Event Outcomes: One-Stage versus Two-Stage Approaches for Estimating the Hazard Ratio under a Random Effects Model},
  shorttitle = {Individual Patient Data Meta-Analysis of Time-to-Event Outcomes},
  author = {Bowden, Jack and Tierney, Jayne F. and Simmonds, Mark and Copas, Andrew J. and Higgins, Julian PT},
  year = {2011},
  volume = {2},
  pages = {150--162},
  issn = {1759-2887},
  doi = {10.1002/jrsm.45},
  abstract = {Meta-analyses of individual patient data (IPD) provide a strong and authoritative basis for evidence synthesis. IPD are particularly useful when the outcome of interest is the time to an event. Methodological developments now enable the meta-analysis of time-to-event IPD using a single model, allowing treatment effect and across-trial heterogeneity parameters to be estimated simultaneously. This differs from the standard approaches used with aggregate data, and also predominantly with IPD. Facilitated by a simulation study, we investigate what these new `one-stage' random-effects models offer over standard `two-stage' approaches. We find that two-stage approaches represent a robust, reliable and easily implementable way to estimate treatment effects and account for heterogeneity. Nevertheless, one-stage models can be used to provide a deeper insight into the data. Software for fitting one-stage Cox models with random effects using Restricted Maximum Likelihood methodology is made available, and its use demonstrated on an IPD meta-analysis assessing post-operative radio therapy for patients with non-small cell lung cancer. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/HR8K7U6U/Bowden et al. - 2011 - Individual patient data meta-analysis of time-to-e.pdf;/Users/rritaz/Zotero/storage/7VQLQAKH/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {3}
}

@article{bowden_modelling_2010,
  title = {Modelling Multiple Sources of Dissemination Bias in Meta-Analysis},
  author = {Bowden, Jack and Jackson, Dan and Thompson, Simon G.},
  year = {2010},
  volume = {29},
  pages = {945--955},
  issn = {1097-0258},
  doi = {10.1002/sim.3813},
  abstract = {Asymmetry in the funnel plot for a meta-analysis suggests the presence of dissemination bias. This may be caused by publication bias through the decisions of journal editors, by selective reporting of research results by authors or by a combination of both. Typically, study results that are statistically significant or have larger estimated effect sizes are more likely to appear in the published literature, hence giving a biased picture of the evidence-base. Previous statistical approaches for addressing dissemination bias have assumed only a single selection mechanism. Here we consider a more realistic scenario in which multiple dissemination processes, involving both the publishing authors and journals, are operating. In practical applications, the methods can be used to provide sensitivity analyses for the potential effects of multiple dissemination biases operating in meta-analysis. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VTESPGSQ/Bowden et al. - 2010 - Modelling multiple sources of dissemination bias i.pdf;/Users/rritaz/Zotero/storage/YHC54G6Y/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {7-8}
}

@article{bowden_quantifying_2011,
  title = {Quantifying, Displaying and Accounting for Heterogeneity in the Meta-Analysis of {{RCTs}} Using Standard and Generalised {{Qstatistics}}},
  author = {Bowden, Jack and Tierney, Jayne F and Copas, Andrew J and Burdett, Sarah},
  year = {2011},
  month = dec,
  volume = {11},
  pages = {41},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-41},
  abstract = {Background: Clinical researchers have often preferred to use a fixed effects model for the primary interpretation of a meta-analysis. Heterogeneity is usually assessed via the well known Q and I2 statistics, along with the random effects estimate they imply. In recent years, alternative methods for quantifying heterogeneity have been proposed, that are based on a `generalised' Q statistic. Methods: We review 18 IPD meta-analyses of RCTs into treatments for cancer, in order to quantify the amount of heterogeneity present and also to discuss practical methods for explaining heterogeneity. Results: Differing results were obtained when the standard Q and I2 statistics were used to test for the presence of heterogeneity. The two meta-analyses with the largest amount of heterogeneity were investigated further, and on inspection the straightforward application of a random effects model was not deemed appropriate. Compared to the standard Q statistic, the generalised Q statistic provided a more accurate platform for estimating the amount of heterogeneity in the 18 meta-analyses. Conclusions: Explaining heterogeneity via the pre-specification of trial subgroups, graphical diagnostic tools and sensitivity analyses produced a more desirable outcome than an automatic application of the random effects model. Generalised Q statistic methods for quantifying and adjusting for heterogeneity should be incorporated as standard into statistical software. Software is provided to help achieve this aim.},
  file = {/Users/rritaz/Zotero/storage/IR92MKNR/Bowden et al. - 2011 - Quantifying, displaying and accounting for heterog.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,Individual Patient Data IPD,quantifying heterogeneity,random-effects},
  language = {en},
  number = {1}
}

@article{bowden_using_2006,
  title = {Using Pseudo-Data to Correct for Publication Bias in Meta-Analysis},
  author = {Bowden, Jack and Thompson, John R. and Burton, Paul},
  year = {2006},
  volume = {25},
  pages = {3798--3813},
  issn = {1097-0258},
  doi = {10.1002/sim.2487},
  abstract = {In many ways, adjustment for publication bias in meta-analysis parallels adjustment for ascertainment bias in genetic studies. We investigate a previously published simulation-based method for dealing with complex ascertainment bias and show that it can be modified for use in meta-analysis when publication bias is suspected. The method involves simulating sets of pseudo-data under the assumed model using guesses for the unknown parameters. The pseudo-data are subjected to the same selection criteria as are believed to have operated on the original data. A conditional likelihood is then used to estimate the adjusted values of the unknown parameters. This method is used to re-analyse a published meta-analysis of the effect of the MTHFR gene on homocysteine levels. Simulation studies show that the pseudo-data method is unbiased; they give an indication of the number of pseudo-data values required and suggest that a two-stage adjustment produces less variable estimates. This method can be thought of as an example of the selection model approach to publication bias correction. As the selection mechanism must be assumed, it is important to investigate the sensitivity of any conclusions to this assumption. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/9UJ64G9A/Bowden et al. - 2006 - Using pseudo-data to correct for publication bias .pdf;/Users/rritaz/Zotero/storage/RSRHFBJC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {22}
}

@article{box_theorems_1954,
  title = {Some {{Theorems}} on {{Quadratic Forms Applied}} in the {{Study}} of {{Analysis}} of {{Variance Problems}}, {{I}}. {{Effect}} of {{Inequality}} of {{Variance}} in the {{One}}-{{Way Classification}}},
  author = {Box, G. E. P.},
  year = {1954},
  month = jun,
  volume = {25},
  pages = {290--302},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177728786},
  abstract = {This is the first of two papers describing a study of the effect of departures from assumptions, other than normality, on the null-distribution of the FFF-statistic in the analysis of variance. In this paper, certain theorems required in the study and concerning the distribution of quadratic forms in multi-normally distributed variables are first enunciated and simple approximations tested numerically. The results are then applied to determine the effect of group-to-group inequality of variance in the one-way classification. It appears that if the groups are equal, moderate inequality of variance does not seriously affect the test. However, with unequal groups, much larger discrepancies appear. In a second paper, similar methods are used to determine the effect of inequality of variance and serial correlation between errors in the two-way classification.},
  file = {/Users/rritaz/Zotero/storage/F6466DBT/Box - 1954 - Some Theorems on Quadratic Forms Applied in the St.pdf;/Users/rritaz/Zotero/storage/G9S6H2GM/1177728786.html},
  journal = {Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR61787},
  number = {2},
  zmnumber = {0055.37305}
}

@article{bradburn_much_2007,
  title = {Much Ado about Nothing: A Comparison of the Performance of Meta-Analytical Methods with Rare Events},
  shorttitle = {Much Ado about Nothing},
  author = {Bradburn, Michael J. and Deeks, Jonathan J. and Berlin, Jesse A. and Localio, A. Russell},
  year = {2007},
  volume = {26},
  pages = {53--77},
  issn = {1097-0258},
  doi = {10.1002/sim.2528},
  abstract = {For rare outcomes, meta-analysis of randomized trials may be the only way to obtain reliable evidence of the effects of healthcare interventions. However, many methods of meta-analysis are based on large sample approximations, and may be unsuitable when events are rare. Through simulation, we evaluated the performance of 12 methods for pooling rare events, considering estimability, bias, coverage and statistical power. Simulations were based on data sets from three case studies with between five and 19 trials, using baseline event rates between 0.1 and 10 per cent and risk ratios of 1, 0.75, 0.5 and 0.2. We found that most of the commonly used meta-analytical methods were biased when data were sparse. The bias was greatest in inverse variance and DerSimonian and Laird odds ratio and risk difference methods, and the Mantel\textendash Haenszel (MH) odds ratio method using a 0.5 zero-cell correction. Risk difference meta-analytical methods tended to show conservative confidence interval coverage and low statistical power at low event rates. At event rates below 1 per cent the Peto one-step odds ratio method was the least biased and most powerful method, and provided the best confidence interval coverage, provided there was no substantial imbalance between treatment and control group sizes within trials, and treatment effects were not exceptionally large. In other circumstances the MH OR without zero-cell corrections, logistic regression and the exact method performed similarly to each other, and were less biased than the Peto method. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/QBRYBTI2/Bradburn et al. - 2007 - Much ado about nothing a comparison of the perfor.pdf;/Users/rritaz/Zotero/storage/HW88W8XF/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),odds ratio,power,random-effects},
  language = {en},
  number = {1}
}

@article{brannick_bayesian_2013,
  title = {Bayesian Meta-Analysis of Coefficient Alpha},
  author = {Brannick, Michael T. and Zhang, Nanhua},
  year = {2013},
  volume = {4},
  pages = {198--207},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1075},
  abstract = {The current paper describes and illustrates a Bayesian approach to the meta-analysis of coefficient alpha. Alpha is the most commonly used estimate of the reliability or consistency (freedom from measurement error) for educational and psychological measures. The conventional approach to meta-analysis uses inverse variance weights to combine information from independent studies to provide an overall estimate. The Bayesian approach provides similar estimates to the conventional approach if a diffuse prior is used. However, the Bayesian approach also provides `shrunken' local estimates of reliability in each context. The amount of shrinkage depends upon both the variability in the underlying populations and the sampling variance of the local estimates. Advantages of the approach are the estimation of individual studies adjusted for sampling error and the application of meta-analytic results to new local studies in which the local study `borrows strength' from the meta-analysis. The ability to borrow strength for the new local studies is particularly useful in applied work in which the estimate of the local parameter is of primary interest. The approach is illustrated by the analysis of studies of the reliability of the General Ethnicity Questionnaire \textendash{} Abridged, a measure of identification with the culture of one's heritage and the culture of one's host country. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JBSJ9CE8/Brannick and Zhang - 2013 - Bayesian meta-analysis of coefficient alpha.pdf;/Users/rritaz/Zotero/storage/W8UBLCD5/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian},
  language = {en},
  number = {2}
}

@article{bravata_coplot_2008,
  title = {{{CoPlot}}: {{A}} Tool for Visualizing Multivariate Data in Medicine},
  shorttitle = {{{CoPlot}}},
  author = {Bravata, Dena M. and Shojania, Kaveh G. and Olkin, Ingram and Raveh, Adi},
  year = {2008},
  volume = {27},
  pages = {2234--2247},
  issn = {1097-0258},
  doi = {10.1002/sim.3078},
  abstract = {Many critical questions in medicine require the analysis of complex multivariate data, often from large data sets describing numerous variables for numerous subjects. In this paper, we describe CoPlot, a tool for visualizing multivariate data in medicine. CoPlot is an adaptation of multidimensional scaling (MDS) that addresses several key limitations of MDS, namely that MDS maps do not allow for visualization of both observations and variables simultaneously and that the axes on an MDS map have no inherent meaning. By addressing these issues, CoPlot facilitates rich interpretation of multivariate data. We present an example using CoPlot on a recently published data set from a systematic review describing clinical features and disease progression of children with anthrax and provide recommendations for the use of CoPlot for evaluating and interpreting other healthcare data sets. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/C2UAFC7D/Bravata et al. - 2008 - CoPlot A tool for visualizing multivariate data i.pdf;/Users/rritaz/Zotero/storage/TNQM6UWE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,multivariate},
  language = {en},
  number = {12}
}

@article{brockhaus_peto_2014,
  title = {The {{Peto}} Odds Ratio Viewed as a New Effect Measure},
  author = {Brockhaus, A. Catharina and Bender, Ralf and Skipka, Guido},
  year = {2014},
  volume = {33},
  pages = {4861--4874},
  issn = {1097-0258},
  doi = {10.1002/sim.6301},
  abstract = {Meta-analysis has generally been accepted as a fundamental tool for combining effect estimates from several studies. For binary studies with rare events, the Peto odds ratio (POR) method has become the relative effect estimator of choice. However, the POR leads to biased estimates for the OR when treatment effects are large or the group size ratio is not balanced. The aim of this work is to derive the limit of the POR estimator for increasing sample size, to investigate whether the POR limit is equal to the true OR and, if this is not the case, in which situations the POR limit is sufficiently close to the OR. It was found that the derived limit of the expected POR is not equivalent to the OR, because it depends on the group size ratio. Thus, the POR represents a different effect measure. We investigated in which situations the POR is reasonably close to the OR and found that this depends only slightly on the baseline risk within the range (0.001; 0.1) yet substantially on the group size ratio and the effect size itself. We derived the maximum effect size of the POR for different group size ratios and tolerated amounts of bias, for which the POR method results in an acceptable estimator of the OR. We conclude that the limit of the expected POR can be regarded as a new effect measure, which can be used in the presented situations as a valid estimate of the true OR. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LR386HHX/Brockhaus et al. - 2014 - The Peto odds ratio viewed as a new effect measure.pdf;/Users/rritaz/Zotero/storage/NHX95HVQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {discrete effect sizes},
  language = {en},
  number = {28}
}

@article{brockwell_comparison_2001,
  title = {A Comparison of Statistical Methods for Meta-Analysis},
  author = {Brockwell, Sarah E. and Gordon, Ian R.},
  year = {2001},
  volume = {20},
  pages = {825--840},
  issn = {1097-0258},
  doi = {10.1002/sim.650},
  abstract = {Meta-analysis may be used to estimate an overall effect across a number of similar studies. A number of statistical techniques are currently used to combine individual study results. The simplest of these is based on a fixed effects model, which assumes the true effect is the same for all studies. A random effects model, however, allows the true effect to vary across studies, with the mean true effect the parameter of interest. We consider three methods currently used for estimation within the framework of a random effects model, and illustrate them by applying each method to a collection of six studies on the effect of aspirin after myocardial infarction. These methods are compared using estimated coverage probabilities of confidence intervals for the overall effect. The techniques considered all generally have coverages below the nominal level, and in particular it is shown that the commonly used DerSimonian and Laird method does not adequately reflect the error associated with parameter estimation, especially when the number of studies is small. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/X76A7E5X/Brockwell and Gordon - 2001 - A comparison of statistical methods for meta-analy.pdf;/Users/rritaz/Zotero/storage/YJURYHTL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random effects models,small meta-analysis},
  language = {en},
  number = {6}
}

@article{brockwell_simple_2007,
  title = {A Simple Method for Inference on an Overall Effect in Meta-Analysis},
  author = {Brockwell, Sarah E. and Gordon, Ian R.},
  year = {2007},
  volume = {26},
  pages = {4531--4543},
  issn = {1097-0258},
  doi = {10.1002/sim.2883},
  abstract = {The random effects approach in meta-analysis due to DerSimonian and Laird is well established and used pervasively. It has been established by Brockwell and Gordon that this method, when used for confidence intervals, leads to coverage probabilities lower than the nominal value. A number of alternatives have been proposed, but these either have the defect of iterative and complicated calculation, or deficient coverage. In this paper we propose a new approach, which is simple to use, and has coverage probabilities better than the alternatives, based on extensive simulation. We call this approach the `quantile approximation' method. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/V68QSDBA/Brockwell and Gordon - 2007 - A simple method for inference on an overall effect.pdf;/Users/rritaz/Zotero/storage/4XW573EI/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,Confidence Intervals,random-effects},
  language = {en},
  number = {25}
}

@article{bujkiewicz_bayesian_2016,
  title = {Bayesian Meta-Analytical Methods to Incorporate Multiple Surrogate Endpoints in Drug Development Process},
  author = {Bujkiewicz, Sylwia and Thompson, John R. and Riley, Richard D. and Abrams, Keith R.},
  year = {2016},
  volume = {35},
  pages = {1063--1089},
  issn = {1097-0258},
  doi = {10.1002/sim.6776},
  abstract = {A number of meta-analytical methods have been proposed that aim to evaluate surrogate endpoints. Bivariate meta-analytical methods can be used to predict the treatment effect for the final outcome from the treatment effect estimate measured on the surrogate endpoint while taking into account the uncertainty around the effect estimate for the surrogate endpoint. In this paper, extensions to multivariate models are developed aiming to include multiple surrogate endpoints with the potential benefit of reducing the uncertainty when making predictions. In this Bayesian multivariate meta-analytic framework, the between-study variability is modelled in a formulation of a product of normal univariate distributions. This formulation is particularly convenient for including multiple surrogate endpoints and flexible for modelling the outcomes which can be surrogate endpoints to the final outcome and potentially to one another. Two models are proposed, first, using an unstructured between-study covariance matrix by assuming the treatment effects on all outcomes are correlated and second, using a structured between-study covariance matrix by assuming treatment effects on some of the outcomes are conditionally independent. While the two models are developed for the summary data on a study level, the individual-level association is taken into account by the use of the Prentice's criteria (obtained from individual patient data) to inform the within study correlations in the models. The modelling techniques are investigated using an example in relapsing remitting multiple sclerosis where the disability worsening is the final outcome, while relapse rate and MRI lesions are potential surrogates to the disability progression. \textcopyright{} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/S7BCTZSD/Bujkiewicz et al. - 2016 - Bayesian meta-analytical methods to incorporate mu.pdf;/Users/rritaz/Zotero/storage/LMMGM6TH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate,random-effects},
  language = {en},
  number = {7}
}

@article{bujkiewicz_bivariate_2019,
  title = {Bivariate Network Meta-Analysis for Surrogate Endpoint Evaluation},
  author = {Bujkiewicz, Sylwia and Jackson, Dan and Thompson, John R. and Turner, Rebecca M. and St{\"a}dler, Nicolas and Abrams, Keith R. and White, Ian R.},
  year = {2019},
  volume = {38},
  pages = {3322--3341},
  issn = {1097-0258},
  doi = {10.1002/sim.8187},
  abstract = {Surrogate endpoints are very important in regulatory decision making in healthcare, in particular if they can be measured early compared to the long-term final clinical outcome and act as good predictors of clinical benefit. Bivariate meta-analysis methods can be used to evaluate surrogate endpoints and to predict the treatment effect on the final outcome from the treatment effect measured on a surrogate endpoint. However, candidate surrogate endpoints are often imperfect, and the level of association between the treatment effects on the surrogate and final outcomes may vary between treatments. This imposes a limitation on methods which do not differentiate between the treatments. We develop bivariate network meta-analysis (bvNMA) methods, which combine data on treatment effects on the surrogate and final outcomes, from trials investigating multiple treatment contrasts. The bvNMA methods estimate the effects on both outcomes for all treatment contrasts individually in a single analysis. At the same time, they allow us to model the trial-level surrogacy patterns within each treatment contrast and treatment-level surrogacy, thus enabling predictions of the treatment effect on the final outcome either for a new study in a new population or for a new treatment. Modelling assumptions about the between-studies heterogeneity and the network consistency, and their impact on predictions, are investigated using an illustrative example in advanced colorectal cancer and in a simulation study. When the strength of the surrogate relationships varies across treatment contrasts, bvNMA has the advantage of identifying treatment comparisons for which surrogacy holds, thus leading to better predictions.},
  file = {/Users/rritaz/Zotero/storage/I3DSWUEW/Bujkiewicz et al. - 2019 - Bivariate network meta-analysis for surrogate endp.pdf;/Users/rritaz/Zotero/storage/YJ8RG3YX/sim.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {18}
}

@article{bujkiewicz_multivariate_2013,
  title = {Multivariate Meta-Analysis of Mixed Outcomes: A {{Bayesian}} Approach},
  shorttitle = {Multivariate Meta-Analysis of Mixed Outcomes},
  author = {Bujkiewicz, Sylwia and Thompson, John R. and Sutton, Alex J. and Cooper, Nicola J. and Harrison, Mark J. and Symmons, Deborah P. M. and Abrams, Keith R.},
  year = {2013},
  volume = {32},
  pages = {3926--3943},
  issn = {1097-0258},
  doi = {10.1002/sim.5831},
  abstract = {Multivariate random effects meta-analysis (MRMA) is an appropriate way for synthesizing data from studies reporting multiple correlated outcomes. In a Bayesian framework, it has great potential for integrating evidence from a variety of sources. In this paper, we propose a Bayesian model for MRMA of mixed outcomes, which extends previously developed bivariate models to the trivariate case and also allows for combination of multiple outcomes that are both continuous and binary. We have constructed informative prior distributions for the correlations by using external evidence. Prior distributions for the within-study correlations were constructed by employing external individual patent data and using a double bootstrap method to obtain the correlations between mixed outcomes. The between-study model of MRMA was parameterized in the form of a product of a series of univariate conditional normal distributions. This allowed us to place explicit prior distributions on the between-study correlations, which were constructed using external summary data. Traditionally, independent `vague' prior distributions are placed on all parameters of the model. In contrast to this approach, we constructed prior distributions for the between-study model parameters in a way that takes into account the inter-relationship between them. This is a flexible method that can be extended to incorporate mixed outcomes other than continuous and binary and beyond the trivariate case. We have applied this model to a motivating example in rheumatoid arthritis with the aim of incorporating all available evidence in the synthesis and potentially reducing uncertainty around the estimate of interest. \textcopyright{} 2013 The Authors. Statistics inMedicine Published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GDMFQ8LD/Bujkiewicz et al. - 2013 - Multivariate meta-analysis of mixed outcomes a Ba.pdf;/Users/rritaz/Zotero/storage/7TPUDFSB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,correlated effects,multivariate},
  language = {en},
  number = {22}
}

@article{burgess_bayesian_2010,
  title = {Bayesian Methods for Meta-Analysis of Causal Relationships Estimated Using Genetic Instrumental Variables},
  author = {Burgess, Stephen and Thompson, Simon G.},
  year = {2010},
  volume = {29},
  pages = {1298--1311},
  issn = {1097-0258},
  doi = {10.1002/sim.3843},
  abstract = {Genetic markers can be used as instrumental variables, in an analogous way to randomization in a clinical trial, to estimate the causal relationship between a phenotype and an outcome variable. Our purpose is to extend the existing methods for such Mendelian randomization studies to the context of multiple genetic markers measured in multiple studies, based on the analysis of individual participant data. First, for a single genetic marker in one study, we show that the usual ratio of coefficients approach can be reformulated as a regression with heterogeneous error in the explanatory variable. This can be implemented using a Bayesian approach, which is next extended to include multiple genetic markers. We then propose a hierarchical model for undertaking a meta-analysis of multiple studies, in which it is not necessary that the same genetic markers are measured in each study. This provides an overall estimate of the causal relationship between the phenotype and the outcome, and an assessment of its heterogeneity across studies. As an example, we estimate the causal relationship of blood concentrations of C-reactive protein on fibrinogen levels using data from 11 studies. These methods provide a flexible framework for efficient estimation of causal relationships derived from multiple studies. Issues discussed include weak instrument bias, analysis of binary outcome data such as disease risk, missing genetic data, and the use of haplotypes. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZH4JDKPW/Burgess and Thompson - 2010 - Bayesian methods for meta-analysis of causal relat.pdf;/Users/rritaz/Zotero/storage/8N8W65JY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,GLM MA models,modeling effect size variation (covariates),physical/biological fields,random-effects},
  language = {en},
  number = {12}
}

@article{burgess_combining_2013,
  title = {Combining Multiple Imputation and Meta-Analysis with Individual Participant Data},
  author = {Burgess, Stephen and White, Ian R. and Resche-Rigon, Matthieu and Wood, Angela M.},
  year = {2013},
  volume = {32},
  pages = {4499--4514},
  issn = {1097-0258},
  doi = {10.1002/sim.5844},
  abstract = {Multiple imputation is a strategy for the analysis of incomplete data such that the impact of the missingness on the power and bias of estimates is mitigated. When data from multiple studies are collated, we can propose both within-study and multilevel imputation models to impute missing data on covariates. It is not clear how to choose between imputation models or how to combine imputation and inverse-variance weighted meta-analysis methods. This is especially important as often different studies measure data on different variables, meaning that we may need to impute data on a variable which is systematically missing in a particular study. In this paper, we consider a simulation analysis of sporadically missing data in a single covariate with a linear analysis model and discuss how the results would be applicable to the case of systematically missing data. We find in this context that ensuring the congeniality of the imputation and analysis models is important to give correct standard errors and confidence intervals. For example, if the analysis model allows between-study heterogeneity of a parameter, then we should incorporate this heterogeneity into the imputation model to maintain the congeniality of the two models. In an inverse-variance weighted meta-analysis, we should impute missing data and apply Rubin's rules at the study level prior to meta-analysis, rather than meta-analyzing each of the multiple imputations and then combining the meta-analysis estimates using Rubin's rules. We illustrate the results using data from the Emerging Risk Factors Collaboration. \textcopyright{} 2013 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/W96D89T2/Burgess et al. - 2013 - Combining multiple imputation and meta-analysis wi.pdf;/Users/rritaz/Zotero/storage/AMWPR5VK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,modeling effect size variation (covariates)},
  language = {en},
  number = {26}
}

@article{burgess_combining_2016,
  title = {Combining Information on Multiple Instrumental Variables in {{Mendelian}} Randomization: Comparison of Allele Score and Summarized Data Methods},
  shorttitle = {Combining Information on Multiple Instrumental Variables in {{Mendelian}} Randomization},
  author = {Burgess, Stephen and Dudbridge, Frank and Thompson, Simon G.},
  year = {2016},
  volume = {35},
  pages = {1880--1906},
  issn = {1097-0258},
  doi = {10.1002/sim.6835},
  abstract = {Mendelian randomization is the use of genetic instrumental variables to obtain causal inferences from observational data. Two recent developments for combining information on multiple uncorrelated instrumental variables (IVs) into a single causal estimate are as follows: (i) allele scores, in which individual-level data on the IVs are aggregated into a univariate score, which is used as a single IV, and (ii) a summary statistic method, in which causal estimates calculated from each IV using summarized data are combined in an inverse-variance weighted meta-analysis. To avoid bias from weak instruments, unweighted and externally weighted allele scores have been recommended. Here, we propose equivalent approaches using summarized data and also provide extensions of the methods for use with correlated IVs. We investigate the impact of different choices of weights on the bias and precision of estimates in simulation studies. We show that allele score estimates can be reproduced using summarized data on genetic associations with the risk factor and the outcome. Estimates from the summary statistic method using external weights are biased towards the null when the weights are imprecisely estimated; in contrast, allele score estimates are unbiased. With equal or external weights, both methods provide appropriate tests of the null hypothesis of no causal effect even with large numbers of potentially weak instruments. We illustrate these methods using summarized data on the causal effect of low-density lipoprotein cholesterol on coronary heart disease risk. It is shown that a more precise causal estimate can be obtained using multiple genetic variants from a single gene region, even if the variants are correlated. \textcopyright{} 2015 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/QZRGH2D3/Burgess et al. - 2016 - Combining information on multiple instrumental var.pdf;/Users/rritaz/Zotero/storage/ZYVB2GYB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,physical/biological fields,random-effects},
  language = {en},
  number = {11}
}

@article{burke_meta-analysis_2017,
  title = {Meta-Analysis Using Individual Participant Data: One-Stage and Two-Stage Approaches, and Why They May Differ},
  shorttitle = {Meta-Analysis Using Individual Participant Data},
  author = {Burke, Danielle L. and Ensor, Joie and Riley, Richard D.},
  year = {2017},
  volume = {36},
  pages = {855--875},
  issn = {1097-0258},
  doi = {10.1002/sim.7141},
  abstract = {Meta-analysis using individual participant data (IPD) obtains and synthesises the raw, participant-level data from a set of relevant studies. The IPD approach is becoming an increasingly popular tool as an alternative to traditional aggregate data meta-analysis, especially as it avoids reliance on published results and provides an opportunity to investigate individual-level interactions, such as treatment-effect modifiers. There are two statistical approaches for conducting an IPD meta-analysis: one-stage and two-stage. The one-stage approach analyses the IPD from all studies simultaneously, for example, in a hierarchical regression model with random effects. The two-stage approach derives aggregate data (such as effect estimates) in each study separately and then combines these in a traditional meta-analysis model. There have been numerous comparisons of the one-stage and two-stage approaches via theoretical consideration, simulation and empirical examples, yet there remains confusion regarding when each approach should be adopted, and indeed why they may differ. In this tutorial paper, we outline the key statistical methods for one-stage and two-stage IPD meta-analyses, and provide 10 key reasons why they may produce different summary results. We explain that most differences arise because of different modelling assumptions, rather than the choice of one-stage or two-stage itself. We illustrate the concepts with recently published IPD meta-analyses, summarise key statistical software and provide recommendations for future IPD meta-analyses. \textcopyright{} 2016 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/VAZGCILP/Burke et al. - 2017 - Meta-analysis using individual participant data o.pdf;/Users/rritaz/Zotero/storage/MLYDVHMA/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,Individual Patient Data IPD},
  language = {en},
  number = {5}
}

@article{burkner_testing_2014,
  title = {Testing for Publication Bias in Diagnostic Meta-Analysis: A Simulation Study},
  shorttitle = {Testing for Publication Bias in Diagnostic Meta-Analysis},
  author = {B{\"u}rkner, Paul-Christian and Doebler, Philipp},
  year = {2014},
  volume = {33},
  pages = {3061--3077},
  issn = {1097-0258},
  doi = {10.1002/sim.6177},
  abstract = {The present study investigates the performance of several statistical tests to detect publication bias in diagnostic meta-analysis by means of simulation. While bivariate models should be used to pool data from primary studies in diagnostic meta-analysis, univariate measures of diagnostic accuracy are preferable for the purpose of detecting publication bias. In contrast to earlier research, which focused solely on the diagnostic odds ratio or its logarithm ( ln {$\omega$}), the tests are combined with four different univariate measures of diagnostic accuracy. For each combination of test and univariate measure, both type I error rate and statistical power are examined under diverse conditions. The results indicate that tests based on linear regression or rank correlation cannot be recommended in diagnostic meta-analysis, because type I error rates are either inflated or power is too low, irrespective of the applied univariate measure. In contrast, the combination of trim and fill and ln {$\omega$} has non-inflated or only slightly inflated type I error rates and medium to high power, even under extreme circumstances (at least when the number of studies per meta-analysis is large enough). Therefore, we recommend the application of trim and fill combined with ln {$\omega$} to detect funnel plot asymmetry in diagnostic meta-analysis. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YKVP8KJH/Bürkner and Doebler - 2014 - Testing for publication bias in diagnostic meta-an.pdf;/Users/rritaz/Zotero/storage/I578A9ZE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,publication bias},
  language = {en},
  number = {18}
}

@article{cafri_meta-analysis_2015,
  title = {Meta-Analysis of Survival Curve Data Using Distributed Health Data Networks: Application to Hip Arthroplasty Studies of the {{International Consortium}} of {{Orthopaedic Registries}}},
  shorttitle = {Meta-Analysis of Survival Curve Data Using Distributed Health Data Networks},
  author = {Cafri, Guy and Banerjee, Samprit and Sedrakyan, Art and Paxton, Liz and Furnes, Ove and Graves, Stephen and Marinac-Dabic, Danica},
  year = {2015},
  volume = {6},
  pages = {347--356},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1159},
  abstract = {The motivating example for this paper comes from a distributed health data network, the International Consortium of Orthopaedic Registries (ICOR), which aims to examine risk factors for orthopedic device failure for registries around the world. Unfortunately, regulatory, privacy, and propriety concerns made sharing of raw data impossible, even if de-identified. Therefore, this article describes an approach to extraction and analysis of aggregate time-to-event data from ICOR. Data extraction is based on obtaining a survival probability and variance estimate for each unique combination of the explanatory variables at each distinct event time for each registry. The extraction procedure allows for a great deal of flexibility; models can be specified after the data have been collected, for example, modeling of interaction effects and selection of subgroups of patients based on their values on the explanatory variables. Our analysis models are adapted from models presented elsewhere \textendash{} but allowing for censoring in the calculation of the correlation between serial survival probabilities and using the square root of the covariance matrix to transform the data to avoid computational problems in model estimation. Simulations and a real-data example are provided with strengths and limitations of the approach discussed. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DNLBR2H4/Cafri et al. - 2015 - Meta-analysis of survival curve data using distrib.pdf;/Users/rritaz/Zotero/storage/WJEX9848/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes},
  language = {en},
  number = {4}
}

@article{cai_meta-analysis_2010,
  title = {Meta-Analysis for Rare Events},
  author = {Cai, Tianxi and Parast, Layla and Ryan, Louise},
  year = {2010},
  volume = {29},
  pages = {2078--2089},
  issn = {1097-0258},
  doi = {10.1002/sim.3964},
  abstract = {Meta-analysis provides a useful framework for combining information across related studies and has been widely utilized to combine data from clinical studies in order to evaluate treatment efficacy. More recently, meta-analysis has also been used to assess drug safety. However, because adverse events are typically rare, standard methods may not work well in this setting. Most popular methods use fixed or random effects models to combine effect estimates obtained separately for each individual study. In the context of very rare outcomes, effect estimates from individual studies may be unstable or even undefined. We propose alternative approaches based on Poisson random effects models to make inference about the relative risk between two treatment groups. Simulation studies show that the proposed methods perform well when the underlying event rates are low. The methods are illustrated using data from a recent meta-analysis (N. Engl. J. Med. 2007; 356(24):2457\textendash 2471) of 48 comparative trials involving rosiglitazone, a type 2 diabetes drug, with respect to its possible cardiovascular toxicity. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Q69FWPW5/Cai et al. - 2010 - Meta-analysis for rare events.pdf;/Users/rritaz/Zotero/storage/LUD5D569/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random-effects},
  language = {en},
  number = {20}
}

@article{callegaro_assessing_2019,
  title = {Assessing Correlates of Protection in Vaccine Trials: Statistical Solutions in the Context of High Vaccine Efficacy},
  shorttitle = {Assessing Correlates of Protection in Vaccine Trials},
  author = {Callegaro, Andrea and Tibaldi, Fabian},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {47},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0687-y},
  abstract = {Background: The use of correlates of protection (CoPs) in vaccination trials offers significant advantages as useful clinical endpoint substitutes. Vaccines with very high vaccine efficacy (VE) are documented in the literature (VE {$\geq$}95\%). The rare events (number of infections) observed in the vaccinated groups of these trials posed challenges when applying conventionally-used statistical methods for CoP assessment. In this paper, we describe the nature of these challenges, and propose easy-to-implement and uniquely-tailored statistical solutions for the assessment of CoPs in the specific context of high VE. Methods: The Prentice criteria and meta-analytic frameworks are standard statistical methods for assessing vaccine CoPs, but can be problematic in high VE cases due to the rare events data available. As a result, lack of fit and the problem of infinite estimates may arise, in the former and latter methods respectively. The use of flexible models within the Prentice framework, and penalized-likelihood methods to solve the issue of infinite estimates can improve the performance of both methods in high VE settings. Results: We have 1) devised flexible non-linear models to counteract the Prentice framework lack of fit, providing sufficient statistical power to the method, and 2) proposed the use of penalised likelihood approaches to make the meta-analytic framework applicable on randomized subgroups, such as regions. The performance of the proposed methods for high VE cases was evaluated by running simulations. Conclusions: As vaccines with high efficacy are documented in the literature, there is a need to identify effective statistical solutions to assess CoPs. Our proposed adaptations are straight-forward and improve the performance of conventional statistical methods for high VE data, leading to more reliable CoP assessments in the context of high VE settings.},
  file = {/Users/rritaz/Zotero/storage/9ZYWGHHF/Callegaro and Tibaldi - 2019 - Assessing correlates of protection in vaccine tria.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {categorical MA models,discrete effect sizes},
  language = {en},
  number = {1}
}

@article{callot_problem_2011,
  title = {The Problem of Natural Funnel Asymmetries: A Simulation Analysis of Meta-Analysis in Macroeconomics},
  shorttitle = {The Problem of Natural Funnel Asymmetries},
  author = {Callot, Laurent and Paldam, Martin},
  year = {2011},
  volume = {2},
  pages = {84--102},
  issn = {1759-2887},
  doi = {10.1002/jrsm.39},
  abstract = {Effect sizes in macroeconomic are estimated by regressions on data published by statistical agencies. Funnel plots are a representation of the distribution of the resulting regression coefficients. They are normally much wider than predicted by the t-ratio of the coefficients and often asymmetric. The standard method of meta-analysts in economics assumes that the asymmetries are because of publication bias causing censoring and adjusts the average accordingly. The paper shows that some funnel asymmetries may be `natural' so that they occur without censoring. We investigate such asymmetries by simulating funnels by pairs of data generating processes (DGPs) and estimating models (EMs), in which the EM has the problem that it disregards a property of the DGP. The problems are data dependency, structural breaks, non-normal residuals, non-linearity, and omitted variables. We show that some of these problems generate funnel asymmetries. When they do, the standard method often fails. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/96KDWE7S/Callot and Paldam - 2011 - The problem of natural funnel asymmetries a simul.pdf;/Users/rritaz/Zotero/storage/M6QVG52S/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,publication bias},
  language = {en},
  number = {2}
}

@article{camilli_noncentral_2010,
  title = {A {{Noncentral}} t {{Regression Model}} for {{Meta}}-{{Analysis}}},
  author = {Camilli, Gregory and {de la Torre}, Jimmy and Chiu, Chia-Yi},
  year = {2010},
  month = apr,
  volume = {35},
  pages = {125--153},
  issn = {1076-9986},
  doi = {10.3102/1076998609346966},
  abstract = {In this article, three multilevel models for meta-analysis are examined. Hedges and Olkin suggested that effect sizes follow a noncentral t distribution and proposed several approximate methods. Raudenbush and Bryk further refined this model; however, this procedure is based on a normal approximation. In the current research literature, this approximate procedure has not been compared to one based directly on the noncentral t distribution, which is the approach taken in this article. A multilevel model is presented, and estimation is carried out on a real data set using the Markov chain Monte Carlo (MCMC) procedure. A simulation study is then conducted to examine the properties of the noncentral t approach in more depth. Finally, an example of code written in WinBUGS is given, which may be useful to researchers across a broad range of disciplines.},
  file = {/Users/rritaz/Zotero/storage/UBK3WKNU/Camilli et al. - 2010 - A Noncentral t Regression Model for Meta-Analysis.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {modeling effect size variation (covariates),multivariate},
  number = {2}
}

@article{casella_intrinsic_2005,
  title = {Intrinsic Meta-Analysis of Contingency Tables},
  author = {Casella, George and Moreno, El{\'i}as},
  year = {2005},
  volume = {24},
  pages = {583--604},
  issn = {1097-0258},
  doi = {10.1002/sim.2038},
  abstract = {Meta-analysis has a natural formulation as a Bayesian hierarchical model. The main theoretical difficulty is the construction of a sensible relationship between the parameters of the individual statistical experiments and the meta-parameter. Since that prior information on such a relationship is typically not available, we argue that this relationship should be dictated by the structure of the model at hand. We then propose a novel procedure based on intrinsic priors which we fully develop for the case of meta-analysis of 2 \texttimes{} 2 contingency tables. Illustrations on real and artificial tables are given. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GXU4V9TK/Casella and Moreno - 2005 - Intrinsic meta-analysis of contingency tables.pdf;/Users/rritaz/Zotero/storage/7M4P6MT3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian},
  language = {en},
  number = {4}
}

@article{chaimani_using_2012,
  title = {Using Network Meta-Analysis to Evaluate the Existence of Small-Study Effects in a Network of Interventions},
  author = {Chaimani, Anna and Salanti, Georgia},
  year = {2012},
  volume = {3},
  pages = {161--176},
  issn = {1759-2887},
  doi = {10.1002/jrsm.57},
  abstract = {Suggested methods for exploring the presence of small-study effects in a meta-analysis and the possibility of publication bias are associated with important limitations. When a meta-analysis comprises only a few studies, funnel plots are difficult to interpret, and regression-based approaches to test and account for small-study effects have low power. Assuming that the cause of funnel plot asymmetry is likely to affect an entire research field rather than only a particular comparison of interventions, we suggest that network meta-regression is employed to account for small-study effects in a set of related meta-analyses. We present several possible models for the direction and distribution of small-study effects and we describe the methods by re-analysing two published networks. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NUJXKFRR/Chaimani and Salanti - 2012 - Using network meta-analysis to evaluate the existe.pdf;/Users/rritaz/Zotero/storage/NT8GHAM8/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {GLM MA models,network meta-analysis},
  language = {en},
  number = {2}
}

@article{charles_correction_2005,
  title = {The {{Correction}} for {{Attenuation Due}} to {{Measurement Error}}: {{Clarifying Concepts}} and {{Creating Confidence Sets}}.},
  shorttitle = {The {{Correction}} for {{Attenuation Due}} to {{Measurement Error}}},
  author = {Charles, Eric P.},
  year = {2005},
  volume = {10},
  pages = {206--226},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.10.2.206},
  abstract = {The correction for attenuation due to measurement error (CAME) has received many historical criticisms, most of which can be traced to the limited ability to use CAME inferentially. Past attempts to determine confidence intervals for CAME are summarized and their limitations discussed. The author suggests that inference requires confidence sets that demarcate those population parameters likely to have produced an obtained value\textemdash rather than indicating the samples likely to be produced by a given population\textemdash and that most researchers tend to confuse these 2 types of confidence sets. Three different Monte-Carlo methods are presented, each offering a different way of examining confidence sets under the new conceptualization. Exploring the implications of these approaches for CAME suggests potential consequences for other statistics.},
  file = {/Users/rritaz/Zotero/storage/7WVMNBIC/Charles - 2005 - The Correction for Attenuation Due to Measurement .pdf},
  journal = {Psychological Methods},
  keywords = {correlation coefficients,diagnostic techniques},
  language = {en},
  number = {2}
}

@article{charoensawat_meta-analysis_2014,
  title = {Meta-Analysis and Meta-Modelling for Diagnostic Problems},
  author = {Charoensawat, Suphada and B{\"o}hning, Walailuck and B{\"o}hning, Dankmar and Holling, Heinz},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {56},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-56},
  abstract = {Background: A proportional hazards measure is suggested in the context of analyzing SROC curves that arise in the meta\textendash analysis of diagnostic studies. The measure can be motivated as a special model: the Lehmann model for ROC curves. The Lehmann model involves study\textendash specific sensitivities and specificities and a diagnostic accuracy parameter which connects the two. Methods: A study\textendash specific model is estimated for each study, and the resulting study-specific estimate of diagnostic accuracy is taken as an outcome measure for a mixed model with a random study effect and other study-level covariates as fixed effects. The variance component model becomes estimable by deriving within-study variances, depending on the outcome measure of choice. In contrast to existing approaches \textendash{} usually of bivariate nature for the outcome measures \textendash{} the suggested approach is univariate and, hence, allows easily the application of conventional mixed modelling. Results: Some simple modifications in the SAS procedure proc mixed allow the fitting of mixed models for meta-analytic data from diagnostic studies. The methodology is illustrated with several meta\textendash analytic diagnostic data sets, including a meta\textendash analysis of the Mini\textendash Mental State Examination as a diagnostic device for dementia and mild cognitive impairment. Conclusions: The proposed methodology allows us to embed the meta-analysis of diagnostic studies into the well\textendash developed area of mixed modelling. Different outcome measures, specifically from the perspective of whether a local or a global measure of diagnostic accuracy should be applied, are discussed as well. In particular, variation in cut-off value is discussed together with recommendations on choosing the best cut-off value. We also show how this problem can be addressed with the proposed methodology.},
  file = {/Users/rritaz/Zotero/storage/ELPTWY5V/Charoensawat et al. - 2014 - Meta-analysis and meta-modelling for diagnostic pr.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,random-effects},
  language = {en},
  number = {1}
}

@article{charpentier_meta-analytic_2016,
  title = {Meta-Analytic Estimation of Measurement Variability and Assessment of Its Impact on Decision-Making: The Case of Perioperative Haemoglobin Concentration Monitoring},
  shorttitle = {Meta-Analytic Estimation of Measurement Variability and Assessment of Its Impact on Decision-Making},
  author = {Charpentier, Emmanuel and Looten, Vincent and Fahlgren, Bj{\"o}rn and Barna, Alexandre and Guillevin, Lo{\"i}c},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0107-5},
  abstract = {Background: As a part of a larger Health Technology Assessment (HTA), the measurement error of a device used to monitor the hemoglobin concentration of a patient undergoing surgery, as well as its decision consequences, were to be estimated from published data. Methods: A Bayesian hierarchical model of measurement error, allowing the meta-analytic estimation of both central and dispersion parameters (under the assumption of normality of measurement errors) is proposed and applied to published data; the resulting potential decision errors are deduced from this estimation. The same method is used to assess the impact of an initial calibration. Results: The posterior distributions are summarized as mean {$\pm$} sd (credible interval). The fitted model exhibits a modest mean expected error (0.24 {$\pm$} 0.73 (-1.23 1.59) g/dL) and a large variability (mean absolute expected error 1.18 {$\pm$} 0.92 (0.05 3.36) g/dL). The initial calibration modifies the bias (-0.20 {$\pm$} 0.87 (-1.99 1.49) g/dL), but the variability remains almost as large (mean absolute expected error 1.05 {$\pm$} 0.87 (0.04 3.21) g/dL). This entails a potential decision error (``false positive'' or ``false negative'') for about one patient out of seven. Conclusions: The proposed hierarchical model allows the estimation of the variability from published aggregates, and allows the modeling of the consequences of this variability in terms of decision errors. For the device under assessment, these potential decision errors are clinically problematic.},
  file = {/Users/rritaz/Zotero/storage/AJTAX254/Charpentier et al. - 2016 - Meta-analytic estimation of measurement variabilit.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,physical/biological fields},
  language = {en},
  number = {1}
}

@article{chen_alternative_2015,
  title = {An Alternative Pseudolikelihood Method for Multivariate Random-Effects Meta-Analysis},
  author = {Chen, Yong and Hong, Chuan and Riley, Richard D.},
  year = {2015},
  volume = {34},
  pages = {361--380},
  issn = {1097-0258},
  doi = {10.1002/sim.6350},
  abstract = {Recently, multivariate random-effects meta-analysis models have received a great deal of attention, despite its greater complexity compared to univariate meta-analyses. One of its advantages is its ability to account for the within-study and between-study correlations. However, the standard inference procedures, such as the maximum likelihood or maximum restricted likelihood inference, require the within-study correlations, which are usually unavailable. In addition, the standard inference procedures suffer from the problem of singular estimated covariance matrix. In this paper, we propose a pseudolikelihood method to overcome the aforementioned problems. The pseudolikelihood method does not require within-study correlations and is not prone to singular covariance matrix problem. In addition, it can properly estimate the covariance between pooled estimates for different outcomes, which enables valid inference on functions of pooled estimates, and can be applied to meta-analysis where some studies have outcomes missing completely at random. Simulation studies show that the pseudolikelihood method provides unbiased estimates for functions of pooled estimates, well-estimated standard errors, and confidence intervals with good coverage probability. Furthermore, the pseudolikelihood method is found to maintain high relative efficiency compared to that of the standard inferences with known within-study correlations. We illustrate the proposed method through three meta-analyses for comparison of prostate cancer treatment, for the association between paraoxonase 1 activities and coronary heart disease, and for the association between homocysteine level and coronary heart disease. \textcopyright{} 2014 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/A99GUV4V/Chen et al. - 2015 - An alternative pseudolikelihood method for multiva.pdf;/Users/rritaz/Zotero/storage/SW43BR5M/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {3}
}

@article{chen_bayesian_2014,
  title = {Bayesian Sequential Meta-Analysis Design in Evaluating Cardiovascular Risk in a New Antidiabetic Drug Development Program},
  author = {Chen, Ming-Hui and Ibrahim, Joseph G. and Xia, H. Amy and Liu, Thomas and Hennessey, Violeta},
  year = {2014},
  volume = {33},
  pages = {1600--1618},
  issn = {1097-0258},
  doi = {10.1002/sim.6067},
  abstract = {AbstractRecently, the Center for Drug Evaluation and Research at the Food and Drug Administration released a guidance that makes recommendations about how to demonstrate that a new antidiabetic therapy to treat type 2 diabetes is not associated with an unacceptable increase in cardiovascular risk. One of the recommendations from the guidance is that phases II and III trials should be appropriately designed and conducted so that a meta-analysis can be performed. In addition, the guidance implies that a sequential meta-analysis strategy could be adopted. That is, the initial meta-analysis could aim at demonstrating the upper bound of a 95\% confidence interval (CI) for the estimated hazard ratio to be {$<$} 1.8 for the purpose of enabling a new drug application or a biologics license application. Subsequently after the marketing authorization, a final meta-analysis would need to show the upper bound to be {$<$} 1.3. In this context, we develop a new Bayesian sequential meta-analysis approach using survival regression models to assess whether the size of a clinical development program is adequate to evaluate a particular safety endpoint. We propose a Bayesian sample size determination methodology for sequential meta-analysis clinical trial design with a focus on controlling the familywise type I error rate and power. We use the partial borrowing power prior to incorporate the historical survival meta-data into the Bayesian design. We examine various properties of the proposed methodology, and simulation-based computational algorithms are developed to generate predictive data at various interim analyses, sample from the posterior distributions, and compute various quantities such as the power and the type I error in the Bayesian sequential meta-analysis trial design. We apply the proposed methodology to the design of a hypothetical antidiabetic drug development program for evaluating cardiovascular risk. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/U9K6CJ9T/Chen et al. - 2014 - Bayesian sequential meta-analysis design in evalua.pdf;/Users/rritaz/Zotero/storage/VU9R55J2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance,power},
  language = {en},
  number = {9}
}

@article{chen_inference_2016,
  title = {Inference for Correlated Effect Sizes Using Multiple Univariate Meta-Analyses},
  author = {Chen, Yong and Cai, Yi and Hong, Chuan and Jackson, Dan},
  year = {2016},
  volume = {35},
  pages = {1405--1422},
  issn = {1097-0258},
  doi = {10.1002/sim.6789},
  abstract = {Multivariate meta-analysis, which involves jointly analyzing multiple and correlated outcomes from separate studies, has received a great deal of attention. One reason to prefer the multivariate approach is its ability to account for the dependence between multiple estimates from the same study. However, nearly all the existing methods for analyzing multivariate meta-analytic data require the knowledge of the within-study correlations, which are usually unavailable in practice. We propose a simple non-iterative method that can be used for the analysis of multivariate meta-analysis datasets, that has no convergence problems, and does not require the use of within-study correlations. Our approach uses standard univariate methods for the marginal effects but also provides valid joint inference for multiple parameters. The proposed method can directly handle missing outcomes under missing completely at random assumption. Simulation studies show that the proposed method provides unbiased estimates, well-estimated standard errors, and confidence intervals with good coverage probability. Furthermore, the proposed method is found to maintain high relative efficiency compared with conventional multivariate meta-analyses where the within-study correlations are known. We illustrate the proposed method through two real meta-analyses where functions of the estimated effects are of interest. \textcopyright{} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/74P8YKBC/Chen et al. - 2016 - Inference for correlated effect sizes using multip.pdf;/Users/rritaz/Zotero/storage/247RY9TG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,correlated effects,missing data},
  language = {en},
  number = {9}
}

@article{chen_meta-analysis_2012,
  title = {Meta-Analysis Methods and Models with Applications in Evaluation of Cholesterol-Lowering Drugs},
  author = {Chen, Ming-Hui and Ibrahim, Joseph G. and Shah, Arvind K. and Lin, Jianxin and Yao, Hui},
  year = {2012},
  volume = {31},
  pages = {3597--3616},
  issn = {1097-0258},
  doi = {10.1002/sim.5462},
  abstract = {In this paper, we propose a class of multivariate random effects models allowing for the inclusion of study-level covariates to carry out meta-analyses. As existing algorithms for computing maximum likelihood estimates often converge poorly or may not converge at all when the random effects are multi-dimensional, we develop an efficient expectation\textendash maximization algorithm for fitting multi-dimensional random effects regression models. In addition, we also develop a new methodology for carrying out variable selection with study-level covariates. We examine the performance of the proposed methodology via a simulation study. We apply the proposed methodology to analyze metadata from 26 studies involving statins as a monotherapy and in combination with ezetimibe. In particular, we compare the low-density lipoprotein cholesterol-lowering efficacy of monotherapy and combination therapy on two patient populations (na\"ive and non-na\"ive patients to statin monotherapy at baseline), controlling for aggregate covariates. The proposed methodology is quite general and can be applied in any meta-analysis setting for a wide range of scientific applications and therefore offers new analytic methods of clinical importance. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CSSMEQCI/Chen et al. - 2012 - Meta-analysis methods and models with applications.pdf;/Users/rritaz/Zotero/storage/GV59B5X9/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),multivariate,random-effects},
  language = {en},
  number = {28}
}

@article{chen_meta-analysis_2016,
  title = {Meta-Analysis of Studies with Bivariate Binary Outcomes: A Marginal Beta-Binomial Model Approach},
  shorttitle = {Meta-Analysis of Studies with Bivariate Binary Outcomes},
  author = {Chen, Yong and Hong, Chuan and Ning, Yang and Su, Xiao},
  year = {2016},
  volume = {35},
  pages = {21--40},
  issn = {1097-0258},
  doi = {10.1002/sim.6620},
  abstract = {When conducting a meta-analysis of studies with bivariate binary outcomes, challenges arise when the within-study correlation and between-study heterogeneity should be taken into account. In this paper, we propose a marginal beta-binomial model for the meta-analysis of studies with binary outcomes. This model is based on the composite likelihood approach and has several attractive features compared with the existing models such as bivariate generalized linear mixed model (Chu and Cole, 2006) and Sarmanov beta-binomial model (Chen et al., 2012). The advantages of the proposed marginal model include modeling the probabilities in the original scale, not requiring any transformation of probabilities or any link function, having closed-form expression of likelihood function, and no constraints on the correlation parameter. More importantly, because the marginal beta-binomial model is only based on the marginal distributions, it does not suffer from potential misspecification of the joint distribution of bivariate study-specific probabilities. Such misspecification is difficult to detect and can lead to biased inference using currents methods. We compare the performance of the marginal beta-binomial model with the bivariate generalized linear mixed model and the Sarmanov beta-binomial model by simulation studies. Interestingly, the results show that the marginal beta-binomial model performs better than the Sarmanov beta-binomial model, whether or not the true model is Sarmanov beta-binomial, and the marginal beta-binomial model is more robust than the bivariate generalized linear mixed model under model misspecifications. Two meta-analyses of diagnostic accuracy studies and a meta-analysis of case\textendash control studies are conducted for illustration. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EZTS3BCP/Chen et al. - 2016 - Meta-analysis of studies with bivariate binary out.pdf;/Users/rritaz/Zotero/storage/ME4QEX8F/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,correlated effects,discrete effect sizes,multivariate},
  language = {en},
  number = {1}
}

@article{chen_simple_2017,
  title = {A Simple and Robust Method for Multivariate Meta-Analysis of Diagnostic Test Accuracy},
  author = {Chen, Yong and Liu, Yulun and Chu, Haitao and Ting Lee, Mei-Ling and Schmid, Christopher H.},
  year = {2017},
  volume = {36},
  pages = {105--121},
  issn = {1097-0258},
  doi = {10.1002/sim.7093},
  abstract = {Meta-analysis of diagnostic test accuracy often involves mixture of case\textendash control and cohort studies. The existing bivariate random-effects models, which jointly model bivariate accuracy indices (e.g., sensitivity and specificity), do not differentiate cohort studies from case\textendash control studies and thus do not utilize the prevalence information contained in the cohort studies. The recently proposed trivariate generalized linear mixed-effects models are only applicable to cohort studies, and more importantly, they assume a common correlation structure across studies and trivariate normality on disease prevalence, test sensitivity, and specificity after transformation by some pre-specified link functions. In practice, very few studies provide justifications of these assumptions, and sometimes these assumptions are violated. In this paper, we evaluate the performance of the commonly used random-effects model under violations of these assumptions and propose a simple and robust method to fully utilize the information contained in case\textendash control and cohort studies. The proposed method avoids making the aforementioned assumptions and can provide valid joint inferences for any functions of overall summary measures of diagnostic accuracy. Through simulation studies, we find that the proposed method is more robust to model misspecifications than the existing methods. We apply the proposed method to a meta-analysis of diagnostic test accuracy for the detection of recurrent ovarian carcinoma. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VKUG5A74/Chen et al. - 2017 - A simple and robust method for multivariate meta-a.pdf;/Users/rritaz/Zotero/storage/WWELEKZT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,correlated effects,multivariate,random-effects},
  language = {en},
  number = {1}
}

@article{cheung_meta-analytic_2005,
  title = {Meta-Analytic Structural Equation Modeling: A Two-Stage Approach.},
  shorttitle = {Meta-Analytic Structural Equation Modeling},
  author = {Cheung, Mike W.-L. and Chan, Wai Yi Kelly},
  year = {2005},
  volume = {10},
  pages = {40--64},
  doi = {10.1037/1082-989X.10.1.40},
  abstract = {To synthesize studies that use structural equation modeling (SEM), researchers usually use Pearson correlations (univariate r), Fisher z scores (univariate z), or generalized least squares (GLS) to combine the correlation matrices. The pooled correlation matrix is then analyzed by the use of SEM. Questionable inferences may occur for these ad hoc procedures. A 2-stage structural equation modeling (TSSEM) method is proposed to incorporate meta-analytic techniques and SEM into a unified framework. Simulation results reveal that the univariate-r, univariate-z, and TSSEM methods perform well in testing the homogeneity of correlation matrices and estimating the pooled correlation matrix. When fitting SEM, only TSSEM works well. The GLS method performed poorly in small to medium samples.},
  file = {/Users/rritaz/Zotero/storage/MD6RBEUF/Cheung and Chan - 2005 - Meta-analytic structural equation modeling a two-.pdf},
  journal = {Psychological methods},
  keywords = {correlated effects,modeling effect size variation (covariates),random-effects},
  number = {1}
}

@article{cheung_model_2008,
  title = {A Model for Integrating Fixed-, Random-, and Mixed-Effects Meta-Analyses into Structural Equation Modeling.},
  author = {Cheung, Mike W.-L.},
  year = {2008},
  volume = {13},
  pages = {182--202},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0013163},
  file = {/Users/rritaz/Zotero/storage/N3A2PMS4/Cheung - 2008 - A model for integrating fixed-, random-, and mixed.pdf},
  journal = {Psychological Methods},
  keywords = {GLM MA models,missing data,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {3}
}

@article{cheung_modeling_2014,
  title = {Modeling Dependent Effect Sizes with Three-Level Meta-Analyses: A Structural Equation Modeling Approach.},
  shorttitle = {Modeling Dependent Effect Sizes with Three-Level Meta-Analyses},
  author = {Cheung, Mike W.-L.},
  year = {2014},
  volume = {19},
  pages = {211--229},
  doi = {10.1037/a0032968},
  abstract = {Meta-analysis is an indispensable tool used to synthesize research findings in the social, educational, medical, management, and behavioral sciences. Most meta-analytic models assume independence among effect sizes. However, effect sizes can be dependent for various reasons. For example, studies might report multiple effect sizes on the same construct, and effect sizes reported by participants from the same cultural group are likely to be more similar than those reported by other cultural groups. This article reviews the problems and common methods to handle dependent effect sizes. The objective of this article is to demonstrate how 3-level meta-analyses can be used to model dependent effect sizes. The advantages of the structural equation modeling approach over the multilevel approach with regard to conducting a 3-level meta-analysis are discussed. This article also seeks to extend the key concepts of Q statistics, I2, and R2 from 2-level meta-analyses to 3-level meta-analyses. The proposed procedures are implemented using the open source metaSEM package for the R statistical environment. Two real data sets are used to illustrate these procedures. New research directions related to 3-level meta-analyses are discussed.},
  file = {/Users/rritaz/Zotero/storage/75H6GNCC/Cheung - 2014 - Modeling dependent effect sizes with three-level m.pdf},
  journal = {Psychological methods},
  keywords = {correlated effects,GLM MA models,modeling effect size variation (covariates),random-effects},
  number = {2}
}

@article{chinn_simple_2000,
  title = {A Simple Method for Converting an Odds Ratio to Effect Size for Use in Meta-Analysis},
  author = {Chinn, Susan},
  year = {2000},
  volume = {19},
  pages = {3127--3131},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20001130)19:22<3127::AID-SIM784>3.0.CO;2-M},
  abstract = {A systematic review may encompass both odds ratios and mean differences in continuous outcomes. A separate meta-analysis of each type of outcome results in loss of information and may be misleading. It is shown that a ln(odds ratio) can be converted to effect size by dividing by 1.81. The validity of effect size, the estimate of interest divided by the residual standard deviation, depends on comparable variation across studies. If researchers routinely report residual standard deviation, any subsequent review can combine both odds ratios and effect sizes in a single meta-analysis when this is justified. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5AD4SV2N/Chinn - 2000 - A simple method for converting an odds ratio to ef.pdf;/Users/rritaz/Zotero/storage/ASIIGLNU/1097-0258(20001130)19223127AID-SIM7843.0.html},
  journal = {Statistics in Medicine},
  keywords = {effect size estimation (series)},
  language = {en},
  number = {22}
}

@article{chootrakool_meta-analysis_2011,
  title = {Meta-Analysis and Sensitivity Analysis for Multi-Arm Trials with Selection Bias},
  author = {Chootrakool, Hathaikan and Shi, Jian Qing and Yue, Rongxian},
  year = {2011},
  volume = {30},
  pages = {1183--1198},
  issn = {1097-0258},
  doi = {10.1002/sim.4143},
  abstract = {Multi-arm trials meta-analysis is a methodology used in combining evidence based on a synthesis of different types of comparisons from all possible similar studies and to draw inferences about the effectiveness of multiple compared-treatments. Studies with statistically significant results are potentially more likely to be submitted and selected than studies with non-significant results; this leads to false-positive results. In meta-analysis, combining only the identified selected studies uncritically may lead to an incorrect, usually over-optimistic conclusion. This problem is known asbiselection bias. In this paper, we first define a random-effect meta-analysis model for multi-arm trials by allowing for heterogeneity among studies. This general model is based on a normal approximation for empirical log-odds ratio. We then address the problem of publication bias by using a sensitivity analysis and by defining a selection model to the available data of a meta-analysis. This method allows for different amounts of selection bias and helps to investigate how sensitive the main interest parameter is when compared with the estimates of the standard model. Throughout the paper, we use binary data from Antiplatelet therapy in maintaining vascular patency of patients to illustrate the methods. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZKU38YYY/Chootrakool et al. - 2011 - Meta-analysis and sensitivity analysis for multi-a.pdf;/Users/rritaz/Zotero/storage/RVM7IB2M/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias,random-effects},
  language = {en},
  number = {11}
}

@article{chowdhry_meta-analysis_2016,
  title = {Meta-Analysis with Missing Study-Level Sample Variance Data},
  author = {Chowdhry, Amit K. and Dworkin, Robert H. and McDermott, Michael P.},
  year = {2016},
  volume = {35},
  pages = {3021--3032},
  issn = {1097-0258},
  doi = {10.1002/sim.6908},
  abstract = {We consider a study-level meta-analysis with a normally distributed outcome variable and possibly unequal study-level variances, where the object of inference is the difference in means between a treatment and control group. A common complication in such an analysis is missing sample variances for some studies. A frequently used approach is to impute the weighted (by sample size) mean of the observed variances (mean imputation). Another approach is to include only those studies with variances reported (complete case analysis). Both mean imputation and complete case analysis are only valid under the missing-completely-at-random assumption, and even then the inverse variance weights produced are not necessarily optimal. We propose a multiple imputation method employing gamma meta-regression to impute the missing sample variances. Our method takes advantage of study-level covariates that may be used to provide information about the missing data. Through simulation studies, we show that multiple imputation, when the imputation model is correctly specified, is superior to competing methods in terms of confidence interval coverage probability and type I error probability when testing a specified group difference. Finally, we describe a similar approach to handling missing variances in cross-over studies. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SF3ETDSY/Chowdhry et al. - 2016 - Meta-analysis with missing study-level sample vari.pdf;/Users/rritaz/Zotero/storage/EFH9QG9Z/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,random-effects},
  language = {en},
  number = {17}
}

@article{chu_meta-analysis_2009,
  title = {Meta-Analysis of Diagnostic Accuracy Studies Accounting for Disease Prevalence: {{Alternative}} Parameterizations and Model Selection},
  shorttitle = {Meta-Analysis of Diagnostic Accuracy Studies Accounting for Disease Prevalence},
  author = {Chu, Haitao and Nie, Lei and Cole, Stephen R. and Poole, Charles},
  year = {2009},
  volume = {28},
  pages = {2384--2399},
  issn = {1097-0258},
  doi = {10.1002/sim.3627},
  abstract = {In a meta-analysis of diagnostic accuracy studies, the sensitivities and specificities of a diagnostic test may depend on the disease prevalence since the severity and definition of disease may differ from study to study due to the design and the population considered. In this paper, we extend the bivariate nonlinear random effects model on sensitivities and specificities to jointly model the disease prevalence, sensitivities and specificities using trivariate nonlinear random-effects models. Furthermore, as an alternative parameterization, we also propose jointly modeling the test prevalence and the predictive values, which reflect the clinical utility of a diagnostic test. These models allow investigators to study the complex relationship among the disease prevalence, sensitivities and specificities; or among test prevalence and the predictive values, which can reveal hidden information about test performance. We illustrate the proposed two approaches by reanalyzing the data from a meta-analysis of radiological evaluation of lymph node metastases in patients with cervical cancer and a simulation study. The latter illustrates the importance of carefully choosing an appropriate normality assumption for the disease prevalence, sensitivities and specificities, or the test prevalence and the predictive values. In practice, it is recommended to use model selection techniques to identify a best-fitting model for making statistical inference. In summary, the proposed trivariate random effects models are novel and can be very useful in practice for meta-analysis of diagnostic accuracy studies. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VW2EY8TJ/Chu et al. - 2009 - Meta-analysis of diagnostic accuracy studies accou.pdf;/Users/rritaz/Zotero/storage/HN866823/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {18}
}

@article{chung_avoiding_2013,
  title = {Avoiding Zero Between-Study Variance Estimates in Random-Effects Meta-Analysis},
  author = {Chung, Yeojin and Rabe-Hesketh, Sophia and Choi, In-Hee},
  year = {2013},
  volume = {32},
  pages = {4071--4089},
  issn = {1097-0258},
  doi = {10.1002/sim.5821},
  abstract = {Fixed-effects meta-analysis has been criticized because the assumption of homogeneity is often unrealistic and can result in underestimation of parameter uncertainty. Random-effects meta-analysis and meta-regression are therefore typically used to accommodate explained and unexplained between-study variability. However, it is not unusual to obtain a boundary estimate of zero for the (residual) between-study standard deviation, resulting in fixed-effects estimates of the other parameters and their standard errors. To avoid such boundary estimates, we suggest using Bayes modal (BM) estimation with a gamma prior on the between-study standard deviation. When no prior information is available regarding the magnitude of the between-study standard deviation, a weakly informative default prior can be used (with shape parameter 2 and rate parameter close to 0) that produces positive estimates but does not overrule the data, leading to only a small decrease in the log likelihood from its maximum. We review the most commonly used estimation methods for meta-analysis and meta-regression including classical and Bayesian methods and apply these methods, as well as our BM estimator, to real datasets. We then perform simulations to compare BM estimation with the other methods and find that BM estimation performs well by (i) avoiding boundary estimates; (ii) having smaller root mean squared error for the between-study standard deviation; and (iii) better coverage for the overall effects than the other methods when the true model has at least a small or moderate amount of unexplained heterogeneity. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GNLB9ESZ/Chung et al. - 2013 - Avoiding zero between-study variance estimates in .pdf;/Users/rritaz/Zotero/storage/SENLP6QG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {23}
}

@article{citkowicz_parsimonious_2017,
  title = {A Parsimonious Weight Function for Modeling Publication Bias},
  author = {Citkowicz, Martyna and Vevea, Jack L.},
  year = {2017},
  month = mar,
  volume = {22},
  pages = {28--41},
  issn = {1082-989X},
  doi = {10.1037/met0000119},
  abstract = {Quantitative research literature is often biased because studies that fail to find a significant effect (or that demonstrate effects in an undesired or unexpected direction) are less likely to be published. This phenomenon, termed publication bias, can cause problems when researchers attempt to synthesize results using meta-analytic methods. Various techniques exist that attempt to estimate and correct meta-analyses for publication bias. However, there is no single method that can (a) account for continuous moderators by including them within the model, (b) allow for substantial data heterogeneity, (c) produce an adjusted mean effect size, (d) include a formal test for publication bias, and (e) allow for correction when only a small number of effects is included in the analysis. This article describes a method that we believe helps fill that gap. The model uses the beta density as a weight function that represents the selection process and provides adjusted parameter estimates that account for publication bias. Use of the beta density allows us to represent selection using fewer parameters than similar models so that the proposed model is suitable for meta-analyses that include relatively few studies. We explain the model and its rationale, illustrate its use with a real data set, and describe the results of a simulation study that shows the model's utility. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/AUA374IK/Citkowicz and Vevea - 2017 - A parsimonious weight function for modeling public.pdf},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,GLM MA models,modeling effect size variation (covariates),publication bias,random-effects},
  number = {1}
}

@article{coburn_publication_2015,
  title = {Publication Bias as a Function of Study Characteristics.},
  author = {Coburn, Kathleen M. and Vevea, Jack L.},
  year = {2015},
  volume = {20},
  pages = {310--330},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000046},
  abstract = {Researchers frequently conceptualize publication bias as a bias against publishing nonsignificant results. However, other factors beyond significance levels can contribute to publication bias. Some of these factors include study characteristics, such as the source of funding for the research project, whether the project was single center or multicenter, and prevailing theories at the time of publication. This article examines the relationship between publication bias and 2 study characteristics by breaking down 2 meta-analytic data sets into levels of the relevant study characteristic and assessing publication bias in each level with funnel plots, trim and fill (Duval \& Tweedie, 2000a, 2000b), Egger's linear regression (Egger, Smith, Schneider, \& Minder, 1997), cumulative meta-analysis (Borenstein, Hedges, Higgins, \& Rothstein, 2009), and the Vevea and Hedges (1995) weight-function model. Using the Vevea and Hedges model, we conducted likelihood ratio tests to determine whether information was lost if only 1 pattern of selection was estimated. Results indicate that publication bias can differ over levels of study characteristics, and that developing a model to accommodate this relationship could be advantageous.},
  file = {/Users/rritaz/Zotero/storage/TTZIPB9K/Coburn and Vevea - 2015 - Publication bias as a function of study characteri.pdf},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,publication bias},
  language = {en},
  number = {3}
}

@article{cohn_how_2003,
  title = {How Meta-Analysis Increases Statistical Power.},
  author = {Cohn, Lawrence D. and Becker, Betsy J.},
  year = {2003},
  volume = {8},
  pages = {243--253},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.8.3.243},
  file = {/Users/rritaz/Zotero/storage/A2YMKUNI/Cohn and Becker - 2003 - How meta-analysis increases statistical power..pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlation coefficients,discrete effect sizes,power,random-effects},
  language = {en},
  number = {3}
}

@article{combescure_meta-analysis_2014,
  title = {Meta-Analysis of Single-Arm Survival Studies: A Distribution-Free Approach for Estimating Summary Survival Curves with Random Effects},
  shorttitle = {Meta-Analysis of Single-Arm Survival Studies},
  author = {Combescure, Christophe and Foucher, Yohann and Jackson, Daniel},
  year = {2014},
  volume = {33},
  pages = {2521--2537},
  issn = {1097-0258},
  doi = {10.1002/sim.6111},
  abstract = {In epidemiologic studies and clinical trials with time-dependent outcome (for instance death or disease progression), survival curves are used to describe the risk of the event over time. In meta-analyses of studies reporting a survival curve, the most informative finding is a summary survival curve. In this paper, we propose a method to obtain a distribution-free summary survival curve by expanding the product-limit estimator of survival for aggregated survival data. The extension of DerSimonian and Laird's methodology for multiple outcomes is applied to account for the between-study heterogeneity. Statistics I2 and H2 are used to quantify the impact of the heterogeneity in the published survival curves. A statistical test for between-strata comparison is proposed, with the aim to explore study-level factors potentially associated with survival. The performance of the proposed approach is evaluated in a simulation study. Our approach is also applied to synthesize the survival of untreated patients with hepatocellular carcinoma from aggregate data of 27 studies and synthesize the graft survival of kidney transplant recipients from individual data from six hospitals. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UKERCFI4/Combescure et al. - 2014 - Meta-analysis of single-arm survival studies a di.pdf;/Users/rritaz/Zotero/storage/EF23RFZE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {discrete effect sizes,random-effects},
  language = {en},
  number = {15}
}

@article{cook_note_2004,
  title = {A {{Note}} on {{Testing}} for {{Homogeneity Among Effect Sizes Sharing}} a {{Common Control Group}}},
  author = {Cook, Samantha R.},
  year = {2004},
  month = dec,
  volume = {9},
  pages = {446--452},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.9.4.446},
  abstract = {L. V. Hedges and I. Olkin (1985) presented a statistic to test for homogeneity among correlated effect sizes and L. J. Gleser and I. Olkin (1994) presented a large-sample approximation to the covariance matrix of the correlated effect sizes. This article presents a more exact expression for this covariance matrix, assuming normally distributed data but not large samples, for the situation where effect sizes are correlated because a single control group was compared with more than one treatment group. After the correlation between effect sizes has been estimated, the standard Q statistic for correlated effect sizes can be used to test for homogeneity. This method is illustrated using results from schizophrenia research. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {correlated effects},
  number = {4}
}

@article{cooper_addressing_2009,
  title = {Addressing Between-Study Heterogeneity and Inconsistency in Mixed Treatment Comparisons: {{Application}} to Stroke Prevention Treatments in Individuals with Non-Rheumatic Atrial Fibrillation},
  shorttitle = {Addressing Between-Study Heterogeneity and Inconsistency in Mixed Treatment Comparisons},
  author = {Cooper, Nicola J. and Sutton, Alex J. and Morris, Danielle and Ades, A. E. and Welton, Nicky J.},
  year = {2009},
  volume = {28},
  pages = {1861--1881},
  issn = {1097-0258},
  doi = {10.1002/sim.3594},
  abstract = {Mixed treatment comparison models extend meta-analysis methods to enable comparisons to be made between all relevant comparators in the clinical area of interest. In such modelling it is imperative that potential sources of variability are explored to explain both heterogeneity (variation in treatment effects between trials within pairwise contrasts) and inconsistency (variation in treatment effects between pairwise contrasts) to ensure the validity of the analysis. The objective of this paper is to extend the mixed treatment comparison framework to allow for the incorporation of study-level covariates in an attempt to explain between-study heterogeneity and reduce inconsistency. Three possible model specifications assuming different assumptions are described and applied to a 17-treatment network for stroke prevention treatments in individuals with non-rheumatic atrial fibrillation. The paper demonstrates the feasibility of incorporating covariates within a mixed treatment comparison framework and using model fit statistics to choose between alternative model specifications. Although such an approach may adjust for inconsistencies in networks, as for standard meta-regression, the analysis will suffer from low power if the number of trials is small compared with the number of treatment comparators. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BSILPMZ5/Cooper et al. - 2009 - Addressing between-study heterogeneity and inconsi.pdf;/Users/rritaz/Zotero/storage/QNWQPJKZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),power,random-effects},
  language = {en},
  number = {14}
}

@book{cooper_handbook_2009,
  title = {The {{Handbook}} of {{Research Synthesis}} and {{Meta}}-{{Analysis}}},
  author = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey C.},
  year = {2009},
  month = feb,
  publisher = {{Russell Sage Foundation}},
  abstract = {Praise for the first edition: "The Handbook is a comprehensive treatment of literature synthesis and provides practical advice for anyone deep in the throes of, just teetering on the brink of, or attempting to decipher a meta-analysis. Given the expanding application and importance of literature synthesis, understanding both its strengths and weaknesses is essential for its practitioners and consumers. This volume is a good beginning for those who wish to gain that understanding." \textemdash Chance "Meta-analysis, as the statistical analysis of a large collection of results from individual studies is called, has now achieved a status of respectability in medicine. This respectability, when combined with the slight hint of mystique that sometimes surrounds meta-analysis, ensures that results of studies that use it are treated with the respect they deserve....The Handbook of Research Synthesis is one of the most important publications in this subject both as a definitive reference book and a practical manual."\textemdash British Medical Journal When the first edition of The Handbook of Research Synthesis was published in 1994, it quickly became the definitive reference for researchers conducting meta-analyses of existing research in both the social and biological sciences. In this fully revised second edition, editors Harris Cooper, Larry Hedges, and Jeff Valentine present updated versions of the Handbook's classic chapters, as well as entirely new sections reporting on the most recent, cutting-edge developments in the field. Research synthesis is the practice of systematically distilling and integrating data from a variety of sources in order to draw more reliable conclusions about a given question or topic. The Handbook of Research Synthesis and Meta-Analysis draws upon years of groundbreaking advances that have transformed research synthesis from a narrative craft into an important scientific process in its own right. Cooper, Hedges, and Valentine have assembled leading authorities in the field to guide the reader through every stage of the research synthesis process\textemdash problem formulation, literature search and evaluation, statistical integration, and report preparation. The Handbook of Research Synthesis and Meta-Analysis incorporates state-of-the-art techniques from all quantitative synthesis traditions. Distilling a vast technical literature and many informal sources, the Handbook provides a portfolio of the most effective solutions to the problems of quantitative data integration. Among the statistical issues addressed by the authors are the synthesis of non-independent data sets, fixed and random effects methods, the performance of sensitivity analyses and model assessments, and the problem of missing data. The Handbook of Research Synthesis and Meta-Analysis also provides a rich treatment of the non-statistical aspects of research synthesis. Topics include searching the literature, and developing schemes for gathering information from study reports. Those engaged in research synthesis will also find useful advice on how tables, graphs, and narration can be used to provide the most meaningful communication of the results of research synthesis. In addition, the editors address the potentials and limitations of research synthesis, and its future directions. The past decade has been a period of enormous growth in the field of research synthesis. The second edition Handbook thoroughly revises original chapters to assure that the volume remains the most authoritative source of information for researchers undertaking meta-analysis today. In response to the increasing use of research synthesis in the formation of public policy, the second edition includes a new chapter on both the strengths and limitations of research synthesis in policy debates},
  googlebooks = {LUGd6B9eyc4C},
  isbn = {978-1-61044-138-4},
  keywords = {Psychology / Experimental Psychology,Social Science / General,Social Science / Methodology,Social Science / Research},
  language = {en}
}

@book{cooper_handbook_2009-1,
  title = {The {{Handbook}} of {{Research Synthesis}} and {{Meta}}-{{Analysis}}},
  author = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey C.},
  year = {2009},
  month = feb,
  publisher = {{Russell Sage Foundation}},
  abstract = {Praise for the first edition: "The Handbook is a comprehensive treatment of literature synthesis and provides practical advice for anyone deep in the throes of, just teetering on the brink of, or attempting to decipher a meta-analysis. Given the expanding application and importance of literature synthesis, understanding both its strengths and weaknesses is essential for its practitioners and consumers. This volume is a good beginning for those who wish to gain that understanding." \textemdash Chance "Meta-analysis, as the statistical analysis of a large collection of results from individual studies is called, has now achieved a status of respectability in medicine. This respectability, when combined with the slight hint of mystique that sometimes surrounds meta-analysis, ensures that results of studies that use it are treated with the respect they deserve....The Handbook of Research Synthesis is one of the most important publications in this subject both as a definitive reference book and a practical manual."\textemdash British Medical Journal When the first edition of The Handbook of Research Synthesis was published in 1994, it quickly became the definitive reference for researchers conducting meta-analyses of existing research in both the social and biological sciences. In this fully revised second edition, editors Harris Cooper, Larry Hedges, and Jeff Valentine present updated versions of the Handbook's classic chapters, as well as entirely new sections reporting on the most recent, cutting-edge developments in the field. Research synthesis is the practice of systematically distilling and integrating data from a variety of sources in order to draw more reliable conclusions about a given question or topic. The Handbook of Research Synthesis and Meta-Analysis draws upon years of groundbreaking advances that have transformed research synthesis from a narrative craft into an important scientific process in its own right. Cooper, Hedges, and Valentine have assembled leading authorities in the field to guide the reader through every stage of the research synthesis process\textemdash problem formulation, literature search and evaluation, statistical integration, and report preparation. The Handbook of Research Synthesis and Meta-Analysis incorporates state-of-the-art techniques from all quantitative synthesis traditions. Distilling a vast technical literature and many informal sources, the Handbook provides a portfolio of the most effective solutions to the problems of quantitative data integration. Among the statistical issues addressed by the authors are the synthesis of non-independent data sets, fixed and random effects methods, the performance of sensitivity analyses and model assessments, and the problem of missing data. The Handbook of Research Synthesis and Meta-Analysis also provides a rich treatment of the non-statistical aspects of research synthesis. Topics include searching the literature, and developing schemes for gathering information from study reports. Those engaged in research synthesis will also find useful advice on how tables, graphs, and narration can be used to provide the most meaningful communication of the results of research synthesis. In addition, the editors address the potentials and limitations of research synthesis, and its future directions. The past decade has been a period of enormous growth in the field of research synthesis. The second edition Handbook thoroughly revises original chapters to assure that the volume remains the most authoritative source of information for researchers undertaking meta-analysis today. In response to the increasing use of research synthesis in the formation of public policy, the second edition includes a new chapter on both the strengths and limitations of research synthesis in policy debates},
  googlebooks = {LUGd6B9eyc4C},
  isbn = {978-1-61044-138-4},
  keywords = {Psychology / Experimental Psychology,Social Science / General,Social Science / Methodology,Social Science / Research},
  language = {en}
}

@article{copas_robust_2008,
  title = {A Robust {{P}}-Value for Treatment Effect in Meta-Analysis with Publication Bias},
  author = {Copas, John B. and Malley, Paul F.},
  year = {2008},
  volume = {27},
  pages = {4267--4278},
  issn = {1097-0258},
  doi = {10.1002/sim.3284},
  abstract = {Publication bias is a major and intractable problem in meta-analysis. There have been several attempts in the literature to adapt methods to allow for such bias, but these are only possible if we are prepared to make strong assumptions about the underlying selection mechanism. We discuss the assumption that the probability that a paper is published may depend in some unspecified way on the P-value being claimed by that study. We suggest a new robust P-value for the overall treatment effect, which turns out to be closely related to the correlation of the associated radial plot. Properties of the method are discussed and illustrated in two examples. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/S2THR93C/Copas and Malley - 2008 - A robust P-value for treatment effect in meta-anal.pdf;/Users/rritaz/Zotero/storage/93A4HT4C/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {21}
}

@article{cope_quantitative_2013,
  title = {Quantitative Summaries of Treatment Effect Estimates Obtained with Network Meta-Analysis of Survival Curves to Inform Decision-Making},
  author = {Cope, Shannon and Jansen, Jeroen P},
  year = {2013},
  month = dec,
  volume = {13},
  pages = {147},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-147},
  abstract = {Background: Increasingly, network meta-analysis (NMA) of published survival data are based on parametric survival curves as opposed to reported hazard ratios to avoid relying on the proportional hazards assumption. If a Bayesian framework is used for the NMA, rank probabilities associated with the alternative treatments can be obtained, which directly support decision-making. In the context of survival analysis multiple treatment effect measures are available to inform the rank probabilities. Methods: A fractional polynomial NMA of overall survival in advanced melanoma was performed as an illustrative example. Rank probabilities were calculated and presented for the following effect measures: 1) median survival; 2) expected survival; 3) mean survival at the follow-up time point of the trial with the shortest follow-up; 4) hazard or hazard ratio over time; 5) cumulative hazard or survival proportions over time; and 6) mean survival at subsequent time points. The advantages and disadvantages of the alternative measures were discussed. Results: Since hazard and survival estimates may vary over time for the compared interventions, calculations of rank probabilities for an NMA of survival curves may depend on the effect measure. With methods 1\textendash 3 rank probabilities do not vary over time, which are easier to understand and communicate than rank probabilities that vary over time as obtained with methods 4\textendash 6. However, rank probabilities based on methods 4\textendash 6 provide useful information regarding the relative treatment effects over time. Conclusions: Different approaches to summarize results of a NMA of survival curves with rank probabilities have pros and cons. Rank probabilities of treatment effects over time provide a more transparent and informative approach to help guide decision-making than single rank probabilities based on collapsed measures, such as median survival or expected survival. Rank probabilities based on survival proportions are the most intuitive and straightforward to communicate, but alternatives based on the hazard function or mean survival over time may also be useful.},
  file = {/Users/rritaz/Zotero/storage/45C8WJMZ/Cope and Jansen - 2013 - Quantitative summaries of treatment effect estimat.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {network meta-analysis},
  language = {en},
  number = {1}
}

@article{crespo_comparative_2014,
  title = {Comparative Efficiency Research ({{COMER}}): Meta-Analysis of Cost-Effectiveness Studies},
  shorttitle = {Comparative Efficiency Research ({{COMER}})},
  author = {Crespo, Carlos and Monleon, Antonio and D{\'i}az, Walter and R{\'i}os, Mart{\'i}n},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {139},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-139},
  abstract = {Background: The aim of this study was to create a new meta-analysis method for cost-effectiveness studies using comparative efficiency research (COMER). Methods: We built a new score named total incremental net benefit (TINB), with inverse variance weighting of incremental net benefits (INB). This permits determination of whether an alternative is cost-effective, given a specific threshold (TINB {$>$} 0 test). Before validation of the model, the structure of dependence between costs and quality-adjusted life years (QoL) was analysed using copula distributions. The goodness-of-fit of a Spanish prospective observational study (n = 498) was analysed using the Independent, Gaussian, T, Gumbel, Clayton, Frank and Placket copulas. Validation was carried out by simulating a copula distribution with log-normal distribution for costs and gamma distribution for disutilities. Hypothetical cohorts were created by varying the sample size (n: 15\textendash 500) and assuming three scenarios (1-cost-effective; 2-non-cost-effective; 3-dominant). The COMER result was compared to the theoretical result according to the incremental cost-effectiveness ratio (ICER) and the INB, assuming a margin of error of 2,000 and 500 monetary units, respectively. Results: The Frank copula with positive dependence (-0.4279) showed a goodness-of-fit sufficient to represent costs and QoL (p-values 0.524 and 0.808). The theoretical INB was within the 95\% confidence interval of the TINB, based on 15 individuals with a probability {$>$} 80\% for scenarios 1 and 2, and {$>$} 90\% for scenario 3. The TINB {$>$} 0 test with 15 individuals showed p-values of 0.0105 (SD: 0.0411) for scenario 1, 0.613 (SD: 0.265) for scenario 2 and {$<$} 0.0001 for scenario 3. Conclusions: COMER is a valid tool for combining cost-effectiveness studies and may be of use to health decision makers.},
  file = {/Users/rritaz/Zotero/storage/8DPLT3PX/Crespo et al. - 2014 - Comparative efficiency research (COMER) meta-anal.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {physical/biological fields},
  language = {en},
  number = {1}
}

@article{crippa_dose-response_2016,
  title = {Dose-Response Meta-Analysis of Differences in Means},
  author = {Crippa, Alessio and Orsini, Nicola},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {91},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0189-0},
  abstract = {Background: Meta-analytical methods are frequently used to combine dose-response findings expressed in terms of relative risks. However, no methodology has been established when results are summarized in terms of differences in means of quantitative outcomes. Methods: We proposed a two-stage approach. A flexible dose-response model is estimated within each study (first stage) taking into account the covariance of the data points (mean differences, standardized mean differences). Parameters describing the study-specific curves are then combined using a multivariate random-effects model (second stage) to address heterogeneity across studies. Results: The method is fairly general and can accommodate a variety of parametric functions. Compared to traditional non-linear models (e.g. Emax, logistic), spline models do not assume any pre-specified dose-response curve. Spline models allow inclusion of studies with a small number of dose levels, and almost any shape, even non monotonic ones, can be estimated using only two parameters. We illustrated the method using dose-response data arising from five clinical trials on an antipsychotic drug, aripiprazole, and improvement in symptoms in shizoaffective patients. Using the Positive and Negative Syndrome Scale (PANSS), pooled results indicated a non-linear association with the maximum change in mean PANSS score equal to 10.40 (95 \% confidence interval 7.48, 13.30) observed for 19.32 mg/day of aripiprazole. No substantial change in PANSS score was observed above this value. An estimated dose of 10.43 mg/day was found to produce 80 \% of the maximum predicted response. Conclusion: The described approach should be adopted to combine correlated differences in means of quantitative outcomes arising from multiple studies. Sensitivity analysis can be a useful tool to assess the robustness of the overall dose-response curve to different modelling strategies. A user-friendly R package has been developed to facilitate applications by practitioners.},
  file = {/Users/rritaz/Zotero/storage/443P97L5/Crippa and Orsini - 2016 - Dose-response meta-analysis of differences in mean.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {physical/biological fields,random-effects},
  language = {en},
  number = {1}
}

@article{crippa_new_2016,
  title = {A New Measure of Between-Studies Heterogeneity in Meta-Analysis},
  author = {Crippa, Alessio and Khudyakov, Polyna and Wang, Molin and Orsini, Nicola and Spiegelman, Donna},
  year = {2016},
  volume = {35},
  pages = {3661--3675},
  issn = {1097-0258},
  doi = {10.1002/sim.6980},
  abstract = {Assessing the magnitude of heterogeneity in a meta-analysis is important for determining the appropriateness of combining results. The most popular measure of heterogeneity, I2, was derived under an assumption of homogeneity of the within-study variances, which is almost never true, and the alternative estimator, , uses the harmonic mean to estimate the average of the within-study variances, which may also lead to bias. This paper thus presents a new measure for quantifying the extent to which the variance of the pooled random-effects estimator is due to between-studies variation, , that overcomes the limitations of the previous approach. We show that this measure estimates the expected value of the proportion of total variance due to between-studies variation and we present its point and interval estimators. The performance of all three heterogeneity measures is evaluated in an extensive simulation study. A negative bias for was observed when the number of studies was very small and became negligible as the number of studies increased, while and I2 showed a tendency to overestimate the impact of heterogeneity. The coverage of confidence intervals based upon was good across different simulation scenarios but was substantially lower for and I2, especially for high values of heterogeneity and when a large number of studies were included in the meta-analysis. The proposed measure is implemented in a user-friendly function available for routine use in r and sas. will be useful in quantifying the magnitude of heterogeneity in meta-analysis and should supplement the p-value for the test of heterogeneity obtained from the Q test. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DH2QBJYW/Crippa et al. - 2016 - A new measure of between-studies heterogeneity in .pdf;/Users/rritaz/Zotero/storage/B6KEQN22/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random-effects},
  language = {en},
  number = {21}
}

@article{crowther_further_2011,
  title = {A Further Use for the {{Harvest}} Plot: A Novel Method for the Presentation of Data Synthesis},
  shorttitle = {A Further Use for the {{Harvest}} Plot},
  author = {Crowther, Mark and Avenell, Alison and MacLennan, Graeme and Mowatt, Graham},
  year = {2011},
  volume = {2},
  pages = {79--83},
  issn = {1759-2887},
  doi = {10.1002/jrsm.37},
  abstract = {When performing a systematic review, whether or not a meta-analysis is performed, graphical displays can be useful. Data do still need to be described, ideally in graphical form. The Harvest plot has been developed to display combined data from several studies that allows demonstration of not only effect but also study quality. We describe a modification to the Harvest plot that allows the presentation of data that normally could not be included in a forest plot meta-analysis and allows extra information to be displayed. Using specific examples, we describe how the arrangement of studies, height of the bars and additional information can be used to enhance the plot. This is an important development, which by fulfilling Tufte's nine requirements for graphical presentation, allows researchers to display evidence in a flexible way. This means readers can follow an argument in a clear and efficient manner without the need for large volumes of descriptive text. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/4B2736BS/Crowther et al. - 2011 - A further use for the Harvest plot a novel method.pdf;/Users/rritaz/Zotero/storage/2MS2AA62/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {2}
}

@article{crowther_individual_2012,
  title = {Individual Patient Data Meta-Analysis of Survival Data Using {{Poisson}} Regression Models},
  author = {Crowther, Michael J and Riley, Richard D and Staessen, Jan A and Wang, Jiguang and Gueyffier, Francois and Lambert, Paul C},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {34},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-34},
  abstract = {Background: An Individual Patient Data (IPD) meta-analysis is often considered the gold-standard for synthesising survival data from clinical trials. An IPD meta-analysis can be achieved by either a two-stage or a one-stage approach, depending on whether the trials are analysed separately or simultaneously. A range of one-stage hierarchical Cox models have been previously proposed, but these are known to be computationally intensive and are not currently available in all standard statistical software. We describe an alternative approach using Poisson based Generalised Linear Models (GLMs). Methods: We illustrate, through application and simulation, the Poisson approach both classically and in a Bayesian framework, in two-stage and one-stage approaches. We outline the benefits of our one-stage approach through extension to modelling treatment-covariate interactions and non-proportional hazards. Ten trials of hypertension treatment, with all-cause death the outcome of interest, are used to apply and assess the approach. Results: We show that the Poisson approach obtains almost identical estimates to the Cox model, is additionally computationally efficient and directly estimates the baseline hazard. Some downward bias is observed in classical estimates of the heterogeneity in the treatment effect, with improved performance from the Bayesian approach. Conclusion: Our approach provides a highly flexible and computationally efficient framework, available in all standard statistical software, to the investigation of not only heterogeneity, but the presence of non-proportional hazards and treatment effect modifiers.},
  file = {/Users/rritaz/Zotero/storage/GXVDZNLQ/Crowther et al. - 2012 - Individual patient data meta-analysis of survival .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,GLM MA models,Individual Patient Data IPD},
  language = {en},
  number = {1}
}

@article{crowther_multilevel_2014,
  title = {Multilevel Mixed Effects Parametric Survival Models Using Adaptive {{Gauss}}\textendash{{Hermite}} Quadrature with Application to Recurrent Events and Individual Participant Data Meta-Analysis},
  author = {Crowther, Michael J. and Look, Maxime P. and Riley, Richard D.},
  year = {2014},
  volume = {33},
  pages = {3844--3858},
  issn = {1097-0258},
  doi = {10.1002/sim.6191},
  abstract = {AbstractMultilevel mixed effects survival models are used in the analysis of clustered survival data, such as repeated events, multicenter clinical trials, and individual participant data (IPD) meta-analyses, to investigate heterogeneity in baseline risk and covariate effects. In this paper, we extend parametric frailty models including the exponential, Weibull and Gompertz proportional hazards (PH) models and the log logistic, log normal, and generalized gamma accelerated failure time models to allow any number of normally distributed random effects. Furthermore, we extend the flexible parametric survival model of Royston and Parmar, modeled on the log-cumulative hazard scale using restricted cubic splines, to include random effects while also allowing for non-PH (time-dependent effects). Maximum likelihood is used to estimate the models utilizing adaptive or nonadaptive Gauss\textendash Hermite quadrature. The methods are evaluated through simulation studies representing clinically plausible scenarios of a multicenter trial and IPD meta-analysis, showing good performance of the estimation method. The flexible parametric mixed effects model is illustrated using a dataset of patients with kidney disease and repeated times to infection and an IPD meta-analysis of prognostic factor studies in patients with breast cancer. User-friendly Stata software is provided to implement the methods. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/N7GIEHWX/Crowther et al. - 2014 - Multilevel mixed effects parametric survival model.pdf;/Users/rritaz/Zotero/storage/7Q3ZBCSL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {22}
}

@article{curtin_meta-analysis_2002,
  title = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{I}}: {{Continuous}} Outcomes},
  shorttitle = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{I}}},
  author = {Curtin, Fran{\c c}ois and Altman, Douglas G. and Elbourne, Diana},
  year = {2002},
  volume = {21},
  pages = {2131--2144},
  issn = {1097-0258},
  doi = {10.1002/sim.1205},
  abstract = {Among clinical trials assessing a given treatment, often parallel and cross-over designs are used together. In the first paper of a series of three, we explore two methods to pool continuous outcomes in a meta-analysis combining parallel and cross-over trial designs: the weighted mean difference (WMD) and the standardized weighted mean difference (SWMD). The combined design meta-analytic formulae are based on a weighted average of the two design treatment estimates. A random effects model can be implemented. Both WMD and SWMD can be used, the choice of the method is determined by the type of outcomes obtained in the trials. Compared to the number of included subjects, the relative weight of the cross-over design is large in combined-design meta-analysis. Differences in the weight estimation between WMD and SWMD can also accentuate the relative weight of cross-over trials, which must be considered a case of design-specific bias. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YB5HF65M/Curtin et al. - 2002 - Meta-analysis combining parallel and cross-over cl.pdf;/Users/rritaz/Zotero/storage/EK5DJWFX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,effect size estimation (series)},
  language = {en},
  number = {15}
}

@article{curtin_meta-analysis_2002-1,
  title = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{II}}: {{Binary}} Outcomes},
  shorttitle = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{II}}},
  author = {Curtin, Fran{\c c}ois and Elbourne, Diana and Altman, Douglas G.},
  year = {2002},
  volume = {21},
  pages = {2145--2159},
  issn = {1097-0258},
  doi = {10.1002/sim.1206},
  abstract = {We examine different methods to pool binary outcomes used both in parallel and cross-over trials. Odds ratio (OR) estimators obtained from joint conditional probabilities in cross-over trials, such as the Mantel\textendash Haenszel and Peto methods, are compared to an OR estimator using marginal results of cross-over trials. When there is correlation between the outcomes in the two cross-over periods, joint conditional ORs differ from marginal ORs and cannot be combined with OR estimates from parallel trials. The marginal OR estimate is independent of the between-period correlation and it includes a correction for cross-over correlation in the variance estimate. As its computation is similar in cross-over and parallel trials, it is the method of choice to pool results from parallel and cross-over trials in a combined design meta-analysis. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/V4H27U8W/Curtin et al. - 2002 - Meta-analysis combining parallel and cross-over cl.pdf;/Users/rritaz/Zotero/storage/G3G7DVQM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,effect size combination (small sample \& discrete)},
  language = {en},
  number = {15}
}

@article{curtin_meta-analysis_2002-2,
  title = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{III}}: {{The}} Issue of Carry-Over},
  shorttitle = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{III}}},
  author = {Curtin, Fran{\c c}ois and Elbourne, Diana and Altman, Douglas G.},
  year = {2002},
  volume = {21},
  pages = {2161--2173},
  issn = {1097-0258},
  doi = {10.1002/sim.1207},
  abstract = {In meta-analysis combining results from parallel and cross-over trials, there is a risk of bias originating from the carry-over effect in cross-over trials. When pooling treatment effects estimated from parallel trials and two-period two-treatment cross-over trials, meta-analytic estimators of treatment effect can be obtained from the combination of parallel trial results either with cross-over trial results based on data of the first period only or with cross-over trial results analysed with data from both periods. Taking data from the first cross-over period protects against carry-over but gives less efficient treatment estimators and may lead to selection bias. This study evaluates in terms of variance reduction and mean square error the cost of calculating meta-analysis estimates with data from the first period instead of data from the two cross-over periods. If the information on cross-over sequence is available, we recommend performing two combined design meta-analyses, one using the first cross-over period data and one based on data from both cross-over periods. To investigate simultaneously the statistical significance of these two estimators as well as the carry-over at meta-analysis level, a method based on a multivariate analysis of the meta-analytic treatment effect and carry-over estimates is proposed. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GST8MWDT/Curtin et al. - 2002 - Meta-analysis combining parallel and cross-over cl.pdf;/Users/rritaz/Zotero/storage/ZE8TD7TR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,multivariate},
  language = {en},
  number = {15}
}

@article{curtin_meta-analysis_2017,
  title = {Meta-Analysis Combining Parallel and Cross-over Trials with Random Effects},
  author = {Curtin, Fran{\c c}ois},
  year = {2017},
  volume = {8},
  pages = {263--274},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1236},
  abstract = {Meta-analysis can necessitate the combination of parallel and cross-over trial designs. Because of the differences in the trial designs and potential biases notably associated with the crossover trials, one often combines trials of the same designs only, which decreases the power of the meta-analysis. To combine results of clinical trials from parallel and cross-over designs, we extend the method proposed in an accompanying study to account for random effects. We propose here a hierarchical mixed model allowing the combination of the 2 types of trial designs and accounting for additional covariates where random effects can be introduced to account for heterogeneity in trial, treatment effect, and interactions. We introduce a multilevel model and a Bayesian hierarchical model for combined trial design meta-analysis. The analysis of the models by restricted iterative generalised least square and Monte Carlo Markov Chain is presented. Methods are compared in a combined design meta-analysis model on salt reduction. Both models and their respective advantages in the perspective of meta-analysis are discussed. However, the access to the trial data, in particular sequence and period data in cross-over trials, remains a major limitation to the meta-analytic combination of trial designs.},
  file = {/Users/rritaz/Zotero/storage/UWQ6UN4P/Curtin - 2017 - Meta-analysis combining parallel and cross-over tr.pdf;/Users/rritaz/Zotero/storage/CGHFQU78/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,random-effects},
  language = {en},
  number = {3}
}

@article{curtin_meta-analysis_2017-1,
  title = {Meta-Analysis Combining Parallel and Crossover Trials Using Generalised Estimating Equation Method},
  author = {Curtin, Fran{\c c}ois},
  year = {2017},
  volume = {8},
  pages = {312--320},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1242},
  abstract = {Clinical trials have different designs: In late stage drug development, the parallel trial design is the most frequent one; however, the crossover design is not rare; different techniques are used to analyse their results. Although both designs measure the same treatment effect, combining parallel and crossover trials in a meta-analysis is not straightforward. We present here a meta-analysis method based on generalised estimating equation (GEE) regression to combine aggregated results of crossover and parallel trials using a marginal estimation approach. This method is based on the fixed effects meta-analytic model; it allows combining average outcomes belonging to the exponential distributions obtained from trials of different designs and in particular from crossover trials with more than 2 periods and 2 treatments. By extending the methods proposed so far to combine the 2 trial designs, the GEE regression allows for adjusting for bias, such as a carry-over effect typical in crossover trials. In this paper, the GEE meta-analysis method is compared to the classical weighted average method with examples of published and simulated meta-analyses. Although the GEE can account for crossover specificities, it is limited by the availability of detailed trial information often encountered with reports of these trials.},
  file = {/Users/rritaz/Zotero/storage/8B8EDME8/Curtin - 2017 - Meta-analysis combining parallel and crossover tri.pdf;/Users/rritaz/Zotero/storage/MDYW67VN/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance},
  language = {en},
  number = {3}
}

@article{da_costa_comparison_2019,
  title = {A Comparison of the Statistical Performance of Different Meta-Analysis Models for the Synthesis of Subgroup Effects from Randomized Clinical Trials},
  author = {{da Costa}, Bruno R. and Sutton, Alex J.},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {198},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0831-8},
  abstract = {Background: When investigating subgroup effects in meta-analysis, it is unclear whether accounting in metaregression for between-trial variation in treatment effects, but not between-trial variation in treatment interaction effects when such effects are present, leads to biased estimates, coverage problems, or wrong standard errors, and whether the use of aggregate data (AD) or individual-patient-data (IPD) influences this assessment. Methods: Seven different models were compared in a simulation study. Models differed regarding the use of AD or IPD, whether they accounted for between-trial variation in interaction effects, and whether they minimized the risk of ecological fallacy. Results: Models that used IPD and that allowed for between-trial variation of the interaction effect had less bias, better coverage, and more accurate standard errors than models that used AD or ignored this variation. The main factor influencing the performance of models was whether they used IPD or AD. The model that used AD had a considerably worse performance than all models that used IPD, especially when a low number of trials was included in the analysis. Conclusions: The results indicate that IPD models that allow for the between-trial variation in interaction effects should be given preference whenever investigating subgroup effects within a meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/735PRRVB/da Costa and Sutton - 2019 - A comparison of the statistical performance of dif.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {1}
}

@article{dagne_testing_2016,
  title = {Testing Moderation in Network Meta-Analysis with Individual Participant Data},
  author = {Dagne, Getachew A. and Brown, C. Hendricks and Howe, George and Kellam, Sheppard G. and Liu, Lei},
  year = {2016},
  volume = {35},
  pages = {2485--2502},
  issn = {1097-0258},
  doi = {10.1002/sim.6883},
  abstract = {Meta-analytic methods for combining data from multiple intervention trials are commonly used to estimate the effectiveness of an intervention. They can also be extended to study comparative effectiveness, testing which of several alternative interventions is expected to have the strongest effect. This often requires network meta-analysis (NMA), which combines trials involving direct comparison of two interventions within the same trial and indirect comparisons across trials. In this paper, we extend existing network methods for main effects to examining moderator effects, allowing for tests of whether intervention effects vary for different populations or when employed in different contexts. In addition, we study how the use of individual participant data may increase the sensitivity of NMA for detecting moderator effects, as compared with aggregate data NMA that employs study-level effect sizes in a meta-regression framework. A new NMA diagram is proposed. We also develop a generalized multilevel model for NMA that takes into account within-trial and between-trial heterogeneity and can include participant-level covariates. Within this framework, we present definitions of homogeneity and consistency across trials. A simulation study based on this model is used to assess effects on power to detect both main and moderator effects. Results show that power to detect moderation is substantially greater when applied to individual participant data as compared with study-level effects. We illustrate the use of this method by applying it to data from a classroom-based randomized study that involved two sub-trials, each comparing interventions that were contrasted with separate control groups. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/IEL8DWKB/Dagne et al. - 2016 - Testing moderation in network meta-analysis with i.pdf;/Users/rritaz/Zotero/storage/W6RCWJBY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {15}
}

@article{dakin_mixed_2011,
  title = {Mixed Treatment Comparison of Repeated Measurements of a Continuous Endpoint: An Example Using Topical Treatments for Primary Open-Angle Glaucoma and Ocular Hypertension},
  shorttitle = {Mixed Treatment Comparison of Repeated Measurements of a Continuous Endpoint},
  author = {Dakin, Helen A. and Welton, Nicky J. and Ades, A. E. and Collins, Sarah and Orme, Michelle and Kelly, Steven},
  year = {2011},
  volume = {30},
  pages = {2511--2535},
  issn = {1097-0258},
  doi = {10.1002/sim.4284},
  abstract = {Mixed treatment comparison (MTC) meta-analyses estimate relative treatment effects from networks of evidence while preserving randomisation. We extend the MTC framework to allow for repeated measurements of a continuous endpoint that varies over time. We used, as a case study, a systematic review and meta-analysis of intraocular pressure (IOP) measurements from randomised controlled trials evaluating topical ocular hypotensives in primary open-angle glaucoma or ocular hypertension because IOP varies over the day and over the treatment course, and repeated measurements are frequently reported. We adopted models for conducting MTC in WinBUGS (The BUGS Project, Cambridge, UK) to allow for repeated IOP measurements and to impute missing standard deviations of the raw data using the predictive distribution from observations with standard deviations. A flexible model with an unconstrained baseline for IOP variations over time and time-invariant random treatment effects fitted the data well. We also adopted repeated measures models to allow for class effects; assuming treatment effects to be exchangeable within classes slightly improved model fit but could bias estimated treatment effects if exchangeability assumptions were not valid. We enabled all timepoints to be included in the analysis, allowing for repeated measures to increase precision around treatment effects and avoid bias associated with selecting timepoints for meta-analysis.The methods we developed for modelling repeated measures and allowing for missing data may be adapted for use in other MTC meta-analyses. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Q46ZYMWF/Dakin et al. - 2011 - Mixed treatment comparison of repeated measurement.pdf;/Users/rritaz/Zotero/storage/A8QER4QZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,network meta-analysis,random-effects},
  language = {en},
  number = {20}
}

@article{darlington_combining_2000,
  title = {Combining Independent p Values: Extensions of the {{Stouffer}} and Binomial Methods.},
  shorttitle = {Combining Independent p Values},
  author = {Darlington, Richard B. and Hayes, Andrew F.},
  year = {2000},
  volume = {5},
  pages = {496--515},
  doi = {10.1037/1082-989X.5.4.496},
  abstract = {The Stouffer z method, and other popular methods for combining p values from independent significance tests, suffer from three problems: vulnerability to criticisms of the individual studies being pooled, difficulty in handling the "file drawer problem," and vague conclusions. These problems can be reduced or eliminated by supplementing a test of combined probability with a variety of new analyses described here. Along with other advantages, these analyses provide a way to address the file drawer problem without making limiting assumptions about the nature of studies not included in the pooled analysis. These analyses can supplement a traditional meta-analysis, yielding conclusions not provided by widely used meta-analytic procedures.},
  journal = {Psychological methods},
  keywords = {combined significance,diagnostic techniques,publication bias},
  number = {4}
}

@article{debray_aggregating_2012,
  title = {Aggregating Published Prediction Models with Individual Participant Data: A Comparison of Different Approaches},
  shorttitle = {Aggregating Published Prediction Models with Individual Participant Data},
  author = {Debray, Thomas P. A. and Koffijberg, Hendrik and Vergouwe, Yvonne and Moons, Karel G. M. and W.  Steyerberg, Ewout},
  year = {2012},
  volume = {31},
  pages = {2697--2712},
  issn = {1097-0258},
  doi = {10.1002/sim.5412},
  abstract = {During the recent decades, interest in prediction models has substantially increased, but approaches to synthesize evidence from previously developed models have failed to keep pace. This causes researchers to ignore potentially useful past evidence when developing a novel prediction model with individual participant data (IPD) from their population of interest. We aimed to evaluate approaches to aggregate previously published prediction models with new data. We consider the situation that models are reported in the literature with predictors similar to those available in an IPD dataset. We adopt a two-stage method and explore three approaches to calculate a synthesis model, hereby relying on the principles of multivariate meta-analysis. The former approach employs a naive pooling strategy, whereas the latter accounts for within-study and between-study covariance. These approaches are applied to a collection of 15 datasets of patients with traumatic brain injury, and to five previously published models for predicting deep venous thrombosis. Here, we illustrated how the generally unrealistic assumption of consistency in the availability of evidence across included studies can be relaxed. Results from the case studies demonstrate that aggregation yields prediction models with an improved discrimination and calibration in a vast majority of scenarios, and result in equivalent performance (compared with the standard approach) in a small minority of situations. The proposed aggregation approaches are particularly useful when few participant data are at hand. Assessing the degree of heterogeneity between IPD and literature findings remains crucial to determine the optimal approach in aggregating previous evidence into new prediction models. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/79SJA2UU/Debray et al. - 2012 - Aggregating published prediction models with indiv.pdf;/Users/rritaz/Zotero/storage/F3NRCZ8D/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,Individual Patient Data IPD,multivariate,prediction intervals,random-effects},
  language = {en},
  number = {23}
}

@article{debray_framework_2013,
  title = {A Framework for Developing, Implementing, and Evaluating Clinical Prediction Models in an Individual Participant Data Meta-Analysis},
  author = {Debray, Thomas P. A. and Moons, Karel G. M. and Ahmed, Ikhlaaq and Koffijberg, Hendrik and Riley, Richard David},
  year = {2013},
  volume = {32},
  pages = {3158--3180},
  issn = {1097-0258},
  doi = {10.1002/sim.5732},
  abstract = {The use of individual participant data (IPD) from multiple studies is an increasingly popular approach when developing a multivariable risk prediction model. Corresponding datasets, however, typically differ in important aspects, such as baseline risk. This has driven the adoption of meta-analytical approaches for appropriately dealing with heterogeneity between study populations. Although these approaches provide an averaged prediction model across all studies, little guidance exists about how to apply or validate this model to new individuals or study populations outside the derivation data. We consider several approaches to develop a multivariable logistic regression model from an IPD meta-analysis (IPD-MA) with potential between-study heterogeneity. We also propose strategies for choosing a valid model intercept for when the model is to be validated or applied to new individuals or study populations. These strategies can be implemented by the IPD-MA developers or future model validators. Finally, we show how model generalizability can be evaluated when external validation data are lacking using internal\textendash external cross-validation and extend our framework to count and time-to-event data. In an empirical evaluation, our results show how stratified estimation allows study-specific model intercepts, which can then inform the intercept to be used when applying the model in practice, even to a population not represented by included studies. In summary, our framework allows the development (through stratified estimation), implementation in new individuals (through focused intercept choice), and evaluation (through internal\textendash external validation) of a single, integrated prediction model from an IPD-MA in order to achieve improved model performance and generalizability. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/IZUZS6L4/Debray et al. - 2013 - A framework for developing, implementing, and eval.pdf;/Users/rritaz/Zotero/storage/WBHSTDGH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {18}
}

@article{debray_meta-analysis_2014,
  title = {Meta-Analysis and Aggregation of Multiple Published Prediction Models},
  author = {Debray, Thomas P. A. and Koffijberg, Hendrik and Nieboer, Daan and Vergouwe, Yvonne and Steyerberg, Ewout W. and Moons, Karel G. M.},
  year = {2014},
  volume = {33},
  pages = {2341--2362},
  issn = {1097-0258},
  doi = {10.1002/sim.6080},
  abstract = {AbstractPublished clinical prediction models are often ignored during the development of novel prediction models despite similarities in populations and intended usage. The plethora of prediction models that arise from this practice may still perform poorly when applied in other populations. Incorporating prior evidence might improve the accuracy of prediction models and make them potentially better generalizable. Unfortunately, aggregation of prediction models is not straightforward, and methods to combine differently specified models are currently lacking. We propose two approaches for aggregating previously published prediction models when a validation dataset is available: model averaging and stacked regressions. These approaches yield user-friendly stand-alone models that are adjusted for the new validation data. Both approaches rely on weighting to account for model performance and between-study heterogeneity but adopt a different rationale (averaging versus combination) to combine the models. We illustrate their implementation in a clinical example and compare them with established methods for prediction modeling in a series of simulation studies. Results from the clinical datasets and simulation studies demonstrate that aggregation yields prediction models with better discrimination and calibration in a vast majority of scenarios, and results in equivalent performance (compared to developing a novel model from scratch) when validation datasets are relatively large. In conclusion, model aggregation is a promising strategy when several prediction models are available from the literature and a validation dataset is at hand. The aggregation methods do not require existing models to have similar predictors and can be applied when relatively few data are at hand. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WFL7W6JX/Debray et al. - 2014 - Meta-analysis and aggregation of multiple publishe.pdf;/Users/rritaz/Zotero/storage/3EYUN6FI/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,prediction intervals},
  language = {en},
  number = {14}
}

@article{dersimonian_meta-analysis_1986,
  title = {Meta-Analysis in Clinical Trials},
  author = {DerSimonian, Rebecca and Laird, Nan},
  year = {1986},
  month = sep,
  volume = {7},
  pages = {177--188},
  publisher = {{Elsevier}},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(86)90046-2},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}p{$>$}This paper examines eight published reviews each reporting results from several related trials. Each review pools the results from the relevant trials in order to evaluate the efficacy of a certain treatment for a specified medical condition. These reviews lack consistent assessment of homogeneity of treatment effect before pooling. We discuss a random effects approach to combining evidence from a series of experiments comparing two treatments. This approach incorporates the heterogeneity of effects in the analysis of the overall treatment efficacy. The model can be extended to include relevant covariates which would reduce the heterogeneity and allow for more specific therapeutic recommendations. We suggest a simple noniterative procedure for characterizing the distribution of treatment effects in a series of studies.{$<$}/p{$>$}},
  file = {/Users/rritaz/Zotero/storage/PVXY6A75/fulltext.html},
  journal = {Controlled Clinical Trials},
  language = {English},
  number = {3},
  pmid = {3802833}
}

@article{dias_checking_2010,
  title = {Checking Consistency in Mixed Treatment Comparison Meta-Analysis},
  author = {Dias, S. and Welton, N. J. and Caldwell, D. M. and Ades, A. E.},
  year = {2010},
  volume = {29},
  pages = {932--944},
  issn = {1097-0258},
  doi = {10.1002/sim.3767},
  abstract = {Pooling of direct and indirect evidence from randomized trials, known as mixed treatment comparisons (MTC), is becoming increasingly common in the clinical literature. MTC allows coherent judgements on which of the several treatments is the most effective and produces estimates of the relative effects of each treatment compared with every other treatment in a network. We introduce two methods for checking consistency of direct and indirect evidence. The first method (back-calculation) infers the contribution of indirect evidence from the direct evidence and the output of an MTC analysis and is useful when the only available data consist of pooled summaries of the pairwise contrasts. The second more general, but computationally intensive, method is based on `node-splitting' which separates evidence on a particular comparison (node) into `direct' and `indirect' and can be applied to networks where trial-level data are available. Methods are illustrated with examples from the literature. We take a hierarchical Bayesian approach to MTC implemented using WinBUGS and R. We show that both methods are useful in identifying potential inconsistencies in different types of network and that they illustrate how the direct and indirect evidence combine to produce the posterior MTC estimates of relative treatment effects. This allows users to understand how MTC synthesis is pooling the data, and what is `driving' the final estimates. We end with some considerations on the modelling assumptions being made, the problems with the extension of the back-calculation method to trial-level data and discuss our methods in the context of the existing literature. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PUY9U253/Dias et al. - 2010 - Checking consistency in mixed treatment comparison.pdf;/Users/rritaz/Zotero/storage/ZRSUCFIG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis},
  language = {en},
  number = {7-8}
}

@article{dimitrakopoulou_accounting_2015,
  title = {Accounting for Uncertainty Due to `Last Observation Carried Forward' Outcome Imputation in a Meta-Analysis Model},
  author = {Dimitrakopoulou, Vasiliki and Efthimiou, Orestis and Leucht, Stefan and Salanti, Georgia},
  year = {2015},
  volume = {34},
  pages = {742--752},
  issn = {1097-0258},
  doi = {10.1002/sim.6364},
  abstract = {Missing outcome data are a problem commonly observed in randomized control trials that occurs as a result of participants leaving the study before its end. Missing such important information can bias the study estimates of the relative treatment effect and consequently affect the meta-analytic results. Therefore, methods on manipulating data sets with missing participants, with regard to incorporating the missing information in the analysis so as to avoid the loss of power and minimize the bias, are of interest. We propose a meta-analytic model that accounts for possible error in the effect sizes estimated in studies with last observation carried forward (LOCF) imputed patients. Assuming a dichotomous outcome, we decompose the probability of a successful unobserved outcome taking into account the sensitivity and specificity of the LOCF imputation process for the missing participants. We fit the proposed model within a Bayesian framework, exploring different prior formulations for sensitivity and specificity. We illustrate our methods by performing a meta-analysis of five studies comparing the efficacy of amisulpride versus conventional drugs (flupenthixol and haloperidol) on patients diagnosed with schizophrenia. Our meta-analytic models yield estimates similar to meta-analysis with LOCF-imputed patients. Allowing for uncertainty in the imputation process, precision is decreased depending on the priors used for sensitivity and specificity. Results on the significance of amisulpride versus conventional drugs differ between the standard LOCF approach and our model depending on prior beliefs on the imputation process. Our method can be regarded as a useful sensitivity analysis that can be used in the presence of concerns about the LOCF process. Copyright \textcopyright{} 2014 JohnWiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/33N439TQ/Dimitrakopoulou et al. - 2015 - Accounting for uncertainty due to ‘last observatio.pdf;/Users/rritaz/Zotero/storage/4D8VWTJM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,missing data},
  language = {en},
  number = {5}
}

@article{dimou_multivariate_2016,
  title = {A Multivariate Method for Meta-Analysis and Comparison of Diagnostic Tests},
  author = {Dimou, Niki L. and Adam, Maria and Bagos, Pantelis G.},
  year = {2016},
  volume = {35},
  pages = {3509--3523},
  issn = {1097-0258},
  doi = {10.1002/sim.6919},
  abstract = {We present here an extension of the classic bivariate random effects meta-analysis for the log-transformed sensitivity and specificity that can be applied for two or more diagnostic tests. The advantage of this method is that a closed-form expression is derived for the calculation of the within-studies covariances. The method allows the direct calculation of sensitivity and specificity, as well as, the diagnostic odds ratio, the area under curve and the parameters of the summary receiver operator's characteristic curve, along with the means for a formal comparison of these quantities for different tests. There is no need for individual patient data or the simultaneous evaluation of both diagnostic tests in all studies. The method is simple and fast; it can be extended for several diagnostic tests and can be fitted in nearly all statistical packages. The method was evaluated in simulations and applied in a meta-analysis for the comparison of anti-cyclic citrullinated peptide antibody and rheumatoid factor for discriminating patients with rheumatoid arthritis, with encouraging results. Simulations suggest that the method is robust and more powerful compared with the standard bivariate approach that ignores the correlation between tests. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/KXUJK8H6/Dimou et al. - 2016 - A multivariate method for meta-analysis and compar.pdf;/Users/rritaz/Zotero/storage/4AMBPLV4/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,random-effects},
  language = {en},
  number = {20}
}

@article{ding_bayesian_2013,
  title = {Bayesian Indirect and Mixed Treatment Comparisons across Longitudinal Time Points},
  author = {Ding, Ying and Fu, Haoda},
  year = {2013},
  volume = {32},
  pages = {2613--2628},
  issn = {1097-0258},
  doi = {10.1002/sim.5688},
  abstract = {Meta-analysis has become an acceptable and powerful tool for pooling quantitative results from multiple studies addressing the same question. It estimates the effect difference between two treatments when they have been compared head-to-head. However, limitations occur when there are more than two treatments of interest, and some of them have not been compared in the same study. Indirect and mixed treatment modeling extends meta-analysis methods to enable data from different treatments and trials to be synthesized, without requiring head-to-head comparisons among all treatments; thus, allowing different treatments to be compared. Traditional indirect and mixed treatment comparison methods consider a single endpoint for each trial. We extend the current methods and propose a Bayesian indirect and mixed treatment comparison longitudinal model that incorporates multiple time points and allows indirect comparisons of treatment effects across different longitudinal studies. The proposed model only uses summary level longitudinal data. This model is particularly useful when a meta-analysis is performed on studies with different durations. It enables the borrowing of information from shorter studies even in the situation where the primary interest is in a time point beyond the duration of some of these shorter studies. We performed simulation studies, which demonstrate that the proposed method performs well and yields better estimations compared with other single time point meta-analysis methods. We apply our method to a set of studies from patients with type 2 diabetes. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WJ2FVB3N/Ding and Fu - 2013 - Bayesian indirect and mixed treatment comparisons .pdf;/Users/rritaz/Zotero/storage/ZS8Q4398/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {15}
}

@article{discacciati_goodness_2017,
  title = {Goodness of Fit Tools for Dose\textendash Response Meta-Analysis of Binary Outcomes},
  author = {Discacciati, Andrea and Crippa, Alessio and Orsini, Nicola},
  year = {2017},
  volume = {8},
  pages = {149--160},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1194},
  abstract = {Goodness of fit evaluation should be a natural step in assessing and reporting dose\textendash response meta-analyses from aggregated data of binary outcomes. However, little attention has been given to this topic in the epidemiological literature, and goodness of fit is rarely, if ever, assessed in practice. We briefly review the two-stage and one-stage methods used to carry out dose\textendash response meta-analyses. We then illustrate and discuss three tools specifically aimed at testing, quantifying, and graphically evaluating the goodness of fit of dose\textendash response meta-analyses. These tools are the deviance, the coefficient of determination, and the decorrelated residuals-versus-exposure plot. Data from two published meta-analyses are used to show how these three tools can improve the practice of quantitative synthesis of aggregated dose\textendash response data. In fact, evaluating the degree of agreement between model predictions and empirical data can help the identification of dose\textendash response patterns, the investigation of sources of heterogeneity, and the assessment of whether the pooled dose\textendash response relation adequately summarizes the published results. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/4ES7VYWK/Discacciati et al. - 2017 - Goodness of fit tools for dose–response meta-analy.pdf;/Users/rritaz/Zotero/storage/VISW7YHA/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {2}
}

@article{doebler_mixed_2012,
  title = {A Mixed Model Approach to Meta-Analysis of Diagnostic Studies with Binary Test Outcome.},
  author = {Doebler, Philipp and Holling, Heinz and B{\"o}hning, Dankmar},
  year = {2012},
  volume = {17},
  pages = {418--436},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0028091},
  file = {/Users/rritaz/Zotero/storage/XGSIFULA/Doebler et al. - 2012 - A mixed model approach to meta-analysis of diagnos.pdf},
  journal = {Psychological Methods},
  keywords = {discrete effect sizes,GLM MA models,random-effects},
  language = {en},
  number = {3}
}

@article{dogo_sequential_2017,
  title = {Sequential Change Detection and Monitoring of Temporal Trends in Random-Effects Meta-Analysis},
  author = {Dogo, Samson Henry and Clark, Allan and Kulinskaya, Elena},
  year = {2017},
  volume = {8},
  pages = {220--235},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1222},
  abstract = {Temporal changes in magnitude of effect sizes reported in many areas of research are a threat to the credibility of the results and conclusions of meta-analysis. Numerous sequential methods for meta-analysis have been proposed to detect changes and monitor trends in effect sizes so that meta-analysis can be updated when necessary and interpreted based on the time it was conducted. The difficulties of sequential meta-analysis under the random-effects model are caused by dependencies in increments introduced by the estimation of the heterogeneity parameter {$\tau$}2. In this paper, we propose the use of a retrospective cumulative sum (CUSUM)-type test with bootstrap critical values. This method allows retrospective analysis of the past trajectory of cumulative effects in random-effects meta-analysis and its visualization on a chart similar to CUSUM chart. Simulation results show that the new method demonstrates good control of Type I error regardless of the number or size of the studies and the amount of heterogeneity. Application of the new method is illustrated on two examples of medical meta-analyses. \textcopyright{} 2016 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/A5DWEX8J/Dogo et al. - 2017 - Sequential change detection and monitoring of temp.pdf;/Users/rritaz/Zotero/storage/PXP7RQ3P/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,random-effects},
  language = {en},
  number = {2}
}

@article{donegan_assessing_2012,
  title = {Assessing the Consistency Assumption by Exploring Treatment by Covariate Interactions in Mixed Treatment Comparison Meta-Analysis: Individual Patient-Level Covariates versus Aggregate Trial-Level Covariates},
  shorttitle = {Assessing the Consistency Assumption by Exploring Treatment by Covariate Interactions in Mixed Treatment Comparison Meta-Analysis},
  author = {Donegan, Sarah and Williamson, Paula and D'Alessandro, Umberto and Smith, Catrin Tudur},
  year = {2012},
  volume = {31},
  pages = {3840--3857},
  issn = {1097-0258},
  doi = {10.1002/sim.5470},
  abstract = {Mixed treatment comparison (MTC) meta-analysis allows several treatments to be compared in a single analysis while utilising direct and indirect evidence. Treatment by covariate interactions can be included in MTC models to explore how the covariate modifies the treatment effects. If interactions exist, the assumptions underlying MTCs may be invalidated. For conventional pair-wise meta-analysis, important benefits regarding the investigation of such interactions, gained from using individual patient data (IPD) rather than aggregate data (AD), have been described. We aim to compare IPD MTC models including patient-level covariates with AD MTC models including study-level covariates. IPD and AD random-effects MTC models for dichotomous outcomes are specified. Three assumptions are made regarding the interactions (i.e. independent, exchangeable and common interactions). The models are applied to a dataset to compare four drugs for treating malaria (i.e. amodiaquine-artesunate, dihydroartemisinin-piperaquine (DHAPQ), artemether-lumefantrine and chlorproguanil-dapsone plus artesunate) using the outcome unadjusted treatment success at day 28. The treatment effects and regression coefficients for interactions from the IPD models were more precise than those from AD models. Using IPD, assuming independent or exchangeable interactions, the regression coefficient for chlorproguanil-dapsone plus artesunate versus DHAPQ was statistically significant and assuming common interactions, the common coefficient was significant; whereas using AD, no coefficients were significant. Using IPD, DHAPQ was the best drug; whereas using AD, the best drug varied. Using AD models, there was no evidence that the consistency assumption was invalid; whereas, the assumption was questionable based on the IPD models. The AD analyses were misleading. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GDDCN2QN/Donegan et al. - 2012 - Assessing the consistency assumption by exploring .pdf;/Users/rritaz/Zotero/storage/7XLTXQXF/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {29}
}

@article{donegan_assessing_2019,
  title = {Assessing the Consistency Assumptions Underlying Network Meta-Regression Using Aggregate Data},
  author = {Donegan, Sarah and Dias, Sofia and Welton, Nicky J.},
  year = {2019},
  volume = {10},
  pages = {207--224},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1327},
  abstract = {When numerous treatments exist for a disease (Treatments 1, 2, 3, etc), network meta-regression (NMR) examines whether each relative treatment effect (eg, mean difference for 2 vs 1, 3 vs 1, and 3 vs 2) differs according to a covariate (eg, disease severity). Two consistency assumptions underlie NMR: consistency of the treatment effects at the covariate value 0 and consistency of the regression coefficients for the treatment by covariate interaction. The NMR results may be unreliable when the assumptions do not hold. Furthermore, interactions may exist but are not found because inconsistency of the coefficients is masking them, for example, when the treatment effect increases as the covariate increases using direct evidence but the effect decreases with the increasing covariate using indirect evidence. We outline existing NMR models that incorporate different types of treatment by covariate interaction. We then introduce models that can be used to assess the consistency assumptions underlying NMR for aggregate data. We extend existing node-splitting models, the unrelated mean effects inconsistency model, and the design by treatment inconsistency model to incorporate covariate interactions. We propose models for assessing both consistency assumptions simultaneously and models for assessing each of the assumptions in turn to gain a more thorough understanding of consistency. We apply the methods in a Bayesian framework to trial-level data comparing antimalarial treatments using the covariate average age and to four fabricated data sets to demonstrate key scenarios. We discuss the pros and cons of the methods and important considerations when applying models to aggregated data.},
  file = {/Users/rritaz/Zotero/storage/A9C9QVZ2/Donegan et al. - 2019 - Assessing the consistency assumptions underlying n.pdf;/Users/rritaz/Zotero/storage/53QNZBZS/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {2}
}

@article{donegan_combining_2013,
  title = {Combining Individual Patient Data and Aggregate Data in Mixed Treatment Comparison Meta-Analysis: {{Individual}} Patient Data May Be Beneficial If Only for a Subset of Trials},
  shorttitle = {Combining Individual Patient Data and Aggregate Data in Mixed Treatment Comparison Meta-Analysis},
  author = {Donegan, Sarah and Williamson, Paula and D'Alessandro, Umberto and Garner, Paul and Smith, Catrin Tudur},
  year = {2013},
  volume = {32},
  pages = {914--930},
  issn = {1097-0258},
  doi = {10.1002/sim.5584},
  abstract = {Background Individual patient data (IPD) meta-analysis is the gold standard. Aggregate data (AD) and IPD can be combined using conventional pairwise meta-analysis when IPD cannot be obtained for all relevant studies. We extend the methodology to combine IPD and AD in a mixed treatment comparison (MTC) meta-analysis. Methods The proposed random-effects MTC models combine IPD and AD for a dichotomous outcome. We study the benefits of acquiring IPD for a subset of trials when assessing the underlying consistency assumption by including treatment-by-covariate interactions in the model. We describe three different model specifications that make increasingly stronger assumptions regarding the interactions. We illustrate the methodology through application to real data sets to compare drugs for treating malaria by using the outcome unadjusted treatment success at day 28. We compare results from AD alone, IPD alone and all data. Results When IPD contributed (i.e. either using IPD alone or combining IPD and AD), the chains converged, and we identified statistically significant regression coefficients for the interactions. Using IPD alone, we were able to compare only three of the six treatments of interest. When models were fitted to AD, the treatment effects and regression coefficients for the interactions were far more imprecise, and the chains did not converge. Conclusions The models combining IPD and AD encapsulated all available evidence. When exploring interactions, it can be beneficial to obtain IPD for a subset of trials and to combine IPD with additional AD. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/IP3USWTB/Donegan et al. - 2013 - Combining individual patient data and aggregate da.pdf;/Users/rritaz/Zotero/storage/SVHDHHV6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {discrete effect sizes,Individual Patient Data IPD,modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {6}
}

@article{donegan_graphs_2018,
  title = {Graphs of Study Contributions and Covariate Distributions for Network Meta-Regression},
  author = {Donegan, Sarah and Dias, Sofia and Tudur-Smith, Catrin and Marinho, Valeria and Welton, Nicky J.},
  year = {2018},
  volume = {9},
  pages = {243--260},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1292},
  abstract = {Background Meta-regression results must be interpreted taking into account the range of covariate values of the contributing studies. Results based on interpolation or extrapolation may be unreliable. In network meta-regression (NMR) models, which include covariates in network meta-analyses, results are estimated using direct and indirect evidence; therefore, it may be unclear which studies and covariate values contribute to which result. We propose graphs to help understand which trials and covariate values contribute to each NMR result and to highlight extrapolation or interpolation. Methods We introduce methods to calculate the contribution that each trial and covariate value makes to each result and compare them with existing methods. We show how to construct graphs including a network covariate distribution diagram, covariate-contribution plot, heat plot, contribution-NMR plot, and heat-NMR plot. We demonstrate the methods using a dataset with treatments for malaria using the covariate average age and a dataset of topical fluoride interventions for preventing dental caries using the covariate randomisation year. Results For the malaria dataset, no contributing trials had an average age between 7\textendash 25 years and therefore results were interpolated within this range. For the fluoride dataset, there are no contributing trials randomised between 1954\textendash 1959 for most comparisons therefore, within this range, results would be extrapolated. Conclusions Even in a fully connected network, an NMR result may be estimated from trials with a narrower covariate range than the range of the whole dataset. Calculating contributions and graphically displaying them aids interpretation of NMR result by highlighting extrapolated or interpolated results.},
  file = {/Users/rritaz/Zotero/storage/GXFS9BHN/Donegan et al. - 2018 - Graphs of study contributions and covariate distri.pdf;/Users/rritaz/Zotero/storage/5CN3ERDI/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {2}
}

@article{du_bayesian_2017,
  title = {A {{Bayesian}} 'fill-in' Method for Correcting for Publication Bias in Meta-Analysis},
  author = {Du, Han and Liu, Fang and Wang, Lijuan},
  year = {2017},
  month = dec,
  volume = {22},
  pages = {799--817},
  issn = {1082-989X},
  doi = {10.1037/met0000164},
  abstract = {Publication bias occurs when the statistical significance or direction of the results between published and unpublished studies differ after controlling for study quality, which threatens the validity of the systematic review and summary of the results on a research topic. Conclusions based on a meta-analysis of published studies without correcting for publication bias are often optimistic and biased toward significance or positivity. We propose a Bayesian fill-in meta-analysis (BALM) method for adjusting publication bias and estimating population effect size that accommodates different assumptions for publication bias. Simulation studies were conducted to examine the performance of BALM and compare it with several commonly used/discussed and recently proposed publication bias correction methods. The simulation results suggested BALM yielded small biases, small RMSE values, and close-to-nominal-level coverage rates in inferring the population effect size and the between-study variance, and outperformed the other examined publication bias correction methods across a wide range of simulation scenarios when the publication bias mechanism is correctly specified. The performance of BALM was relatively sensitive to the assumed publication bias mechanism. Even with a misspecified publication bias mechanism, BALM still outperformed the naive methods without correcting for publication in inferring the overall population effect size. BALM was applied to 2 meta-analysis case studies to illustrate the use of BALM in real life situations. R functions are provided to facilitate the implementation of BALM. Guidelines on how to specify the publication bias mechanisms in BALM and how to report overall effect size estimates are provided. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {bayesian,diagnostic techniques,publication bias,random-effects},
  number = {4},
  series = {Bayesian {{Data Analysis}}: {{Part II}}}
}

@article{edwardes_generalization_2000,
  title = {The Generalization of the Odds Ratio, Risk Ratio and Risk Difference to r \texttimes{} k Tables},
  author = {deB Edwardes, Michael D. and Baltzan, Marc},
  year = {2000},
  volume = {19},
  pages = {1901--1914},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20000730)19:14<1901::AID-SIM514>3.0.CO;2-V},
  abstract = {A Correction has been published for this article in Statisitcs in Medicine 2000; 19(21): 3017. Familiar measures of association for 2 \texttimes{} 2 tables are the odds ratio, the risk ratio and the risk difference. Analagous measures of outcome\textendash exposure association are desirable when there are several degrees of severity of both exposure and disease outcome. One such measure ({$\alpha$}), which we label the general odds ratio (ORG), was proposed by Agresti. Convenient methods are given for calculation of both standard error and 95 per cent confidence intervals for ORG. Other approaches to generalizing the odds ratio entail fitting statistical models which might not fit the data, and cannot handle some zero frequencies. We propose a generalization of the risk ratio (RRG) following the statistical approaches of Agresti, Goodman and Kruskal. A method of calculating the standard error and 95 per cent confidence interval for RRG is provided. A known statistic, Somers' d, fulfils the characteristics necessary for a generalized risk difference (RDG). These measures have straightforward interpretations, are easily computed, are at least as precise as other methods and do not require fitting statistical models to the data. We also examine the pooling of such measures as in, for example, meta-analysis. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/82J5ZDBU/Edwardes and Baltzan - 2000 - The generalization of the odds ratio, risk ratio a.pdf;/Users/rritaz/Zotero/storage/53ZVE5ZF/1097-0258(20000730)19141901AID-SIM5143.0.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),effect size estimation (series)},
  language = {en},
  number = {14}
}

@article{edwardes_meta-analysis_2014,
  title = {Meta-Analysis and the Reversed {{Theorem}} of the {{Means}}},
  author = {deB Edwardes, Michael D.},
  year = {2014},
  volume = {5},
  pages = {313--321},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1118},
  abstract = {Conventional meta-analysis estimators are weighted means of study measures, meant to estimate an overall population measure. For measures such as means, mean differences and risk differences, a weighted arithmetic mean is the conventional estimator. When the measures are ratios, such as odds ratios, logarithms of the study measures are most frequently used, and the back-transform is a weighted geometric mean, rather than the arithmetic mean. For numbers needed to treat, a weighted harmonic mean is the back-transform. The Theorem of the Means effectively states that unless all of the studies have an equal result, the arithmetic mean must be greater than the geometric mean, which must be greater than the harmonic mean. When the weights are fixed sampling weights, the inequalities are in the expected direction. However, when the weights are the usual reciprocal variance estimates, the inequalities go in the opposite direction. The use of reciprocal variance weights is therefore questioned as perhaps having a fundamental flaw. An example is shown of a meta-analysis of frequencies of two classes of drug-resistant HIV-1 mutations. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JE7GEE76/Edwardes - 2014 - Meta-analysis and the reversed Theorem of the Mean.pdf;/Users/rritaz/Zotero/storage/KQ44GLKP/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,weights},
  language = {en},
  number = {4}
}

@article{efthimiou_approach_2014,
  title = {An Approach for Modelling Multiple Correlated Outcomes in a Network of Interventions Using Odds Ratios},
  author = {Efthimiou, Orestis and Mavridis, Dimitris and Cipriani, Andrea and Leucht, Stefan and Bagos, Pantelis and Salanti, Georgia},
  year = {2014},
  volume = {33},
  pages = {2275--2287},
  issn = {1097-0258},
  doi = {10.1002/sim.6117},
  abstract = {AbstractA multivariate meta-analysis of two or more correlated outcomes is expected to improve precision compared with a series of independent, univariate meta-analyses especially when there are studies reporting some but not all outcomes. Multivariate meta-analysis requires estimates of the within-study correlations, which are seldom available. Existing methods for analysing multiple outcomes simultaneously are limited to pairwise treatment comparisons. We propose a model for a joint, simultaneous synthesis of multiple dichotomous outcomes in a network of interventions and introduce a simple way to elicit expert opinion for the within-study correlations by utilizing a set of conditional probability parameters. We implement our multiple-outcomes network meta-analysis model within a Bayesian framework, which allows incorporation of expert information. As an example, we analyse two correlated dichotomous outcomes, response to the treatment and dropout rate, in a network of pharmacological interventions for acute mania. The produced estimates have narrower confidence intervals compared with the simple network meta-analysis. We conclude that the proposed model and the suggested prior elicitation method for correlations constitute a useful framework for performing network meta-analysis for multiple outcomes. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/L49G94BN/Efthimiou et al. - 2014 - An approach for modelling multiple correlated outc.pdf;/Users/rritaz/Zotero/storage/KWH8XE7C/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,discrete effect sizes,network meta-analysis},
  language = {en},
  number = {13}
}

@article{efthimiou_combining_2017,
  title = {Combining Randomized and Non-Randomized Evidence in Network Meta-Analysis},
  author = {Efthimiou, Orestis and Mavridis, Dimitris and Debray, Thomas P. A. and Samara, Myrto and Belger, Mark and Siontis, George C. M. and Leucht, Stefan and Salanti, Georgia},
  year = {2017},
  volume = {36},
  pages = {1210--1226},
  issn = {1097-0258},
  doi = {10.1002/sim.7223},
  abstract = {Non-randomized studies aim to reveal whether or not interventions are effective in real-life clinical practice, and there is a growing interest in including such evidence in the decision-making process. We evaluate existing methodologies and present new approaches to using non-randomized evidence in a network meta-analysis of randomized controlled trials (RCTs) when the aim is to assess relative treatment effects. We first discuss how to assess compatibility between the two types of evidence. We then present and compare an array of alternative methods that allow the inclusion of non-randomized studies in a network meta-analysis of RCTs: the na\"ive data synthesis, the design-adjusted synthesis, the use of non-randomized evidence as prior information and the use of three-level hierarchical models. We apply some of the methods in two previously published clinical examples comparing percutaneous interventions for the treatment of coronary in-stent restenosis and antipsychotics in patients with schizophrenia. We discuss in depth the advantages and limitations of each method, and we conclude that the inclusion of real-world evidence from non-randomized studies has the potential to corroborate findings from RCTs, increase precision and enhance the decision-making process. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CH84X3G3/Efthimiou et al. - 2017 - Combining randomized and non-randomized evidence i.pdf;/Users/rritaz/Zotero/storage/IXBB9WN7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis},
  language = {en},
  number = {8}
}

@article{efthimiou_network_2019,
  title = {Network Meta-Analysis of Rare Events Using the {{Mantel}}-{{Haenszel}} Method},
  author = {Efthimiou, Orestis and R{\"u}cker, Gerta and Schwarzer, Guido and Higgins, Julian P. T. and Egger, Matthias and Salanti, Georgia},
  year = {2019},
  volume = {38},
  pages = {2992--3012},
  issn = {1097-0258},
  doi = {10.1002/sim.8158},
  abstract = {The Mantel-Haenszel (MH) method has been used for decades to synthesize data obtained from studies that compare two interventions with respect to a binary outcome. It has been shown to perform better than the inverse-variance method or Peto's odds ratio when data is sparse. Network meta-analysis (NMA) is increasingly used to compare the safety of medical interventions, synthesizing, eg, data on mortality or serious adverse events. In this setting, sparse data occur often and yet there is to-date, no extension of the MH method for the case of NMA. In this paper, we fill this gap by presenting a MH-NMA method for odds ratios. Similarly to the pairwise MH method, we assume common treatment effects. We implement our approach in R, and we provide freely available easy-to-use routines. We illustrate our approach using data from two previously published networks. We compare our results to those obtained from three other approaches to NMA, namely, NMA with noncentral hypergeometric likelihood, an inverse-variance NMA, and a Bayesian NMA with a binomial likelihood. We also perform simulations to assess the performance of our method and compare it with alternative methods. We conclude that our MH-NMA method offers a reliable approach to the NMA of binary outcomes, especially in the case or sparse data, and when the assumption of methodological and clinical homogeneity is justifiable.},
  file = {/Users/rritaz/Zotero/storage/FDJCN7KE/Efthimiou et al. - 2019 - Network meta-analysis of rare events using the Man.pdf;/Users/rritaz/Zotero/storage/8IMKX8DS/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),network meta-analysis},
  language = {en},
  number = {16}
}

@article{eilers_data_2007,
  title = {Data Exploration in Meta-Analysis with Smooth Latent Distributions},
  author = {Eilers, Paul H. C.},
  year = {2007},
  volume = {26},
  pages = {3358--3368},
  issn = {1097-0258},
  doi = {10.1002/sim.2817},
  abstract = {Meta-analysis with discrete outcomes is interpreted as the estimation (in one or two dimensions) of a non-parametric smooth latent distribution of event probabilities (or rates). A simple but efficient EM algorithm is presented. A fine grid is used and fast smoothing is done by penalized least squares. Data exploration is the primary goal, but the estimated distribution can also be used to compute useful statistics of treatment effects. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/M2N54QJG/Eilers - 2007 - Data exploration in meta-analysis with smooth late.pdf;/Users/rritaz/Zotero/storage/3CZ823ZC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,discrete effect sizes},
  language = {en},
  number = {17}
}

@article{elst_exploring_2016,
  title = {Exploring the Relationship between the Causal-Inference and Meta-Analytic Paradigms for the Evaluation of Surrogate Endpoints},
  author = {der Elst, Wim Van and Molenberghs, Geert and Alonso, Ariel},
  year = {2016},
  volume = {35},
  pages = {1281--1298},
  issn = {1097-0258},
  doi = {10.1002/sim.6807},
  abstract = {Nowadays, two main frameworks for the evaluation of surrogate endpoints, based on causal-inference and meta-analysis, dominate the scene. Earlier work showed that the metrics of surrogacy introduced in both paradigms are related, although in a complex way that is difficult to study analytically. In the present work, this relationship is further examined using simulations and the analysis of a case study. The results indicate that the extent to which both paradigms lead to similar conclusions regarding the validity of the surrogate, depends on a complex interplay between multiple factors like the ratio of the between and within trial variability and the unidentifiable correlations between the potential outcomes. All the analyses were carried out using the newly developed R package Surrogate, which is freely available via CRAN. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/93R7PGBJ/Elst et al. - 2016 - Exploring the relationship between the causal-infe.pdf;/Users/rritaz/Zotero/storage/WA7IQP9S/sim.html},
  journal = {Statistics in Medicine},
  keywords = {causality,correlated effects,random-effects},
  language = {en},
  number = {8}
}

@article{ensor_meta-analysis_2018,
  title = {Meta-Analysis of Test Accuracy Studies Using Imputation for Partial Reporting of Multiple Thresholds},
  author = {Ensor, J. and Deeks, J. J. and Martin, E. C. and Riley, R. D.},
  year = {2018},
  volume = {9},
  pages = {100--115},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1276},
  abstract = {Introduction For tests reporting continuous results, primary studies usually provide test performance at multiple but often different thresholds. This creates missing data when performing a meta-analysis at each threshold. A standard meta-analysis (no imputation [NI]) ignores such missing data. A single imputation (SI) approach was recently proposed to recover missing threshold results. Here, we propose a new method that performs multiple imputation of the missing threshold results using discrete combinations (MIDC). Methods The new MIDC method imputes missing threshold results by randomly selecting from the set of all possible discrete combinations which lie between the results for 2 known bounding thresholds. Imputed and observed results are then synthesised at each threshold. This is repeated multiple times, and the multiple pooled results at each threshold are combined using Rubin's rules to give final estimates. We compared the NI, SI, and MIDC approaches via simulation. Results Both imputation methods outperform the NI method in simulations. There was generally little difference in the SI and MIDC methods, but the latter was noticeably better in terms of estimating the between-study variances and generally gave better coverage, due to slightly larger standard errors of pooled estimates. Given selective reporting of thresholds, the imputation methods also reduced bias in the summary receiver operating characteristic curve. Simulations demonstrate the imputation methods rely on an equal threshold spacing assumption. A real example is presented. Conclusions The SI and, in particular, MIDC methods can be used to examine the impact of missing threshold results in meta-analysis of test accuracy studies.},
  file = {/Users/rritaz/Zotero/storage/323X2JP3/Ensor et al. - 2018 - Meta-analysis of test accuracy studies using imput.pdf;/Users/rritaz/Zotero/storage/SSV4A5KH/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {missing data},
  language = {en},
  number = {1}
}

@article{ensor_simulation-based_2018,
  title = {Simulation-Based Power Calculations for Planning a Two-Stage Individual Participant Data Meta-Analysis},
  author = {Ensor, Joie and Burke, Danielle L. and Snell, Kym I. E. and Hemming, Karla and Riley, Richard D.},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {41},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0492-z},
  abstract = {Background: Researchers and funders should consider the statistical power of planned Individual Participant Data (IPD) meta-analysis projects, as they are often time-consuming and costly. We propose simulation-based power calculations utilising a two-stage framework, and illustrate the approach for a planned IPD meta-analysis of randomised trials with continuous outcomes where the aim is to identify treatment-covariate interactions. Methods: The simulation approach has four steps: (i) specify an underlying (data generating) statistical model for trials in the IPD meta-analysis; (ii) use readily available information (e.g. from publications) and prior knowledge (e. g. number of studies promising IPD) to specify model parameter values (e.g. control group mean, intervention effect, treatment-covariate interaction); (iii) simulate an IPD meta-analysis dataset of a particular size from the model, and apply a two-stage IPD meta-analysis to obtain the summary estimate of interest (e.g. interaction effect) and its associated p-value; (iv) repeat the previous step (e.g. thousands of times), then estimate the power to detect a genuine effect by the proportion of summary estimates with a significant p-value. Results: In a planned IPD meta-analysis of lifestyle interventions to reduce weight gain in pregnancy, 14 trials (1183 patients) promised their IPD to examine a treatment-BMI interaction (i.e. whether baseline BMI modifies intervention effect on weight gain). Using our simulation-based approach, a two-stage IPD meta-analysis has {$<$} 60\% power to detect a reduction of 1 kg weight gain for a 10-unit increase in BMI. Additional IPD from ten other published trials (containing 1761 patients) would improve power to over 80\%, but only if a fixed-effect meta-analysis was appropriate. Pre-specified adjustment for prognostic factors would increase power further. Incorrect dichotomisation of BMI would reduce power by over 20\%, similar to immediately throwing away IPD from ten trials. Conclusions: Simulation-based power calculations could inform the planning and funding of IPD projects, and should be used routinely.},
  file = {/Users/rritaz/Zotero/storage/JC6NEFR4/Ensor et al. - 2018 - Simulation-based power calculations for planning a.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Individual Patient Data IPD,power},
  language = {en},
  number = {1}
}

@article{estes_meta-analysis_2017,
  title = {Meta-Analysis of Gene-Environment Interaction Exploiting Gene-Environment Independence across Multiple Case-Control Studies},
  author = {Estes, Jason P. and Rice, John D. and Li, Shi and Stringham, Heather M. and Boehnke, Michael and Mukherjee, Bhramar},
  year = {2017},
  volume = {36},
  pages = {3895--3909},
  issn = {1097-0258},
  doi = {10.1002/sim.7398},
  abstract = {Multiple papers have studied the use of gene-environment (G-E) independence to enhance power for testing gene-environment interaction in case-control studies. However, studies that evaluate the role of G-E independence in a meta-analysis framework are limited. In this paper, we extend the single-study empirical Bayes type shrinkage estimators proposed by Mukherjee and Chatterjee (2008) to a meta-analysis setting that adjusts for uncertainty regarding the assumption of G-E independence across studies. We use the retrospective likelihood framework to derive an adaptive combination of estimators obtained under the constrained model (assuming G-E independence) and unconstrained model (without assumptions of G-E independence) with weights determined by measures of G-E association derived from multiple studies. Our simulation studies indicate that this newly proposed estimator has improved average performance across different simulation scenarios than the standard alternative of using inverse variance (covariance) weighted estimators that combines study-specific constrained, unconstrained, or empirical Bayes estimators. The results are illustrated by meta-analyzing 6 different studies of type 2 diabetes investigating interactions between genetic markers on the obesity related FTO gene and environmental factors body mass index and age.},
  file = {/Users/rritaz/Zotero/storage/EJC75HA7/Estes et al. - 2017 - Meta-analysis of gene-environment interaction expl.pdf;/Users/rritaz/Zotero/storage/6PAACXEK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,physical/biological fields},
  language = {en},
  number = {24}
}

@article{eusebi_latent_2014,
  title = {Latent Class Bivariate Model for the Meta-Analysis of Diagnostic Test Accuracy Studies},
  author = {Eusebi, Paolo and Reitsma, Johannes B and Vermunt, Jeroen K},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {88},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-88},
  abstract = {Background: Several types of statistical methods are currently available for the meta-analysis of studies on diagnostic test accuracy. One of these methods is the Bivariate Model which involves a simultaneous analysis of the sensitivity and specificity from a set of studies. In this paper, we review the characteristics of the Bivariate Model and demonstrate how it can be extended with a discrete latent variable. The resulting clustering of studies yields additional insight into the accuracy of the test of interest. Methods: A Latent Class Bivariate Model is proposed. This model captures the between-study variability in sensitivity and specificity by assuming that studies belong to one of a small number of latent classes. This yields both an easier to interpret and a more precise description of the heterogeneity between studies. Latent classes may not only differ with respect to the average sensitivity and specificity, but also with respect to the correlation between sensitivity and specificity. Results: The Latent Class Bivariate Model identifies clusters of studies with their own estimates of sensitivity and specificity. Our simulation study demonstrated excellent parameter recovery and good performance of the model selection statistics typically used in latent class analysis. Application in a real data example on coronary artery disease showed that the inclusion of latent classes yields interesting additional information. Conclusions: Our proposed new meta-analysis method can lead to a better fit of the data set of interest, less biased estimates and more reliable confidence intervals for sensitivities and specificities. But even more important, it may serve as an exploratory tool for subsequent sub-group meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/K5C6FW8F/Eusebi et al. - 2014 - Latent class bivariate model for the meta-analysis.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {GLM MA models,multivariate},
  language = {en},
  number = {1}
}

@article{feingold_effect_2009,
  title = {Effect {{Sizes}} for {{Growth}}-{{Modeling Analysis}} for {{Controlled Clinical Trials}} in the {{Same Metric}} as for {{Classical Analysis}}},
  author = {Feingold, Alan},
  year = {2009},
  month = mar,
  volume = {14},
  pages = {43--53},
  issn = {1082-989X},
  doi = {10.1037/a0014699},
  abstract = {The use of growth-modeling analysis (GMA)--including Hierarchical Linear Models, Latent Growth Models, and General Estimating Equations--to evaluate interventions in psychology, psychiatry, and prevention science has grown rapidly over the last decade. However, an effect size associated with the difference between the trajectories of the intervention and control groups that captures the treatment effect is rarely reported. This article first reviews two classes of formulas for effect sizes associated with classical repeated-measures designs that use the standard deviation of either change scores or raw scores for the denominator. It then broadens the scope to subsume GMA, and demonstrates that the independent groups, within-subjects, pretest-posttest control-group, and GMA designs all estimate the same effect size when the standard deviation of raw scores is uniformly used. Finally, it is shown that the correct effect size for treatment efficacy in GMA--the difference between the estimated means of the two groups at end of study (determined from the coefficient for the slope difference and length of study) divided by the baseline standard deviation--is not reported in clinical trials.},
  file = {/Users/rritaz/Zotero/storage/DRN3VQ6E/Feingold - 2009 - Effect Sizes for Growth-Modeling Analysis for Cont.pdf},
  journal = {Psychological methods},
  keywords = {continuous effect sizes},
  number = {1},
  pmcid = {PMC2712654},
  pmid = {19271847}
}

@article{ferguson_publication_2012,
  title = {Publication Bias in Psychological Science: {{Prevalence}}, Methods for Identifying and Controlling, and Implications for the Use of Meta-Analyses},
  shorttitle = {Publication Bias in Psychological Science},
  author = {Ferguson, Christopher J. and Brannick, Michael T.},
  year = {2012},
  month = mar,
  volume = {17},
  pages = {120--128},
  issn = {1082-989X},
  doi = {10.1037/a0024445},
  abstract = {The issue of publication bias in psychological science is one that has remained difficult to address despite decades of discussion and debate. The current article examines a sample of 91 recent meta-analyses published in American Psychological Association and Association for Psychological Science journals and the methods used in these analyses to identify and control for publication bias. Of the 91 studies analyzed, 64 (70\%) made some effort to analyze publication bias, and 26 (41\%) reported finding evidence of bias. Approaches to controlling publication bias were heterogeneous among studies. Of these studies, 57 (63\%) attempted to find unpublished studies to control for publication bias. Nonetheless, those studies that included unpublished studies were just as likely to find evidence for publication bias as those that did not. Furthermore, authors of meta-analyses themselves were overrepresented in unpublished studies acquired, as compared with published studies, suggesting that searches for unpublished studies may increase rather than decrease some sources of bias. A subset of 48 meta-analyses for which study sample sizes and effect sizes were available was further analyzed with a conservative and newly developed tandem procedure of assessing publication bias. Results indicated that publication bias was worrisome in about 25\% of meta-analyses. Meta-analyses that included unpublished studies were more likely to show bias than those that did not, likely due to selection bias in unpublished literature searches. Sources of publication bias and implications for the use of meta-analysis are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/JMUY4ZQ3/Ferguson and Brannick - 2012 - Publication bias in psychological science Prevale.pdf},
  journal = {Psychological Methods},
  keywords = {meta-meta-analyses,publication bias},
  number = {1}
}

@article{field_is_2005,
  title = {Is the {{Meta}}-{{Analysis}} of {{Correlation Coefficients Accurate When Population Correlations Vary}}?},
  author = {Field, Andy P.},
  year = {2005},
  month = dec,
  volume = {10},
  pages = {444--467},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.10.4.444},
  file = {/Users/rritaz/Zotero/storage/C8335IEQ/Field - 2005 - Is the Meta-Analysis of Correlation Coefficients A.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlation coefficients,random-effects},
  language = {en},
  number = {4}
}

@article{fiocco_meta-analysis_2009,
  title = {Meta-Analysis of Pairs of Survival Curves under Heterogeneity: {{A Poisson}} Correlated Gamma-Frailty Approach},
  shorttitle = {Meta-Analysis of Pairs of Survival Curves under Heterogeneity},
  author = {Fiocco, M. and Putter, H. and van Houwelingen, J. C.},
  year = {2009},
  volume = {28},
  pages = {3782--3797},
  issn = {1097-0258},
  doi = {10.1002/sim.3752},
  abstract = {We address the problem of meta-analysis of pairs of survival curves under heterogeneity. Starting point for the meta-analysis is a set of studies, each comparing the same two treatments, containing information about multiple survival outcomes. Under heterogeneity, we model the number of events using an extension of the Poisson correlated gamma-frailty model with serial within-arm and positive between-arm correlations. The parameters of the models are estimated following a two-stage estimation procedure. In the first stage the underlying hazards and between-study variance are estimated using the marginals, while a second stage is used to estimate both within-arm and between-arm correlations. The methodology is illustrated with an observational study on breast cancer. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BLM74RS4/Fiocco et al. - 2009 - Meta-analysis of pairs of survival curves under he.pdf;/Users/rritaz/Zotero/storage/T365W4RF/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,correlation coefficients,multivariate,random-effects},
  language = {en},
  number = {30}
}

@article{follmann_valid_1999,
  title = {Valid {{Inference}} in {{Random Effects Meta}}-{{Analysis}}},
  author = {Follmann, Dean A. and Proschan, Michael A.},
  year = {1999},
  volume = {55},
  pages = {732--737},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.1999.00732.x},
  abstract = {Summary. The standard approach to inference for random effects meta-analysis relies on approximating the null distribution of a test statistic by a standard normal distribution. This approximation is asymptotic on k, the number of studies, and can be substantially in error in medical meta-analyses, which often have only a few studies. This paper proposes permutation and ad hoc methods for testing with the random effects model. Under the group permutation method, we randomly switch the treatment and control group labels in each trial. This idea is similar to using a permutation distribution for a community intervention trial where communities are randomized in pairs. The permutation method theoretically controls the type I error rate for typical meta-analyses scenarios. We also suggest two ad hoc procedures. Our first suggestion is to use a t-reference distribution with k - 1 degrees of freedom rather than a standard normal distribution for the usual random effects test statistic. We also investigate the use of a simple t-statistic on the reported treatment effects.},
  file = {/Users/rritaz/Zotero/storage/THYH4CHR/Follmann and Proschan - 1999 - Valid Inference in Random Effects Meta-Analysis.pdf;/Users/rritaz/Zotero/storage/FS2J2R7L/j.0006-341X.1999.00732.html},
  journal = {Biometrics},
  keywords = {Clinical trials,Permutation test,Randomization test,t-test},
  language = {en},
  number = {3}
}

@article{follmann_valid_1999-1,
  title = {Valid {{Inference}} in {{Random Effects Meta}}-{{Analysis}}},
  author = {Follmann, Dean A. and Proschan, Michael A.},
  year = {1999},
  volume = {55},
  pages = {732--737},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.1999.00732.x},
  abstract = {Summary. The standard approach to inference for random effects meta-analysis relies on approximating the null distribution of a test statistic by a standard normal distribution. This approximation is asymptotic on k, the number of studies, and can be substantially in error in medical meta-analyses, which often have only a few studies. This paper proposes permutation and ad hoc methods for testing with the random effects model. Under the group permutation method, we randomly switch the treatment and control group labels in each trial. This idea is similar to using a permutation distribution for a community intervention trial where communities are randomized in pairs. The permutation method theoretically controls the type I error rate for typical meta-analyses scenarios. We also suggest two ad hoc procedures. Our first suggestion is to use a t-reference distribution with k - 1 degrees of freedom rather than a standard normal distribution for the usual random effects test statistic. We also investigate the use of a simple t-statistic on the reported treatment effects.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.1999.00732.x},
  file = {/Users/rritaz/Zotero/storage/SPMX5PZJ/Follmann and Proschan - 1999 - Valid Inference in Random Effects Meta-Analysis.pdf;/Users/rritaz/Zotero/storage/WD8N4SYC/j.0006-341X.1999.00732.html},
  journal = {Biometrics},
  keywords = {Clinical trials,Permutation test,Randomization test,t-test},
  language = {en},
  number = {3}
}

@article{for_the_3cia_collaboration_multiple_2017,
  title = {Multiple {{Score Comparison}}: A Network Meta-Analysis Approach to Comparison and External Validation of Prognostic Scores},
  shorttitle = {Multiple {{Score Comparison}}},
  author = {{for the 3CIA collaboration} and Haile, Sarah R. and Guerra, Beniamino and Soriano, Joan B. and Puhan, Milo A.},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {172},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0433-2},
  abstract = {Background: Prediction models and prognostic scores have been increasingly popular in both clinical practice and clinical research settings, for example to aid in risk-based decision making or control for confounding. In many medical fields, a large number of prognostic scores are available, but practitioners may find it difficult to choose between them due to lack of external validation as well as lack of comparisons between them. Methods: Borrowing methodology from network meta-analysis, we describe an approach to Multiple Score Comparison meta-analysis (MSC) which permits concurrent external validation and comparisons of prognostic scores using individual patient data (IPD) arising from a large-scale international collaboration. We describe the challenges in adapting network meta-analysis to the MSC setting, for instance the need to explicitly include correlations between the scores on a cohort level, and how to deal with many multi-score studies. We propose first using IPD to make cohort-level aggregate discrimination or calibration scores, comparing all to a common comparator. Then, standard network meta-analysis techniques can be applied, taking care to consider correlation structures in cohorts with multiple scores. Transitivity, consistency and heterogeneity are also examined. Results: We provide a clinical application, comparing prognostic scores for 3-year mortality in patients with chronic obstructive pulmonary disease using data from a large-scale collaborative initiative. We focus on the discriminative properties of the prognostic scores. Our results show clear differences in performance, with ADO and eBODE showing higher discrimination with respect to mortality than other considered scores. The assumptions of transitivity and local and global consistency were not violated. Heterogeneity was small. Conclusions: We applied a network meta-analytic methodology to externally validate and concurrently compare the prognostic properties of clinical scores. Our large-scale external validation indicates that the scores with the best discriminative properties to predict 3 year mortality in patients with COPD are ADO and eBODE.},
  file = {/Users/rritaz/Zotero/storage/SMPHVL2A/for the 3CIA collaboration et al. - 2017 - Multiple Score Comparison a network meta-analysis.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Individual Patient Data IPD,network meta-analysis,physical/biological fields},
  language = {en},
  number = {1}
}

@article{franchini_accounting_2012,
  title = {Accounting for Correlation in Network Meta-Analysis with Multi-Arm Trials},
  author = {Franchini, A. J. and Dias, S. and Ades, A. E. and Jansen, J. P. and Welton, N. J.},
  year = {2012},
  volume = {3},
  pages = {142--160},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1049},
  abstract = {Multi-arm trials (trials with more than two arms) are particularly valuable forms of evidence for network meta-analysis (NMA). Trial results are available either as arm-level summaries, where effect measures are reported for each arm, or as contrast-level summaries, where the differences in effect between arms compare with the control arm chosen for the trial. We show that likelihood-based inference in both contrast-level and arm-level formats is identical if there are only two-arm trials, but that if there are multi-arm trials, results from the contrast-level format will be incorrect unless correlations are accounted for in the likelihood. We review Bayesian and frequentist software for NMA with multi-arm trials that can account for this correlation and give an illustrative example of the difference in estimates that can be introduced if the correlations are not incorporated. We discuss methods of imputing correlations when they cannot be derived from the reported results and urge trialists to report the standard error for the control arm even if only contrast-level summaries are reported. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/XA4RVW6L/Franchini et al. - 2012 - Accounting for correlation in network meta-analysi.pdf;/Users/rritaz/Zotero/storage/Q63PMB83/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlated effects,network meta-analysis},
  language = {en},
  number = {2}
}

@article{freeman_identifying_2019,
  title = {Identifying Inconsistency in Network Meta-Analysis: {{Is}} the Net Heat Plot a Reliable Method?},
  shorttitle = {Identifying Inconsistency in Network Meta-Analysis},
  author = {Freeman, Suzanne C. and Fisher, David and White, Ian R. and Auperin, Anne and Carpenter, James R.},
  year = {2019},
  volume = {38},
  pages = {5547--5564},
  issn = {1097-0258},
  doi = {10.1002/sim.8383},
  abstract = {One of the biggest challenges for network meta-analysis is inconsistency, which occurs when the direct and indirect evidence conflict. Inconsistency causes problems for the estimation and interpretation of treatment effects and treatment contrasts. Krahn and colleagues proposed the net heat approach as a graphical tool for identifying and locating inconsistency within a network of randomized controlled trials. For networks with a treatment loop, the net heat plot displays statistics calculated by temporarily removing each design one at a time, in turn, and assessing the contribution of each remaining design to the inconsistency. The net heat plot takes the form of a matrix which is displayed graphically with coloring indicating the degree of inconsistency in the network. Applied to a network of individual participant data assessing overall survival in 7531 patients with lung cancer, we were surprised to find no evidence of important inconsistency from the net heat approach; this contradicted other approaches for assessing inconsistency such as the Bucher approach, Cochran's Q statistic, node-splitting, and the inconsistency parameter approach, which all suggested evidence of inconsistency within the network at the 5\% level. Further theoretical work shows that the calculations underlying the net heat plot constitute an arbitrary weighting of the direct and indirect evidence which may be misleading. We illustrate this further using a simulation study and a network meta-analysis of 10 treatments for diabetes. We conclude that the net heat plot does not reliably signal inconsistency or identify designs that cause inconsistency.},
  file = {/Users/rritaz/Zotero/storage/JJMBWEN2/Freeman et al. - 2019 - Identifying inconsistency in network meta-analysis.pdf;/Users/rritaz/Zotero/storage/USHPLUUM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,network meta-analysis},
  language = {en},
  number = {29}
}

@article{friede_meta-analysis_2017,
  title = {Meta-Analysis of Few Small Studies in Orphan Diseases},
  author = {Friede, Tim and R{\"o}ver, Christian and Wandel, Simon and Neuenschwander, Beat},
  year = {2017},
  volume = {8},
  pages = {79--91},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1217},
  abstract = {Meta-analyses in orphan diseases and small populations generally face particular problems, including small numbers of studies, small study sizes and heterogeneity of results. However, the heterogeneity is difficult to estimate if only very few studies are included. Motivated by a systematic review in immunosuppression following liver transplantation in children, we investigate the properties of a range of commonly used frequentist and Bayesian procedures in simulation studies. Furthermore, the consequences for interval estimation of the common treatment effect in random-effects meta-analysis are assessed. The Bayesian credibility intervals using weakly informative priors for the between-trial heterogeneity exhibited coverage probabilities in excess of the nominal level for a range of scenarios considered. However, they tended to be shorter than those obtained by the Knapp\textendash Hartung method, which were also conservative. In contrast, methods based on normal quantiles exhibited coverages well below the nominal levels in many scenarios. With very few studies, the performance of the Bayesian credibility intervals is of course sensitive to the specification of the prior for the between-trial heterogeneity. In conclusion, the use of weakly informative priors as exemplified by half-normal priors (with a scale of 0.5 or 1.0) for log odds ratios is recommended for applications in rare diseases. \textcopyright{} 2016 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/52B4Y2QF/Friede et al. - 2017 - Meta-analysis of few small studies in orphan disea.pdf;/Users/rritaz/Zotero/storage/3R3MLYW2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,effect size combination (small sample \& discrete),small meta-analysis},
  language = {en},
  number = {1}
}

@article{friedman_estimators_2000,
  title = {Estimators of {{Random Effects Variance Components}} in {{Meta}}-{{Analysis}}},
  author = {Friedman, Lynn},
  year = {2000},
  month = mar,
  volume = {25},
  pages = {1--12},
  issn = {1076-9986},
  doi = {10.3102/10769986025001001},
  abstract = {In meta-analyses, groups of study effect sizes often do not fit the model of a single population with only sampling, or estimation, variance differentiating the estimates. If the effect sizes in a group of studies are not homogeneous, a random effects model should be calculated, and a variance component for the random effect estimated. This estimate can be made in several ways, but two closed form estimators are in common use. The comparative efficiency of the two is the focus of this report. We show here that these estimators vary in relative efficiency with the actual size of the random effects model variance component. The latter depends on the study effect sizes. The closed form estimators are linear functions of quadratic forms whose moments can be calculated according to a well-known theorem in linear models. We use this theorem to derive the variances of the estimators, and show that one of them is smaller when the random effects model variance is near zero; however, the variance of the other is smaller when the model variance is larger. This leads to conclusions about their relative efficiency.},
  file = {/Users/rritaz/Zotero/storage/RLN6C5LE/Friedman - 2000 - Estimators of Random Effects Variance Components i.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {heterogeneity estimators,random effects models,random-effects},
  number = {1}
}

@article{friedrich_inclusion_2007,
  title = {Inclusion of Zero Total Event Trials in Meta-Analyses Maintains Analytic Consistency and Incorporates All Available Data},
  author = {Friedrich, Jan O and Adhikari, Neill KJ and Beyene, Joseph},
  year = {2007},
  month = dec,
  volume = {7},
  pages = {5},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-7-5},
  abstract = {Background: Meta-analysis handles randomized trials with no outcome events in both treatment and control arms inconsistently, including them when risk difference (RD) is the effect measure but excluding them when relative risk (RR) or odds ratio (OR) are used. This study examined the influence of such trials on pooled treatment effects. Methods: Analysis with and without zero total event trials of three illustrative published metaanalyses with a range of proportions of zero total event trials, treatment effects, and heterogeneity using inverse variance weighting and random effects that incorporates between-study heterogeneity. Results: Including zero total event trials in meta-analyses moves the pooled estimate of treatment effect closer to nil, decreases its confidence interval and decreases between-study heterogeneity. For RR and OR, inclusion of such trials causes small changes, even when they comprise the large majority of included trials. For RD, the changes are more substantial, and in extreme cases can eliminate a statistically significant effect estimate. Conclusion: To include all relevant data regardless of effect measure chosen, reviewers should also include zero total event trials when calculating pooled estimates using OR and RR.},
  file = {/Users/rritaz/Zotero/storage/36AJKN6X/Friedrich et al. - 2007 - Inclusion of zero total event trials in meta-analy.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,effect size combination (small sample \& discrete)},
  language = {en},
  number = {1}
}

@article{friedrich_ratio_2012,
  title = {Ratio of Geometric Means to Analyze Continuous Outcomes in Meta-Analysis: Comparison to Mean Differences and Ratio of Arithmetic Means Using Empiric Data and Simulation},
  shorttitle = {Ratio of Geometric Means to Analyze Continuous Outcomes in Meta-Analysis},
  author = {Friedrich, Jan O. and Adhikari, Neill K. J. and Beyene, Joseph},
  year = {2012},
  volume = {31},
  pages = {1857--1886},
  issn = {1097-0258},
  doi = {10.1002/sim.4501},
  abstract = {Meta-analyses pooling continuous outcomes can use mean differences (MD), standardized MD (MD in pooled standard deviation units, SMD), or ratio of arithmetic means (RoM). Recently, ratio of geometric means using ad hoc (RoGM ad hoc) or Taylor series (RoGM Taylor) methods for estimating variances have been proposed as alternative effect measures for skewed continuous data. Skewed data are suggested for summary measures of clinical parameters restricted to positive values which have large coefficients of variation (CV). Our objective was to compare performance characteristics of RoGM ad hoc and RoGM Taylor to MD, SMD, and RoM. We used empiric data from systematic reviews reporting continuous outcomes and selected from each the meta-analysis with the most and at least 5 trials (Cochrane Database [2008, Issue 1]). We supplemented this with simulations conducted with representative parameters. Pooled results were calculated using each effect measure. Of the reviews, 232/5053 met the inclusion criteria. Empiric data and simulation showed that RoGM ad hoc exhibits more extreme treatment effects and greater heterogeneity than all other effect measures. Compared with MD, SMD, and RoM, RoGM Taylor exhibits similar treatment effects, more heterogeneity when CV {$\leq$}0.7, and less heterogeneity when CV {$>$} 0.7. In conclusion, RoGM Taylor may be considered for pooling continuous outcomes in meta-analysis when data are skewed, but RoGM ad hoc should not be used. However, clinicians' lack of familiarity with geometric means combined with acceptable performance characteristics of RoM in most situations suggests that RoM may be the preferable ratio method for pooling continuous outcomes in meta-analysis. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2NAZ53IF/Friedrich et al. - 2012 - Ratio of geometric means to analyze continuous out.pdf;/Users/rritaz/Zotero/storage/29HHUK2K/sim.html},
  journal = {Statistics in Medicine},
  keywords = {continuous effect sizes,effect size estimation (series)},
  language = {en},
  number = {17}
}

@article{fu_estimating_2007,
  title = {Estimating Risk of Breast Cancer in Carriers of {{BRCA1}} and {{BRCA2}} Mutations: A Meta-Analytic Approach},
  shorttitle = {Estimating Risk of Breast Cancer in Carriers of {{BRCA1}} and {{BRCA2}} Mutations},
  author = {Fu, Rongwei and Harris, Emily L. and Helfand, Mark and Nelson, Heidi D.},
  year = {2007},
  volume = {26},
  pages = {1775--1787},
  issn = {1097-0258},
  doi = {10.1002/sim.2811},
  abstract = {Estimates of penetrance (or risk) of breast cancer among BRCA mutation carriers in published studies are heterogeneous, prohibiting direct combined estimates. Estimates of prevalence of BRCA mutations are more homogeneous and could allow combined estimates of prevalence. We propose a combined estimator of penetrance from combined estimates of the prevalence of BRCA mutations in women with and without breast cancer and from the probability of breast cancer by using Bayes' Theorem. The relative risk of having breast cancer with positive family history and the prevalence of positive family history contribute to the combined estimate of penetrance if family history is present. The combined estimate incorporates variation in estimates from different resources. The method is illustrated by using data from Ashkenazi Jewish women unselected for family history and for those with family history. Risks of breast cancer conferred by BRCA1 and BRCA2 mutations are estimated to be 8.39 per cent (6.56, 10.68 per cent) and 2.66 per cent (1.85, 3.82 per cent) by 40 years old, and 47.45 per cent (37.39, 57.72 per cent) and 31.85 per cent (23.72, 41.26 per cent) by 75 years old, respectively. For those with family history, risks of breast cancer conferred by BRCA mutations appear to be higher. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YE8F4VI6/Fu et al. - 2007 - Estimating risk of breast cancer in carriers of BR.pdf;/Users/rritaz/Zotero/storage/A9VDZ4PN/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),physical/biological fields},
  language = {en},
  number = {8}
}

@article{furlow_meta-analytic_2005,
  title = {Meta-{{Analytic Methods}} of {{Pooling Correlation Matrices}} for {{Structural Equation Modeling Under Different Patterns}} of {{Missing Data}}},
  author = {Furlow, Carolyn F. and Beretvas, S. Natasha},
  year = {2005},
  month = jun,
  volume = {10},
  pages = {227--254},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.10.2.227},
  abstract = {Three methods of synthesizing correlations for meta-analytic structural equation modeling (SEM) under different degrees and mechanisms of missingness were compared for the estimation of correlation and SEM parameters and goodness-of-fit indices by using Monte Carlo simulation techniques. A revised generalized least squares (GLS) method for synthesizing correlations, weighted-covariance GLS (W-COV GLS), was compared with univariate weighting with untransformed correlations (univariate r) and univariate weighting with Fisher's z-transformed correlations (univariate z). These 3 methods were crossed with listwise and pairwise deletion. Univariate z and W-COV GLS performed similarly, with W-COV GLS providing slightly better estimation of parameters and more correct model rejection rates. Missing not at random data produced high levels of relative bias in correlation and model parameter estimates and higher incorrect SEM model rejection rates. Pairwise deletion resulted in inflated standard errors for all synthesis methods and higher incorrect rejection rates for the SEM model with univariate weighting procedures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/LVENI7MR/Furlow and Beretvas - 2005 - Meta-Analytic Methods of Pooling Correlation Matri.pdf},
  journal = {Psychological Methods},
  keywords = {correlated effects,correlation coefficients,missing data,modeling effect size variation (covariates),multivariate},
  number = {2}
}

@article{galwey_supplementation_2017,
  title = {Supplementation of a Clinical Trial by Historical Control Data: Is the Prospect of Dynamic Borrowing an Illusion?},
  shorttitle = {Supplementation of a Clinical Trial by Historical Control Data},
  author = {Galwey, N. W.},
  year = {2017},
  volume = {36},
  pages = {899--916},
  issn = {1097-0258},
  doi = {10.1002/sim.7180},
  abstract = {There are strong arguments, ethical, logistical and financial, for supplementing the evidence from a new clinical trial using data from previous trials with similar control treatments. There is a consensus that historical information should be down-weighted or discounted relative to information from the new trial, but the determination of the appropriate degree of discounting is a major difficulty. The degree of discounting can be represented by a bias parameter with specified variance, but a comparison between the historical and new data gives only a poor estimate of this variance. Hence, if no strong assumption is made concerning its value (i.e. if `dynamic borrowing' is practiced), there may be little or no gain from using the historical data, in either frequentist terms (type I error rate and power) or Bayesian terms (posterior distribution of the treatment effect). It is therefore best to compare the consequences of a range of assumptions. This paper presents a clear, simple graphical tool for doing so on the basis of the mean square error, and illustrates its use with historical data from clinical trials in amyotrophic lateral sclerosis. This approach makes it clear that different assumptions can lead to very different conclusions. External information can sometimes provide strong additional guidance, but different stakeholders may still make very different judgements concerning the appropriate degree of discounting. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GMGQ5H6N/Galwey - 2017 - Supplementation of a clinical trial by historical .pdf;/Users/rritaz/Zotero/storage/RGUVG84U/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques},
  language = {en},
  number = {6}
}

@article{gasparrini_multivariate_2012,
  title = {Multivariate Meta-Analysis for Non-Linear and Other Multi-Parameter Associations},
  author = {Gasparrini, A. and Armstrong, B. and Kenward, M. G.},
  year = {2012},
  volume = {31},
  pages = {3821--3839},
  issn = {1097-0258},
  doi = {10.1002/sim.5471},
  abstract = {In this paper, we formalize the application of multivariate meta-analysis and meta-regression to synthesize estimates of multi-parameter associations obtained from different studies. This modelling approach extends the standard two-stage analysis used to combine results across different sub-groups or populations. The most straightforward application is for the meta-analysis of non-linear relationships, described for example by regression coefficients of splines or other functions, but the methodology easily generalizes to any setting where complex associations are described by multiple correlated parameters. The modelling framework of multivariate meta-analysis is implemented in the package mvmeta within the statistical environment R. As an illustrative example, we propose a two-stage analysis for investigating the non-linear exposure\textendash response relationship between temperature and non-accidental mortality using time-series data from multiple cities. Multivariate meta-analysis represents a useful analytical tool for studying complex associations through a two-stage procedure. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/I5GW6LA5/Gasparrini et al. - 2012 - Multivariate meta-analysis for non-linear and othe.pdf;/Users/rritaz/Zotero/storage/7VR8KLUP/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),multivariate},
  language = {en},
  number = {29}
}

@article{gasparrini_reducing_2013,
  title = {Reducing and Meta-Analysing Estimates from Distributed Lag Non-Linear Models},
  author = {Gasparrini, Antonio and Armstrong, Ben},
  year = {2013},
  pages = {10},
  abstract = {Background: The two-stage time series design represents a powerful analytical tool in environmental epidemiology. Recently, models for both stages have been extended with the development of distributed lag non-linear models (DLNMs), a methodology for investigating simultaneously non-linear and lagged relationships, and multivariate meta-analysis, a methodology to pool estimates of multi-parameter associations. However, the application of both methods in two-stage analyses is prevented by the high-dimensional definition of DLNMs. Methods: In this contribution we propose a method to synthesize DLNMs to simpler summaries, expressed by a reduced set of parameters of one-dimensional functions, which are compatible with current multivariate meta-analytical techniques. The methodology and modelling framework are implemented in R through the packages dlnm and mvmeta. Results: As an illustrative application, the method is adopted for the two-stage time series analysis of temperature-mortality associations using data from 10 regions in England and Wales. R code and data are available as supplementary online material. Discussion and Conclusions: The methodology proposed here extends the use of DLNMs in two-stage analyses, obtaining meta-analytical estimates of easily interpretable summaries from complex non-linear and delayed associations. The approach relaxes the assumptions and avoids simplifications required by simpler modelling approaches.},
  file = {/Users/rritaz/Zotero/storage/HA42PD8P/Gasparrini and Armstrong - 2013 - Reducing and meta-analysing estimates from distrib.pdf},
  keywords = {multivariate},
  language = {en}
}

@article{gebregziabher_fitting_2012,
  title = {Fitting Parametric Random Effects Models in Very Large Data Sets with Application to {{VHA}} National Data},
  author = {Gebregziabher, Mulugeta and Egede, Leonard and Gilbert, Gregory E and Hunt, Kelly and Nietert, Paul J and Mauldin, Patrick},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {163},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-163},
  abstract = {Background: With the current focus on personalized medicine, patient/subject level inference is often of key interest in translational research. As a result, random effects models (REM) are becoming popular for patient level inference. However, for very large data sets that are characterized by large sample size, it can be difficult to fit REM using commonly available statistical software such as SAS since they require inordinate amounts of computer time and memory allocations beyond what are available preventing model convergence. For example, in a retrospective cohort study of over 800,000 Veterans with type 2 diabetes with longitudinal data over 5 years, fitting REM via generalized linear mixed modeling using currently available standard procedures in SAS (e.g. PROC GLIMMIX) was very difficult and same problems exist in Stata's gllamm or R's lme packages. Thus, this study proposes and assesses the performance of a meta regression approach and makes comparison with methods based on sampling of the full data. Data: We use both simulated and real data from a national cohort of Veterans with type 2 diabetes (n=890,394) which was created by linking multiple patient and administrative files resulting in a cohort with longitudinal data collected over 5 years. Methods and results: The outcome of interest was mean annual HbA1c measured over a 5 years period. Using this outcome, we compared parameter estimates from the proposed random effects meta regression (REMR) with estimates based on simple random sampling and VISN (Veterans Integrated Service Networks) based stratified sampling of the full data. Our results indicate that REMR provides parameter estimates that are less likely to be biased with tighter confidence intervals when the VISN level estimates are homogenous. Conclusion: When the interest is to fit REM in repeated measures data with very large sample size, REMR can be used as a good alternative. It leads to reasonable inference for both Gaussian and non-Gaussian responses if parameter estimates are homogeneous across VISNs.},
  file = {/Users/rritaz/Zotero/storage/MZKMJSNE/Gebregziabher et al. - 2012 - Fitting parametric random effects models in very l.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {GLM MA models,Individual Patient Data IPD,physical/biological fields,random-effects},
  language = {en},
  number = {1}
}

@article{gelfand_bias_2006,
  title = {Bias Resulting from the Use of `Assay Sensitivity' as an Inclusion Criterion for Meta-Analysis},
  author = {Gelfand, Lois A. and Strunk, Daniel R. and Tu, Xin M. and Noble, Ronald E. S. and DeRubeis, Robert J.},
  year = {2006},
  volume = {25},
  pages = {943--955},
  issn = {1097-0258},
  doi = {10.1002/sim.2240},
  abstract = {Assay sensitivity has been proposed as a criterion for including psychiatric clinical outcome studies in meta-analyses. The authors assess the performance of assay sensitivity as a method for determining study appropriateness for meta-analysis by calculating expected standard drug vs placebo effect sizes for various combinations of high quality and flawed studies. In the absence of flawed studies, expected effect sizes are close to unbiased only when sample sizes are very large. In the presence of flawed studies, expected effect sizes tend to be substantially biased except under simultaneous conditions of high power, a large proportion of flawed studies, and a population standard vs placebo effect size of flawed studies considerably lower than that of high quality studies. The authors conclude that this method is not robust and can lead to serious bias. Unless it can be shown that specific conditions hold, assay sensitivity should not be used to make quality judgments of studies. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/T7QM3L9I/Gelfand et al. - 2006 - Bias resulting from the use of ‘assay sensitivity’.pdf;/Users/rritaz/Zotero/storage/A8SHWSFH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques},
  language = {en},
  number = {6}
}

@article{ghidey_semi-parametric_2007,
  title = {Semi-Parametric Modelling of the Distribution of the Baseline Risk in Meta-Analysis},
  author = {Ghidey, W. and Lesaffre, E. and Stijnen, T.},
  year = {2007},
  volume = {26},
  pages = {5434--5444},
  issn = {1097-0258},
  doi = {10.1002/sim.3066},
  abstract = {In meta-analysis of clinical trials, often meta-regression analyses are performed to explain the heterogeneity in treatment effects that usually exist between trials. A popular explanatory variable is the risk observed in the control group, the baseline risk. The relationship between the treatment effect and the baseline risk is investigated by fitting a linear model that allows randomness on the true baseline risk by assuming a normal distribution with unknown mean and variance. However, the normality assumption could be too strong to adequately describe the underlying distribution. Therefore, we developed a new semi-parametric method that relaxes the normality assumption to a more flexible and general distribution. We applied a penalized Gaussian mixture distribution to represent the baseline risk distribution. Furthermore, a bivariate hierarchical model is formulated in order to take into account the correlation between the baseline and treatment effect. To fit the proposed model, a penalized likelihood function is maximized by an Expectation Maximization (EM) algorithm. We illustrate our method on a number of simulated data sets and on a published meta-analysis data set. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SNILD247/Ghidey et al. - 2007 - Semi-parametric modelling of the distribution of t.pdf;/Users/rritaz/Zotero/storage/PY8KFSJV/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,modeling effect size variation (covariates)},
  language = {en},
  number = {30}
}

@book{gillett_metric_nodate,
  title = {The {{Metric Comparability}} of {{Meta}}-{{Analytic Effect}}-{{Size Estimators From Factorial Designs}}},
  author = {Gillett, Raphael},
  abstract = {The primary studies in a meta-analysis of standardized mean differences generally include a mixture of single-factor t tests and multifactor analysis of variance de-signs. Accordingly, there is a need for effect-size measures in multifactor designs that are metrically comparable with measures in single-factor t tests. Two models, the variance-preservation model and the variance-reduction model, provide a for-mal description of the 2 principal routes by which a single-factor design may evolve into a higher order factorial design. New metrically comparable effect-size mea-sures and estimators are developed for designs that contain variance-preservation factors, variance-reduction factors, or a mixture of both types of factors. A statis-tical test for checking the validity of model assumptions is presented. A problem encountered by every meta-analyst is how to obtain estimates of the size of a psychological effect that are comparable with each other, from stud-ies in the literature that have used different statistical},
  file = {/Users/rritaz/Zotero/storage/9BQ84WCF/Gillett - The Metric Comparability of Meta-Analytic Effect-S.pdf;/Users/rritaz/Zotero/storage/W6VIUZCT/summary.html},
  keywords = {continuous effect sizes,random-effects}
}

@article{giraudeau_sample_2016,
  title = {Sample Size Calculation for Meta-Epidemiological Studies},
  author = {Giraudeau, Bruno and Higgins, Julian P. T. and Tavernier, Elsa and Trinquart, Ludovic},
  year = {2016},
  volume = {35},
  pages = {239--250},
  issn = {1097-0258},
  doi = {10.1002/sim.6627},
  abstract = {Meta-epidemiological studies are used to compare treatment effect estimates between randomized clinical trials with and without a characteristic of interest. To our knowledge, there is presently nothing to help researchers to a priori specify the required number of meta-analyses to be included in a meta-epidemiological study. We derived a theoretical power function and sample size formula in the framework of a hierarchical model that allows for variation in the impact of the characteristic between trials within a meta-analysis and between meta-analyses. A simulation study revealed that the theoretical function overestimated power (because of the assumption of equal weights for each trial within and between meta-analyses). We also propose a simulation approach that allows for relaxing the constraints used in the theoretical approach and is more accurate. We illustrate that the two variables that mostly influence power are the number of trials per meta-analysis and the proportion of trials with the characteristic of interest. We derived a closed-form power function and sample size formula for estimating the impact of trial characteristics in meta-epidemiological studies. Our analytical results can be used as a `rule of thumb' for sample size calculation for a meta-epidemiologic study. A more accurate sample size can be derived with a simulation study. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/FRGJK2J8/Giraudeau et al. - 2016 - Sample size calculation for meta-epidemiological s.pdf;/Users/rritaz/Zotero/storage/RK3Q3YWW/sim.html},
  journal = {Statistics in Medicine},
  keywords = {power},
  language = {en},
  number = {2}
}

@article{gjerdevik_improving_2014,
  title = {Improving the Error Rates of the {{Begg}} and {{Mazumdar}} Test for Publication Bias in Fixed Effects Meta-Analysis},
  author = {Gjerdevik, Miriam and Heuch, Ivar},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {109},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-109},
  abstract = {Background: The rank correlation test introduced by Begg and Mazumdar is extensively used in meta-analysis to test for publication bias in clinical and epidemiological studies. It is based on correlating the standardized treatment effect with the variance of the treatment effect using Kendall's tau as the measure of association. To our knowledge, the operational characteristics regarding the significance level of the test have not, however, been fully assessed. Methods: We propose an alternative rank correlation test to improve the error rates of the original Begg and Mazumdar test. This test is based on the simulated distribution of the estimated measure of association, conditional on sampling variances. Furthermore, Spearman's rho is suggested as an alternative rank correlation coefficient. The attained level and power of the tests are studied by simulations of meta-analyses assuming the fixed effects model. Results: The significance levels of the original Begg and Mazumdar test often deviate considerably from the nominal level, the null hypothesis being rejected too infrequently. It is proven mathematically that the assumptions for using the rank correlation test are not strictly satisfied. The pairs of variables fail to be independent, and there is a correlation between the standardized effect sizes and sampling variances under the null hypothesis of no publication bias. In the meta-analysis setting, the adverse consequences of a false negative test are more profound than the disadvantages of a false positive test. Our alternative test improves the error rates in fixed effects meta-analysis. Its significance level equals the nominal value, and the Type II error rate is reduced. In small data sets Spearman's rho should be preferred to Kendall's tau as the measure of association. Conclusions: As the attained significance levels of the test introduced by Begg and Mazumdar often deviate greatly from the nominal level, modified rank correlation tests, improving the error rates, should be preferred when testing for publication bias assuming fixed effects meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/IA94GFSJ/Gjerdevik and Heuch - 2014 - Improving the error rates of the Begg and Mazumdar.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {publication bias},
  language = {en},
  number = {1}
}

@article{gnambs_parameter_2016,
  title = {Parameter Accuracy in Meta-Analyses of Factor Structures},
  author = {Gnambs, Timo and Staufenbiel, Thomas},
  year = {2016},
  volume = {7},
  pages = {168--186},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1190},
  abstract = {Two new methods for the meta-analysis of factor loadings are introduced and evaluated by Monte Carlo simulations. The direct method pools each factor loading individually, whereas the indirect method synthesizes correlation matrices reproduced from factor loadings. The results of the two simulations demonstrated that the accuracy of meta-analytical derived factor loadings is primarily affected by characteristics of the pooled factor structures (e.g., model error, communality) and to a lesser degree by the sample size of the primary studies and the number of included samples. The choice of the meta-analytical method had a minor impact. In general, the indirect method produced somewhat less biased estimates, particularly for small-sample studies. Thus, the indirect method presents a viable alternative for the meta-analysis of factor structures that could also address moderator hypotheses. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/7TZGNPLQ/Gnambs and Staufenbiel - 2016 - Parameter accuracy in meta-analyses of factor stru.pdf;/Users/rritaz/Zotero/storage/JUW7CAZV/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlation coefficients},
  language = {en},
  number = {2}
}

@article{gomes_handling_2016,
  title = {Handling Incomplete Correlated Continuous and Binary Outcomes in Meta-Analysis of Individual Participant Data},
  author = {Gomes, Manuel and Hatfield, Laura and Normand, Sharon-Lise},
  year = {2016},
  volume = {35},
  pages = {3676--3689},
  issn = {1097-0258},
  doi = {10.1002/sim.6969},
  abstract = {Meta-analysis of individual participant data (IPD) is increasingly utilised to improve the estimation of treatment effects, particularly among different participant subgroups. An important concern in IPD meta-analysis relates to partially or completely missing outcomes for some studies, a problem exacerbated when interest is on multiple discrete and continuous outcomes. When leveraging information from incomplete correlated outcomes across studies, the fully observed outcomes may provide important information about the incompleteness of the other outcomes. In this paper, we compare two models for handling incomplete continuous and binary outcomes in IPD meta-analysis: a joint hierarchical model and a sequence of full conditional mixed models. We illustrate how these approaches incorporate the correlation across the multiple outcomes and the between-study heterogeneity when addressing the missing data. Simulations characterise the performance of the methods across a range of scenarios which differ according to the proportion and type of missingness, strength of correlation between outcomes and the number of studies. The joint model provided confidence interval coverage consistently closer to nominal levels and lower mean squared error compared with the fully conditional approach across the scenarios considered. Methods are illustrated in a meta-analysis of randomised controlled trials comparing the effectiveness of implantable cardioverter-defibrillator devices alone to implantable cardioverter-defibrillator combined with cardiac resynchronisation therapy for treating patients with chronic heart failure. \textcopyright{} 2016 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/XUXC8248/Gomes et al. - 2016 - Handling incomplete correlated continuous and bina.pdf;/Users/rritaz/Zotero/storage/3PZ7QYIH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,missing data},
  language = {en},
  number = {21}
}

@article{goring_disconnected_2016,
  title = {Disconnected by Design: Analytic Approach in Treatment Networks Having No Common Comparator},
  shorttitle = {Disconnected by Design},
  author = {Goring, S. M. and Gustafson, P. and Liu, Y. and Saab, S. and Cline, S. K. and Platt, R. W.},
  year = {2016},
  volume = {7},
  pages = {420--432},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1204},
  abstract = {In a network meta-analysis, comparators of interest are ideally connected either directly or via one or more common comparators. However, in some therapeutic areas, the evidence base can produce networks that are disconnected, in which there is neither direct evidence nor an indirect route for comparing certain treatments within the network. Disconnected networks may occur when there is no accepted standard of care, when there has been a major paradigm shift in treatment, when use of a standard of care or placebo is debated, when a product receives orphan drug designation, or when there is a large number of available treatments and many accepted standards of care. These networks pose a challenge to decision makers and clinicians who want to estimate the relative efficacy and safety of newly available agents against alternatives. A currently recommended approach is to insert a distribution for the unknown treatment effect(s) into a network meta-analysis model of treatment effect. In this paper, we describe this approach along with two alternative Bayesian models that can accommodate disconnected networks. Additionally, we present a theoretical framework to guide the choice between modeling approaches. This paper presents researchers with the tools and framework for selecting appropriate models for indirect comparison of treatment efficacies when challenged with a disconnected framework. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Y4RAHD68/Goring et al. - 2016 - Disconnected by design analytic approach in treat.pdf;/Users/rritaz/Zotero/storage/8LAV4SSL/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis},
  language = {en},
  number = {4}
}

@article{govan_controlling_2010,
  title = {Controlling Ecological Bias in Evidence Synthesis of Trials Reporting on Collapsed and Overlapping Covariate Categories},
  author = {Govan, L. and Ades, A. E. and Weir, C. J. and Welton, N. J. and Langhorne, P.},
  year = {2010},
  volume = {29},
  pages = {1340--1356},
  issn = {1097-0258},
  doi = {10.1002/sim.3869},
  abstract = {Meta-analysis of randomized controlled trials based on aggregated data is vulnerable to ecological bias if trial results are pooled over covariates that influence the outcome variable, even when the covariate does not modify the treatment effect, or is not associated with the treatment. This paper shows how, when trial results are aggregated over different levels of covariates, the within-study covariate distribution, and the effects of both covariates and treatments can be simultaneously estimated, and ecological bias reduced. Bayesian Markov chain Monte Carlo methods are used. The method is applied to a mixed treatment comparison evidence synthesis of six alternative approaches to post-stroke inpatient care. Results are compared with a model using only the stratified covariate data available, where each stratum is treated as a separate trial, and a model using fully aggregated data, where no covariate data are used. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/6J2E7YEH/Govan et al. - 2010 - Controlling ecological bias in evidence synthesis .pdf;/Users/rritaz/Zotero/storage/W4IPWUSC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,modeling effect size variation (covariates)},
  language = {en},
  number = {12}
}

@article{groenwold_subgroup_2010,
  title = {Subgroup Effects despite Homogeneous Heterogeneity Test Results},
  author = {Groenwold, Rolf HH and Rovers, Maroeska M and Lubsen, Jacobus and van der Heijden, Geert JMG},
  year = {2010},
  month = may,
  volume = {10},
  pages = {43},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-10-43},
  abstract = {Background Statistical tests of heterogeneity are very popular in meta-analyses, as heterogeneity might indicate subgroup effects. Lack of demonstrable statistical heterogeneity, however, might obscure clinical heterogeneity, meaning clinically relevant subgroup effects. Methods A qualitative, visual method to explore the potential for subgroup effects was provided by a modification of the forest plot, i.e., adding a vertical axis indicating the proportion of a subgroup variable in the individual trials. Such a plot was used to assess the potential for clinically relevant subgroup effects and was illustrated by a clinical example on the effects of antibiotics in children with acute otitis media. Results Statistical tests did not indicate heterogeneity in the meta-analysis on the effects of amoxicillin on acute otitis media (Q = 3.29, p = 0.51; I2 = 0\%; T2 = 0). Nevertheless, in a modified forest plot, in which the individual trials were ordered by the proportion of children with bilateral otitis, a clear relation between bilaterality and treatment effects was observed (which was also found in an individual patient data meta-analysis of the included trials: p-value for interaction 0.021). Conclusions A modification of the forest plot, by including an additional (vertical) axis indicating the proportion of a certain subgroup variable, is a qualitative, visual, and easy-to-interpret method to explore potential subgroup effects in studies included in meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/M8X9AHI9/Groenwold et al. - 2010 - Subgroup effects despite homogeneous heterogeneity.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,modeling effect size variation (covariates)},
  pmcid = {PMC2900281},
  pmid = {20478021}
}

@article{gsteiger_using_2013,
  title = {Using Historical Control Information for the Design and Analysis of Clinical Trials with Overdispersed Count Data},
  author = {Gsteiger, Sandro and Neuenschwander, Beat and Mercier, Francois and Schmidli, Heinz},
  year = {2013},
  volume = {32},
  pages = {3609--3622},
  issn = {1097-0258},
  doi = {10.1002/sim.5851},
  abstract = {Results from clinical trials are never interpreted in isolation. Previous studies in a similar setting provide valuable information for designing a new trial. For the analysis, however, the use of trial-external information is challenging and therefore controversial, although it seems attractive from an ethical or efficiency perspective. Here, we consider the formal use of historical control data on lesion counts in a multiple sclerosis trial. The approach to incorporating historical data is Bayesian, in that historical information is captured in a prior that accounts for between-trial variability and hence leads to discounting of historical data. We extend the meta-analytic-predictive approach, a random-effects meta-analysis of historical data combined with the prediction of the parameter in the new trial, from normal to overdispersed count data of individual-patient or aggregate-trial format. We discuss the prior derivation for the lesion mean count in the control group of the new trial for two populations. For the general population (without baseline enrichment), with 1936 control patients from nine historical trials, between-trial variability was moderate to substantial, leading to a prior effective sample size of about 45 control patients. For the more homogenous population (with enrichment), with 412 control patients from five historical trials, the prior effective sample size was approximately 63 patients. Although these numbers are small relative to the historical data, they are fairly typical in settings where between-trial heterogeneity is moderate. For phase II, reducing the number of control patients by 45 or by 63 may be an attractive option in many multiple sclerosis trials. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VJILZ6GI/Gsteiger et al. - 2013 - Using historical control information for the desig.pdf;/Users/rritaz/Zotero/storage/PAFVTBID/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {21}
}

@article{guevara_meta-analytic_2004,
  title = {Meta-Analytic Methods for Pooling Rates When Follow-up Duration Varies: A Case Study},
  shorttitle = {Meta-Analytic Methods for Pooling Rates When Follow-up Duration Varies},
  author = {Guevara, James P and Berlin, Jesse A and Wolf, Fredric M},
  year = {2004},
  month = dec,
  volume = {4},
  pages = {17},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-4-17},
  abstract = {Background: Meta-analysis can be used to pool rate measures across studies, but challenges arise when follow-up duration varies. Our objective was to compare different statistical approaches for pooling count data of varying follow-up times in terms of estimates of effect, precision, and clinical interpretability. Methods: We examined data from a published Cochrane Review of asthma self-management education in children. We selected two rate measures with the largest number of contributing studies: school absences and emergency room (ER) visits. We estimated fixed- and random-effects standardized weighted mean differences (SMD), stratified incidence rate differences (IRD), and stratified incidence rate ratios (IRR). We also fit Poisson regression models, which allowed for further adjustment for clustering by study. Results: For both outcomes, all methods gave qualitatively similar estimates of effect in favor of the intervention. For school absences, SMD showed modest results in favor of the intervention (SMD -0.14, 95\% CI -0.23 to -0.04). IRD implied that the intervention reduced school absences by 1.8 days per year (IRD -0.15 days/child-month, 95\% CI -0.19 to -0.11), while IRR suggested a 14\% reduction in absences (IRR 0.86, 95\% CI 0.83 to 0.90). For ER visits, SMD showed a modest benefit in favor of the intervention (SMD -0.27, 95\% CI: -0.45 to -0.09). IRD implied that the intervention reduced ER visits by 1 visit every 2 years (IRD -0.04 visits/child-month, 95\% CI: -0.05 to -0.03), while IRR suggested a 34\% reduction in ER visits (IRR 0.66, 95\% CI 0.59 to 0.74). In Poisson models, adjustment for clustering lowered the precision of the estimates relative to stratified IRR results. For ER visits but not school absences, failure to incorporate study indicators resulted in a different estimate of effect (unadjusted IRR 0.77, 95\% CI 0.59 to 0.99). Conclusions: Choice of method among the ones presented had little effect on inference but affected the clinical interpretability of the findings. Incidence rate methods gave more clinically interpretable results than SMD. Poisson regression allowed for further adjustment for heterogeneity across studies. These data suggest that analysts who want to improve the clinical interpretability of their findings should consider incidence rate methods.},
  file = {/Users/rritaz/Zotero/storage/IA8DALKR/Guevara et al. - 2004 - Meta-analytic methods for pooling rates when follo.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,physical/biological fields,random-effects},
  language = {en},
  number = {1}
}

@article{gumedze_random_2011,
  title = {A Random Effects Variance Shift Model for Detecting and Accommodating Outliers in Meta-Analysis},
  author = {Gumedze, Freedom N and Jackson, Dan},
  year = {2011},
  month = dec,
  volume = {11},
  pages = {19},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-19},
  abstract = {Background: Meta-analysis typically involves combining the estimates from independent studies in order to estimate a parameter of interest across a population of studies. However, outliers often occur even under the random effects model. The presence of such outliers could substantially alter the conclusions in a meta-analysis. This paper proposes a methodology for identifying and, if desired, downweighting studies that do not appear representative of the population they are thought to represent under the random effects model. Methods: An outlier is taken as an observation (study result) with an inflated random effect variance. We used the likelihood ratio test statistic as an objective measure for determining whether observations have inflated variance and are therefore considered outliers. A parametric bootstrap procedure was used to obtain the sampling distribution of the likelihood ratio test statistics and to account for multiple testing. Our methods were applied to three illustrative and contrasting meta-analytic data sets. Results: For the three meta-analytic data sets our methods gave robust inferences when the identified outliers were downweighted. Conclusions: The proposed methodology provides a means to identify and, if desired, downweight outliers in meta-analysis. It does not eliminate them from the analysis however and we consider the proposed approach preferable to simply removing any or all apparently outlying results. We do not however propose that our methods in any way replace or diminish the standard random effects methodology that has proved so useful, rather they are helpful when used in conjunction with the random effects model.},
  file = {/Users/rritaz/Zotero/storage/TVPQE5VT/Gumedze and Jackson - 2011 - A random effects variance shift model for detectin.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,random-effects,robust variance estimation},
  language = {en},
  number = {1}
}

@article{gunhan_design-by-treatment_2018,
  title = {A Design-by-Treatment Interaction Model for Network Meta-Analysis and Meta-Regression with Integrated Nested {{Laplace}} Approximations},
  author = {G{\"u}nhan, Burak K{\"u}rsad and Friede, Tim and Held, Leonhard},
  year = {2018},
  volume = {9},
  pages = {179--194},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1285},
  abstract = {Network meta-analysis (NMA) is gaining popularity for comparing multiple treatments in a single analysis. Generalized linear mixed models provide a unifying framework for NMA, allow us to analyze datasets with dichotomous, continuous or count endpoints, and take into account multiarm trials, potential heterogeneity between trials and network inconsistency. To perform inference within such NMA models, the use of Bayesian methods is often advocated. The standard inference tool is Markov chain Monte Carlo (MCMC), which is computationally expensive and requires convergence diagnostics. A deterministic approach to do fully Bayesian inference for latent Gaussian models can be achieved by integrated nested Laplace approximations (INLA), which is a fast and accurate alternative to MCMC. We show how NMA models fit in the class of latent Gaussian models and how NMA models are implemented using INLA and demonstrate that the estimates obtained by INLA are in close agreement with the ones obtained by MCMC. Specifically, we emphasize the design-by-treatment interaction model with random inconsistency parameters (also known as the Jackson model). Also, we have proposed a network meta-regression model, which is constructed by incorporating trial-level covariates to the Jackson model to explain possible sources of heterogeneity and/or inconsistency in the network. A publicly available R package, nmaINLA, is developed to automate the INLA implementation of NMA models, which are considered in this paper. Three applications illustrate the use of INLA for a NMA.},
  file = {/Users/rritaz/Zotero/storage/LJM4QHQQ/Günhan et al. - 2018 - A design-by-treatment interaction model for networ.pdf;/Users/rritaz/Zotero/storage/XW4PG66I/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis},
  language = {en},
  number = {2}
}

@article{guo_bayesian_2017,
  title = {Bayesian Bivariate Meta-Analysis of Diagnostic Test Studies with Interpretable Priors},
  author = {Guo, Jingyi and Riebler, Andrea and Rue, H{\aa}vard},
  year = {2017},
  volume = {36},
  pages = {3039--3058},
  issn = {1097-0258},
  doi = {10.1002/sim.7313},
  abstract = {In a bivariate meta-analysis, the number of diagnostic studies involved is often very low so that frequentist methods may result in problems. Using Bayesian inference is particularly attractive as informative priors that add a small amount of information can stabilise the analysis without overwhelming the data. However, Bayesian analysis is often computationally demanding and the selection of the prior for the covariance matrix of the bivariate structure is crucial with little data. The integrated nested Laplace approximations method provides an efficient solution to the computational issues by avoiding any sampling, but the important question of priors remain. We explore the penalised complexity (PC) prior framework for specifying informative priors for the variance parameters and the correlation parameter. PC priors facilitate model interpretation and hyperparameter specification as expert knowledge can be incorporated intuitively. We conduct a simulation study to compare the properties and behaviour of differently defined PC priors to currently used priors in the field. The simulation study shows that the PC prior seems beneficial for the variance parameters. The use of PC priors for the correlation parameter results in more precise estimates when specified in a sensible neighbourhood around the truth. To investigate the usage of PC priors in practice, we reanalyse a meta-analysis using the telomerase marker for the diagnosis of bladder cancer and compare the results with those obtained by other commonly used modelling approaches. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3HZLFWHJ/Guo et al. - 2017 - Bayesian bivariate meta-analysis of diagnostic tes.pdf;/Users/rritaz/Zotero/storage/FQ2DF47G/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate,random-effects},
  language = {en},
  number = {19}
}

@article{guolo_double_2017,
  title = {A Double {{SIMEX}} Approach for Bivariate Random-Effects Meta-Analysis of Diagnostic Accuracy Studies},
  author = {Guolo, Annamaria},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {6},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0284-2},
  abstract = {Background: Bivariate random-effects models represent a widely accepted and recommended approach for meta-analysis of test accuracy studies. Standard likelihood methods routinely used for inference are prone to several drawbacks. Small sample size can give rise to unreliable inferential conclusions and convergence issues make the approach unappealing. This paper suggests a different methodology to address such difficulties. Methods: A SIMEX methodology is proposed. The method is a simulation-based technique originally developed as a correction strategy within the measurement error literature. It suits the meta-analysis framework as the diagnostic accuracy measures provided by each study are prone to measurement error. SIMEX can be straightforwardly adapted to cover different measurement error structures and to deal with covariates. The effortless implementation with standard software is an interesting feature of the method. Results: Extensive simulation studies highlight the improvement provided by SIMEX over likelihood approach in terms of empirical coverage probabilities of confidence intervals under different scenarios, independently of the sample size and the values of the correlation between sensitivity and specificity. A remarkable amelioration is obtained in case of deviations from the normality assumption for the random-effects distribution. From a computational point of view, the application of SIMEX is shown to be neither involved nor subject to the convergence issues affecting likelihood-based alternatives. Application of the method to a diagnostic review of the performance of transesophageal echocardiography for assessing ascending aorta atherosclerosis enables overcoming limitations of the likelihood procedure. Conclusions: The SIMEX methodology represents an interesting alternative to likelihood-based procedures for inference in meta-analysis of diagnostic accuracy studies. The approach can provide more accurate inferential conclusions, while avoiding convergence failure and numerical instabilities. The application of the method in the R programming language is possible through the code which is made available and illustrated using the real data example.},
  file = {/Users/rritaz/Zotero/storage/MFYNZAVG/Guolo - 2017 - A double SIMEX approach for bivariate random-effec.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,physical/biological fields,random-effects},
  language = {en},
  number = {1}
}

@article{guolo_flexibly_2013,
  title = {Flexibly Modeling the Baseline Risk in Meta-Analysis},
  author = {Guolo, A.},
  year = {2013},
  volume = {32},
  pages = {40--50},
  issn = {1097-0258},
  doi = {10.1002/sim.5506},
  abstract = {This paper investigates a likelihood-based approach in meta-analysis of clinical trials involving the baseline risk as explanatory variable. The approach takes account of the errors affecting the measure of either the treatment effect or the baseline risk, while facing the potential misspecification of the baseline risk distribution. To this aim, we suggest to model the baseline risk through a flexible family of distributions represented by the skew-normal. We describe how to carry out inference within this framework and evaluate the performance of the approach through simulation. The method is compared with the routine likelihood approach based on the restrictive normality assumption for the baseline risk distribution and with the weighted least-squares regression. We apply the competing approaches to the analysis of two published datasets. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JL6A9NUW/Guolo - 2013 - Flexibly modeling the baseline risk in meta-analys.pdf;/Users/rritaz/Zotero/storage/H2HN72UH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {1}
}

@article{guolo_higher-order_2012,
  title = {Higher-Order Likelihood Inference in Meta-Analysis and Meta-Regression},
  author = {Guolo, Annamaria},
  year = {2012},
  volume = {31},
  pages = {313--327},
  issn = {1097-0258},
  doi = {10.1002/sim.4451},
  abstract = {This paper investigates the use of likelihood methods for meta-analysis, within the random-effects models framework. We show that likelihood inference relying on first-order approximations, while improving common meta-analysis techniques, can be prone to misleading results. This drawback is very evident in the case of small sample sizes, which are typical in meta-analysis. We alleviate the problem by exploiting the theory of higher-order asymptotics. In particular, we focus on a second-order adjustment to the log-likelihood ratio statistic. Simulation studies in meta-analysis and meta-regression show that higher-order likelihood inference provides much more accurate results than its first-order counterpart, while being of a computationally feasible form. We illustrate the application of the proposed approach on a real example. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/AUGAZQNN/Guolo - 2012 - Higher-order likelihood inference in meta-analysis.pdf;/Users/rritaz/Zotero/storage/DY3G3ICB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),random effects models,random-effects},
  language = {en},
  number = {4}
}

@article{guolo_improving_2018,
  title = {Improving Likelihood-Based Inference in Control Rate Regression},
  author = {Guolo, Annamaria},
  year = {2018},
  volume = {37},
  pages = {157--166},
  issn = {1097-0258},
  doi = {10.1002/sim.7511},
  abstract = {Control rate regression is a diffuse approach to account for heterogeneity among studies in meta-analysis by including information about the outcome risk of patients in the control condition. Correcting for the presence of measurement error affecting risk information in the treated and in the control group has been recognized as a necessary step to derive reliable inferential conclusions. Within this framework, the paper considers the problem of small sample size as an additional source of misleading inference about the slope of the control rate regression. Likelihood procedures relying on first-order approximations are shown to be substantially inaccurate, especially when dealing with increasing heterogeneity and correlated measurement errors. We suggest to address the problem by relying on higher-order asymptotics. In particular, we derive Skovgaard's statistic as an instrument to improve the accuracy of the approximation of the signed profile log-likelihood ratio statistic to the standard normal distribution. The proposal is shown to provide much more accurate results than standard likelihood solutions, with no appreciable computational effort. The advantages of Skovgaard's statistic in control rate regression are shown in a series of simulation experiments and illustrated in a real data example. R code for applying first- and second-order statistic for inference on the slope on the control rate regression is provided.},
  file = {/Users/rritaz/Zotero/storage/AETDUJAU/Guolo - 2018 - Improving likelihood-based inference in control ra.pdf;/Users/rritaz/Zotero/storage/K6KAMRSC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{guolo_simex_2014,
  title = {The {{SIMEX}} Approach to Measurement Error Correction in Meta-Analysis with Baseline Risk as Covariate},
  author = {Guolo, A.},
  year = {2014},
  volume = {33},
  pages = {2062--2076},
  issn = {1097-0258},
  doi = {10.1002/sim.6076},
  abstract = {This paper investigates the use of SIMEX, a simulation-based measurement error correction technique, for meta-analysis of studies involving the baseline risk of subjects in the control group as explanatory variable. The approach accounts for the measurement error affecting the information about either the outcome in the treatment group or the baseline risk available from each study, while requiring no assumption about the distribution of the true unobserved baseline risk. This robustness property, together with the feasibility of computation, makes SIMEX very attractive. The approach is suggested as an alternative to the usual likelihood analysis, which can provide misleading inferential results when the commonly assumed normal distribution for the baseline risk is violated. The performance of SIMEX is compared to the likelihood method and to the moment-based correction through an extensive simulation study and the analysis of two datasets from the medical literature. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3YLSQL26/Guolo - 2014 - The SIMEX approach to measurement error correction.pdf;/Users/rritaz/Zotero/storage/UPW3KPZC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {12}
}

@article{hafdahl_combining_2007,
  title = {Combining {{Correlation Matrices}}: {{Simulation Analysis}} of {{Improved Fixed}}-{{Effects Methods}}},
  shorttitle = {Combining {{Correlation Matrices}}},
  author = {Hafdahl, Adam R.},
  year = {2007},
  month = jun,
  volume = {32},
  pages = {180--205},
  issn = {1076-9986},
  doi = {10.3102/1076998606298041},
  abstract = {The originally proposed multivariate meta-analysis approach for correlation matrices?analyze Pearson correlations, with each study?s observed correlations replacing their population counterparts in its conditional-covariance matrix?performs poorly. Two refinements are considered: Analyze Fisher Z-transformed correlations, and substitute better estimates of correlations in the conditional covariances. Fixed-effects methods with and without each refinement were examined in a Monte Carlo study; number of studies and the distribution of within-study sample sizes were varied. Both refinements improved element-wise point and interval estimates, as well as Type I error control for homogeneity tests, especially with many small studies. Practical recommendations and suggestions for future methodological work are offered. An appendix describes how to transform Fisher-Z (co)variances to the Pearson-r metric.},
  file = {/Users/rritaz/Zotero/storage/7MY7MT84/Hafdahl - 2007 - Combining Correlation Matrices Simulation Analysi.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {correlation coefficients,multivariate},
  number = {2}
}

@article{hafdahl_combining_2008,
  title = {Combining {{Heterogeneous Correlation Matrices}}: {{Simulation Analysis}} of {{Fixed}}-{{Effects Methods}}},
  shorttitle = {Combining {{Heterogeneous Correlation Matrices}}},
  author = {Hafdahl, Adam R.},
  year = {2008},
  month = dec,
  volume = {33},
  pages = {507--533},
  issn = {1076-9986},
  doi = {10.3102/1076998607309472},
  abstract = {Monte Carlo studies of several fixed-effects methods for combining and comparing correlation matrices have shown that two refinements improve estimation and inference substantially. With rare exception, however, these simulations have involved homogeneous data analyzed using conditional meta-analytic procedures. The present study builds on previous evidence about these methods? relative performance by examining their behavior under heterogeneity, which is more realistic in practice. Results based on both conditional and unconditional estimands indicate that of the two refinements, using estimated correlations in conditional (co)variances improves point and interval estimates of mean correlations more than analyzing Fisher Z correlations, despite the latter?s superiority for testing homogeneity. Recommended choices among methods are offered.},
  file = {/Users/rritaz/Zotero/storage/EK49KEX5/Hafdahl - 2008 - Combining Heterogeneous Correlation Matrices Simu.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {correlation coefficients},
  number = {4}
}

@article{hafdahl_meta-analysis_2009,
  title = {Meta-Analysis of Correlations Revisited: {{Attempted}} Replication and Extension of {{Field}}'s (2001) Simulation Studies.},
  shorttitle = {Meta-Analysis of Correlations Revisited},
  author = {Hafdahl, Adam R. and Williams, Michelle A.},
  year = {2009},
  volume = {14},
  pages = {24--42},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0014697},
  file = {/Users/rritaz/Zotero/storage/SJ5D6LUR/Hafdahl and Williams - 2009 - Meta-analysis of correlations revisited Attempted.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlation coefficients,random-effects},
  language = {en},
  number = {1}
}

@article{hahn_assessing_2000,
  title = {Assessing the Potential for Bias in Meta-Analysis Due to Selective Reporting of Subgroup Analyses within Studies},
  author = {Hahn, Seokyung and Williamson, Paula R. and Hutton, Jane L. and Garner, Paul and Flynn, E. Victor},
  year = {2000},
  volume = {19},
  pages = {3325--3336},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20001230)19:24<3325::AID-SIM827>3.0.CO;2-D},
  abstract = {Subgroup analysis is frequently used to investigate heterogeneity in meta-analysis. Subgroup data are not always available, and researchers should record what data were available for each trial. If data were not available, and it is known that the subgroup data were collected, the potential for selective reporting should be considered. Bias due to selective publishing of reports is widely recognized in meta-analysis. In contrast, selective reporting within studies is little discussed but potentially important. We explored this problem by evaluating the effect of potential bias in subgroup analysis due to within-study selective reporting with an existing meta-analysis. The review addressed malaria chemoprophylaxis in pregnancy. The conclusion in the original review, that benefit is limited to primigravidae, was based on subgroup analysis using the three trials out of five which reported on subgroups. We developed a method of sensitivity analysis that imputes data for the missing subgroups to assess the robustness of the results and the conclusions drawn. In this particular example, our analysis indicates that the estimate of effect reported in the review is most likely to overestimate the true effect and the conclusion that benefit is limited to primigravidae may be false. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/73DMKK99/Hahn et al. - 2000 - Assessing the potential for bias in meta-analysis .pdf;/Users/rritaz/Zotero/storage/WQMXG99F/1097-0258(20001230)19243325AID-SIM8273.0.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {24}
}

@article{hamling_facilitating_2008,
  title = {Facilitating Meta-Analyses by Deriving Relative Effect and Precision Estimates for Alternative Comparisons from a Set of Estimates Presented by Exposure Level or Disease Category},
  author = {Hamling, Jan and Lee, Peter and Weitkunat, Rolf and Amb{\"u}hl, Mathias},
  year = {2008},
  volume = {27},
  pages = {954--970},
  issn = {1097-0258},
  doi = {10.1002/sim.3013},
  abstract = {Epidemiological studies relating a particular exposure to a specified disease may present their results in a variety of ways. Often, results are presented as estimated odds ratios (or relative risks) and confidence intervals (CIs) for a number of categories of exposure, for example, by duration or level of exposure, compared with a single reference category, often the unexposed. For systematic literature review, and particularly meta-analysis, estimates for an alternative comparison of the categories, such as any exposure versus none, may be required. Obtaining these alternative comparisons is not straightforward, as the initial set of estimates is correlated. This paper describes a method for estimating these alternative comparisons based on the ideas originally put forward by Greenland and Longnecker, and provides implementations of the method, developed using Microsoft Excel and SAS. Examples of the method based on studies of smoking and cancer are given. The method also deals with results given by categories of disease (such as histological types of a cancer). The method allows the use of a more consistent comparison when summarizing published evidence, thus potentially improving the reliability of a meta-analysis. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/6CXWV7CH/Hamling et al. - 2008 - Facilitating meta-analyses by deriving relative ef.pdf;/Users/rritaz/Zotero/storage/BEMZLDEU/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,effect size estimation (series),random-effects},
  language = {en},
  number = {7}
}

@article{hamza_multivariate_2009,
  title = {Multivariate Random Effects Meta-Analysis of Diagnostic Tests with Multiple Thresholds},
  author = {Hamza, Taye H and Arends, Lidia R and {van Houwelingen}, Hans C and Stijnen, Theo},
  year = {2009},
  month = dec,
  volume = {9},
  pages = {73},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-73},
  abstract = {Background: Bivariate random effects meta-analysis of diagnostic tests is becoming a well established approach when studies present one two-by-two table or one pair of sensitivity and specificity. When studies present multiple thresholds for test positivity, usually meta-analysts reduce the data to a two-by-two table or take one threshold value at a time and apply the well developed meta-analytic approaches. However, this approach does not fully exploit the data. Methods: In this paper we generalize the bivariate random effects approach to the situation where test results are presented with k thresholds for test positivity, resulting in a 2 by (k+1) table per study. The model can be fitted with standard likelihood procedures in statistical packages such as SAS (Proc NLMIXED). We follow a multivariate random effects approach; i.e., we assume that each study estimates a study specific ROC curve that can be viewed as randomly sampled from the population of all ROC curves of such studies. In contrast to the bivariate case, where nothing can be said about the shape of study specific ROC curves without additional untestable assumptions, the multivariate model can be used to describe study specific ROC curves. The models are easily extended with study level covariates. Results: The method is illustrated using published meta-analysis data. The SAS NLMIXED syntax is given in the appendix. Conclusion: We conclude that the multivariate random effects meta-analysis approach is an appropriate and convenient framework to meta-analyse studies with multiple threshold without losing any information by dichotomizing the test results.},
  file = {/Users/rritaz/Zotero/storage/DYPHEYYP/Hamza et al. - 2009 - Multivariate random effects meta-analysis of diagn.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,multivariate,random-effects},
  language = {en},
  number = {1}
}

@article{harbord_modified_2006,
  title = {A Modified Test for Small-Study Effects in Meta-Analyses of Controlled Trials with Binary Endpoints},
  author = {Harbord, Roger M. and Egger, Matthias and Sterne, Jonathan A. C.},
  year = {2006},
  volume = {25},
  pages = {3443--3457},
  issn = {1097-0258},
  doi = {10.1002/sim.2380},
  abstract = {Publication bias and related bias in meta-analysis is often examined by visually checking for asymmetry in funnel plots of treatment effect against its standard error. Formal statistical tests of funnel plot asymmetry have been proposed, but when applied to binary outcome data these can give false-positive rates that are higher than the nominal level in some situations (large treatment effects, or few events per trial, or all trials of similar sizes). We develop a modified linear regression test for funnel plot asymmetry based on the efficient score and its variance, Fisher's information. The performance of this test is compared to the other proposed tests in simulation analyses based on the characteristics of published controlled trials. When there is little or no between-trial heterogeneity, this modified test has a false-positive rate close to the nominal level while maintaining similar power to the original linear regression test (`Egger' test). When the degree of between-trial heterogeneity is large, none of the tests that have been proposed has uniformly good properties. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YK4WR6KS/Harbord et al. - 2006 - A modified test for small-study effects in meta-an.pdf;/Users/rritaz/Zotero/storage/D8IC3W28/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,publication bias},
  language = {en},
  number = {20}
}

@article{hardy_likelihood_1996,
  title = {A {{Likelihood Approach}} to {{Meta}}-{{Analysis}} with {{Random Effects}}},
  author = {Hardy, Rebecca J. and Thompson, Simon G.},
  year = {1996},
  volume = {15},
  pages = {619--629},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19960330)15:6<619::AID-SIM188>3.0.CO;2-A},
  abstract = {In a meta-analysis of a set of clinical trials, a crucial but problematic component is providing an estimate and confidence interval for the overall treatment effect \texttheta. Since in the presence of heterogeneity a fixed effect approach yields an artificially narrow confidence interval for \texttheta, the random effects method of DerSimonian and Laird, which incorporates a moment estimator of the between-trial components of variance {$\sigma$}2B, has been advocated. With the additional distributional assumptions of normality, a confidence interval for \texttheta{} may be obtained. However, this method does not provide a confidence interval for {$\sigma$}2B, nor a confidence interval for \texttheta{} which takes account of the fact that {$\sigma$}2B has to be estimated from the data. We show how a likelihood based method can be used to overcome these problems, and use profile likelihoods to construct likelihood based confidence intervals. This approach yields an appropriately widened confidence interval compared with the standard random effects method. Examples of application to a published meta-analysis and a multicentre clinical trial are discussed. It is concluded that likelihood based methods are preferred to the standard method in undertaking random effects meta-analysis when the value of {$\sigma$}2B has an important effect on the overall estimated treatment effect.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819960330\%2915\%3A6\%3C619\%3A\%3AAID-SIM188\%3E3.0.CO\%3B2-A},
  file = {/Users/rritaz/Zotero/storage/UDA48W9S/Hardy and Thompson - 1996 - A Likelihood Approach to Meta-Analysis with Random.pdf;/Users/rritaz/Zotero/storage/CBDA4BKD/(SICI)1097-0258(19960330)156619AID-SIM1883.0.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {6}
}

@article{harrison_albatross_2017,
  title = {The Albatross Plot: {{A}} Novel Graphical Tool for Presenting Results of Diversely Reported Studies in a Systematic Review},
  shorttitle = {The Albatross Plot},
  author = {Harrison, Sean and Jones, Hayley E. and Martin, Richard M. and Lewis, Sarah J. and Higgins, Julian P. T.},
  year = {2017},
  volume = {8},
  pages = {281--289},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1239},
  abstract = {Meta-analyses combine the results of multiple studies of a common question. Approaches based on effect size estimates from each study are generally regarded as the most informative. However, these methods can only be used if comparable effect sizes can be computed from each study, and this may not be the case due to variation in how the studies were done or limitations in how their results were reported. Other methods, such as vote counting, are then used to summarize the results of these studies, but most of these methods are limited in that they do not provide any indication of the magnitude of effect. We propose a novel plot, the albatross plot, which requires only a 1-sided P value and a total sample size from each study (or equivalently a 2-sided P value, direction of effect and total sample size). The plot allows an approximate examination of underlying effect sizes and the potential to identify sources of heterogeneity across studies. This is achieved by drawing contours showing the range of effect sizes that might lead to each P value for given sample sizes, under simple study designs. We provide examples of albatross plots using data from previous meta-analyses, allowing for comparison of results, and an example from when a meta-analysis was not possible.},
  file = {/Users/rritaz/Zotero/storage/Z7IWN5DR/Harrison et al. - 2017 - The albatross plot A novel graphical tool for pre.pdf;/Users/rritaz/Zotero/storage/F9GYRUYF/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {3}
}

@article{hartung_alternative_1999,
  title = {An {{Alternative Method}} for {{Meta}}-{{Analysis}}},
  author = {Hartung, Joachim},
  year = {1999},
  volume = {41},
  pages = {901--916},
  issn = {1521-4036},
  doi = {10.1002/(SICI)1521-4036(199912)41:8<901::AID-BIMJ901>3.0.CO;2-W},
  abstract = {In many fields of applications, test statistics are obtained by combining estimates from several experiments, studies or centres of a multi-centre trial. The commonly used test procedure to judge the evidence of a common overall effect can result in a considerable overestimation of the significance level, leading to a high rate of too liberal decisions. Alternative test statistics are presented and better approximating test distributions are derived. Explicitly discussed are the methods in the unbalanced heteroscedastic 1-way random ANOVA model and for the probability difference method, including interaction treatment by centres or studies. Numerical results are presented by simulation studies.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291521-4036\%28199912\%2941\%3A8\%3C901\%3A\%3AAID-BIMJ901\%3E3.0.CO\%3B2-W},
  copyright = {\textcopyright{} 1999 WILEY-VCH Verlag Berlin GmbH, Fed. Rep. of Germany},
  file = {/Users/rritaz/Zotero/storage/KS4S8UKN/Hartung - 1999 - An Alternative Method for Meta-Analysis.pdf;/Users/rritaz/Zotero/storage/TZ62HQKU/(SICI)1521-4036(199912)418901AID-BIMJ9013.0.html},
  journal = {Biometrical Journal},
  keywords = {Combining experiments,Heteroscedastic ANOVA model,Interaction treatment by centres,Log odds ratio,Log relative risk,Meta-analysis,Multi-centre study,Probability difference method,Random effects},
  language = {en},
  number = {8}
}

@article{hartung_alternative_1999-1,
  title = {An {{Alternative Method}} for {{Meta}}-{{Analysis}}},
  author = {Hartung, Joachim},
  year = {1999},
  volume = {41},
  pages = {901--916},
  issn = {1521-4036},
  doi = {10.1002/(SICI)1521-4036(199912)41:8<901::AID-BIMJ901>3.0.CO;2-W},
  abstract = {In many fields of applications, test statistics are obtained by combining estimates from several experiments, studies or centres of a multi-centre trial. The commonly used test procedure to judge the evidence of a common overall effect can result in a considerable overestimation of the significance level, leading to a high rate of too liberal decisions. Alternative test statistics are presented and better approximating test distributions are derived. Explicitly discussed are the methods in the unbalanced heteroscedastic 1-way random ANOVA model and for the probability difference method, including interaction treatment by centres or studies. Numerical results are presented by simulation studies.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291521-4036\%28199912\%2941\%3A8\%3C901\%3A\%3AAID-BIMJ901\%3E3.0.CO\%3B2-W},
  file = {/Users/rritaz/Zotero/storage/3JWSQS8W/Hartung - 1999 - An Alternative Method for Meta-Analysis.pdf;/Users/rritaz/Zotero/storage/B3Z82LQ7/(SICI)1521-4036(199912)418901AID-BIMJ9013.0.html},
  journal = {Biometrical Journal},
  keywords = {Combining experiments,Heteroscedastic ANOVA model,Interaction treatment by centres,Log odds ratio,Log relative risk,Meta-analysis,Multi-centre study,Probability difference method,Random effects},
  language = {en},
  number = {8}
}

@article{hartung_refined_2001,
  title = {A Refined Method for the Meta-Analysis of Controlled Clinical Trials with Binary Outcome},
  author = {Hartung, Joachim and Knapp, Guido},
  year = {2001},
  volume = {20},
  pages = {3875--3889},
  issn = {1097-0258},
  doi = {10.1002/sim.1009},
  abstract = {For the meta-analysis of controlled clinical trials with binary outcome a test statistic for testing an overall treatment effect is proposed, which is based on a refined estimator for the variance of the treatment effect estimator usually used in the random-effects model of meta-analysis. In simulation studies it is shown that the proposed test keeps the prescribed significance level much better than the commonly used tests in the fixed-effects and random-effects model, respectively. Moreover, when using the test it is not necessary to choose between fixed effects and random effects approaches in advance. The proposed method applies in the same way to the analysis of a controlled multi-centre study with binary outcome, including a possible interaction between drugs and centres. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/TVMZSUUA/Hartung and Knapp - 2001 - A refined method for the meta-analysis of controll.pdf;/Users/rritaz/Zotero/storage/47LVN44M/sim.html},
  journal = {Statistics in Medicine},
  keywords = {binary outcome,combined significance},
  language = {en},
  number = {24}
}

@article{hartung_tests_2001,
  title = {On Tests of the Overall Treatment Effect in Meta-Analysis with Normally Distributed Responses},
  author = {Hartung, Joachim and Knapp, Guido},
  year = {2001},
  volume = {20},
  pages = {1771--1782},
  issn = {1097-0258},
  doi = {10.1002/sim.791},
  abstract = {For the meta-analysis of controlled clinical trials or epidemiological studies, in which the responses are at least approximately normally distributed, a refined test for the hypothesis of no overall treatment effect is proposed. The test statistic is based on a direct estimation function for the variance of the overall treatment effect estimator. As outcome measures, the absolute and the standardized difference between means are considered. In simulation studies it is shown that the proposed test keeps the prescribed significance level very well in contrast to the commonly used tests in the fixed effects and random effects model, respectively, which can become very liberal. Furthermore, just for using the proposed test it is not necessary to choose between the fixed effects and the random effects approach in advance. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZBIXPXSX/Hartung and Knapp - 2001 - On tests of the overall treatment effect in meta-a.pdf;/Users/rritaz/Zotero/storage/WYDGZ6GR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random effects models,standardized mean difference},
  language = {en},
  number = {12}
}

@article{hartung_tests_2001-1,
  title = {On Tests of the Overall Treatment Effect in Meta-Analysis with Normally Distributed Responses},
  author = {Hartung, Joachim and Knapp, Guido},
  year = {2001},
  volume = {20},
  pages = {1771--1782},
  issn = {1097-0258},
  doi = {10.1002/sim.791},
  abstract = {For the meta-analysis of controlled clinical trials or epidemiological studies, in which the responses are at least approximately normally distributed, a refined test for the hypothesis of no overall treatment effect is proposed. The test statistic is based on a direct estimation function for the variance of the overall treatment effect estimator. As outcome measures, the absolute and the standardized difference between means are considered. In simulation studies it is shown that the proposed test keeps the prescribed significance level very well in contrast to the commonly used tests in the fixed effects and random effects model, respectively, which can become very liberal. Furthermore, just for using the proposed test it is not necessary to choose between the fixed effects and the random effects approach in advance. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.791},
  copyright = {Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PUZ9EYF4/Hartung and Knapp - 2001 - On tests of the overall treatment effect in meta-a.pdf;/Users/rritaz/Zotero/storage/ZYMHFI3Y/sim.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {12}
}

@article{harville_maximum_1977,
  title = {Maximum {{Likelihood Approaches}} to {{Variance Component Estimation}} and to {{Related Problems}}},
  author = {Harville, David A.},
  year = {1977},
  month = jun,
  volume = {72},
  pages = {320--338},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1977.10480998},
  abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ml) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (reml) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ml estimators of variance components. There are many iterative algorithms that can be considered for computing the ml or reml estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
  file = {/Users/rritaz/Zotero/storage/J2QVS5UD/01621459.1977.html},
  journal = {Journal of the American Statistical Association},
  number = {358}
}

@article{hattori_evaluation_2016,
  title = {Evaluation of Predictive Capacities of Biomarkers Based on Research Synthesis},
  author = {Hattori, Satoshi and Zhou, Xiao-Hua},
  year = {2016},
  volume = {35},
  pages = {4559--4572},
  issn = {1097-0258},
  doi = {10.1002/sim.7018},
  abstract = {The objective of diagnostic studies or prognostic studies is to evaluate and compare predictive capacities of biomarkers. Suppose we are interested in evaluation and comparison of predictive capacities of continuous biomarkers for a binary outcome based on research synthesis. In analysis of each study, subjects are often classified into two groups of the high-expression and low-expression groups according to a cut-off value, and statistical analysis is based on a 2 \texttimes{} 2 table defined by the response and the high expression or low expression of the biomarker. Because the cut-off is study specific, it is difficult to interpret a combined summary measure such as an odds ratio based on the standard meta-analysis techniques. The summary receiver operating characteristic curve is a useful method for meta-analysis of diagnostic studies in the presence of heterogeneity of cut-off values to examine discriminative capacities of biomarkers. We develop a method to estimate positive or negative predictive curves, which are alternative to the receiver operating characteristic curve based on information reported in published papers of each study. These predictive curves provide a useful graphical presentation of pairs of positive and negative predictive values and allow us to compare predictive capacities of biomarkers of different scales in the presence of heterogeneity in cut-off values among studies. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/9H9TKPZP/Hattori and Zhou - 2016 - Evaluation of predictive capacities of biomarkers .pdf;/Users/rritaz/Zotero/storage/U2JRDT8X/sim.html},
  journal = {Statistics in Medicine},
  keywords = {physical/biological fields,random-effects},
  language = {en},
  number = {25}
}

@article{hattori_sensitivity_2018,
  title = {Sensitivity Analysis for Publication Bias in Meta-Analysis of Diagnostic Studies for a Continuous Biomarker},
  author = {Hattori, Satoshi and Zhou, Xiao-Hua},
  year = {2018},
  volume = {37},
  pages = {327--342},
  issn = {1097-0258},
  doi = {10.1002/sim.7510},
  abstract = {Publication bias is one of the most important issues in meta-analysis. For standard meta-analyses to examine intervention effects, the funnel plot and the trim-and-fill method are simple and widely used techniques for assessing and adjusting for the influence of publication bias, respectively. However, their use may be subjective and can then produce misleading insights. To make a more objective inference for publication bias, various sensitivity analysis methods have been proposed, including the Copas selection model. For meta-analysis of diagnostic studies evaluating a continuous biomarker, the summary receiver operating characteristic (sROC) curve is a very useful method in the presence of heterogeneous cutoff values. To our best knowledge, no methods are available for evaluation of influence of publication bias on estimation of the sROC curve. In this paper, we introduce a Copas-type selection model for meta-analysis of diagnostic studies and propose a sensitivity analysis method for publication bias. Our method enables us to assess the influence of publication bias on the estimation of the sROC curve and then judge whether the result of the meta-analysis is sufficiently confident or should be interpreted with much caution. We illustrate our proposed method with real data.},
  file = {/Users/rritaz/Zotero/storage/FIREKLDV/Hattori and Zhou - 2018 - Sensitivity analysis for publication bias in meta-.pdf;/Users/rritaz/Zotero/storage/983XWYXR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {3}
}

@article{hattori_time-dependent_2016,
  title = {Time-Dependent Summary Receiver Operating Characteristics for Meta-Analysis of Prognostic Studies},
  author = {Hattori, Satoshi and Zhou, Xiao-Hua},
  year = {2016},
  volume = {35},
  pages = {4746--4763},
  issn = {1097-0258},
  doi = {10.1002/sim.7029},
  abstract = {Prognostic studies are widely conducted to examine whether biomarkers are associated with patient's prognoses and play important roles in medical decisions. Because findings from one prognostic study may be very limited, meta-analyses may be useful to obtain sound evidence. However, prognostic studies are often analyzed by relying on a study-specific cut-off value, which can lead to difficulty in applying the standard meta-analysis techniques. In this paper, we propose two methods to estimate a time-dependent version of the summary receiver operating characteristics curve for meta-analyses of prognostic studies with a right-censored time-to-event outcome. We introduce a bivariate normal model for the pair of time-dependent sensitivity and specificity and propose a method to form inferences based on summary statistics reported in published papers. This method provides a valid inference asymptotically. In addition, we consider a bivariate binomial model. To draw inferences from this bivariate binomial model, we introduce a multiple imputation method. The multiple imputation is found to be approximately proper multiple imputation, and thus the standard Rubin's variance formula is justified from a Bayesian view point. Our simulation study and application to a real dataset revealed that both methods work well with a moderate or large number of studies and the bivariate binomial model coupled with the multiple imputation outperforms the bivariate normal model with a small number of studies. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/T23V7NAJ/Hattori and Zhou - 2016 - Time-dependent summary receiver operating characte.pdf;/Users/rritaz/Zotero/storage/BLXFNPS2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,physical/biological fields,random-effects},
  language = {en},
  number = {26}
}

@article{hedges_consistency_2019,
  title = {Consistency of Effects Is Important in Replication: {{Rejoinder}} to {{Mathur}} and {{VanderWeele}} (2019)},
  shorttitle = {Consistency of Effects Is Important in Replication},
  author = {Hedges, Larry V. and Schauer, Jacob M.},
  year = {2019},
  month = oct,
  volume = {24},
  pages = {576--577},
  issn = {1082-989X},
  doi = {10.1037/met0000237},
  abstract = {In this rejoinder, we discuss Mathur and VanderWeele's response to our article, 'Statistical Analyses for Studying Replication: Meta-Analytic Perspectives,' which appears in this current issue. We attempt to clarify a point of confusion regarding the inclusion of an original study in an analysis of replication, and the potential impact of publication bias. We then discuss the methods used by Mathur and VanderWeele to conduct an alternative analysis of the Gambler's Fallacy example from our article. We highlight that there are some potential statistical and conceptual differences to their approach compared to what we propose in our article. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {effect size estimation (series),publication bias},
  number = {5}
}

@article{hedges_distribution_1981,
  title = {Distribution {{Theory}} for {{Glass}}'s {{Estimator}} of {{Effect Size}} and {{Related Estimators}}},
  author = {Hedges, Larry V.},
  year = {1981},
  volume = {6},
  pages = {107--128},
  publisher = {{[Sage Publications, Inc., American Educational Research Association, American Statistical Association]}},
  issn = {0362-9791},
  doi = {10.2307/1164588},
  abstract = {Glass's estimator of effect size, the sample mean difference divided by the sample standard deviation, is studied in the context of an explicit statistical model. The exact distribution of Glass's estimator is obtained and the estimator is shown to have a small sample bias. The minimum variance unbiased estimator is obtained and shown to have uniformly smaller variance than Glass's (biased) estimator. Measurement error is shown to attenuate estimates of effect size and a correction is given. The effects of measurement invalidity are discussed. Expressions for weights that yield the most precise weighted estimate of effect size are also derived.},
  journal = {Journal of Educational Statistics},
  number = {2}
}

@article{hedges_effect_2007,
  title = {Effect {{Sizes}} in {{Cluster}}-{{Randomized Designs}}},
  author = {Hedges, Larry V.},
  year = {2007},
  month = dec,
  volume = {32},
  pages = {341--370},
  issn = {1076-9986},
  doi = {10.3102/1076998606298043},
  abstract = {Multisite research designs involving cluster randomization are becoming increasingly important in educational and behavioral research. Researchers would like to compute effect size indexes based on the standardized mean difference to compare the results of cluster-randomized studies (and corresponding quasi-experiments) with other studies and to combine information across studies in meta-analyses. This article addresses the problem of defining effect sizes in multilevel designs and computing estimates of those effect sizes and their standard errors from information that is likely to be reported in journal articles. Three effect sizes are defined corresponding to different standardizations. Estimators of each effect size index are also presented along with their sampling distributions (including standard errors).},
  file = {/Users/rritaz/Zotero/storage/W6IJNENT/Hedges - 2007 - Effect Sizes in Cluster-Randomized Designs.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {effect size estimation (series),rrita-future},
  number = {4}
}

@article{hedges_effect_2011,
  title = {Effect {{Sizes}} in {{Three}}-{{Level Cluster}}-{{Randomized Experiments}}},
  author = {Hedges, Larry V.},
  year = {2011},
  month = jun,
  volume = {36},
  pages = {346--380},
  issn = {1076-9986},
  doi = {10.3102/1076998610376617},
  abstract = {Research designs involving cluster randomization are becoming increasingly important in educational and behavioral research. Many of these designs involve two levels of clustering or nesting (students within classes and classes within schools). Researchers would like to compute effect size indexes based on the standardized mean difference to compare the results of cluster-randomized studies with other studies and to combine information across studies in meta-analyses. This article addresses the problem of defining effect sizes in designs with two levels of clustering and computing estimates of those effect sizes and their standard errors from information that is likely to be reported in journal articles. Five effect sizes are defined corresponding to different standardizations. Estimators of each effect size index are also presented along with their sampling distributions (including standard errors).},
  file = {/Users/rritaz/Zotero/storage/HFU4FBKS/Hedges - 2011 - Effect Sizes in Three-Level Cluster-Randomized Exp.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {effect size estimation (series),rrita-future},
  number = {3}
}

@article{hedges_fixed-_1998,
  title = {Fixed- and Random-Effects Models in Meta-Analysis},
  author = {Hedges, Larry V. and Vevea, Jack L.},
  year = {1998},
  month = dec,
  volume = {3},
  pages = {486--504},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.3.4.486},
  abstract = {There are 2 families of statistical procedures in meta-analysis: fixed- and random-effects procedures. They were developed for somewhat different inference goals: making inferences about the effect parameters in the studies that have been observed versus making inferences about the distribution of effect parameters in a population of studies from a random sample of studies. The authors evaluate the performance of confidence intervals and hypothesis tests when each type of statistical procedure is used for each type of inference and confirm that each procedure is best for making the kind of inference for which it was designed. Conditionally random-effects procedures (a hybrid type) are shown to have properties in between those of fixed- and random-effects procedures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {confidence intervals \& hypothesis tests using fixed- \& random-effects procedures \& models in meta-analysis,Confidence Limits (Statistics),Hypothesis Testing,Meta Analysis,Models,Statistical Analysis},
  number = {4}
}

@article{hedges_overlap_2016,
  title = {Overlap between Treatment and Control Distributions as an Effect Size Measure in Experiments},
  author = {Hedges, Larry V. and Olkin, Ingram},
  year = {2016},
  month = mar,
  volume = {21},
  pages = {61--68},
  issn = {1082-989X},
  doi = {10.1037/met0000042},
  abstract = {The proportion {$\pi$} of treatment group observations that exceed the control group mean has been proposed as an effect size measure for experiments that randomly assign independent units into 2 groups. We give the exact distribution of a simple estimator of {$\pi$} based on the standardized mean difference and use it to study the small sample bias of this estimator. We also give the minimum variance unbiased estimator of {$\pi$} under 2 models, one in which the variance of the mean difference is known and one in which the variance is unknown. We show how to use the relation between the standardized mean difference and the overlap measure to compute confidence intervals for {$\pi$} and show that these results can be used to obtain unbiased estimators, large sample variances, and confidence intervals for 3 related effect size measures based on the overlap. Finally, we show how the effect size {$\pi$} can be used in a meta-analysis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes},
  number = {1},
  series = {Naturally {{Occurring Section}} on {{Statistical Inference Topics}}}
}

@article{hedges_plausibility_2017,
  title = {Plausibility and Influence in Selection Models: {{A}} Comment on {{Citkowicz}} and {{Vevea}} (2017)},
  shorttitle = {Plausibility and Influence in Selection Models},
  author = {Hedges, Larry V.},
  year = {2017},
  month = mar,
  volume = {22},
  pages = {42--46},
  issn = {1082-989X},
  doi = {10.1037/met0000108},
  abstract = {I discuss how methods that adjust for publication selection involve implicit or explicit selection models. Such models describe the relation between the studies conducted and those actually observed. I argue that the evaluation of selection models should include an evaluation of the plausibility of the empirical implications of that model. This includes how many studies would have had to exist to yield the observed sample of studies. I also argue that the amount of influence that one or a small number of studies might have on the overall results is also important to understand. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,GLM MA models,modeling effect size variation (covariates),publication bias,random-effects},
  number = {1}
}

@article{hedges_power_2001,
  title = {The Power of Statistical Tests in Meta-Analysis.},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  year = {2001},
  volume = {6},
  pages = {203--217},
  doi = {10.1037/1082-989X.6.3.203},
  abstract = {Calculations of the power of statistical tests are important in planning research studies (including meta-analyses) and in interpreting situations in which a result has not proven to be statistically significant. The authors describe procedures to compute statistical power of fixed- and random-effects tests of the mean effect size, tests for heterogeneity (or variation) of effect size parameters across studies, and tests for contrasts among effect sizes of different studies. Examples are given using 2 published meta-analyses. The examples illustrate that statistical power is not always high in meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/773QQDG4/Hedges and Pigott - 2001 - The power of statistical tests in meta-analysis..pdf},
  journal = {Psychological methods},
  keywords = {modeling effect size variation (covariates),power,random effects models,random-effects},
  number = {3}
}

@article{hedges_power_2004,
  title = {The Power of Statistical Tests for Moderators in Meta-Analysis.},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  year = {2004},
  volume = {9},
  pages = {426--445},
  doi = {10.1037/1082-989X.9.4.426},
  abstract = {Calculation of the statistical power of statistical tests is important in planning and interpreting the results of research studies, including meta-analyses. It is particularly important in moderator analyses in meta-analysis, which are often used as sensitivity analyses to rule out moderator effects but also may have low statistical power. This article describes how to compute statistical power of both fixed- and mixed-effects moderator tests in meta-analysis that are analogous to the analysis of variance and multiple regression analysis for effect sizes. It also shows how to compute power of tests for goodness of fit associated with these models. Examples from a published meta-analysis demonstrate that power of moderator tests and goodness-of-fit tests is not always high.},
  file = {/Users/rritaz/Zotero/storage/ZNYR722Z/Hedges and Pigott - 2004 - The power of statistical tests for moderators in m.pdf},
  journal = {Psychological methods},
  keywords = {categorical MA models,GLM MA models,modeling effect size variation (covariates),power},
  number = {4}
}

@article{hedges_random_1983,
  title = {A Random Effects Model for Effect Sizes},
  author = {Hedges, Larry V.},
  year = {1983},
  month = mar,
  volume = {93},
  pages = {388--395},
  publisher = {{American Psychological Association}},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.93.2.388},
  abstract = {Recent interest in quantitative research synthesis has led to the development of rigorous statistical theory for some of the methods used in meta-analysis. Statistical theory proposed previously has stressed the estimation of fixed but unknown population effect sizes (standardized mean differences). Theoretical considerations often suggest that treatment effects are not fixed but vary across different implementations of a treatment. The present author presents a random effects model (analogous to random effects ANOVA) in which the population effect sizes are not fixed but are sample realizations from a distribution of possible population effect sizes. An analogy to variance component estimation is used to derive an unbiased estimator of the variance of the effect-size distribution. An example shows that these methods may suggest insights that are not available from inspection of means and standard deviation of effect-size estimates. (13 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Bulletin},
  keywords = {Estimation,Mathematical Modeling,random effects model for estimation of variance of effect-size distribution,Statistical Analysis},
  number = {2}
}

@article{hedges_robust_2010,
  title = {Robust Variance Estimation in Meta-Regression with Dependent Effect Size Estimates},
  author = {Hedges, Larry V. and Tipton, Elizabeth and Johnson, Matthew C.},
  year = {2010},
  volume = {1},
  pages = {39--65},
  issn = {1759-2887},
  doi = {10.1002/jrsm.5},
  abstract = {Conventional meta-analytic techniques rely on the assumption that effect size estimates from different studies are independent and have sampling distributions with known conditional variances. The independence assumption is violated when studies produce several estimates based on the same individuals or there are clusters of studies that are not independent (such as those carried out by the same investigator or laboratory). This paper provides an estimator of the covariance matrix of meta-regression coefficients that are applicable when there are clusters of internally correlated estimates. It makes no assumptions about the specific form of the sampling distributions of the effect sizes, nor does it require knowledge of the covariance structure of the dependent estimates. Moreover, this paper demonstrates that the meta-regression coefficients are consistent and asymptotically normally distributed and that the robust variance estimator is valid even when the covariates are random. The theory is asymptotic in the number of studies, but simulations suggest that the theory may yield accurate results with as few as 20\textendash 40 studies. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/HUIL4MHC/Hedges et al. - 2010 - Robust variance estimation in meta-regression with.pdf;/Users/rritaz/Zotero/storage/Z7BD22YA/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlated effects,multivariate,random-effects,robust variance estimation},
  language = {en},
  number = {1}
}

@book{hedges_statistical_1985,
  title = {Statistical Methods for Meta-Analysis},
  author = {Hedges, Larry V.},
  year = {1985},
  publisher = {{Academic Press}},
  address = {{Orlando}},
  collaborator = {Olkin, Ingram},
  isbn = {978-0-12-336380-0},
  keywords = {Social sciences,Statistical methods},
  language = {eng},
  lccn = {300.72 H453s}
}

@book{hedges_statistical_2014,
  title = {Statistical {{Methods}} for {{Meta}}-{{Analysis}}},
  author = {Hedges, Larry V. and Olkin, Ingram},
  year = {2014},
  month = jun,
  publisher = {{Academic Press}},
  abstract = {The main purpose of this book is to address the statistical issues for integrating independent studies. There exist a number of papers and books that discuss the mechanics of collecting, coding, and preparing data for a meta-analysis , and we do not deal with these. Because this book concerns methodology, the content necessarily is statistical, and at times mathematical. In order to make the material accessible to a wider audience, we have not provided proofs in the text. Where proofs are given, they are placed as commentary at the end of a chapter. These can be omitted at the discretion of the reader.Throughout the book we describe computational procedures whenever required. Many computations can be completed on a hand calculator, whereas some require the use of a standard statistical package such as SAS, SPSS, or BMD. Readers with experience using a statistical package or who conduct analyses such as multiple regression or analysis of variance should be able to carry out the analyses described with the aid of a statistical package.},
  googlebooks = {7GviBQAAQBAJ},
  isbn = {978-0-08-057065-5},
  keywords = {Education / Testing \& Measurement,Mathematics / Probability \& Statistics / General,Social Science / Research},
  language = {en}
}

@article{hedges_statistical_2019,
  title = {Statistical Analyses for Studying Replication: {{Meta}}-Analytic Perspectives},
  shorttitle = {Statistical Analyses for Studying Replication},
  author = {Hedges, Larry V. and Schauer, Jacob M.},
  year = {2019},
  month = oct,
  volume = {24},
  pages = {557--570},
  issn = {1082-989X},
  doi = {10.1037/met0000189},
  abstract = {Formal empirical assessments of replication have recently become more prominent in several areas of science, including psychology. These assessments have used different statistical approaches to determine if a finding has been replicated. The purpose of this article is to provide several alternative conceptual frameworks that lead to different statistical analyses to test hypotheses about replication. All of these analyses are based on statistical methods used in meta-analysis. The differences among the methods described involve whether the burden of proof is placed on replication or nonreplication, whether replication is exact or allows for a small amount of 'negligible heterogeneity,' and whether the studies observed are assumed to be fixed (constituting the entire body of relevant evidence) or are a sample from a universe of possibly relevant studies. The statistical power of each of these tests is computed and shown to be low in many cases, raising issues of the interpretability of tests for replication. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,effect size estimation (series),power},
  number = {5}
}

@article{hemila_duration_2017,
  title = {Duration of the Common Cold and Similar Continuous Outcomes Should Be Analyzed on the Relative Scale: A Case Study of Two Zinc Lozenge Trials},
  shorttitle = {Duration of the Common Cold and Similar Continuous Outcomes Should Be Analyzed on the Relative Scale},
  author = {Hemil{\"a}, Harri},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {82},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0356-y},
  abstract = {Background: The relative scale has been used for decades in analysing binary data in epidemiology. In contrast, there has been a long tradition of carrying out meta-analyses of continuous outcomes on the absolute, original measurement, scale. The biological rationale for using the relative scale in the analysis of binary outcomes is that it adjusts for baseline variations; however, similar baseline variations can occur in continuous outcomes and relative effect scale may therefore be often useful also for continuous outcomes. The aim of this study was to determine whether the relative scale is more consistent with empirical data on treating the common cold than the absolute scale. Methods: Individual patient data was available for 2 randomized trials on zinc lozenges for the treatment of the common cold. Mossad (Ann Intern Med 125:81\textendash 8, 1996) found 4.0 days and 43\% reduction, and Petrus (Curr Ther Res 59:595\textendash 607, 1998) found 1.77 days and 25\% reduction, in the duration of colds. In both trials, variance in the placebo group was significantly greater than in the zinc lozenge group. The effect estimates were applied to the common cold distributions of the placebo groups, and the resulting distributions were compared with the actual zinc lozenge group distributions. Results: When the absolute effect estimates, 4.0 and 1.77 days, were applied to the placebo group common cold distributions, negative and zero (i.e., impossible) cold durations were predicted, and the high level variance remained. In contrast, when the relative effect estimates, 43 and 25\%, were applied, impossible common cold durations were not predicted in the placebo groups, and the cold distributions became similar to those of the zinc lozenge groups. Conclusions: For some continuous outcomes, such as the duration of illness and the duration of hospital stay, the relative scale leads to a more informative statistical analysis and more effective communication of the study findings. The transformation of continuous data to the relative scale is simple with a spreadsheet program, after which the relative scale data can be analysed using standard meta-analysis software. The option for the analysis of relative effects of continuous outcomes directly from the original data should be implemented in standard meta-analysis programs.},
  file = {/Users/rritaz/Zotero/storage/R9AU6SCG/Hemilä - 2017 - Duration of the common cold and similar continuous.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,Individual Patient Data IPD},
  language = {en},
  number = {1}
}

@article{hemming_meta-regression_2010,
  title = {Meta-Regression with Partial Information on Summary Trial or Patient Characteristics},
  author = {Hemming, K. and Hutton, J. L. and Maguire, M. G. and Marson, A. G.},
  year = {2010},
  volume = {29},
  pages = {1312--1324},
  issn = {1097-0258},
  doi = {10.1002/sim.3848},
  abstract = {We present a model for meta-regression in the presence of missing information on some of the study level covariates, obtaining inferences using Bayesian methods. In practice, when confronted with missing covariate data in a meta-regression, it is common to carry out a complete case or available case analysis. We propose to use the full observed data, modelling the joint density as a factorization of a meta-regression model and a conditional factorization of the density for the covariates. With the inclusion of several covariates, inter-relations between these covariates are modelled. Under this joint likelihood-based approach, it is shown that the lesser assumption of the covariates being Missing At Random is imposed, instead of the more usual Missing Completely At Random (MCAR) assumption. The model is easily programmable in WinBUGS, and we examine, through the analysis of two real data sets, sensitivity and robustness of results to the MCAR assumption. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/HHX64899/Hemming et al. - 2010 - Meta-regression with partial information on summar.pdf;/Users/rritaz/Zotero/storage/BLGHVU4N/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance,missing data,modeling effect size variation (covariates)},
  language = {en},
  number = {12}
}

@article{hemming_pooling_2012,
  title = {Pooling Systematic Reviews of Systematic Reviews: A {{Bayesian}} Panoramic Meta-Analysis},
  shorttitle = {Pooling Systematic Reviews of Systematic Reviews},
  author = {Hemming, Karla and Bowater, Russell James and Lilford, Richard J.},
  year = {2012},
  volume = {31},
  pages = {201--216},
  issn = {1097-0258},
  doi = {10.1002/sim.4372},
  abstract = {Systematic reviews and meta-analyses usually synthesise evidence from studies reporting outcomes from particular interventions in specific diseases. For example, a meta-analysis of prophylactic antibiotics (intervention) in elective arterial reconstruction (disease) for rates of wound infection (outcome). However, because systematic reviews and meta-analyses are so widespread, a body of evidence often exists around specific intervention effects on particular outcomes over a range of diseases. So for example, a multitude of independent meta-analyses have evaluated rates of wound infection with and without the use of prophylactic antibiotics over multiple surgery types. A systematic review of systematic reviews is a means of synthesising evidence for the same intervention over multiple disease types. We propose a panoramic meta-analysis as a means of pooling effect estimates over systematic reviews of systematic reviews. We explore several methods ranging from a simple two-step approach, to a meta-regression or mixed effects approach, where variation between diseases are modelled as fixed covariate effects and between-study variation by random effects, and to a three-level hierarchical model in which exchangeability is assumed, which allows both a between-disease component of variance and a between-study (within disease) component of variance. In the surgery example, we pool 18 meta-analyses (each including between 4 and 26 studies) of prophylactic antibiotics reporting rates of wound infection from 18 different surgery sites to obtain a single pooled estimate of effect and estimates of between-disease, within-disease and within-study variability. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/C3QTSL79/Hemming et al. - 2012 - Pooling systematic reviews of systematic reviews .pdf;/Users/rritaz/Zotero/storage/8U6HKRSP/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,meta-meta-analyses,modeling effect size variation (covariates)},
  language = {en},
  number = {3}
}

@article{henmi_confidence_2010,
  title = {Confidence Intervals for Random Effects Meta-Analysis and Robustness to Publication Bias},
  author = {Henmi, Masayuki and Copas, John B.},
  year = {2010},
  volume = {29},
  pages = {2969--2983},
  issn = {1097-0258},
  doi = {10.1002/sim.4029},
  abstract = {The DerSimonian\textendash Laird confidence interval for the average treatment effect in meta-analysis is widely used in practice when there is heterogeneity between studies. However, it is well known that its coverage probability (the probability that the interval actually includes the true value) can be substantially below the target level of 95 per cent. It can also be very sensitive to publication bias. In this paper, we propose a new confidence interval that has better coverage than the DerSimonian\textendash Laird method, and that is less sensitive to publication bias. The key idea is to note that fixed effects estimates are less sensitive to such biases than random effects estimates, since they put relatively more weight on the larger studies and relatively less weight on the smaller studies. Whereas the DerSimonian\textendash Laird interval is centred on a random effects estimate, we centre our confidence interval on a fixed effects estimate, but allow for heterogeneity by including an assessment of the extra uncertainty induced by the random effects setting. Properties of the resulting confidence interval are studied by simulation and compared with other random effects confidence intervals that have been proposed in the literature. An example is briefly discussed. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/F83RG9CV/Henmi and Copas - 2010 - Confidence intervals for random effects meta-analy.pdf;/Users/rritaz/Zotero/storage/I7UIRHUL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,Confidence Intervals,publication bias,random-effects},
  language = {en},
  number = {29}
}

@article{hepworth_debiased_2009,
  title = {Debiased Estimation of Proportions in Group Testing},
  author = {Hepworth, Graham and Watson, Ray},
  year = {2009},
  month = feb,
  volume = {58},
  pages = {105--121},
  issn = {1467-9876},
  doi = {10.1111/j.1467-9876.2008.00639.x},
  abstract = {Summary.  In the assessment of disease, estimation of the proportion of infected units in a population can sometimes be facilitated by pooling units into groups for testing. Such group testing was us...},
  file = {/Users/rritaz/Zotero/storage/DU8DNRPN/Login.html},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  keywords = {debiased},
  language = {en},
  number = {1}
}

@article{higgins_consistency_2012,
  title = {Consistency and Inconsistency in Network Meta-Analysis: Concepts and Models for Multi-Arm Studies},
  shorttitle = {Consistency and Inconsistency in Network Meta-Analysis},
  author = {Higgins, J. P. T. and Jackson, D. and Barrett, J. K. and Lu, G. and Ades, A. E. and White, I. R.},
  year = {2012},
  volume = {3},
  pages = {98--110},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1044},
  abstract = {Meta-analyses that simultaneously compare multiple treatments (usually referred to as network meta-analyses or mixed treatment comparisons) are becoming increasingly common. An important component of a network meta-analysis is an assessment of the extent to which different sources of evidence are compatible, both substantively and statistically. A simple indirect comparison may be confounded if the studies involving one of the treatments of interest are fundamentally different from the studies involving the other treatment of interest. Here, we discuss methods for addressing inconsistency of evidence from comparative studies of different treatments. We define and review basic concepts of heterogeneity and inconsistency, and attempt to introduce a distinction between `loop inconsistency' and `design inconsistency'. We then propose that the notion of design-by-treatment interaction provides a useful general framework for investigating inconsistency. In particular, using design-by-treatment interactions successfully addresses complications that arise from the presence of multi-arm trials in an evidence network. We show how the inconsistency model proposed by Lu and Ades is a restricted version of our full design-by-treatment interaction model and that there may be several distinct Lu\textendash Ades models for any particular data set. We introduce novel graphical methods for depicting networks of evidence, clearly depicting multi-arm trials and illustrating where there is potential for inconsistency to arise. We apply various inconsistency models to data from trials of different comparisons among four smoking cessation interventions and show that models seeking to address loop inconsistency alone can run into problems. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UW3GHSRN/Higgins et al. - 2012 - Consistency and inconsistency in network meta-anal.pdf;/Users/rritaz/Zotero/storage/R7UDXE5U/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,network meta-analysis},
  language = {en},
  number = {2}
}

@article{higgins_controlling_2004,
  title = {Controlling the Risk of Spurious Findings from Meta-Regression},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  year = {2004},
  volume = {23},
  pages = {1663--1682},
  issn = {1097-0258},
  doi = {10.1002/sim.1752},
  abstract = {Meta-regression has become a commonly used tool for investigating whether study characteristics may explain heterogeneity of results among studies in a systematic review. However, such explorations of heterogeneity are prone to misleading false-positive results. It is unclear how many covariates can reliably be investigated, and how this might depend on the number of studies, the extent of the heterogeneity and the relative weights awarded to the different studies. Our objectives in this paper are two-fold. First, we use simulation to investigate the type I error rate of meta-regression in various situations. Second, we propose a permutation test approach for assessing the true statistical significance of an observed meta-regression finding. Standard meta-regression methods suffer from substantially inflated false-positive rates when heterogeneity is present, when there are few studies and when there are many covariates. These are typical of situations in which meta-regressions are routinely employed. We demonstrate in particular that fixed effect meta-regression is likely to produce seriously misleading results in the presence of heterogeneity. The permutation test appropriately tempers the statistical significance of meta-regression findings. We recommend its use before a statistically significant relationship is claimed from a standard meta-regression analysis. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/XYCXL2RH/Higgins and Thompson - 2004 - Controlling the risk of spurious findings from met.pdf;/Users/rritaz/Zotero/storage/QHVF3D5W/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,modeling effect size variation (covariates)},
  language = {en},
  number = {11}
}

@article{higgins_meta-analysis_2001,
  title = {Meta-Analysis of Continuous Outcome Data from Individual Patients},
  author = {Higgins, Julian P. T. and Whitehead, Anne and Turner, Rebecca M. and Omar, Rumana Z. and Thompson, Simon G.},
  year = {2001},
  volume = {20},
  pages = {2219--2241},
  issn = {1097-0258},
  doi = {10.1002/sim.918},
  abstract = {Meta-analyses using individual patient data are becoming increasingly common and have several advantages over meta-analyses of summary statistics. We explore the use of multilevel or hierarchical models for the meta-analysis of continuous individual patient outcome data from clinical trials. A general framework is developed which encompasses traditional meta-analysis, as well as meta-regression and the inclusion of patient-level covariates for investigation of heterogeneity. Unexplained variation in treatment differences between trials is considered as random. We focus on models with fixed trial effects, although an extension to a random effect for trial is described. The methods are illustrated on an example in Alzheimer's disease in a classical framework using SAS PROC MIXED and MLwiN, and in a Bayesian framework using BUGS. Relative merits of the three software packages for such meta-analyses are discussed, as are the assessment of model assumptions and extensions to incorporate more than two treatments. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EQM5VVS7/Higgins et al. - 2001 - Meta-analysis of continuous outcome data from indi.pdf;/Users/rritaz/Zotero/storage/72BCAYXZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {15}
}

@article{higgins_meta-analysis_2008,
  title = {Meta-Analysis of Skewed Data: {{Combining}} Results Reported on Log-Transformed or Raw Scales},
  shorttitle = {Meta-Analysis of Skewed Data},
  author = {Higgins, Julian P. T. and White, Ian R. and Anzures-Cabrera, Judith},
  year = {2008},
  volume = {27},
  pages = {6072--6092},
  issn = {1097-0258},
  doi = {10.1002/sim.3427},
  abstract = {When literature-based meta-analyses involve outcomes with skewed distributions, the best available data can sometimes be a mixture of results presented on the raw scale and results presented on the logarithmic scale. We review and develop methods for transforming between these results for two-group studies, such as clinical trials and prospective or cross-sectional epidemiological studies. These allow meta-analyses to be conducted using all studies and on a common scale. The methods can also be used to produce a meta-analysis of ratios of geometric means when skewed data are reported on the raw scale for every study. We compare three methods, two of which have alternative standard error formulae, in an application and in a series of simulation studies. We conclude that an approach based on a log-normal assumption for the raw data is reasonably robust to different true distributions; and we provide new standard error approximations for this method. An assumption can be made that the standard deviations in the two groups are equal. This increases precision of the estimates, but if incorrect can lead to very misleading results. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/D84J8ZZZ/Higgins et al. - 2008 - Meta-analysis of skewed data Combining results re.pdf;/Users/rritaz/Zotero/storage/WDGHHS77/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,effect size estimation (series),random-effects},
  language = {en},
  number = {29}
}

@article{higgins_quantifying_2002,
  title = {Quantifying Heterogeneity in a Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  year = {2002},
  volume = {21},
  pages = {1539--1558},
  issn = {1097-0258},
  doi = {10.1002/sim.1186},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the {$\chi$}2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of H that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EMDUFVUM/Higgins and Thompson - 2002 - Quantifying heterogeneity in a meta-analysis.pdf;/Users/rritaz/Zotero/storage/VKK62D4X/sim.html},
  journal = {Statistics in Medicine},
  keywords = {quantifying heterogeneity,random-effects},
  language = {en},
  number = {11}
}

@article{higgins_sequential_2011,
  title = {Sequential Methods for Random-Effects Meta-Analysis},
  author = {Higgins, Julian P. T. and Whitehead, Anne and Simmonds, Mark},
  year = {2011},
  volume = {30},
  pages = {903--921},
  issn = {1097-0258},
  doi = {10.1002/sim.4088},
  abstract = {Although meta-analyses are typically viewed as retrospective activities, they are increasingly being applied prospectively to provide up-to-date evidence on specific research questions. When meta-analyses are updated account should be taken of the possibility of false-positive findings due to repeated significance tests. We discuss the use of sequential methods for meta-analyses that incorporate random effects to allow for heterogeneity across studies. We propose a method that uses an approximate semi-Bayes procedure to update evidence on the among-study variance, starting with an informative prior distribution that might be based on findings from previous meta-analyses. We compare our methods with other approaches, including the traditional method of cumulative meta-analysis, in a simulation study and observe that it has Type I and Type II error rates close to the nominal level. We illustrate the method using an example in the treatment of bleeding peptic ulcers. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NDSKCSJ4/Higgins et al. - 2011 - Sequential methods for random-effects meta-analysi.pdf;/Users/rritaz/Zotero/storage/2HSTD87V/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance,power,random-effects},
  language = {en},
  number = {9}
}

@article{hinchliffe_using_2013,
  title = {Using Meta-Analysis to Inform the Design of Subsequent Studies of Diagnostic Test Accuracy},
  author = {Hinchliffe, Sally R. and Crowther, Michael J. and Phillips, Robert S. and Sutton, Alex J.},
  year = {2013},
  volume = {4},
  pages = {156--168},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1066},
  abstract = {An individual diagnostic accuracy study rarely provides enough information to make conclusive recommendations about the accuracy of a diagnostic test; particularly when the study is small. Meta-analysis methods provide a way of combining information from multiple studies, reducing uncertainty in the result and hopefully providing substantial evidence to underpin reliable clinical decision-making. Very few investigators consider any sample size calculations when designing a new diagnostic accuracy study. However, it is important to consider the number of subjects in a new study in order to achieve a precise measure of accuracy. Sutton et al. have suggested previously that when designing a new therapeutic trial, it could be more beneficial to consider the power of the updated meta-analysis including the new trial rather than of the new trial itself. The methodology involves simulating new studies for a range of sample sizes and estimating the power of the updated meta-analysis with each new study added. Plotting the power values against the range of sample sizes allows the clinician to make an informed decision about the sample size of a new trial. This paper extends this approach from the trial setting and applies it to diagnostic accuracy studies. Several meta-analytic models are considered including bivariate random effects meta-analysis that models the correlation between sensitivity and specificity. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EP7ALNFP/Hinchliffe et al. - 2013 - Using meta-analysis to inform the design of subseq.pdf;/Users/rritaz/Zotero/storage/UWLMEIU4/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes,power},
  language = {en},
  number = {2}
}

@article{hirji_calculating_2011,
  title = {Calculating Unreported Confidence Intervals for Paired Data},
  author = {Hirji, Karim F and Fagerland, Morten W},
  year = {2011},
  month = dec,
  volume = {11},
  pages = {66},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-66},
  abstract = {Background: Confidence intervals (or associated standard errors) facilitate assessment of the practical importance of the findings of a health study, and their incorporation into a meta-analysis. For paired design studies, these items are often not reported. Since the descriptive statistics for such studies are usually presented in the same way as for unpaired designs, direct computation of the standard error is not possible without additional information. Methods: Elementary, well-known relationships between standard errors and p-values were used to develop computation schemes for paired mean difference, risk difference, risk ratio and odds ratio. Results: Unreported confidence intervals for large sample paired binary and numeric data can be computed fairly accurately using simple methods provided the p-value is given. In the case of paired binary data, the design based 2 \texttimes{} 2 table can be reconstructed as well. Conclusions: Our results will facilitate appropriate interpretation of paired design studies, and their incorporation into meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/KWRDB9JX/Hirji and Fagerland - 2011 - Calculating unreported confidence intervals for pa.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,missing data},
  language = {en},
  number = {1}
}

@article{hofler_re-interpreting_2006,
  title = {Re-Interpreting Conventional Interval Estimates Taking into Account Bias and Extra-Variation},
  author = {H{\"o}fler, Michael and Seaman, Shaun R},
  year = {2006},
  month = dec,
  volume = {6},
  pages = {51},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-6-51},
  abstract = {Background: The study design with the smallest bias for causal inference is a perfect randomized clinical trial. Since this design is often not feasible in epidemiologic studies, an important challenge is to model bias properly and take random and systematic variation properly into account. A value for a target parameter might be said to be "incompatible" with the data (under the model used) if the parameter's confidence interval excludes it. However, this "incompatibility" may be due to bias and/or extra-variation.},
  file = {/Users/rritaz/Zotero/storage/2TW62QBK/Höfler and Seaman - 2006 - Re-interpreting conventional interval estimates ta.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{holzhauer_evidence_2018,
  title = {Evidence Synthesis from Aggregate Recurrent Event Data for Clinical Trial Design and Analysis},
  author = {Holzhauer, Bj{\"o}rn and Wang, Craig and Schmidli, Heinz},
  year = {2018},
  volume = {37},
  pages = {867--882},
  issn = {1097-0258},
  doi = {10.1002/sim.7549},
  abstract = {Information from historical trials is important for the design, interim monitoring, analysis, and interpretation of clinical trials. Meta-analytic models can be used to synthesize the evidence from historical data, which are often only available in aggregate form. We consider evidence synthesis methods for trials with recurrent event endpoints, which are common in many therapeutic areas. Such endpoints are typically analyzed by negative binomial regression. However, the individual patient data necessary to fit such a model are usually unavailable for historical trials reported in the medical literature. We describe approaches for back-calculating model parameter estimates and their standard errors from available summary statistics with various techniques, including approximate Bayesian computation. We propose to use a quadratic approximation to the log-likelihood for each historical trial based on 2 independent terms for the log mean rate and the log of the dispersion parameter. A Bayesian hierarchical meta-analysis model then provides the posterior predictive distribution for these parameters. Simulations show this approach with back-calculated parameter estimates results in very similar inference as using parameter estimates from individual patient data as an input. We illustrate how to design and analyze a new randomized placebo-controlled exacerbation trial in severe eosinophilic asthma using data from 11 historical trials.},
  file = {/Users/rritaz/Zotero/storage/CBLBMBA5/Holzhauer et al. - 2018 - Evidence synthesis from aggregate recurrent event .pdf;/Users/rritaz/Zotero/storage/TQ8QXFPL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance},
  language = {en},
  number = {6}
}

@article{holzhauer_meta-analysis_2017,
  title = {Meta-Analysis of Aggregate Data on Medical Events},
  author = {Holzhauer, Bj{\"o}rn},
  year = {2017},
  volume = {36},
  pages = {723--737},
  issn = {1097-0258},
  doi = {10.1002/sim.7181},
  abstract = {Meta-analyses of clinical trials often treat the number of patients experiencing a medical event as binomially distributed when individual patient data for fitting standard time-to-event models are unavailable. Assuming identical drop-out time distributions across arms, random censorship, and low proportions of patients with an event, a binomial approach results in a valid test of the null hypothesis of no treatment effect with minimal loss in efficiency compared with time-to-event methods. To deal with differences in follow-up\textemdash at the cost of assuming specific distributions for event and drop-out times\textemdash we propose a hierarchical multivariate meta-analysis model using the aggregate data likelihood based on the number of cases, fatal cases, and discontinuations in each group, as well as the planned trial duration and groups sizes. Such a model also enables exchangeability assumptions about parameters of survival distributions, for which they are more appropriate than for the expected proportion of patients with an event across trials of substantially different length. Borrowing information from other trials within a meta-analysis or from historical data is particularly useful for rare events data. Prior information or exchangeability assumptions also avoid the parameter identifiability problems that arise when using more flexible event and drop-out time distributions than the exponential one. We discuss the derivation of robust historical priors and illustrate the discussed methods using an example. We also compare the proposed approach against other aggregate data meta-analysis methods in a simulation study. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GMB98HKP/Holzhauer - 2017 - Meta-analysis of aggregate data on medical events.pdf;/Users/rritaz/Zotero/storage/5J6NW2VN/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),multivariate},
  language = {en},
  number = {5}
}

@article{honekopp_meaning_2006,
  title = {The Meaning and Suitability of Various Effect Sizes for Structured Rater \texttimes{} Ratee Designs.},
  author = {H{\"o}nekopp, Johannes and Becker, Betsy Jane and Oswald, Frederick L.},
  year = {2006},
  month = mar,
  volume = {11},
  pages = {72--86},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.1.72},
  file = {/Users/rritaz/Zotero/storage/28ZCMHQH/Hönekopp et al. - 2006 - The meaning and suitability of various effect size.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes},
  language = {en},
  number = {1}
}

@article{hong_bayesian_2016,
  title = {A {{Bayesian}} Missing Data Framework for Generalized Multiple Outcome Mixed Treatment Comparisons},
  author = {Hong, Hwanhee and Chu, Haitao and Zhang, Jing and Carlin, Bradley P.},
  year = {2016},
  volume = {7},
  pages = {6--22},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1153},
  abstract = {Bayesian statistical approaches to mixed treatment comparisons (MTCs) are becoming more popular because of their flexibility and interpretability. Many randomized clinical trials report multiple outcomes with possible inherent correlations. Moreover, MTC data are typically sparse (although richer than standard meta-analysis, comparing only two treatments), and researchers often choose study arms based upon which treatments emerge as superior in previous trials. In this paper, we summarize existing hierarchical Bayesian methods for MTCs with a single outcome and introduce novel Bayesian approaches for multiple outcomes simultaneously, rather than in separate MTC analyses. We do this by incorporating partially observed data and its correlation structure between outcomes through contrast-based and arm-based parameterizations that consider any unobserved treatment arms as missing data to be imputed. We also extend the model to apply to all types of generalized linear model outcomes, such as count or continuous responses. We offer a simulation study under various missingness mechanisms (e.g., missing completely at random, missing at random, and missing not at random) providing evidence that our models outperform existing models in terms of bias, mean squared error, and coverage probability then illustrate our methods with a real MTC dataset. We close with a discussion of our results, several contentious issues in MTC analysis, and a few avenues for future methodological development. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ECIG4T9U/Hong et al. - 2016 - A Bayesian missing data framework for generalized .pdf;/Users/rritaz/Zotero/storage/R63J4CDK/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,missing data,network meta-analysis},
  language = {en},
  number = {1}
}

@article{hong_improved_2018,
  title = {An Improved Method for Bivariate Meta-Analysis When within-Study Correlations Are Unknown},
  author = {Hong, Chuan and Riley, Richard D. and Chen, Yong},
  year = {2018},
  volume = {9},
  pages = {73--88},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1274},
  abstract = {Multivariate meta-analysis, which jointly analyzes multiple and possibly correlated outcomes in a single analysis, is becoming increasingly popular in recent years. An attractive feature of the multivariate meta-analysis is its ability to account for the dependence between multiple estimates from the same study. However, standard inference procedures for multivariate meta-analysis require the knowledge of within-study correlations, which are usually unavailable. This limits standard inference approaches in practice. Riley et al proposed a working model and an overall synthesis correlation parameter to account for the marginal correlation between outcomes, where the only data needed are those required for a separate univariate random-effects meta-analysis. As within-study correlations are not required, the Riley method is applicable to a wide variety of evidence synthesis situations. However, the standard variance estimator of the Riley method is not entirely correct under many important settings. As a consequence, the coverage of a function of pooled estimates may not reach the nominal level even when the number of studies in the multivariate meta-analysis is large. In this paper, we improve the Riley method by proposing a robust variance estimator, which is asymptotically correct even when the model is misspecified (ie, when the likelihood function is incorrect). Simulation studies of a bivariate meta-analysis, in a variety of settings, show a function of pooled estimates has improved performance when using the proposed robust variance estimator. In terms of individual pooled estimates themselves, the standard variance estimator and robust variance estimator give similar results to the original method, with appropriate coverage. The proposed robust variance estimator performs well when the number of studies is relatively large. Therefore, we recommend the use of the robust method for meta-analyses with a relatively large number of studies (eg, m{$\geq$}50). When the sample size is relatively small, we recommend the use of the robust method under the working independence assumption. We illustrate the proposed method through 2 meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/GF72H7ZQ/Hong et al. - 2018 - An improved method for bivariate meta-analysis whe.pdf;/Users/rritaz/Zotero/storage/H5J8XSNI/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {1}
}

@article{hong_incorporation_2015,
  title = {Incorporation of Individual-Patient Data in Network Meta-Analysis for Multiple Continuous Endpoints, with Application to Diabetes Treatment},
  author = {Hong, Hwanhee and Fu, Haoda and Price, Karen L. and Carlin, Bradley P.},
  year = {2015},
  volume = {34},
  pages = {2794--2819},
  issn = {1097-0258},
  doi = {10.1002/sim.6519},
  abstract = {Availability of individual patient-level data (IPD) broadens the scope of network meta-analysis (NMA) and enables us to incorporate patient-level information. Although IPD is a potential gold mine in biomedical areas, methodological development has been slow owing to limited access to such data. In this paper, we propose a Bayesian IPD NMA modeling framework for multiple continuous outcomes under both contrast-based and arm-based parameterizations. We incorporate individual covariate-by-treatment interactions to facilitate personalized decision making. Furthermore, we can find subpopulations performing well with a certain drug in terms of predictive outcomes. We also impute missing individual covariates via an MCMC algorithm. We illustrate this approach using diabetes data that include continuous bivariate efficacy outcomes and three baseline covariates and show its practical implications. Finally, we close with a discussion of our results, a review of computational challenges, and a brief description of areas for future research. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GPYIEVCF/Hong et al. - 2015 - Incorporation of individual-patient data in networ.pdf;/Users/rritaz/Zotero/storage/LAPBBI6G/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,correlated effects,Individual Patient Data IPD,modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {20}
}

@article{horn_estimating_1975,
  title = {Estimating {{Heteroscedastic Variances}} in {{Linear Models}}},
  author = {Horn, Susan D. and Horn, Roger A. and Duncan, David B.},
  year = {1975},
  month = jun,
  volume = {70},
  pages = {380--385},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1975.10479877},
  file = {/Users/rritaz/Zotero/storage/HJE7ECCL/Horn et al. - 1975 - Estimating Heteroscedastic Variances in Linear Mod.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {350}
}

@article{hoyer_meta-analysis_2015,
  title = {Meta-Analysis of Diagnostic Tests Accounting for Disease Prevalence: A New Model Using Trivariate Copulas},
  shorttitle = {Meta-Analysis of Diagnostic Tests Accounting for Disease Prevalence},
  author = {Hoyer, A. and Kuss, O.},
  year = {2015},
  volume = {34},
  pages = {1912--1924},
  issn = {1097-0258},
  doi = {10.1002/sim.6463},
  abstract = {In real life and somewhat contrary to biostatistical textbook knowledge, sensitivity and specificity (and not only predictive values) of diagnostic tests can vary with the underlying prevalence of disease. In meta-analysis of diagnostic studies, accounting for this fact naturally leads to a trivariate expansion of the traditional bivariate logistic regression model with random study effects. In this paper, a new model is proposed using trivariate copulas and beta-binomial marginal distributions for sensitivity, specificity, and prevalence as an expansion of the bivariate model. Two different copulas are used, the trivariate Gaussian copula and a trivariate vine copula based on the bivariate Plackett copula. This model has a closed-form likelihood, so standard software (e.g., SAS PROC NLMIXED) can be used. The results of a simulation study have shown that the copula models perform at least as good but frequently better than the standard model. The methods are illustrated by two examples. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/TEHSCHVC/Hoyer and Kuss - 2015 - Meta-analysis of diagnostic tests accounting for d.pdf;/Users/rritaz/Zotero/storage/XCE97C26/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,multivariate},
  language = {en},
  number = {11}
}

@article{hoyer_meta-analysis_2018,
  title = {Meta-Analysis for the Comparison of Two Diagnostic Tests\textemdash{{A}} New Approach Based on Copulas},
  author = {Hoyer, Annika and Kuss, Oliver},
  year = {2018},
  volume = {37},
  pages = {739--748},
  issn = {1097-0258},
  doi = {10.1002/sim.7556},
  abstract = {Meta-analysis of diagnostic studies is still field of ongoing biometrical research. Especially, clinical researchers call for methods that allow for a comparison of different diagnostic tests to a common gold standard. Focussing on two diagnostic tests, the main parameters of interest are differences of sensitivities and specificities (with their corresponding confidence intervals) between the two diagnostic tests while accounting for the various associations across the two tests and the single studies. Similar to our previous work using generalized linear mixed models to this task, we propose a model with a quadrivariate response consisting of the two sensitivities and the two specificities of both tests. This new approach uses the ideas of copula modelling, and especially a quadrivariate Gaussian copula and a quadrivariate vine copula, which is built from bivariate Plackett copulas. The different copulas are compared in a simulation study and illustrated by the application of population-based screening for type 2 diabetes.},
  file = {/Users/rritaz/Zotero/storage/386Q9ZJP/Hoyer and Kuss - 2018 - Meta-analysis for the comparison of two diagnostic.pdf;/Users/rritaz/Zotero/storage/JA4BNX7S/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate},
  language = {en},
  number = {5}
}

@article{hoyer_meta-analysis_2018-1,
  title = {Meta-Analysis of Full {{ROC}} Curves Using Bivariate Time-to-Event Models for Interval-Censored Data},
  author = {Hoyer, Annika and Hirt, Stefan and Kuss, Oliver},
  year = {2018},
  volume = {9},
  pages = {62--72},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1273},
  abstract = {Systematic reviews and meta-analyses are the cornerstones of evidence-based medicine and inform treatment, diagnosis, or prevention of individual patients as well as policy decisions in health care. Statistical methods for the meta-analysis of intervention studies are well established today. Meta-analysis for diagnostic accuracy trials has also been a vivid research area in recent years, which is especially due to the increased complexity of their bivariate outcome of sensitivity and specificity. The situation is even more challenging when single studies report a full ROC curve with several pairs of sensitivity and specificity, each pair for a different threshold. Researchers frequently ignore this information and use only 1 pair of sensitivity and specificity from each study to arrive at meta-analytic estimates. Although methods to deal with the full information have been proposed, they have some disadvantages, eg, the numbers or values of thresholds have to be identical across studies, or the precise values of thresholds are ignored. We propose an approach for the meta-analysis of full ROC curves including the information from all thresholds by using bivariate time-to-event models for interval-censored data with random effects. This approach avoids the problems of previous methods and comes with the additional advantage that it allows for various distributions of the underlying continuous test values. The results from a small simulation study are given, which show that the approach works well in practice. Furthermore, we illustrate our new model using an example based on the population-based screening for type 2 diabetes mellitus.},
  file = {/Users/rritaz/Zotero/storage/N5BG76AX/Hoyer et al. - 2018 - Meta-analysis of full ROC curves using bivariate t.pdf;/Users/rritaz/Zotero/storage/I9XDBXMC/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {multivariate,random-effects},
  language = {en},
  number = {1}
}

@article{hoyer_meta-analysis_2019,
  title = {Meta-Analysis of Full {{ROC}} Curves: {{Additional}} Flexibility by Using Semiparametric Distributions of Diagnostic Test Values},
  shorttitle = {Meta-Analysis of Full {{ROC}} Curves},
  author = {Hoyer, Annika and Kuss, Oliver},
  year = {2019},
  volume = {10},
  pages = {528--538},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1364},
  abstract = {Diagnostic test accuracy studies frequently report on sensitivities and specificities for more than one threshold of the diagnostic test under study. Although it is obvious that the information from all thresholds should be used for a meta-analysis, in practice, frequently, only a single pair of sensitivity and specificity is selected. To overcome this disadvantage, we recently proposed a statistical model for the meta-analysis of such full receiver operating characteristic (ROC) curves that uses the relationship between a ROC curve and a bivariate model for interval-censored data. In this model, diagnostic tests values reported by the single studies were assumed to follow a parametric distribution. We propose a generalization of this model that allows for a flexible semiparametric modelling of the underlying distribution of the diagnostic test values by using the idea of piecewise constant hazard modelling. We show the results of a simulation study that indicates that the approach works reasonably well in practice. Finally, we illustrate the model by the example of population-based screening for type 2 diabetes mellitus.},
  file = {/Users/rritaz/Zotero/storage/AI9SA38X/Hoyer and Kuss - 2019 - Meta-analysis of full ROC curves Additional flexi.pdf;/Users/rritaz/Zotero/storage/LDRWA9K3/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance},
  language = {en},
  number = {4}
}

@article{hozo_estimating_2005,
  title = {Estimating the Mean and Variance from the Median, Range, and the Size of a Sample},
  author = {Hozo, Stela Pudar and Djulbegovic, Benjamin and Hozo, Iztok},
  year = {2005},
  month = dec,
  volume = {5},
  pages = {13},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-5-13},
  abstract = {Background: Usually the researchers performing meta-analysis of continuous outcomes from clinical trials need their mean value and the variance (or standard deviation) in order to pool data. However, sometimes the published reports of clinical trials only report the median, range and the size of the trial. Methods: In this article we use simple and elementary inequalities and approximations in order to estimate the mean and the variance for such trials. Our estimation is distribution-free, i.e., it makes no assumption on the distribution of the underlying data. Results: We found two simple formulas that estimate the mean using the values of the median (m), low and high end of the range (a and b, respectively), and n (the sample size). Using simulations, we show that median can be used to estimate mean when the sample size is larger than 25. For smaller samples our new formula, devised in this paper, should be used. We also estimated the variance of an unknown sample using the median, low and high end of the range, and the sample size. Our estimate is performing as the best estimate in our simulations for very small samples (n {$\leq$} 15). For moderately sized samples (15 {$<$}n {$\leq$} 70), our simulations show that the formula range/4 is the best estimator for the standard deviation (variance). For large samples (n {$>$} 70), the formula range/6 gives the best estimator for the standard deviation (variance). We also include an illustrative example of the potential value of our method using reports from the Cochrane review on the role of erythropoietin in anemia due to malignancy. Conclusion: Using these formulas, we hope to help meta-analysts use clinical trials in their analysis even when not all of the information is available and/or reported.},
  file = {/Users/rritaz/Zotero/storage/KBIKLJND/Hozo et al. - 2005 - Estimating the mean and variance from the median, .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes},
  language = {en},
  number = {1}
}

@article{hozo_use_2005,
  title = {Use of Re-Randomized Data in Meta-Analysis},
  author = {Hozo, Iztok and Djulbegovic, Benjamin and Clark, Otavio and Lyman, Gary H},
  year = {2005},
  month = dec,
  volume = {5},
  pages = {17},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-5-17},
  abstract = {Background: Outcomes collected in randomized clinical trials are observations of random variables that should be independent and identically distributed. However, in some trials, the patients are randomized more than once thus violating both of these assumptions. The probability of an event is not always the same when a patient is re-randomized; there is probably a non-zero covariance coming from observations on the same patient. This is of particular importance to the meta-analysts. Methods: We developed a method to estimate the relative error in the risk differences with and without re-randomization of the patients. The relative error can be estimated by an expression depending on the percentage of the patients who were re-randomized, multipliers (how many times more likely it is to repeat an event) for the probability of reoccurrences, and the ratio of the total events reported and the initial number of patients entering the trial. Results: We illustrate our methods using two randomized trials testing growth factors in febrile neutropenia. We showed that under some circumstances the relative error of taking into account re-randomized patients was sufficiently small to allow using the results in the meta-analysis. Our findings indicate that if the study in question is of similar size to other studies included in the metaanalysis, the error introduced by re-randomization will only minimally affect meta-analytic summary point estimate. We also show that in our model the risk ratio remains constant during the re-randomization, and therefore, if a meta-analyst is concerned about the effect of re-randomization on the meta-analysis, one way to sidestep the issue and still obtain reliable results is to use risk ratio as the measure of interest. Conclusion: Our method should be helpful in the understanding of the results of clinical trials and particularly helpful to the meta-analysts to assess if re-randomized patient data can be used in their analyses.},
  file = {/Users/rritaz/Zotero/storage/PL4CUJYR/Hozo et al. - 2005 - Use of re-randomized data in meta-analysis.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes},
  language = {en},
  number = {1}
}

@article{hsu_biases_2004,
  title = {Biases of {{Success Rate Differences Shown}} in {{Binomial Effect Size Displays}}.},
  author = {Hsu, Louis M.},
  year = {2004},
  volume = {9},
  pages = {183--197},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.183},
  file = {/Users/rritaz/Zotero/storage/UG2KIPT5/Hsu - 2004 - Biases of Success Rate Differences Shown in Binomi.pdf},
  journal = {Psychological Methods},
  keywords = {correlation coefficients},
  language = {en},
  number = {2}
}

@article{hsu_effects_2000,
  title = {Effects of Directionality of Significance Tests on the Bias of Accessible Effect Sizes.},
  author = {Hsu, Louis M.},
  year = {2000},
  volume = {5},
  pages = {333--342},
  doi = {10.1037//1082-989X.5.3.333},
  abstract = {The proportion of studies that use one-tailed statistical significance tests (pi) in a population of studies targeted by a meta-analysis can affect the bias of the sample effect sizes (sample ESs, or ds) that are accessible to the meta-analyst. H. C. Kraemer, C. Gardner, J. O. Brooks, and J. A. Yesavage (1998) found that, assuming pi = 1.0, for small studies (small Ns) the overestimation bias was large for small population ESs (delta {$<$} or = 0.2) and reached a maximum for the smallest population ES (viz., delta = 0). The present article shows (with a minor modification of H. C. Kraemer et al.'s model) that when pi = 0, the small-N bias of accessible sample ESs is relatively small for delta {$<$} or = 0.2, and a minimum (in fact, nonexistent) for delta = 0. Implications are discussed for interpretations of meta-analyses of (a) therapy efficacy and therapy effectiveness studies, (b) comparative outcome studies, and (c) studies targeting small but important population ESs.},
  file = {/Users/rritaz/Zotero/storage/8CC9YP2D/Hsu - 2000 - Effects of directionality of significance tests on.pdf},
  journal = {Psychological methods},
  keywords = {diagnostic techniques},
  number = {3}
}

@article{hsu_properties_2005,
  title = {Some Properties of Requivalent: {{A}} Simple Effect Size Indicator},
  shorttitle = {Some Properties of Requivalent},
  author = {Hsu, Louis M.},
  year = {2005},
  month = dec,
  volume = {10},
  pages = {420--427},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.10.4.420},
  abstract = {One version of requivalent, calculated from Fisher's exact test p values and recommended for small samples, is considered 'a more realistic . . . [and] a more accurate estimate of the population correlation than . . . the sample correlation, rsample' (R. Rosenthal \& D. B. Rubin, 2003, p. 494). Small sample properties of rsample and of two effect size estimators (requivalent and rhybrid) that use requivalent were examined: rsample is preferable to requivalent (defined as requivalent used without restrictions) in terms of bias and mean squared error (MSE); rhybrid (defined as requivalent only when rsample = 1.0) is generally preferable to requivalent, and preferable to rsample in terms of MSEs, except when population correlations are very large. Conditions favoring rsample over requivalent and rhybrid in meta-analyses are noted. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {combined significance,correlation coefficients},
  number = {4}
}

@article{hua_one-stage_2017,
  title = {One-Stage Individual Participant Data Meta-Analysis Models: Estimation of Treatment-Covariate Interactions Must Avoid Ecological Bias by Separating out within-Trial and across-Trial Information},
  shorttitle = {One-Stage Individual Participant Data Meta-Analysis Models},
  author = {Hua, Hairui and Burke, Danielle L. and Crowther, Michael J. and Ensor, Joie and Smith, Catrin Tudur and Riley, Richard D.},
  year = {2017},
  volume = {36},
  pages = {772--789},
  issn = {1097-0258},
  doi = {10.1002/sim.7171},
  abstract = {Stratified medicine utilizes individual-level covariates that are associated with a differential treatment effect, also known as treatment-covariate interactions. When multiple trials are available, meta-analysis is used to help detect true treatment-covariate interactions by combining their data. Meta-regression of trial-level information is prone to low power and ecological bias, and therefore, individual participant data (IPD) meta-analyses are preferable to examine interactions utilizing individual-level information. However, one-stage IPD models are often wrongly specified, such that interactions are based on amalgamating within- and across-trial information. We compare, through simulations and an applied example, fixed-effect and random-effects models for a one-stage IPD meta-analysis of time-to-event data where the goal is to estimate a treatment-covariate interaction. We show that it is crucial to centre patient-level covariates by their mean value in each trial, in order to separate out within-trial and across-trial information. Otherwise, bias and coverage of interaction estimates may be adversely affected, leading to potentially erroneous conclusions driven by ecological bias. We revisit an IPD meta-analysis of five epilepsy trials and examine age as a treatment effect modifier. The interaction is -0.011 (95\% CI: -0.019 to -0.003; p = 0.004), and thus highly significant, when amalgamating within-trial and across-trial information. However, when separating within-trial from across-trial information, the interaction is -0.007 (95\% CI: -0.019 to 0.005; p = 0.22), and thus its magnitude and statistical significance are greatly reduced. We recommend that meta-analysts should only use within-trial information to examine individual predictors of treatment effect and that one-stage IPD models should separate within-trial from across-trial information to avoid ecological bias. \textcopyright{} 2016 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/A4RUW5DD/Hua et al. - 2017 - One-stage individual participant data meta-analysi.pdf;/Users/rritaz/Zotero/storage/YP3E7S4Y/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {5}
}

@article{huedo-medina_assessing_2006,
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I}}{$^2$} Index?},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  author = {{Huedo-Medina}, Tania B. and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and Botella, Juan},
  year = {2006},
  volume = {11},
  pages = {193--206},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.2.193},
  abstract = {In meta-analysis, the usual way of assessing whether a set of single studies are homogeneous is by means of the Q test. However, the Q test only informs us about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I2 index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this paper, the performances of the Q test and the confidence interval around the I2 index are compared by means of a Monte Carlo simulation. The results show the utility of the I2 index as a complement to the Q test, although it has the same problems of power with a small number of studies.},
  file = {/Users/rritaz/Zotero/storage/XU96M2YE/Huedo-Medina et al. - 2006 - Assessing heterogeneity in meta-analysis Q statis.pdf},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,modeling effect size variation (covariates),random-effects,small meta-analysis},
  language = {en},
  number = {2}
}

@article{hwang_multivariate_2018,
  title = {Multivariate Network Meta-Analysis to Mitigate the Effects of Outcome Reporting Bias},
  author = {Hwang, Hyunsoo and DeSantis, Stacia M.},
  year = {2018},
  volume = {37},
  pages = {3254--3266},
  issn = {1097-0258},
  doi = {10.1002/sim.7815},
  abstract = {Outcome reporting bias (ORB) is recognized as a threat to the validity of both pairwise and network meta-analysis (NMA). In recent years, multivariate meta-analytic methods have been proposed to reduce the impact of ORB in the pairwise setting. These methods have shown that multivariate meta-analysis can reduce bias and increase efficiency of pooled effect sizes. However, it is unknown whether multivariate NMA (MNMA) can similarly reduce the impact of ORB. Additionally, it is quite challenging to implement MNMA due to the fact that correlation between treatments and outcomes must be modeled; thus, the dimension of the covariance matrix and number of components to estimate grows quickly with the number of treatments and number of outcomes. To determine whether MNMA can reduce the effects of ORB on pooled treatment effect sizes, we present an extensive simulation study of Bayesian MNMA. Via simulation studies, we show that MNMA reduces the bias of pooled effect sizes under a variety of outcome missingness scenarios, including missing at random and missing not at random. Further, MNMA improves the precision of estimates, producing narrower credible intervals. We demonstrate the applicability of the approach via application of MNMA to a multi-treatment systematic review of randomized controlled trials of anti-depressants for the treatment of depression in older adults.},
  file = {/Users/rritaz/Zotero/storage/CEZJN7S9/Hwang and DeSantis - 2018 - Multivariate network meta-analysis to mitigate the.pdf;/Users/rritaz/Zotero/storage/F8MJ77RU/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate,network meta-analysis,random-effects},
  language = {en},
  number = {22}
}

@misc{idostatistics_hedges_2016,
  title = {Hedges, {{L}}. {{V}}., \& {{Olkin}}, {{I}}. (1985). {{Statistical}} Methods for Meta-Analysis},
  author = {{idostatistics}},
  year = {2016},
  month = mar,
  abstract = {Hedges, L. V., \& Olkin, I. (1985). Statistical methods for meta-analysis. San Diego, CA: Academic Press. Abstract The main purpose of this book is to address the statistical issues for integrating independent studies. There exist a number of papers and books that discuss the mechanics of collecting, coding, and preparing data for a meta-analysis , Read More},
  file = {/Users/rritaz/Zotero/storage/9RZL9ZBH/hedges-olkin-1985-statistical-methods-for-meta-analysis.html},
  howpublished = {https://idostatistics.com/hedges-olkin-1985-statistical-methods-for-meta-analysis/},
  journal = {IDoStatistics},
  language = {en-US}
}

@article{inthout_hartung-knapp-sidik-jonkman_2014,
  title = {The {{Hartung}}-{{Knapp}}-{{Sidik}}-{{Jonkman}} Method for Random Effects Meta-Analysis Is Straightforward and Considerably Outperforms the Standard {{DerSimonian}}-{{Laird}} Method},
  author = {IntHout, Joanna and Ioannidis, John PA and Borm, George F},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {25},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-25},
  abstract = {Background: The DerSimonian and Laird approach (DL) is widely used for random effects meta-analysis, but this often results in inappropriate type I error rates. The method described by Hartung, Knapp, Sidik and Jonkman (HKSJ) is known to perform better when trials of similar size are combined. However evidence in realistic situations, where one trial might be much larger than the other trials, is lacking. We aimed to evaluate the relative performance of the DL and HKSJ methods when studies of different sizes are combined and to develop a simple method to convert DL results to HKSJ results. Methods: We evaluated the performance of the HKSJ versus DL approach in simulated meta-analyses of 2\textendash 20 trials with varying sample sizes and between-study heterogeneity, and allowing trials to have various sizes, e.g. 25\% of the trials being 10-times larger than the smaller trials. We also compared the number of ``positive'' (statistically significant at p {$<$} 0.05) findings using empirical data of recent meta-analyses with {$>$} = 3 studies of interventions from the Cochrane Database of Systematic Reviews. Results: The simulations showed that the HKSJ method consistently resulted in more adequate error rates than the DL method. When the significance level was 5\%, the HKSJ error rates at most doubled, whereas for DL they could be over 30\%. DL, and, far less so, HKSJ had more inflated error rates when the combined studies had unequal sizes and between-study heterogeneity. The empirical data from 689 meta-analyses showed that 25.1\% of the significant findings for the DL method were non-significant with the HKSJ method. DL results can be easily converted into HKSJ results. Conclusions: Our simulations showed that the HKSJ method consistently results in more adequate error rates than the DL method, especially when the number of studies is small, and can easily be applied routinely in meta-analyses. Even with the HKSJ method, extra caution is needed when there are = {$<$}5 studies of very unequal sizes.},
  file = {/Users/rritaz/Zotero/storage/WWUTYPP7/IntHout et al. - 2014 - The Hartung-Knapp-Sidik-Jonkman method for random .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {random effects models,random-effects},
  language = {en},
  number = {1}
}

@article{inthout_small_2015,
  title = {Small Studies Are More Heterogeneous than Large Ones: A Meta-Meta-Analysis},
  shorttitle = {Small Studies Are More Heterogeneous than Large Ones},
  author = {IntHout, Joanna and Ioannidis, John P.A. and Borm, George F. and Goeman, Jelle J.},
  year = {2015},
  month = aug,
  volume = {68},
  pages = {860--869},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2015.03.017},
  abstract = {Objectives: Between-study heterogeneity plays an important role in random-effects models for meta-analysis. Most clinical trials are small, and small trials are often associated with larger effect sizes. We empirically evaluated whether there is also a relationship between trial size and heterogeneity (t). Study Design and Setting: We selected the first meta-analysis per intervention review of the Cochrane Database of Systematic Reviews Issues 2009e2013 with a dichotomous (n 5 2,009) or continuous (n 5 1,254) outcome. The association between estimated t and trial size was evaluated across meta-analyses using regression and within meta-analyses using a Bayesian approach. Small trials were predefined as those having standard errors (SEs) over 0.2 standardized effects. Results: Most meta-analyses were based on few (median 4) trials. Within the same meta-analysis, the small study tS2 was larger than the large-study tL2 [average ratio 2.11; 95\% credible interval (1.05, 3.87) for dichotomous and 3.11 (2.00, 4.78) for continuous metaanalyses]. The imprecision of tS was larger than of tL: median SE 0.39 vs. 0.20 for dichotomous and 0.22 vs. 0.13 for continuous small-study and large-study meta-analyses. Conclusion: Heterogeneity between small studies is larger than between larger studies. The large imprecision with which t is estimated in a typical small-studies' meta-analysis is another reason for concern, and sensitivity analyses are recommended. \'O 2015 Elsevier Inc. All rights reserved.},
  file = {/Users/rritaz/Zotero/storage/DIKWUK6X/IntHout et al. - 2015 - Small studies are more heterogeneous than large on.pdf},
  journal = {Journal of Clinical Epidemiology},
  language = {en},
  number = {8}
}

@article{isaman_indirect_2009,
  title = {Indirect Estimation of a Discrete-State Discrete-Time Model Using Secondary Data Analysis of Regression Data},
  author = {Isaman, Deanna J. M. and Barhak, Jacob and Ye, Wen},
  year = {2009},
  volume = {28},
  pages = {2095--2115},
  issn = {1097-0258},
  doi = {10.1002/sim.3599},
  abstract = {Multi-state models of chronic disease are becoming increasingly important in medical research to describe the progression of complicated diseases. However, studies seldom observe health outcomes over long time periods. Therefore, current clinical research focuses on the secondary data analysis of the published literature to estimate a single transition probability within the entire model. Unfortunately, there are many difficulties when using secondary data, especially since the states and transitions of published studies may not be consistent with the proposed multi-state model. Early approaches to reconciling published studies with the theoretical framework of a multi-state model have been limited to data available as cumulative counts of progression. This paper presents an approach that allows the use of published regression data in a multi-state model when the published study may have ignored intermediary states in the multi-state model. Colloquially, we call this approach the Lemonade Method since when study data give you lemons, make lemonade. The approach uses maximum likelihood estimation. An example is provided for the progression of heart disease in people with diabetes. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LSX6KFRP/Isaman et al. - 2009 - Indirect estimation of a discrete-state discrete-t.pdf;/Users/rritaz/Zotero/storage/E8X2CVAE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {discrete effect sizes,modeling effect size variation (covariates)},
  language = {en},
  number = {16}
}

@article{ishak_impact_2008,
  title = {Impact of Approximating or Ignoring Within-Study Covariances in Multivariate Meta-Analyses},
  author = {Ishak, K. Jack and Platt, Robert W. and Joseph, Lawrence and Hanley, James A.},
  year = {2008},
  volume = {27},
  pages = {670--686},
  issn = {1097-0258},
  doi = {10.1002/sim.2913},
  abstract = {Multivariate meta-analyses are used to derive summary estimates of treatment effects for two or more outcomes from a joint model. In addition to treatment effects, these models also quantify the correlations between outcomes across studies. To be fully specified, the model requires an estimate of the covariance or correlations between outcomes observed in each study. These are rarely available in published reports, so that analysts must either approximate these or ignore correlations between effect estimates from the same studies. We examined the impact of errors in approximating within-study covariances on the parameters of multivariate models in a simulation study. We found that treatment effect and heterogeneity estimates were not strongly affected by inaccurate approximations, but estimates of the correlation between outcomes were sometimes highly biased. The potential for error is greatest when the covariance between outcomes within- and between-studies are of comparable scale. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/P3RY4LU2/Ishak et al. - 2008 - Impact of approximating or ignoring within-study c.pdf;/Users/rritaz/Zotero/storage/NA2EBNBL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate},
  language = {en},
  number = {5}
}

@article{islas_addressing_2018,
  title = {Addressing the Estimation of Standard Errors in Fixed Effects Meta-Analysis},
  author = {Islas, Clara Dom{\'i}nguez and Rice, Kenneth M.},
  year = {2018},
  volume = {37},
  pages = {1788--1809},
  issn = {1097-0258},
  doi = {10.1002/sim.7625},
  abstract = {Standard methods for fixed effects meta-analysis assume that standard errors for study-specific estimates are known, not estimated. While the impact of this simplifying assumption has been shown in a few special cases, its general impact is not well understood, nor are general-purpose tools available for inference under more realistic assumptions. In this paper, we aim to elucidate the impact of using estimated standard errors in fixed effects meta-analysis, showing why it does not go away in large samples and quantifying how badly miscalibrated standard inference will be if it is ignored. We also show the important role of a particular measure of heterogeneity in this miscalibration. These developments lead to confidence intervals for fixed effects meta-analysis with improved performance for both location and scale parameters.},
  file = {/Users/rritaz/Zotero/storage/5PKC4EVJ/Islas and Rice - 2018 - Addressing the estimation of standard errors in fi.pdf;/Users/rritaz/Zotero/storage/NVA8CTRY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,fixed effects model,random-effects},
  language = {en},
  number = {11}
}

@article{jackson_approximate_2015,
  title = {Approximate Confidence Intervals for Moment-Based Estimators of the between-Study Variance in Random Effects Meta-Analysis},
  author = {Jackson, Dan and Bowden, Jack and Baker, Rose},
  year = {2015},
  volume = {6},
  pages = {372--382},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1162},
  abstract = {Moment-based estimators of the between-study variance are very popular when performing random effects meta-analyses. This type of estimation has many advantages including computational and conceptual simplicity. Furthermore, by using these estimators in large samples, valid meta-analyses can be performed without the assumption that the treatment effects follow a normal distribution. Recently proposed moment-based confidence intervals for the between-study variance are exact under the random effects model but are quite elaborate. Here, we present a much simpler method for calculating approximate confidence intervals of this type. This method uses variance-stabilising transformations as its basis and can be used for a very wide variety of moment-based estimators in both the random effects meta-analysis and meta-regression models. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/X8ZIDINF/Jackson et al. - 2015 - Approximate confidence intervals for moment-based .pdf;/Users/rritaz/Zotero/storage/PBSEA3Q9/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Confidence Intervals,random-effects},
  language = {en},
  number = {4}
}

@article{jackson_comparison_2018,
  title = {A Comparison of Seven Random-Effects Models for Meta-Analyses That Estimate the Summary Odds Ratio},
  author = {Jackson, Dan and Law, Martin and Stijnen, Theo and Viechtbauer, Wolfgang and White, Ian R.},
  year = {2018},
  volume = {37},
  pages = {1059--1085},
  issn = {1097-0258},
  doi = {10.1002/sim.7588},
  abstract = {Comparative trials that report binary outcome data are commonly pooled in systematic reviews and meta-analyses. This type of data can be presented as a series of 2-by-2 tables. The pooled odds ratio is often presented as the outcome of primary interest in the resulting meta-analysis. We examine the use of 7 models for random-effects meta-analyses that have been proposed for this purpose. The first of these models is the conventional one that uses normal within-study approximations and a 2-stage approach. The other models are generalised linear mixed models that perform the analysis in 1 stage and have the potential to provide more accurate inference. We explore the implications of using these 7 models in the context of a Cochrane Review, and we also perform a simulation study. We conclude that generalised linear mixed models can result in better statistical inference than the conventional 2-stage approach but also that this type of model presents issues and difficulties. These challenges include more demanding numerical methods and determining the best way to model study specific baseline risks. One possible approach for analysts is to specify a primary model prior to performing the systematic review but also to present the results using other models in a sensitivity analysis. Only one of the models that we investigate is found to perform poorly so that any of the other models could be considered for either the primary or the sensitivity analysis.},
  file = {/Users/rritaz/Zotero/storage/B7QSMWAD/Jackson et al. - 2018 - A comparison of seven random-effects models for me.pdf;/Users/rritaz/Zotero/storage/B29PIIRG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {odds ratio,random effects models},
  language = {en},
  number = {7}
}

@article{jackson_confidence_2013,
  title = {Confidence Intervals for the Between-Study Variance in Random Effects Meta-Analysis Using Generalised {{Cochran}} Heterogeneity Statistics},
  author = {Jackson, Dan},
  year = {2013},
  volume = {4},
  pages = {220--229},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1081},
  abstract = {Statistical inference is problematic in the common situation in meta-analysis where the random effects model is fitted to just a handful of studies. In particular, the asymptotic theory of maximum likelihood provides a poor approximation, and Bayesian methods are sensitive to the prior specification. Hence, less efficient, but easily computed and exact, methods are an attractive alternative. Here, methodology is developed to compute exact confidence intervals for the between-study variance using generalised versions of Cochran's heterogeneity statistic. If some between-study is anticipated, but it is unclear how much, then a pragmatic approach is to use the reciprocals of the within-study standard errors as weights when computing the confidence interval. \textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NWSL6ZNW/Jackson - 2013 - Confidence intervals for the between-study varianc.pdf;/Users/rritaz/Zotero/storage/E7YSWN65/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Confidence Intervals,random-effects},
  language = {en},
  number = {3}
}

@article{jackson_confidence_2016,
  title = {Confidence Intervals for the Between-Study Variance in Random-Effects Meta-Analysis Using Generalised Heterogeneity Statistics: Should We Use Unequal Tails?},
  shorttitle = {Confidence Intervals for the Between-Study Variance in Random-Effects Meta-Analysis Using Generalised Heterogeneity Statistics},
  author = {Jackson, Dan and Bowden, Jack},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {118},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0219-y},
  abstract = {Background: Confidence intervals for the between study variance are useful in random-effects meta-analyses because they quantify the uncertainty in the corresponding point estimates. Methods for calculating these confidence intervals have been developed that are based on inverting hypothesis tests using generalised heterogeneity statistics. Whilst, under the random effects model, these new methods furnish confidence intervals with the correct coverage, the resulting intervals are usually very wide, making them uninformative. Methods: We discuss a simple strategy for obtaining 95 \% confidence intervals for the between-study variance with a markedly reduced width, whilst retaining the nominal coverage probability. Specifically, we consider the possibility of using methods based on generalised heterogeneity statistics with unequal tail probabilities, where the tail probability used to compute the upper bound is greater than 2.5 \%. This idea is assessed using four real examples and a variety of simulation studies. Supporting analytical results are also obtained. Results: Our results provide evidence that using unequal tail probabilities can result in shorter 95 \% confidence intervals for the between-study variance. We also show some further results for a real example that illustrates how shorter confidence intervals for the between-study variance can be useful when performing sensitivity analyses for the average effect, which is usually the parameter of primary interest. Conclusions: We conclude that using unequal tail probabilities when computing 95 \% confidence intervals for the between-study variance, when using methods based on generalised heterogeneity statistics, can result in shorter confidence intervals. We suggest that those who find the case for using unequal tail probabilities convincing should use the `1\textendash 4 \% split', where greater tail probability is allocated to the upper confidence bound. The `width-optimal' interval that we present deserves further investigation.},
  file = {/Users/rritaz/Zotero/storage/V53FBBZR/Jackson and Bowden - 2016 - Confidence intervals for the between-study varianc.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {confidence intervals for heterogeneity,random-effects},
  language = {en},
  number = {1}
}

@article{jackson_design-by-treatment_2014,
  title = {A Design-by-Treatment Interaction Model for Network Meta-Analysis with Random Inconsistency Effects},
  author = {Jackson, Dan and Barrett, Jessica K. and Rice, Stephen and White, Ian R. and Higgins, Julian P. T.},
  year = {2014},
  volume = {33},
  pages = {3639--3654},
  issn = {1097-0258},
  doi = {10.1002/sim.6188},
  abstract = {Network meta-analysis is becoming more popular as a way to analyse multiple treatments simultaneously and, in the right circumstances, rank treatments. A difficulty in practice is the possibility of `inconsistency' or `incoherence', where direct evidence and indirect evidence are not in agreement. Here, we develop a random-effects implementation of the recently proposed design-by-treatment interaction model, using these random effects to model inconsistency and estimate the parameters of primary interest. Our proposal is a generalisation of the model proposed by Lumley and allows trials with three or more arms to be included in the analysis. Our methods also facilitate the ranking of treatments under inconsistency. We derive R and I2 statistics to quantify the impact of the between-study heterogeneity and the inconsistency. We apply our model to two examples. \textcopyright{} 2014 The Authors. Statistics in Medicine published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3JWNP7GI/Jackson et al. - 2014 - A design-by-treatment interaction model for networ.pdf;/Users/rritaz/Zotero/storage/FET2NZL6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {21}
}

@article{jackson_extending_2010,
  title = {Extending {{DerSimonian}} and {{Laird}}'s Methodology to Perform Multivariate Random Effects Meta-Analyses},
  author = {Jackson, Dan and White, Ian R. and Thompson, Simon G.},
  year = {2010},
  volume = {29},
  pages = {1282--1297},
  issn = {1097-0258},
  doi = {10.1002/sim.3602},
  abstract = {Multivariate meta-analysis is increasingly used in medical statistics. In the univariate setting, the non-iterative method proposed by DerSimonian and Laird is a simple and now standard way of performing random effects meta-analyses. We propose a natural and easily implemented multivariate extension of this procedure which is accessible to applied researchers and provides a much less computationally intensive alternative to existing methods. In a simulation study, the proposed procedure performs similarly in almost all ways to the more established iterative restricted maximum likelihood approach. The method is applied to some real data sets and an extension to multivariate meta-regression is described. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UT63QAI5/Jackson et al. - 2010 - Extending DerSimonian and Laird's methodology to p.pdf;/Users/rritaz/Zotero/storage/KLHFTDT4/sim.html},
  journal = {Statistics in Medicine},
  keywords = {DL extension,GLM MA models,modeling effect size variation (covariates),multivariate,random-effects},
  language = {en},
  number = {12}
}

@article{jackson_extending_2016,
  title = {Extending {{DerSimonian}} and {{Laird}}'s Methodology to Perform Network Meta-Analyses with Random Inconsistency Effects},
  author = {Jackson, Dan and Law, Martin and Barrett, Jessica K. and Turner, Rebecca and Higgins, Julian P. T. and Salanti, Georgia and White, Ian R.},
  year = {2016},
  volume = {35},
  pages = {819--839},
  issn = {1097-0258},
  doi = {10.1002/sim.6752},
  abstract = {Network meta-analysis is becoming more popular as a way to compare multiple treatments simultaneously. Here, we develop a new estimation method for fitting models for network meta-analysis with random inconsistency effects. This method is an extension of the procedure originally proposed by DerSimonian and Laird. Our methodology allows for inconsistency within the network. The proposed procedure is semi-parametric, non-iterative, fast and highly accessible to applied researchers. The methodology is found to perform satisfactorily in a simulation study provided that the sample size is large enough and the extent of the inconsistency is not very severe. We apply our approach to two real examples. \textcopyright{} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/GZHA4SDD/Jackson et al. - 2016 - Extending DerSimonian and Laird's methodology to p.pdf;/Users/rritaz/Zotero/storage/5T4J6ABN/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {6}
}

@article{jackson_hartung-knapp_2017,
  title = {The {{Hartung}}-{{Knapp}} Modification for Random-Effects Meta-Analysis: {{A}} Useful Refinement but Are There Any Residual Concerns?},
  shorttitle = {The {{Hartung}}-{{Knapp}} Modification for Random-Effects Meta-Analysis},
  author = {Jackson, Dan and Law, Martin and R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2017},
  volume = {36},
  pages = {3923--3934},
  issn = {1097-0258},
  doi = {10.1002/sim.7411},
  abstract = {The modified method for random-effects meta-analysis, usually attributed to Hartung and Knapp and also proposed by Sidik and Jonkman, is easy to implement and is becoming advocated for general use. Here, we examine a range of potential concerns about the widespread adoption of this method. Motivated by these issues, a variety of different conventions can be adopted when using the modified method in practice. We describe and investigate the use of a variety of these conventions using a new taxonomy of meta-analysis datasets. We conclude that the Hartung and Knapp modification may be a suitable replacement for the standard method. Despite this, analysts who advocate the modified method should be ready to defend its use against the possible objections to it that we present. We further recommend that the results from more conventional approaches should be used as sensitivity analyses when using the modified method. It has previously been suggested that a common-effect analysis should be used for this purpose but we suggest amending this recommendation and argue that a standard random-effects analysis should be used instead.},
  file = {/Users/rritaz/Zotero/storage/W359X9F8/Jackson et al. - 2017 - The Hartung-Knapp modification for random-effects .pdf;/Users/rritaz/Zotero/storage/SEEEIIT5/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random effects models,random-effects},
  language = {en},
  number = {25}
}

@article{jackson_implications_2006,
  title = {The Implications of Publication Bias for Meta-Analysis' Other Parameter},
  author = {Jackson, Dan},
  year = {2006},
  volume = {25},
  pages = {2911--2921},
  issn = {1097-0258},
  doi = {10.1002/sim.2293},
  abstract = {Perhaps the greatest threat to the validity of a meta-analysis is the possibility of publication bias, where studies that are interesting or statistically significant are more likely to be published than those with less encouraging results. In particular, there is the concern that this bias might be `one-sided', where studies indicating that the treatment is beneficial have a greater probability of publication. The impact that this type of bias has on the estimate of treatment effect has received a great deal of attention but this also has implications for estimates of between-study variance. Using step functions to model the bias it can be demonstrated that it is impossible to make generalizations concerning how we should revise estimates of between-study variance when presented with the possibility of publication bias. To determine this, assumptions must be made concerning the form that the bias takes, which is unknown in practice. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NE2D4PMF/Jackson - 2006 - The implications of publication bias for meta-anal.pdf;/Users/rritaz/Zotero/storage/XKIKC9FB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias,random-effects},
  language = {en},
  number = {17}
}

@article{jackson_methods_2014,
  title = {Methods for Calculating Confidence and Credible Intervals for the Residual Between-Study Variance in Random Effects Meta-Regression Models},
  author = {Jackson, Dan and Turner, Rebecca and Rhodes, Kirsty and Viechtbauer, Wolfgang},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {103},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-103},
  abstract = {Background: Meta-regression is becoming increasingly used to model study level covariate effects. However this type of statistical analysis presents many difficulties and challenges. Here two methods for calculating confidence intervals for the magnitude of the residual between-study variance in random effects meta-regression models are developed. A further suggestion for calculating credible intervals using informative prior distributions for the residual between-study variance is presented. Methods: Two recently proposed and, under the assumptions of the random effects model, exact methods for constructing confidence intervals for the between-study variance in random effects meta-analyses are extended to the meta-regression setting. The use of Generalised Cochran heterogeneity statistics is extended to the meta-regression setting and a Newton-Raphson procedure is developed to implement the Q profile method for meta-analysis and meta-regression. WinBUGS is used to implement informative priors for the residual between-study variance in the context of Bayesian meta-regressions. Results: Results are obtained for two contrasting examples, where the first example involves a binary covariate and the second involves a continuous covariate. Intervals for the residual between-study variance are wide for both examples. Conclusions: Statistical methods, and R computer software, are available to compute exact confidence intervals for the residual between-study variance under the random effects model for meta-regression. These frequentist methods are almost as easily implemented as their established counterparts for meta-analysis. Bayesian meta-regressions are also easily performed by analysts who are comfortable using WinBUGS. Estimates of the residual between-study variance in random effects meta-regressions should be routinely reported and accompanied by some measure of their uncertainty. Confidence and/or credible intervals are well-suited to this purpose.},
  file = {/Users/rritaz/Zotero/storage/X59RC8TI/Jackson et al. - 2014 - Methods for calculating confidence and credible in.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,GLM MA models,random-effects},
  language = {en},
  number = {1}
}

@article{jackson_multivariate_2014,
  title = {A Multivariate Model for the Meta-Analysis of Study Level Survival Data at Multiple Times},
  author = {Jackson, Dan and Rollins, Katie and Coughlin, Patrick},
  year = {2014},
  volume = {5},
  pages = {264--272},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1112},
  abstract = {Motivated by our meta-analytic dataset involving survival rates after treatment for critical leg ischemia, we develop and apply a new multivariate model for the meta-analysis of study level survival data at multiple times. Our data set involves 50 studies that provide mortality rates at up to seven time points, which we model simultaneously, and we compare the results to those obtained from standard methodologies. Our method uses exact binomial within-study distributions and enforces the constraints that both the study specific and the overall mortality rates must not decrease over time. We directly model the probabilities of mortality at each time point, which are the quantities of primary clinical interest. We also present I2 statistics that quantify the impact of the between-study heterogeneity, which is very considerable in our data set. \textcopyright{} 2014 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BGIJAC6Q/Jackson et al. - 2014 - A multivariate model for the meta-analysis of stud.pdf;/Users/rritaz/Zotero/storage/NHPGZWBA/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes,multivariate},
  language = {en},
  number = {3}
}

@article{jackson_paule-mandel_2017,
  title = {Paule-{{Mandel}} Estimators for Network Meta-Analysis with Random Inconsistency Effects},
  author = {Jackson, Dan and Veroniki, Areti Angeliki and Law, Martin and Tricco, Andrea C. and Baker, Rose},
  year = {2017},
  volume = {8},
  pages = {416--434},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1244},
  abstract = {Network meta-analysis is used to simultaneously compare multiple treatments in a single analysis. However, network meta-analyses may exhibit inconsistency, where direct and different forms of indirect evidence are not in agreement with each other, even after allowing for between-study heterogeneity. Models for network meta-analysis with random inconsistency effects have the dual aim of allowing for inconsistencies and estimating average treatment effects across the whole network. To date, two classical estimation methods for fitting this type of model have been developed: a method of moments that extends DerSimonian and Laird's univariate method and maximum likelihood estimation. However, the Paule and Mandel estimator is another recommended classical estimation method for univariate meta-analysis. In this paper, we extend the Paule and Mandel method so that it can be used to fit models for network meta-analysis with random inconsistency effects. We apply all three estimation methods to a variety of examples that have been used previously and we also examine a challenging new dataset that is highly heterogenous. We perform a simulation study based on this new example. We find that the proposed Paule and Mandel method performs satisfactorily and generally better than the previously proposed method of moments because it provides more accurate inferences. Furthermore, the Paule and Mandel method possesses some advantages over likelihood-based methods because it is both semiparametric and requires no convergence diagnostics. Although restricted maximum likelihood estimation remains the gold standard, the proposed methodology is a fully viable alternative to this and other estimation methods.},
  file = {/Users/rritaz/Zotero/storage/ZAV2NXPJ/Jackson et al. - 2017 - Paule-Mandel estimators for network meta-analysis .pdf;/Users/rritaz/Zotero/storage/K28PJT8N/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {4}
}

@article{jackson_power_2006,
  title = {The Power of the Standard Test for the Presence of Heterogeneity in Meta-Analysis},
  author = {Jackson, Dan},
  year = {2006},
  volume = {25},
  pages = {2688--2699},
  issn = {1097-0258},
  doi = {10.1002/sim.2481},
  abstract = {It has been suggested that the standard test for the presence of heterogeneity in meta-analysis has low power. Although this has been investigated using simulation, there is little direct analytical evidence of the validity of this claim. Using an established approximate distribution for the test statistic, a procedure for obtaining the power of the test is described. From this, a simple formula for the power is obtained. Although this applies to a special case, the formula gives an indication of the power of the test more generally. In particular, for a given significance level, the power can be calibrated in terms of the proportion of the studies' variances that is provided by between-study variation. A consideration of this quantity confirms that the test does, in general, have low power. It is suggested that practitioners, who wish to conduct the standard test, use the ideas provided in order to investigate the operating characteristics of the test prior to performing it. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/KDTQKQZ6/Jackson - 2006 - The power of the standard test for the presence of.pdf;/Users/rritaz/Zotero/storage/DV63AVPW/sim.html},
  journal = {Statistics in Medicine},
  keywords = {power,random effects models,random-effects},
  language = {en},
  number = {15}
}

@article{jackson_power_2017,
  title = {Power Analysis for Random-Effects Meta-Analysis},
  author = {Jackson, Dan and Turner, Rebecca},
  year = {2017},
  volume = {8},
  pages = {290--302},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1240},
  abstract = {One of the reasons for the popularity of meta-analysis is the notion that these analyses will possess more power to detect effects than individual studies. This is inevitably the case under a fixed-effect model. However, the inclusion of the between-study variance in the random-effects model, and the need to estimate this parameter, can have unfortunate implications for this power. We develop methods for assessing the power of random-effects meta-analyses, and the average power of the individual studies that contribute to meta-analyses, so that these powers can be compared. In addition to deriving new analytical results and methods, we apply our methods to 1991 meta-analyses taken from the Cochrane Database of Systematic Reviews to retrospectively calculate their powers. We find that, in practice, 5 or more studies are needed to reasonably consistently achieve powers from random-effects meta-analyses that are greater than the studies that contribute to them. Not only is statistical inference under the random-effects model challenging when there are very few studies but also less worthwhile in such cases. The assumption that meta-analysis will result in an increase in power is challenged by our findings.},
  file = {/Users/rritaz/Zotero/storage/7GAUBC5Z/Jackson and Turner - 2017 - Power analysis for random-effects meta-analysis.pdf;/Users/rritaz/Zotero/storage/ULF295ZL/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {power,random effects models},
  language = {en},
  number = {3}
}

@article{jackson_quantifying_2012,
  title = {Quantifying the Impact of Between-Study Heterogeneity in Multivariate Meta-Analyses},
  author = {Jackson, Dan and White, Ian R. and Riley, Richard D.},
  year = {2012},
  volume = {31},
  pages = {3805--3820},
  issn = {1097-0258},
  doi = {10.1002/sim.5453},
  abstract = {Measures that quantify the impact of heterogeneity in univariate meta-analysis, including the very popular I2 statistic, are now well established. Multivariate meta-analysis, where studies provide multiple outcomes that are pooled in a single analysis, is also becoming more commonly used. The question of how to quantify heterogeneity in the multivariate setting is therefore raised. It is the univariate R2 statistic, the ratio of the variance of the estimated treatment effect under the random and fixed effects models, that generalises most naturally, so this statistic provides our basis. This statistic is then used to derive a multivariate analogue of I2, which we call . We also provide a multivariate H2 statistic, the ratio of a generalisation of Cochran's heterogeneity statistic and its associated degrees of freedom, with an accompanying generalisation of the usual I2 statistic, . Our proposed heterogeneity statistics can be used alongside all the usual estimates and inferential procedures used in multivariate meta-analysis. We apply our methods to some real datasets and show how our statistics are equally appropriate in the context of multivariate meta-regression, where study level covariate effects are included in the model. Our heterogeneity statistics may be used when applying any procedure for fitting the multivariate random effects model. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5TA87YHS/Jackson et al. - 2012 - Quantifying the impact of between-study heterogene.pdf;/Users/rritaz/Zotero/storage/UDHAPYK4/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,random-effects},
  language = {en},
  number = {29}
}

@article{jackson_re-evaluation_2009,
  title = {A Re-Evaluation of the `Quantile Approximation Method' for Random Effects Meta-Analysis},
  author = {Jackson, Dan and Bowden, Jack},
  year = {2009},
  volume = {28},
  pages = {338--348},
  issn = {1097-0258},
  doi = {10.1002/sim.3487},
  abstract = {The quantile approximation method has recently been proposed as a simple method for deriving confidence intervals for the treatment effect in a random effects meta-analysis. Although easily implemented, the quantiles used to construct intervals are derived from a single simulation study. Here it is shown that altering the study parameters, and in particular introducing changes to the distribution of the within-study variances, can have a dramatic impact on the resulting quantiles. This is further illustrated analytically by examining the scenario where all trials are assumed to be the same size. A more cautious approach is therefore suggested, where the conventional standard normal quantile is used in the primary analysis, but where the use of alternative quantiles is also considered in a sensitivity analysis. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/G5GFA6FG/Jackson and Bowden - 2009 - A re-evaluation of the ‘quantile approximation met.pdf;/Users/rritaz/Zotero/storage/7F3NTD5L/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Confidence Intervals,random-effects},
  language = {en},
  number = {2}
}

@article{jackson_refined_2014,
  title = {A Refined Method for Multivariate Meta-Analysis and Meta-Regression},
  author = {Jackson, Daniel and Riley, Richard D.},
  year = {2014},
  volume = {33},
  pages = {541--554},
  issn = {1097-0258},
  doi = {10.1002/sim.5957},
  abstract = {Making inferences about the average treatment effect using the random effects model for meta-analysis is problematic in the common situation where there is a small number of studies. This is because estimates of the between-study variance are not precise enough to accurately apply the conventional methods for testing and deriving a confidence interval for the average effect. We have found that a refined method for univariate meta-analysis, which applies a scaling factor to the estimated effects' standard error, provides more accurate inference. We explain how to extend this method to the multivariate scenario and show that our proposal for refined multivariate meta-analysis and meta-regression can provide more accurate inferences than the more conventional approach. We explain how our proposed approach can be implemented using standard output from multivariate meta-analysis software packages and apply our methodology to two real examples. \textcopyright{} 2013 The Authors. Statistics in Medicine published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CQRA6VG2/Jackson and Riley - 2014 - A refined method for multivariate meta-analysis an.pdf;/Users/rritaz/Zotero/storage/82LUPP28/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,modeling effect size variation (covariates),multivariate,random-effects,small meta-analysis},
  language = {en},
  number = {4}
}

@article{jackson_sensitivity_2013,
  title = {A Sensitivity Analysis Framework for the Treatment Effect Measure Used in the Meta-Analysis of Comparative Binary Data from Randomised Controlled Trials},
  author = {Jackson, Dan and Baker, Rose and Bowden, Jack},
  year = {2013},
  volume = {32},
  pages = {931--940},
  issn = {1097-0258},
  doi = {10.1002/sim.5591},
  abstract = {The process of undertaking a meta-analysis involves a sequence of decisions, one of which is deciding which measure of treatment effect to use. In particular, for comparative binary data from randomised controlled trials, a wide variety of measures are available such as the odds ratio and the risk difference. It is often of interest to know whether important conclusions would have been substantively different if an alternative measure had been used. Here we develop a new type of sensitivity analysis that incorporates standard measures of treatment effect. Thus, rather than examining the implications of a variety of measures in an ad hoc manner, we can simultaneously examine an entire family of possibilities, including the odds ratio, the arcsine difference and the risk difference. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CP7KY3UG/Jackson et al. - 2013 - A sensitivity analysis framework for the treatment.pdf;/Users/rritaz/Zotero/storage/ZIHNGV56/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,discrete effect sizes},
  language = {en},
  number = {6}
}

@article{jacobs_estimation_2017,
  title = {Estimation of the Biserial Correlation and Its Sampling Variance for Use in Meta-Analysis},
  author = {Jacobs, Perke and Viechtbauer, Wolfgang},
  year = {2017},
  volume = {8},
  pages = {161--180},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1218},
  abstract = {Meta-analyses are often used to synthesize the findings of studies examining the correlational relationship between two continuous variables. When only dichotomous measurements are available for one of the two variables, the biserial correlation coefficient can be used to estimate the product\textendash moment correlation between the two underlying continuous variables. Unlike the point-biserial correlation coefficient, biserial correlation coefficients can therefore be integrated with product\textendash moment correlation coefficients in the same meta-analysis. The present article describes the estimation of the biserial correlation coefficient for meta-analytic purposes and reports simulation results comparing different methods for estimating the coefficient's sampling variance. The findings indicate that commonly employed methods yield inconsistent estimates of the sampling variance across a broad range of research situations. In contrast, consistent estimates can be obtained using two methods that appear to be unknown in the meta-analytic literature. A variance-stabilizing transformation for the biserial correlation coefficient is described that allows for the construction of confidence intervals for individual coefficients with close to nominal coverage probabilities in most of the examined conditions. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WMWQN8RI/Jacobs and Viechtbauer - 2017 - Estimation of the biserial correlation and its sam.pdf;/Users/rritaz/Zotero/storage/WJ467U93/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlation coefficients},
  language = {en},
  number = {2}
}

@article{jansen_meta-regression_2012,
  title = {Meta-Regression Models to Address Heterogeneity and Inconsistency in Network Meta-Analysis of Survival Outcomes},
  author = {Jansen, Jeroen P and Cope, Shannon},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {152},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-152},
  abstract = {Background: Recently, network meta-analysis of survival data with a multidimensional treatment effect was introduced. With these models the hazard ratio is not assumed to be constant over time, thereby reducing the possibility of violating transitivity in indirect comparisons. However, bias is still present if there are systematic differences in treatment effect modifiers across comparisons. Methods: In this paper we present multidimensional network meta-analysis models for time-to-event data that are extended with covariates to explain heterogeneity and adjust for confounding bias in the synthesis of evidence networks of randomized controlled trials. The impact of a covariate on the treatment effect can be assumed to be treatment specific or constant for all treatments compared. Results: An illustrative example analysis is presented for a network of randomized controlled trials evaluating different interventions for advanced melanoma. Incorporating a covariate related to the study date resulted in different estimates for the hazard ratios over time than an analysis without this covariate, indicating the importance of adjusting for changes in contextual factors over time. Conclusion: Adding treatment-by-covariate interactions to multidimensional network meta-analysis models for published survival curves can be worthwhile to explain systematic differences across comparisons, thereby reducing inconsistencies and bias. An additional advantage is that heterogeneity in treatment effects can be explored.},
  file = {/Users/rritaz/Zotero/storage/4GAITMGH/Jansen and Cope - 2012 - Meta-regression models to address heterogeneity an.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,GLM MA models,modeling effect size variation (covariates),network meta-analysis,random-effects},
  language = {en},
  number = {1}
}

@article{jansen_network_2011,
  title = {Network Meta-Analysis of Survival Data with Fractional Polynomials},
  author = {Jansen, Jeroen P},
  year = {2011},
  month = dec,
  volume = {11},
  pages = {61},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-61},
  abstract = {Background: Pairwise meta-analysis, indirect treatment comparisons and network meta-analysis for aggregate level survival data are often based on the reported hazard ratio, which relies on the proportional hazards assumption. This assumption is implausible when hazard functions intersect, and can have a huge impact on decisions based on comparisons of expected survival, such as cost-effectiveness analysis. Methods: As an alternative to network meta-analysis of survival data in which the treatment effect is represented by the constant hazard ratio, a multi-dimensional treatment effect approach is presented. With fractional polynomials the hazard functions of interventions compared in a randomized controlled trial are modeled, and the difference between the parameters of these fractional polynomials within a trial are synthesized (and indirectly compared) across studies. Results: The proposed models are illustrated with an analysis of survival data in non-small-cell lung cancer. Fixed and random effects first and second order fractional polynomials were evaluated. Conclusion: (Network) meta-analysis of survival data with models where the treatment effect is represented with several parameters using fractional polynomials can be more closely fitted to the available data than meta-analysis based on the constant hazard ratio.},
  file = {/Users/rritaz/Zotero/storage/TBNJZ6NH/Jansen - 2011 - Network meta-analysis of survival data with fracti.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {1}
}

@article{jansen_network_2012,
  title = {Network Meta-Analysis of Individual and Aggregate Level Data},
  author = {Jansen, Jeroen P.},
  year = {2012},
  volume = {3},
  pages = {177--190},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1048},
  abstract = {Network meta-analysis is often performed with aggregate-level data (AgD). A challenge in using AgD is that the association between a patient-level covariate and treatment effects at the study level may not reflect the individual-level effect modification. In this paper, non-linear network meta-analysis models for combining individual patient data (IPD) and AgD are presented to reduce bias and uncertainty of direct and indirect treatment effects in the presence of heterogeneity. The first method uses the same model form for IPD and AgD. With the second method, the model for AgD is obtained by integrating an underlying IPD model over the joint within-study distribution of covariates, in line with the method by Jackson et al. for ecological inferences. With simulated examples, the models are illustrated. Having IPD for a subset of studies improves estimation of treatment effects in the presence of patient-level heterogeneity. Of the two proposed non-linear models for combining IPD and AgD, the second seems less affected by bias in situations with large treatment-by-patient-level-covariate interactions, probably at the cost of greater uncertainty. Additional studies are needed to better understand when one model is favorable over the other. For network meta-analysis, it is recommended to use IPD when available. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JQJJGWZL/Jansen - 2012 - Network meta-analysis of individual and aggregate .pdf;/Users/rritaz/Zotero/storage/3URSZZ5F/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Individual Patient Data IPD,network meta-analysis},
  language = {en},
  number = {2}
}

@article{jansen_network_2015,
  title = {Network Meta-Analysis of Longitudinal Data Using Fractional Polynomials},
  author = {Jansen, J. P. and Vieira, M. C. and Cope, S.},
  year = {2015},
  volume = {34},
  pages = {2294--2311},
  issn = {1097-0258},
  doi = {10.1002/sim.6492},
  abstract = {Network meta-analysis of randomized controlled trials (RCTs) are often based on one treatment effect measure per study. However, many studies report data at multiple time points. Furthermore, not all studies measure the outcomes at the same time points. As an alternative to a network meta-analysis based on a synthesis of the results at one time point, a network meta-analysis method is presented that allows for the simultaneous analysis of outcomes at multiple time points. The development of outcomes over time of interventions compared in an RCT is modeled with fractional polynomials, and the differences between the parameters of these polynomials within a trial are synthesized across studies with a Bayesian network meta-analysis. The proposed models are illustrated with an analysis of RCTs evaluating interventions for osteoarthritis of the knee. Fixed and random effects second order fractional polynomials were applied to the case study. Network meta-analysis with models that represent the treatment effects in terms of several parameters using fractional polynomials can be considered a useful addition to models for network meta-analysis of repeated measures previously proposed. When RCTs report treatment effects at multiple follow-up times, these models can be used to synthesize the results even if reporting times differ across the studies. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VQCG9EC2/Jansen et al. - 2015 - Network meta-analysis of longitudinal data using f.pdf;/Users/rritaz/Zotero/storage/FIX8WZ9S/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {15}
}

@article{jin_modified_2014,
  title = {A Modified Regression Method to Test Publication Bias in Meta-Analyses with Binary Outcomes},
  author = {Jin, Zhi-Chao and Wu, Cheng and Zhou, Xiao-Hua and He, Jia},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {132},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-132},
  abstract = {Background: The tendency towards publication bias is greater for observational studies than for randomized clinical trials. Several statistical methods have been developed to test the publication bias. However, almost all existing methods exhibit rather low power or have inappropriate type I error rates. Methods: We propose a modified regression method, which used a smoothed variance to estimate the precision of a study, to test for publication bias in meta-analyses of observational studies. A comprehensive simulation study is carried out, and a real-world example is considered. Results: The simulation results indicate that the performance of tests varies with the number of included studies, level of heterogeneity, event rates, and sample size ratio between two groups. Neither the existing tests nor the newly developed method is particularly powerful in all simulation scenarios. However, our proposed method has a more robust performance across different settings. In the presence of heterogeneity, the arcsine-Thompson test is a suitable alternative, and Peters' test can be considered as a complementary method when mild or no heterogeneity is present. Conclusions: Several factors should be taken into consideration when employing asymmetry tests for publication bias. Based on our simulation results, we provide a concise table to show the appropriate use of regression methods to test for publication bias based on our simulation results.},
  file = {/Users/rritaz/Zotero/storage/45YI46ZW/Jin et al. - 2014 - A modified regression method to test publication b.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,GLM MA models,publication bias},
  language = {en},
  number = {1}
}

@article{jolani_imputation_2015,
  title = {Imputation of Systematically Missing Predictors in an Individual Participant Data Meta-Analysis: A Generalized Approach Using {{MICE}}},
  shorttitle = {Imputation of Systematically Missing Predictors in an Individual Participant Data Meta-Analysis},
  author = {Jolani, Shahab and Debray, Thomas P. A. and Koffijberg, Hendrik and van Buuren, Stef and Moons, Karel G. M.},
  year = {2015},
  volume = {34},
  pages = {1841--1863},
  issn = {1097-0258},
  doi = {10.1002/sim.6451},
  abstract = {Individual participant data meta-analyses (IPD-MA) are increasingly used for developing and validating multivariable (diagnostic or prognostic) risk prediction models. Unfortunately, some predictors or even outcomes may not have been measured in each study and are thus systematically missing in some individual studies of the IPD-MA. As a consequence, it is no longer possible to evaluate between-study heterogeneity and to estimate study-specific predictor effects, or to include all individual studies, which severely hampers the development and validation of prediction models.Here, we describe a novel approach for imputing systematically missing data and adopt a generalized linear mixed model to allow for between-study heterogeneity. This approach can be viewed as an extension of Resche-Rigon's method (Stat Med 2013), relaxing their assumptions regarding variance components and allowing imputation of linear and nonlinear predictors.We illustrate our approach using a case study with IPD-MA of 13 studies to develop and validate a diagnostic prediction model for the presence of deep venous thrombosis. We compare the results after applying four methods for dealing with systematically missing predictors in one or more individual studies: complete case analysis where studies with systematically missing predictors are removed, traditional multiple imputation ignoring heterogeneity across studies, stratified multiple imputation accounting for heterogeneity in predictor prevalence, and multilevel multiple imputation (MLMI) fully accounting for between-study heterogeneity.We conclude that MLMI may substantially improve the estimation of between-study heterogeneity parameters and allow for imputation of systematically missing predictors in IPD-MA aimed at the development and validation of prediction models. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PRFLSEFK/Jolani et al. - 2015 - Imputation of systematically missing predictors in.pdf;/Users/rritaz/Zotero/storage/K8JNE8NX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,Individual Patient Data IPD,missing data},
  language = {en},
  number = {11}
}

@article{jones_meta-analysis_2008,
  title = {Meta-{{Analysis}} 101: {{What You Want}} to {{Know}} in the {{Era}} of {{Comparative Effectiveness}}},
  shorttitle = {Meta-{{Analysis}} 101},
  author = {Jones, J.B. and Blecker, Saul and Shah, Nirav R.},
  year = {2008},
  month = apr,
  volume = {1},
  pages = {38--43},
  issn = {1942-2962},
  abstract = {In the era of ``comparative effectiveness'' research, each of the major stakeholders in healthcare\textemdash payors, patients, providers, and government\textemdash face a similar challenge. When making a decision about whether a new device, drug, or a diagnostic modality should be considered for use or coverage, what choices are best supported by the evidence? Medical evidence is defined by randomized controlled trials and by observational studies that vary greatly in their design, the accuracy of their analyses, and the relevance of their conclusions and recommendations. Hence, key decision makers increasingly rely on systematic reviews and meta-analyses to facilitate the interpretation and application of research evidence. Knowing how to evaluate meta-analyses and understanding the potential pitfalls of the method are crucial for those involved in designing drug benefits. The authors highlight the process, strengths, and weaknesses of meta-analysis and explain how to judge the value of the results.},
  file = {/Users/rritaz/Zotero/storage/L6J5F7AR/Jones et al. - 2008 - Meta-Analysis 101 What You Want to Know in the Er.pdf},
  journal = {American Health \& Drug Benefits},
  number = {3},
  pmcid = {PMC4115319},
  pmid = {25126223}
}

@article{jones_quantifying_2019,
  title = {Quantifying How Diagnostic Test Accuracy Depends on Threshold in a Meta-Analysis},
  author = {Jones, Hayley E. and Gatsonsis, Constantine A. and Trikalinos, Thomas A. and Welton, Nicky J. and Ades, A. E.},
  year = {2019},
  volume = {38},
  pages = {4789--4803},
  issn = {1097-0258},
  doi = {10.1002/sim.8301},
  abstract = {Tests for disease often produce a continuous measure, such as the concentration of some biomarker in a blood sample. In clinical practice, a threshold C is selected such that results, say, greater than C are declared positive and those less than C negative. Measures of test accuracy such as sensitivity and specificity depend crucially on C, and the optimal value of this threshold is usually a key question for clinical practice. Standard methods for meta-analysis of test accuracy (i) do not provide summary estimates of accuracy at each threshold, precluding selection of the optimal threshold, and furthermore, (ii) do not make use of all available data. We describe a multinomial meta-analysis model that can take any number of pairs of sensitivity and specificity from each study and explicitly quantifies how accuracy depends on C. Our model assumes that some prespecified or Box-Cox transformation of test results in the diseased and disease-free populations has a logistic distribution. The Box-Cox transformation parameter can be estimated from the data, allowing for a flexible range of underlying distributions. We parameterise in terms of the means and scale parameters of the two logistic distributions. In addition to credible intervals for the pooled sensitivity and specificity across all thresholds, we produce prediction intervals, allowing for between-study heterogeneity in all parameters. We demonstrate the model using two case study meta-analyses, examining the accuracy of tests for acute heart failure and preeclampsia. We show how the model can be extended to explore reasons for heterogeneity using study-level covariates.},
  file = {/Users/rritaz/Zotero/storage/X8GT8VC3/Jones et al. - 2019 - Quantifying how diagnostic test accuracy depends o.pdf;/Users/rritaz/Zotero/storage/7GVWXPJT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {categorical MA models},
  language = {en},
  number = {24}
}

@article{jones_use_2018,
  title = {Use of a Random Effects Meta-Analysis in the Design and Analysis of a New Clinical Trial},
  author = {Jones, Hayley E. and Ades, A. E. and Sutton, Alex J. and Welton, Nicky J.},
  year = {2018},
  volume = {37},
  pages = {4665--4679},
  issn = {1097-0258},
  doi = {10.1002/sim.7948},
  abstract = {In designing a randomized controlled trial, it has been argued that trialists should consider existing evidence about the likely intervention effect. One approach is to form a prior distribution for the intervention effect based on a meta-analysis of previous studies and then power the trial on its ability to affect the posterior distribution in a Bayesian analysis. Alternatively, methods have been proposed to calculate the power of the trial to influence the ``pooled'' estimate in an updated meta-analysis. These two approaches can give very different results if the existing evidence is heterogeneous, summarised using a random effects meta-analysis. We argue that the random effects mean will rarely represent the trialist's target parameter, and so, it will rarely be appropriate to power a trial based on its impact upon the random effects mean. Furthermore, the random effects mean will not generally provide an appropriate prior distribution. More appropriate alternatives include the predictive distribution and shrinkage estimate for the most similar study. Consideration of the impact of the trial on the entire random effects distribution might sometimes be appropriate. We describe how beliefs about likely sources of heterogeneity have implications for how the previous evidence should be used and can have a profound impact on the expected power of the new trial. We conclude that the likely causes of heterogeneity among existing studies need careful consideration. In the absence of explanations for heterogeneity, we suggest using the predictive distribution from the meta-analysis as the basis for a prior distribution for the intervention effect.},
  file = {/Users/rritaz/Zotero/storage/MX8RITJI/Jones et al. - 2018 - Use of a random effects meta-analysis in the desig.pdf;/Users/rritaz/Zotero/storage/NKPM8PLB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,power,random-effects},
  language = {en},
  number = {30}
}

@article{kacker_combining_2004,
  title = {Combining Information from Interlaboratory Evaluations Using a Random Effects Model},
  author = {Kacker, Raghu N.},
  year = {2004},
  month = feb,
  volume = {41},
  pages = {132--136},
  publisher = {{IOP Publishing}},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/41/3/004},
  abstract = {This paper compares leading methods for combining information from interlaboratory evaluations of a common measurand through a random effects model of classical statistics. The leading methods are those of Cochran, Paule and Mandel, and DerSimonian and Laird. We show that all three methods are special cases of a unifying identity. The unifying identity suggests a new two-step method. This makes four methods for comparison. The comparison is based on six published data sets from three key comparisons. The method of Paule and Mandel is optimal in the sense of being conditionally restricted maximum likelihood under normality, the condition being that the estimated intralaboratory variances be treated as the true variances. The method of Paule and Mandel requires a simple iteration that can be easily done on a spreadsheet program. Therefore, it is the preferred method for combining results of interlaboratory evaluations through a random effects model. We compare the other three methods relative to the method of Paule and Mandel. The two-step method approximates the optimal method of Paule and Mandel better than the earlier methods of Cochran, and DerSimonian and Laird.},
  file = {/Users/rritaz/Zotero/storage/ACD6GG6C/Kacker - 2004 - Combining information from interlaboratory evaluat.pdf},
  journal = {Metrologia},
  language = {en},
  number = {3}
}

@article{kang_evaluation_2018,
  title = {Evaluation of Biomarkers for Treatment Selection Using Individual Participant Data from Multiple Clinical Trials},
  author = {Kang, Chaeryon and Janes, Holly and Tajik, Parvin and Groen, Henk and Mol, Ben and Koopmans, Corine and Broekhuijsen, Kim and Zwertbroek, Eva and {van Pampus}, Maria and Franssen, Maureen},
  year = {2018},
  volume = {37},
  pages = {1439--1453},
  issn = {1097-0258},
  doi = {10.1002/sim.7608},
  abstract = {Biomarkers that predict treatment effects may be used to guide treatment decisions, thus improving patient outcomes. A meta-analysis of individual participant data (IPD) is potentially more powerful than a single-study data analysis in evaluating markers for treatment selection. Our study was motivated by the IPD that were collected from 2 randomized controlled trials of hypertension and preeclampsia among pregnant women to evaluate the effect of labor induction over expectant management of the pregnancy in preventing progression to severe maternal disease. The existing literature on statistical methods for biomarker evaluation in IPD meta-analysis have evaluated a marker's performance in terms of its ability to predict risk of disease outcome, which do not directly apply to the treatment selection problem. In this study, we propose a statistical framework for evaluating a marker for treatment selection given IPD from a small number of individual clinical trials. We derive marker-based treatment rules by minimizing the average expected outcome across studies. The application of the proposed methods to the IPD from 2 studies in women with hypertension in pregnancy is presented.},
  file = {/Users/rritaz/Zotero/storage/LU8HQYAB/Kang et al. - 2018 - Evaluation of biomarkers for treatment selection u.pdf;/Users/rritaz/Zotero/storage/PFENMBLX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {categorical MA models,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {9}
}

@article{karabatsos_bayesian_2015,
  title = {A {{Bayesian}} Nonparametric Meta-Analysis Model},
  author = {Karabatsos, George and Talbott, Elizabeth and Walker, Stephen G.},
  year = {2015},
  volume = {6},
  pages = {28--44},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1117},
  abstract = {In a meta-analysis, it is important to specify a model that adequately describes the effect-size distribution of the underlying population of studies. The conventional normal fixed-effect and normal random-effects models assume a normal effect-size population distribution, conditionally on parameters and covariates. For estimating the mean overall effect size, such models may be adequate, but for prediction, they surely are not if the effect-size distribution exhibits non-normal behavior. To address this issue, we propose a Bayesian nonparametric meta-analysis model, which can describe a wider range of effect-size distributions, including unimodal symmetric distributions, as well as skewed and more multimodal distributions. We demonstrate our model through the analysis of real meta-analytic data arising from behavioral-genetic research. We compare the predictive performance of the Bayesian nonparametric model against various conventional and more modern normal fixed-effects and random-effects models. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/MUTIH8TW/Karabatsos et al. - 2015 - A Bayesian nonparametric meta-analysis model.pdf;/Users/rritaz/Zotero/storage/YZE35ZT4/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,combined significance},
  language = {en},
  number = {1}
}

@article{karabatsos_bayesian_2018,
  title = {A {{Bayesian}} Nonparametric Test of Significance Chasing Biases},
  author = {Karabatsos, George},
  year = {2018},
  volume = {9},
  pages = {51--61},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1269},
  abstract = {There is a growing concern that much of the published research literature is distorted by the pursuit of statistically significant results. In a seminal article, Ioannidis and Trikalinos (2007, Clinical Trials) proposed an omnibus (I\&T) test for significance chasing (SC) biases. This test compares the observed number of studies that report statistically significant results, against their expected number based on study power, assuming a common effect size across studies. The current article extends this approach by developing a Bayesian nonparametric (BNP) meta-regression model and test of SC bias, which can diagnose bias at the individual study level. This new BNP test is based on a flexible model of the predictive distribution of study power, conditionally on study-level covariates which account for study diversity, including diversity due to heterogeneous effect sizes across studies. A test of SC bias proceeds by comparing each study's significant outcome report indicator against its estimated posterior predictive distribution of study power, conditionally on the study's covariates. The BNP model and SC bias test are illustrated through the analyses of 3 meta-analytic data sets and through a simulation study. Software code for the BNP model and test, and the data sets, are provided as Supporting Information.},
  file = {/Users/rritaz/Zotero/storage/KD29LA72/Karabatsos - 2018 - A Bayesian nonparametric test of significance chas.pdf;/Users/rritaz/Zotero/storage/63SD2LQJ/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,modeling effect size variation (covariates),publication bias},
  language = {en},
  number = {1}
}

@article{kelley_sample_2006,
  title = {Sample Size Planning for the Standardized Mean Difference: {{Accuracy}} in Parameter Estimation via Narrow Confidence Intervals.},
  shorttitle = {Sample Size Planning for the Standardized Mean Difference},
  author = {Kelley, Ken and Rausch, Joseph R.},
  year = {2006},
  volume = {11},
  pages = {363--385},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.4.363},
  abstract = {Methods for planning sample size (SS) for the standardized mean difference so that a narrow confidence interval (CI) can be obtained via the accuracy in parameter estimation (AIPE) approach are developed. One method plans SS so that the expected width of the CI is sufficiently narrow. A modification adjusts the SS so that the obtained CI is no wider than desired with some specified degree of certainty (e.g., 99\% certain the 95\% CI will be no wider than ␻). The rationale of the AIPE approach to SS planning is given, as is a discussion of the analytic approach to CI formation for the population standardized mean difference. Tables with values of necessary SS are provided. The freely available Methods for the Behavioral, Educational, and Social Sciences (K. Kelley, 2006a) R (R Development Core Team, 2006) software package easily implements the methods discussed.},
  file = {/Users/rritaz/Zotero/storage/N5ISF3L5/Kelley and Rausch - 2006 - Sample size planning for the standardized mean dif.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes},
  language = {en},
  number = {4}
}

@article{kenny_unappreciated_2019,
  title = {The Unappreciated Heterogeneity of Effect Sizes: {{Implications}} for Power, Precision, Planning of Research, and Replication.},
  shorttitle = {The Unappreciated Heterogeneity of Effect Sizes},
  author = {Kenny, David A. and Judd, Charles M.},
  year = {2019},
  doi = {10.1037/met0000209},
  abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  journal = {Psychological methods},
  keywords = {effect size estimation (series),power,random-effects}
}

@article{kim_bayesian_2013,
  title = {Bayesian Inference for Multivariate Meta-Analysis {{Box}}\textendash{{Cox}} Transformation Models for Individual Patient Data with Applications to Evaluation of Cholesterol-Lowering Drugs},
  author = {Kim, Sungduk and Chen, Ming-Hui and Ibrahim, Joseph G. and Shah, Arvind K. and Lin, Jianxin},
  year = {2013},
  volume = {32},
  pages = {3972--3990},
  issn = {1097-0258},
  doi = {10.1002/sim.5814},
  abstract = {In this paper, we propose a class of Box\textendash Cox transformation regression models with multidimensional random effects for analyzing multivariate responses for individual patient data in meta-analysis. Our modeling formulation uses a multivariate normal response meta-analysis model with multivariate random effects, in which each response is allowed to have its own Box\textendash Cox transformation. Prior distributions are specified for the Box\textendash Cox transformation parameters as well as the regression coefficients in this complex model, and the deviance information criterion is used to select the best transformation model. Because the model is quite complex, we develop a novel Monte Carlo Markov chain sampling scheme to sample from the joint posterior of the parameters. This model is motivated by a very rich dataset comprising 26 clinical trials involving cholesterol-lowering drugs where the goal is to jointly model the three-dimensional response consisting of low density lipoprotein cholesterol (LDL-C), high density lipoprotein cholesterol (HDL-C), and triglycerides (TG) (LDL-C, HDL-C, TG). Because the joint distribution of (LDL-C, HDL-C, TG) is not multivariate normal and in fact quite skewed, a Box\textendash Cox transformation is needed to achieve normality. In the clinical literature, these three variables are usually analyzed univariately; however, a multivariate approach would be more appropriate because these variables are correlated with each other. We carry out a detailed analysis of these data by using the proposed methodology. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LHZA8GGD/Kim et al. - 2013 - Bayesian inference for multivariate meta-analysis .pdf;/Users/rritaz/Zotero/storage/GIHWXUCZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,Individual Patient Data IPD,multivariate,random-effects},
  language = {en},
  number = {23}
}

@article{kirkham_multivariate_2012,
  title = {A Multivariate Meta-Analysis Approach for Reducing the Impact of Outcome Reporting Bias in Systematic Reviews},
  author = {Kirkham, Jamie J. and Riley, Richard D. and Williamson, Paula R.},
  year = {2012},
  volume = {31},
  pages = {2179--2195},
  issn = {1097-0258},
  doi = {10.1002/sim.5356},
  abstract = {Multivariate meta-analysis allows the joint synthesis of multiple correlated outcomes from randomised trials, and is an alternative to a separate univariate meta-analysis of each outcome independently. Usually not all trials report all outcomes; furthermore, outcome reporting bias (ORB) within trials, where an outcome is measured and analysed but not reported on the basis of the results, may cause a biased set of the evidence to be available for some outcomes, potentially affecting the significance and direction of meta-analysis results. The multivariate approach, however, allows one to `borrow strength' across correlated outcomes, to potentially reduce the impact of ORB. Assuming ORB missing data mechanisms, we aim to investigate the magnitude of bias in the pooled treatment effect estimates for multiple outcomes using univariate meta-analysis, and to determine whether the `borrowing of strength' from multivariate meta-analysis can reduce the impact of ORB. A simulation study was conducted for a bivariate fixed effect meta-analysis of two correlated outcomes. The approach is illustrated by application to a Cochrane systematic review. Results show that the `borrowing of strength' from a multivariate meta-analysis can reduce the impact of ORB on the pooled treatment effect estimates. We also examine the use of the Pearson correlation as a novel approach for dealing with missing within-study correlations, and provide an extension to bivariate random-effects models that reduce ORB in the presence of heterogeneity. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/9IEHEZ3L/Kirkham et al. - 2012 - A multivariate meta-analysis approach for reducing.pdf;/Users/rritaz/Zotero/storage/DRFZISAZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {20}
}

@article{kline_comparing_2017,
  title = {Comparing Multiple Imputation Methods for Systematically Missing Subject-Level Data},
  author = {Kline, David and Andridge, Rebecca and Kaizar, Eloise},
  year = {2017},
  volume = {8},
  pages = {136--148},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1192},
  abstract = {When conducting research synthesis, the collection of studies that will be combined often do not measure the same set of variables, which creates missing data. When the studies to combine are longitudinal, missing data can occur on the observation-level (time-varying) or the subject-level (non-time-varying). Traditionally, the focus of missing data methods for longitudinal data has been on missing observation-level variables. In this paper, we focus on missing subject-level variables and compare two multiple imputation approaches: a joint modeling approach and a sequential conditional modeling approach. We find the joint modeling approach to be preferable to the sequential conditional approach, except when the covariance structure of the repeated outcome for each individual has homogenous variance and exchangeable correlation. Specifically, the regression coefficient estimates from an analysis incorporating imputed values based on the sequential conditional method are attenuated and less efficient than those from the joint method. Remarkably, the estimates from the sequential conditional method are often less efficient than a complete case analysis, which, in the context of research synthesis, implies that we lose efficiency by combining studies. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/KATZEXJI/Kline et al. - 2017 - Comparing multiple imputation methods for systemat.pdf;/Users/rritaz/Zotero/storage/QPBG4IJR/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {missing data},
  language = {en},
  number = {2}
}

@article{klingenberg_new_2014,
  title = {A New and Improved Confidence Interval for the {{Mantel}}\textendash{{Haenszel}} Risk Difference},
  author = {Klingenberg, Bernhard},
  year = {2014},
  volume = {33},
  pages = {2968--2983},
  issn = {1097-0258},
  doi = {10.1002/sim.6122},
  abstract = {Writing the variance of the Mantel\textendash Haenszel estimator under the null of homogeneity and inverting the corresponding test, we arrive at an improved confidence interval for the common risk difference in stratified 2 \texttimes{} 2 tables. This interval outperforms a variety of other intervals currently recommended in the literature and implemented in software. We also discuss a score-type confidence interval that allows to incorporate strata/study weights. Both of these intervals work very well under many scenarios common in stratified trials or in a meta-analysis, including situations with a mixture of both small and large strata sample sizes, unbalanced treatment allocation, or rare events. The new interval has the advantage that it is available in closed form with a simple formula. In addition, it applies to matched pairs data. We illustrate the methodology with various stratified clinical trials and a meta-analysis. R code to reproduce all analysis is provided in the Appendix. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Y6C5JL75/Klingenberg - 2014 - A new and improved confidence interval for the Man.pdf;/Users/rritaz/Zotero/storage/ARLXQSC7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,discrete effect sizes,risk difference},
  language = {en},
  number = {17}
}

@article{knapp_improved_2003,
  title = {{Improved tests for a random effects meta-regression with a single covariate}},
  author = {Knapp, Guido and Hartung, Joachim},
  year = {2003},
  volume = {22},
  pages = {2693--2710},
  issn = {1097-0258},
  doi = {10.1002/sim.1482},
  abstract = {The explanation of heterogeneity plays an important role in meta-analysis. The random effects meta-regression model allows the inclusion of trial-specific covariates which may explain a part of the heterogeneity. We examine the commonly used tests on the parameters in the random effects meta-regression with one covariate and propose some new test statistics based on an improved estimator of the variance of the parameter estimates. The approximation of the distribution of the newly proposed tests is based on some theoretical considerations. Moreover, the newly proposed tests can easily be extended to the case of more than one covariate. In a simulation study, we compare the tests with regard to their actual significance level and we consider the log relative risk as the parameter of interest. Our simulation study reflects the meta-analysis of the efficacy of a vaccine for the prevention of tuberculosis originally discussed in Berkey et al. The simulation study shows that the newly proposed tests are superior to the commonly used test in holding the nominal significance level. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Z4RKKKR9/Knapp and Hartung - 2003 - Improved tests for a random effects meta-regressio.pdf;/Users/rritaz/Zotero/storage/NK55SVTR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,modeling effect size variation (covariates),random-effects},
  language = {fr},
  number = {17}
}

@article{knapp_improved_2003-1,
  title = {{Improved tests for a random effects meta-regression with a single covariate}},
  author = {Knapp, Guido and Hartung, Joachim},
  year = {2003},
  volume = {22},
  pages = {2693--2710},
  issn = {1097-0258},
  doi = {10.1002/sim.1482},
  abstract = {The explanation of heterogeneity plays an important role in meta-analysis. The random effects meta-regression model allows the inclusion of trial-specific covariates which may explain a part of the heterogeneity. We examine the commonly used tests on the parameters in the random effects meta-regression with one covariate and propose some new test statistics based on an improved estimator of the variance of the parameter estimates. The approximation of the distribution of the newly proposed tests is based on some theoretical considerations. Moreover, the newly proposed tests can easily be extended to the case of more than one covariate. In a simulation study, we compare the tests with regard to their actual significance level and we consider the log relative risk as the parameter of interest. Our simulation study reflects the meta-analysis of the efficacy of a vaccine for the prevention of tuberculosis originally discussed in Berkey et al. The simulation study shows that the newly proposed tests are superior to the commonly used test in holding the nominal significance level. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1482},
  copyright = {Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LF4GMYCZ/Knapp and Hartung - 2003 - Improved tests for a random effects meta-regressio.pdf;/Users/rritaz/Zotero/storage/DQIGN23Z/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity,meta-analysis,meta-regression,trial-specific covariate},
  language = {fr},
  number = {17}
}

@article{knapp_improved_2003-2,
  title = {{Improved tests for a random effects meta-regression with a single covariate}},
  author = {Knapp, Guido and Hartung, Joachim},
  year = {2003},
  volume = {22},
  pages = {2693--2710},
  issn = {1097-0258},
  doi = {10.1002/sim.1482},
  abstract = {The explanation of heterogeneity plays an important role in meta-analysis. The random effects meta-regression model allows the inclusion of trial-specific covariates which may explain a part of the heterogeneity. We examine the commonly used tests on the parameters in the random effects meta-regression with one covariate and propose some new test statistics based on an improved estimator of the variance of the parameter estimates. The approximation of the distribution of the newly proposed tests is based on some theoretical considerations. Moreover, the newly proposed tests can easily be extended to the case of more than one covariate. In a simulation study, we compare the tests with regard to their actual significance level and we consider the log relative risk as the parameter of interest. Our simulation study reflects the meta-analysis of the efficacy of a vaccine for the prevention of tuberculosis originally discussed in Berkey et al. The simulation study shows that the newly proposed tests are superior to the commonly used test in holding the nominal significance level. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1482},
  file = {/Users/rritaz/Zotero/storage/UJHXZ4YC/Knapp and Hartung - 2003 - Improved tests for a random effects meta-regressio.pdf;/Users/rritaz/Zotero/storage/2GKGN667/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity,meta-analysis,meta-regression,trial-specific covariate},
  language = {fr},
  number = {17}
}

@article{koch_general_1967,
  title = {A {{General Approach}} to the {{Estimation}} of {{Variance Components}}},
  author = {Koch, Gary G.},
  year = {1967},
  month = feb,
  volume = {9},
  pages = {93--118},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1967.10490444},
  abstract = {A general method of estimation of variance components in random-effects models of the nested and/or classification type is considered. If a given parameter is estimable with respect to some particular experimental design (i.e., an unbiased estimate of the parameter may be obtained from the experiment), then the suggested estimator may be readily computed with only the aid of a desk calculator. The estimates are always unbiased and consistent (with respect to the structure of the experimental design); in the case of balanced experiments, they coincide with those obtained from the analysis of variance. Secondly, the problem of designing experiments to estimate variance components is briefly discussed from the point-of-view of the suggested estimation procedure. As a result, certain non-balanced designs are seen to yield more efficient estimators of particular parameters in specified situations than the corresponding balanced design using the same number of observations. Finally, the method of estimation is shown to be applicable to models more general than the variance component one. Again it is readily computed and is unbiased and consistent.},
  file = {/Users/rritaz/Zotero/storage/VXDAIJA2/Koch - 1967 - A General Approach to the Estimation of Variance C.pdf;/Users/rritaz/Zotero/storage/7JWCYP9D/00401706.1967.html},
  journal = {Technometrics},
  number = {1}
}

@article{konig_visualizing_2013,
  title = {Visualizing the Flow of Evidence in Network Meta-Analysis and Characterizing Mixed Treatment Comparisons},
  author = {K{\"o}nig, Jochem and Krahn, Ulrike and Binder, Harald},
  year = {2013},
  volume = {32},
  pages = {5414--5429},
  issn = {1097-0258},
  doi = {10.1002/sim.6001},
  abstract = {Network meta-analysis techniques allow for pooling evidence from different studies with only partially overlapping designs for getting a broader basis for decision support. The results are network-based effect estimates that take indirect evidence into account for all pairs of treatments. The results critically depend on homogeneity and consistency assumptions, which are sometimes difficult to investigate. To support such evaluation, we propose a display of the flow of evidence and introduce new measures that characterize the structure of a mixed treatment comparison. Specifically, a linear fixed effects model for network meta-analysis is considered, where the network estimates for two treatments are linear combinations of direct effect estimates comparing these or other treatments. The linear coefficients can be seen as the generalization of weights known from classical meta-analysis. We summarize properties of these coefficients and display them as a weighted directed acyclic graph, representing the flow of evidence. Furthermore, measures are introduced that quantify the direct evidence proportion, the mean path length, and the minimal parallelism of mixed treatment comparisons. The graphical display and the measures are illustrated for two published network meta-analyses. In these applications, the proposed methods are seen to render transparent the process of data pooling in mixed treatment comparisons. They can be expected to be more generally useful for guiding and facilitating the validity assessment in network meta-analysis. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/H8LCHJY5/König et al. - 2013 - Visualizing the flow of evidence in network meta-a.pdf;/Users/rritaz/Zotero/storage/2V8Z9NNY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,network meta-analysis},
  language = {en},
  number = {30}
}

@article{konstantopoulos_fixed_2011,
  title = {Fixed Effects and Variance Components Estimation in Three-Level Meta-Analysis},
  author = {Konstantopoulos, Spyros},
  year = {2011},
  volume = {2},
  pages = {61--76},
  issn = {1759-2887},
  doi = {10.1002/jrsm.35},
  abstract = {Meta-analytic methods have been widely applied to education, medicine, and the social sciences. Much of meta-analytic data are hierarchically structured because effect size estimates are nested within studies, and in turn, studies can be nested within level-3 units such as laboratories or investigators, and so forth. Thus, multilevel models are a natural framework for analyzing meta-analytic data. This paper discusses the application of a Fisher scoring method in two-level and three-level meta-analysis that takes into account random variation at the second and third levels. The usefulness of the model is demonstrated using data that provide information about school calendar types. sas proc mixed and hlm can be used to compute the estimates of fixed effects and variance components. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DRPBJ2T4/Konstantopoulos - 2011 - Fixed effects and variance components estimation i.pdf;/Users/rritaz/Zotero/storage/JEAW2JS9/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {1}
}

@article{kontopantelis_comparison_2018,
  title = {A Comparison of One-Stage vs Two-Stage Individual Patient Data Meta-Analysis Methods: {{A}} Simulation Study},
  shorttitle = {A Comparison of One-Stage vs Two-Stage Individual Patient Data Meta-Analysis Methods},
  author = {Kontopantelis, Evangelos},
  year = {2018},
  volume = {9},
  pages = {417--430},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1303},
  abstract = {Background Individual patient data (IPD) meta-analysis allows for the exploration of heterogeneity and can identify subgroups that most benefit from an intervention (or exposure), much more successfully than meta-analysis of aggregate data. One-stage or two-stage IPD meta-analysis is possible, with the former using mixed-effects regression models and the latter obtaining study estimates through simpler regression models before aggregating using standard meta-analysis methodology. However, a comprehensive comparison of the two methods, in practice, is lacking. Methods We generated 1000 datasets for each of many simulation scenarios covering different IPD sizes and different between-study variance (heterogeneity) assumptions at various levels (intercept and exposure). Numerous simulation settings of different assumptions were also used, while we evaluated performance both on main effects and interaction effects. Performance was assessed on mean bias, mean error, coverage, and power. Results Fully specified one-stage models (random study intercept or fixed study-specific intercept; random exposure effect; and fixed study-specific effects for covariate) were the best performers overall, especially when investigating interactions. For main effects, performance was almost identical across models unless intercept heterogeneity was present, in which case the fully specified one-stage and the two-stage models performed better. For interaction effects, differences across models were greater with the two-stage model consistently outperformed by the two fully specified one-stage models. Conclusions A fully specified one-stage model should be preferred (accounting for potential exposure, intercept, and, possibly, interaction heterogeneity), especially when investigating interactions. If non-convergence is encountered with a random study intercept, the fixed study-specific intercept one-stage model should be used instead.},
  file = {/Users/rritaz/Zotero/storage/ANYLGNBB/Kontopantelis - 2018 - A comparison of one-stage vs two-stage individual .pdf;/Users/rritaz/Zotero/storage/CAPVV8E7/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,Individual Patient Data IPD},
  language = {en},
  number = {3}
}

@article{kosch_conducting_2019,
  title = {Conducting Gene Set Tests in Meta-Analyses of Transcriptome Expression Data},
  author = {Kosch, Robin and Jung, Klaus},
  year = {2019},
  volume = {10},
  pages = {99--112},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1337},
  abstract = {Research synthesis, eg, by meta-analysis, is more and more considered in the area of high-dimensional data from molecular research such as gene and protein expression data, especially because most studies and experiments are performed with very small sample sizes. In contrast to most clinical and epidemiological trials, raw data are often available for high-dimensional expression data. Therefore, direct data merging followed by a joint analysis of selected studies can be an alternative to meta-analysis by P value or effect-size merging or, more generally spoken, the merging of results. While several methods for meta-analysis of differential expression studies have been proposed, meta-analysis of gene set tests has very rarely been considered, although gene set tests are standard in the analysis of individual gene expression studies. We compare in this work the different strategies of research synthesis of gene set tests, in particularly the ``early merging'' of data cleaned from batch effects versus the ``late merging'' of individual results. In simulation studies and in examples of manipulated real-world data, we found that in most scenarios, the early merging has a higher sensitivity of detecting a gene set enrichment than the late merging. However, in scenarios with few studies, large batch effect, moderate and large sample sizes of late merging are more sensitive than early merging.},
  file = {/Users/rritaz/Zotero/storage/YEXBTVF4/Kosch and Jung - 2019 - Conducting gene set tests in meta-analyses of tran.pdf;/Users/rritaz/Zotero/storage/CW5NUBEP/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {physical/biological fields},
  language = {en},
  number = {1}
}

@article{kovacic_graphical_2016,
  title = {A Graphical Model Approach to Systematically Missing Data in Meta-Analysis of Observational Studies},
  author = {Kova{\v c}i{\'c}, Jelena and Varnai, Veda Marija},
  year = {2016},
  volume = {35},
  pages = {4443--4458},
  issn = {1097-0258},
  doi = {10.1002/sim.7010},
  abstract = {When studies in meta-analysis include different sets of confounders, simple analyses can cause a bias (omitting confounders that are missing in certain studies) or precision loss (omitting studies with incomplete confounders, i.e. a complete-case meta-analysis). To overcome these types of issues, a previous study proposed modelling the high correlation between partially and fully adjusted regression coefficient estimates in a bivariate meta-analysis. When multiple differently adjusted regression coefficient estimates are available, we propose exploiting such correlations in a graphical model. Compared with a previously suggested bivariate meta-analysis method, such a graphical model approach is likely to reduce the number of parameters in complex missing data settings by omitting the direct relationships between some of the estimates. We propose a structure-learning rule whose justification relies on the missingness pattern being monotone. This rule was tested using epidemiological data from a multi-centre survey. In the analysis of risk factors for early retirement, the method showed a smaller difference from a complete data odds ratio and greater precision than a commonly used complete-case meta-analysis. Three real-world applications with monotone missing patterns are provided, namely, the association between (1) the fibrinogen level and coronary heart disease, (2) the intima media thickness and vascular risk and (3) allergic asthma and depressive episodes. The proposed method allows for the inclusion of published summary data, which makes it particularly suitable for applications involving both microdata and summary data. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VPIE32KT/Kovačić and Varnai - 2016 - A graphical model approach to systematically missi.pdf;/Users/rritaz/Zotero/storage/5MPGLVNZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data},
  language = {en},
  number = {24}
}

@article{kraemer_simple_2005,
  title = {A {{Simple Effect Size Indicator}} for {{Two}}-{{Group Comparisons}}? {{A Comment}} on Requivalent},
  shorttitle = {A {{Simple Effect Size Indicator}} for {{Two}}-{{Group Comparisons}}?},
  author = {Kraemer, Helena Chmura},
  year = {2005},
  month = dec,
  volume = {10},
  pages = {413--419},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.10.4.413},
  abstract = {R. Rosenthal and D. B. Rubin (2003) proposed an effect size, requivalent, to be used when (a) only sample size and p values are known for a study, (b) there are no generally accepted effect size indicators, or (c) sample sizes are so small or the data so non-normal that the directly computed effect sizes would be more misleading than the simple effect size. The limitations of their proposal, however, are many, and much more serious than the authors suggested, and should be carefully considered before this effect size is applied, as well as in developing other effect sizes using similar methods. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {combined significance,correlation coefficients,publication bias},
  number = {4}
}

@article{krahn_graphical_2013,
  title = {A Graphical Tool for Locating Inconsistency in Network Meta-Analyses},
  author = {Krahn, Ulrike and Binder, Harald and K{\"o}nig, Jochem},
  year = {2013},
  month = dec,
  volume = {13},
  pages = {35},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-35},
  abstract = {Background: In network meta-analyses, several treatments can be compared by connecting evidence from clinical trials that have investigated two or more treatments. The resulting trial network allows estimating the relative effects of all pairs of treatments taking indirect evidence into account. For a valid analysis of the network, consistent information from different pathways is assumed. Consistency can be checked by contrasting effect estimates from direct comparisons with the evidence of the remaining network. Unfortunately, one deviating direct comparison may have side effects on the network estimates of others, thus producing hot spots of inconsistency. Methods: We provide a tool, the net heat plot, to render transparent which direct comparisons drive each network estimate and to display hot spots of inconsistency: this permits singling out which of the suspicious direct comparisons are sufficient to explain the presence of inconsistency. We base our methods on fixed-effects models. For disclosure of potential drivers, the plot comprises the contribution of each direct estimate to network estimates resulting from regression diagnostics. In combination, we show heat colors corresponding to the change in agreement between direct and indirect estimate when relaxing the assumption of consistency for one direct comparison. A clustering procedure is applied to the heat matrix in order to find hot spots of inconsistency. Results: The method is shown to work with several examples, which are constructed by perturbing the effect of single study designs, and with two published network meta-analyses. Once the possible sources of inconsistencies are identified, our method also reveals which network estimates they affect. Conclusion: Our proposal is seen to be useful for identifying sources of inconsistencies in the network together with the interrelatedness of effect estimates. It opens the way for a further analysis based on subject matter considerations.},
  file = {/Users/rritaz/Zotero/storage/XBCKUN7U/Krahn et al. - 2013 - A graphical tool for locating inconsistency in net.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,network meta-analysis},
  language = {en},
  number = {1}
}

@article{krahn_visualizing_2014,
  title = {Visualizing Inconsistency in Network Meta-Analysis by Independent Path Decomposition},
  author = {Krahn, Ulrike and Binder, Harald and K{\"o}nig, Jochem},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {131},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-131},
  abstract = {Background: In network meta-analysis, several alternative treatments can be compared by pooling the evidence of all randomised comparisons made in different studies. Incorporated indirect conclusions require a consistent network of treatment effects. An assessment of this assumption and of the influence of deviations is fundamental for the validity evaluation. Methods: We show that network estimates for single pairwise treatment comparisons can be approximated by the evidence of a subnet that is decomposable into independent paths. Path-based estimates and the estimate of the residual evidence can be used with their contribution to the network estimate to set up a forest plot for the consistency assessment. Using a network meta-analysis of twelve antidepressants and controlled perturbations in the real and constructed consistent data, we discuss the consistency assessment by the independent path decomposition in contrast to an approach using a recently presented graphical tool, the net heat plot. In addition, we define influence functions that describe how changes in study effects are translated into network estimates. Results: While the consistency assessment by the net heat plot comprises all network estimates, an independent path decomposition and visualisation in a forest plot is tailored to one specific treatment comparison. It allows for the recognition as to whether inconsistencies between different paths of evidence and outlier effects do affect the considered treatment comparison. Conclusions: The approximation of the network estimate for a single comparison by the evidence of a subnet and the visualisation of the decomposition into independent paths provide the applicability of a graphical validation instrument that is known from classical meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/N7KSLQEP/Krahn et al. - 2014 - Visualizing inconsistency in network meta-analysis.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,network meta-analysis},
  language = {en},
  number = {1}
}

@article{kuan_simple_2013,
  title = {A Simple and Robust Method for Partially Matched Samples Using the P-Values Pooling Approach},
  author = {Kuan, Pei Fen and Huang, Bo},
  year = {2013},
  volume = {32},
  pages = {3247--3259},
  issn = {1097-0258},
  doi = {10.1002/sim.5758},
  abstract = {This paper focuses on statistical analyses in scenarios where some samples from the matched pairs design are missing, resulting in partially matched samples. Motivated by the idea of meta-analysis, we recast the partially matched samples as coming from two experimental designs and propose a simple yet robust approach based on the weighted Z-test to integrate the p-values computed from these two designs. We show that the proposed approach achieves better operating characteristics in simulations and a case study, compared with existing methods for partially matched samples. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/7LTVGXC7/Kuan and Huang - 2013 - A simple and robust method for partially matched s.pdf;/Users/rritaz/Zotero/storage/728PKEPX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance},
  language = {en},
  number = {19}
}

@article{kuhnert_comparison_2007,
  title = {A Comparison of Three Different Models for Estimating Relative Risk in Meta-Analysis of Clinical Trials under Unobserved Heterogeneity},
  author = {Kuhnert, Ronny and B{\"o}hning, Dankmar},
  year = {2007},
  volume = {26},
  pages = {2277--2296},
  issn = {1097-0258},
  doi = {10.1002/sim.2710},
  abstract = {We focus on the comparison of three statistical models used to estimate the treatment effect in meta-analysis when individually pooled data are available. The models are two conventional models, namely a multi-level and a model based upon an approximate likelihood, and a newly developed model, the profile likelihood model which might be viewed as an extension of the Mantel\textendash Haenszel approach. To exemplify these methods, we use results from a meta-analysis of 22 trials to prevent respiratory tract infections. We show that by using the multi-level approach, in the case of baseline heterogeneity, the number of clusters or components is considerably over-estimated. The approximate and profile likelihood method showed nearly the same pattern for the treatment effect distribution. To provide more evidence two simulation studies are accomplished. The profile likelihood can be considered as a clear alternative to the approximate likelihood model. In the case of strong baseline heterogeneity, the profile likelihood method shows superior behaviour when compared with the multi-level model. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3SCTB3J6/Kuhnert and Böhning - 2007 - A comparison of three different models for estimat.pdf;/Users/rritaz/Zotero/storage/EL2RDPNQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random effects models,relative risk},
  language = {en},
  number = {11}
}

@article{kulinskaya_accurate_2015,
  title = {An Accurate Test for Homogeneity of Odds Ratios Based on {{Cochran}}'s {{Q}}-Statistic},
  author = {Kulinskaya, Elena and Dollinger, Michael B},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {49},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0034-x},
  abstract = {Background: A frequently used statistic for testing homogeneity in a meta-analysis of K independent studies is Cochran's Q. For a standard test of homogeneity the Q statistic is referred to a chi-square distribution with K - 1 degrees of freedom. For the situation in which the effects of the studies are logarithms of odds ratios, the chi-square distribution is much too conservative for moderate size studies, although it may be asymptotically correct as the individual studies become large. Methods: Using a mixture of theoretical results and simulations, we provide formulas to estimate the shape and scale parameters of a gamma distribution to fit the distribution of Q. Results: Simulation studies show that the gamma distribution is a good approximation to the distribution for Q. Conclusions: Use of the gamma distribution instead of the chi-square distribution for Q should eliminate inaccurate inferences in assessing homogeneity in a meta-analysis. (A computer program for implementing this test is provided.) This hypothesis test is competitive with the Breslow-Day test both in accuracy of level and in power.},
  file = {/Users/rritaz/Zotero/storage/6YR8M38U/Kulinskaya and Dollinger - 2015 - An accurate test for homogeneity of odds ratios ba.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {odds ratio,Q distribution,random-effects},
  language = {en},
  number = {1}
}

@article{kulinskaya_combining_2010,
  title = {Combining the Evidence Using Stable Weights},
  author = {Kulinskaya, Elena and Morgenthaler, Stephan and Staudte, Robert G.},
  year = {2010},
  volume = {1},
  pages = {284--296},
  issn = {1759-2887},
  doi = {10.1002/jrsm.20},
  abstract = {In a meta-analysis one seeks to combine the results of several studies in order to improve the accuracy of decisions. Here we compare by simulation four methods for combining estimates of the risk difference, namely the Cochran and Mantel\textendash Haenszel (MH) methods, the inverse-variance weights approach and a recent variance-stabilized weights approach. Both the level and power of corresponding test statistics, as well as the coverage of related confidence intervals are compared over a wide range of sample size and parameter configurations. We found that the inverse-variance weights methodology is unreliable and is not recommended, while for equal risks, the Cochran test and the associated confidence intervals are the most reliable. Under alternatives of unequal risks, the coverage probabilities of the variance-stabilized confidence intervals are almost uniformly more reliable than those based on other methods except when the average risk is small in which case the MH confidence intervals are preferable. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JHD9KNEY/Kulinskaya et al. - 2010 - Combining the evidence using stable weights.pdf;/Users/rritaz/Zotero/storage/89CAF9EZ/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,power,risk difference,variance-stabilized},
  language = {en},
  number = {3-4}
}

@article{kulinskaya_confidence_2007,
  title = {Confidence Intervals for the Standardized Effect Arising in the Comparison of Two Normal Populations},
  author = {Kulinskaya, Elena and Staudte, Robert G.},
  year = {2007},
  volume = {26},
  pages = {2853--2871},
  issn = {1097-0258},
  doi = {10.1002/sim.2751},
  abstract = {Confidence intervals for a standardized effect are derived after stabilizing the variance of the Welch t-statistic. Simulation studies demonstrate the viability of the resulting intervals for a wide range of parameter values and sample sizes as small as five. The methodology is extended to the combination of results from several studies, so as to obtain a confidence interval for a representative standardized effect for all the studies. The methods are illustrated on a recent meta-analytic study of systolic blood pressure reduction during a weight reducing regime, as well as the classical Mumford data on psychological intervention and hospital length of stay. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/75472ERM/Kulinskaya and Staudte - 2007 - Confidence intervals for the standardized effect a.pdf;/Users/rritaz/Zotero/storage/GEHCM2VA/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Confidence Intervals,continuous effect sizes,random-effects,variance-stabilized},
  language = {en},
  number = {14}
}

@article{kulinskaya_moments_2011,
  title = {On the Moments of {{Cochran}}'s {{Q}} Statistic under the Null Hypothesis, with Application to the Meta-Analysis of Risk Difference},
  author = {Kulinskaya, Elena and Dollinger, Michael B. and Bj{\o}rkest{\o}l, Kirsten},
  year = {2011},
  volume = {2},
  pages = {254--270},
  issn = {1759-2887},
  doi = {10.1002/jrsm.54},
  abstract = {W. G. Cochran's Q statistic was introduced in 1937 to test for equality of means under heteroscedasticity. Today, the use of Q is widespread in tests for homogeneity of effects in meta-analysis, but often these effects (such as risk differences and odds ratios) are not normally distributed. It is common to assume that Q follows a chi-square distribution, but it has long been known that this asymptotic distribution for Q is not accurate for moderate sample sizes. In this paper, the effect and weight for an individual study may depend on two parameters: the effect and a nuisance parameter. We present expansions for the first two moments of Q without any normality assumptions. Our expansions will have wide applicability in testing for homogeneity in meta-analysis. As an important example, we present a homogeneity test when the effects are the differences of risks between treatment and control arms of the several studies\textemdash a test which is substantially more accurate than that currently used. In this situation, we approximate the distribution of Q with a gamma distribution. We provide the results of simulations to verify the accuracy of our proposal and an example of a meta-analysis of medical data. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PY237HHI/Kulinskaya et al. - 2011 - On the moments of Cochran's Q statistic under the .pdf;/Users/rritaz/Zotero/storage/S74SQ5M2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Q distribution,random-effects},
  language = {en},
  number = {4}
}

@article{kulinskaya_sequential_2016,
  title = {Sequential Biases in Accumulating Evidence},
  author = {Kulinskaya, Elena and Huggins, Richard and Dogo, Samson Henry},
  year = {2016},
  volume = {7},
  pages = {294--305},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1185},
  abstract = {Whilst it is common in clinical trials to use the results of tests at one phase to decide whether to continue to the next phase and to subsequently design the next phase, we show that this can lead to biased results in evidence synthesis. Two new kinds of bias associated with accumulating evidence, termed `sequential decision bias' and `sequential design bias', are identified. Both kinds of bias are the result of making decisions on the usefulness of a new study, or its design, based on the previous studies. Sequential decision bias is determined by the correlation between the value of the current estimated effect and the probability of conducting an additional study. Sequential design bias arises from using the estimated value instead of the clinically relevant value of an effect in sample size calculations. We considered both the fixed-effect and the random-effects models of meta-analysis and demonstrated analytically and by simulations that in both settings the problems due to sequential biases are apparent. According to our simulations, the sequential biases increase with increased heterogeneity. Minimisation of sequential biases arises as a new and important research area necessary for successful evidence-based approaches to the development of science. \textcopyright{} 2015 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/LXYCZHCM/Kulinskaya et al. - 2016 - Sequential biases in accumulating evidence.pdf;/Users/rritaz/Zotero/storage/84TU6AZ2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance},
  language = {en},
  number = {3}
}

@article{kulinskaya_trial_2014,
  title = {Trial Sequential Methods for Meta-Analysis},
  author = {Kulinskaya, Elena and Wood, John},
  year = {2014},
  volume = {5},
  pages = {212--220},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1104},
  abstract = {Statistical methods for sequential meta-analysis have applications also for the design of new trials. Existing methods are based on group sequential methods developed for single trials and start with the calculation of a required information size. This works satisfactorily within the framework of fixed effects meta-analysis, but conceptual difficulties arise in the random effects model. One approach applying sequential meta-analysis to design is `trial sequential analysis', developed by Wetterslev, Thorlund, Brok, Gluud and others from the Copenhagen Trial Unit. In trial sequential analysis, information size is based on the required sample size of a single new trial, which, in the random effects model, is obtained by simply inflating it in comparison with fixed effects meta-analysis. However, this is not sufficient as, depending on the amount of heterogeneity, a minimum of several new trials may be indicated, and the total number of new patients needed may be substantially reduced by planning an even larger number of small trials. We provide explicit formulae to determine the requisite minimum number of trials and their sample sizes within this framework, which also exemplify the conceptual difficulties referred to. We illustrate all these points with two practical examples, including the well-known meta-analysis of magnesium for myocardial infarction. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JZE4NP9N/Kulinskaya and Wood - 2014 - Trial sequential methods for meta-analysis.pdf;/Users/rritaz/Zotero/storage/LJNZKGFU/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,power},
  language = {en},
  number = {3}
}

@article{kulinskaya_use_2010,
  title = {Use of Quality Control Charts for Detection of Outliers and Temporal Trends in Cumulative Meta-Analysis},
  author = {Kulinskaya, Elena and Koricheva, Julia},
  year = {2010},
  volume = {1},
  pages = {297--307},
  issn = {1759-2887},
  doi = {10.1002/jrsm.29},
  abstract = {Cumulative meta-analysis (CMA) aims to aggregate accumulating evidence. Essentially a visual tool, CMA should be supplemented by formal statistical methods for assessment of the significance of the accumulating evidence, and for detection of temporal trends in effect sizes. These methods should also take into account multiple testing inherent in CMA. We review the existing methods for detection of temporal trends in effect sizes and suggest a new approach, namely the use of standard quality control (QC) charts, in particular X charts and CUSUM charts, to detect possible outliers and trends over time. We discuss the application of the QC charts to four popular measures of effect size: the odds ratios, the relative risks, the correlation coefficients and the standardized mean differences. Applications of QC charts are illustrated by three meta-analysis examples from medicine, ecology and evolutionary biology. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/QK7EXXBJ/Kulinskaya and Koricheva - 2010 - Use of quality control charts for detection of out.pdf;/Users/rritaz/Zotero/storage/9SGPRAHX/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {3-4}
}

@article{kulinskaya_welch-type_2004,
  title = {A {{Welch}}-Type Test for Homogeneity of Contrasts under Heteroscedasticity with Application to Meta-Analysis},
  author = {Kulinskaya, E. and Dollinger, M. B. and Knight, E. and Gao, H.},
  year = {2004},
  volume = {23},
  pages = {3655--3670},
  issn = {1097-0258},
  doi = {10.1002/sim.1929},
  abstract = {A common problem that arises in the meta-analysis of several studies, each with independent treatment and control groups, is to test for the homogeneity of effect sizes without the assumptions of equal variances of the treatment and the control groups and of equal variances among the separate studies. A commonly used test statistic, frequently denoted as Q, is the weighted sum of squares of the differences of the individual effect sizes from the mean effect size, with weights inversely proportional to the variances of the effect sizes. The primary contributions of this article are the presentation of improved and very accurate approximations to the distributions of the Q statistic when the effect size is a linear contrast such as the difference between the treatment and control means. Our improved approximation to the distribution of Q under the null hypothesis is based on a multiple of an F-distribution; its use yields a substantial reduction in the type I error rate of the homogeneity test. Our improved approximation to the distribution of Q under an alternative hypothesis is based on a shift of a chi-square distribution; its use allows for much greater accuracy in the computation of the power of the homogeneity test. These two improved approximate distributions are developed using the Welch methodology of approximating the moments of Q by the use of multivariate Taylor expansions. The quality of these approximations is studied by simulation. A secondary contribution of this article is a study of how best to combine the variances of the treatment and control groups (needed for the calculation of weights in the Q statistic). Our conclusion, based on simulations, is that use of pooled variances can result in substantially erroneous conclusions. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/R9TUTGS5/Kulinskaya et al. - 2004 - A Welch-type test for homogeneity of contrasts und.pdf;/Users/rritaz/Zotero/storage/5JEBACQS/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,power,Q distribution,random-effects,small meta-analysis},
  language = {en},
  number = {23}
}

@article{kunkel_comparison_2017,
  title = {A Comparison of Existing Methods for Multiple Imputation in Individual Participant Data Meta-Analysis},
  author = {Kunkel, Deborah and Kaizar, Eloise E.},
  year = {2017},
  volume = {36},
  pages = {3507--3532},
  issn = {1097-0258},
  doi = {10.1002/sim.7388},
  abstract = {Multiple imputation is a popular method for addressing missing data, but its implementation is difficult when data have a multilevel structure and one or more variables are systematically missing. This systematic missing data pattern may commonly occur in meta-analysis of individual participant data, where some variables are never observed in some studies, but are present in other hierarchical data settings. In these cases, valid imputation must account for both relationships between variables and correlation within studies. Proposed methods for multilevel imputation include specifying a full joint model and multiple imputation with chained equations (MICE). While MICE is attractive for its ease of implementation, there is little existing work describing conditions under which this is a valid alternative to specifying the full joint model. We present results showing that for multilevel normal models, MICE is rarely exactly equivalent to joint model imputation. Through a simulation study and an example using data from a traumatic brain injury study, we found that in spite of theoretical differences, MICE imputations often produce results similar to those obtained using the joint model. We also assess the influence of prior distributions in MICE imputation methods and find that when missingness is high, prior choices in MICE models tend to affect estimation of across-study variability more than compatibility of conditional likelihoods. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GNLZW9KM/Kunkel and Kaizar - 2017 - A comparison of existing methods for multiple impu.pdf;/Users/rritaz/Zotero/storage/23L6CFLT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,missing data},
  language = {en},
  number = {22}
}

@article{kunz_meta-analysis_2015,
  title = {Meta-Analysis of Rate Ratios with Differential Follow-up by Treatment Arm: Inferring Comparative Effectiveness of Medical Devices},
  shorttitle = {Meta-Analysis of Rate Ratios with Differential Follow-up by Treatment Arm},
  author = {Kunz, Lauren M. and Normand, Sharon-Lise T. and Sedrakyan, Art},
  year = {2015},
  volume = {34},
  pages = {2913--2925},
  issn = {1097-0258},
  doi = {10.1002/sim.6530},
  abstract = {Modeling events requires accounting for differential follow-up duration, especially when combining randomized and observational studies. Although events occur at any point over a follow-up period and censoring occurs throughout, most applied researchers use odds ratios as association measures, assuming follow-up duration is similar across treatment groups. We derive the bias of the rate ratio when incorrectly assuming equal follow-up duration in the single study binary treatment setting. Simulations illustrate bias, efficiency, and coverage and demonstrate that bias and coverage worsen rapidly as the ratio of follow-up duration between arms moves away from one. Combining study rate ratios with hierarchical Poisson regression models, we examine bias and coverage for the overall rate ratio via simulation in three cases: when average arm-specific follow-up duration is available for all studies, some studies, and no study. In the null case, bias and coverage are poor when the study average follow-up is used and improve even if some arm-specific follow-up information is available. As the rate ratio gets further from the null, bias and coverage remain poor. We investigate the effectiveness of cardiac resynchronization therapy devices compared with those with cardioverter-defibrillator capacity where three of eight studies report arm-specific follow-up duration. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/IYZ54487/Kunz et al. - 2015 - Meta-analysis of rate ratios with differential fol.pdf;/Users/rritaz/Zotero/storage/3CPRJEW7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,discrete effect sizes},
  language = {en},
  number = {21}
}

@article{kuss_meta-analysis_2014,
  title = {Meta-Analysis for Diagnostic Accuracy Studies: A New Statistical Model Using Beta-Binomial Distributions and Bivariate Copulas},
  shorttitle = {Meta-Analysis for Diagnostic Accuracy Studies},
  author = {Kuss, Oliver and Hoyer, Annika and Solms, Alexander},
  year = {2014},
  volume = {33},
  pages = {17--30},
  issn = {1097-0258},
  doi = {10.1002/sim.5909},
  abstract = {There are still challenges when meta-analyzing data from studies on diagnostic accuracy. This is mainly due to the bivariate nature of the response where information on sensitivity and specificity must be summarized while accounting for their correlation within a single trial. In this paper, we propose a new statistical model for the meta-analysis for diagnostic accuracy studies. This model uses beta-binomial distributions for the marginal numbers of true positives and true negatives and links these margins by a bivariate copula distribution. The new model comes with all the features of the current standard model, a bivariate logistic regression model with random effects, but has the additional advantages of a closed likelihood function and a larger flexibility for the correlation structure of sensitivity and specificity. In a simulation study, which compares three copula models and two implementations of the standard model, the Plackett and the Gauss copula do rarely perform worse but frequently better than the standard model. We use an example from a meta-analysis to judge the diagnostic accuracy of telomerase (a urinary tumor marker) for the diagnosis of primary bladder cancer for illustration. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VNZ5WMT7/Kuss et al. - 2014 - Meta-analysis for diagnostic accuracy studies a n.pdf;/Users/rritaz/Zotero/storage/E3GAUQXU/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,GLM MA models,multivariate},
  language = {en},
  number = {1}
}

@article{kuss_statistical_2015,
  title = {Statistical Methods for Meta-Analyses Including Information from Studies without Any Events\textemdash Add Nothing to Nothing and Succeed Nevertheless},
  author = {Kuss, O.},
  year = {2015},
  volume = {34},
  pages = {1097--1116},
  issn = {1097-0258},
  doi = {10.1002/sim.6383},
  abstract = {Meta-analyses with rare events, especially those that include studies with no event in one (`single-zero') or even both (`double-zero') treatment arms, are still a statistical challenge. In the case of double-zero studies, researchers in general delete these studies or use continuity corrections to avoid them. A number of arguments against both options has been given, and statistical methods that use the information from double-zero studies without using continuity corrections have been proposed. In this paper, we collect them and compare them by simulation. This simulation study tries to mirror real-life situations as completely as possible by deriving true underlying parameters from empirical data on actually performed meta-analyses. It is shown that for each of the commonly encountered effect estimators valid statistical methods are available that use the information from double-zero studies without using continuity corrections. Interestingly, all of them are truly random effects models, and so also the current standard method for very sparse data as recommended from the Cochrane collaboration, the Yusuf\textendash Peto odds ratio, can be improved on. For actual analysis, we recommend to use beta-binomial regression methods to arrive at summary estimates for the odds ratio, the relative risk, or the risk difference. Methods that ignore information from double-zero studies or use continuity corrections should no longer be used. We illustrate the situation with an example where the original analysis ignores 35 double-zero studies, and a superior analysis discovers a clinically relevant advantage of off-pump surgery in coronary artery bypass grafting. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JEQAM97C/Kuss - 2015 - Statistical methods for meta-analyses including in.pdf;/Users/rritaz/Zotero/storage/9IMR6SNR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,effect size combination (small sample \& discrete)},
  language = {en},
  number = {7}
}

@article{kwon_simulation-based_2015,
  title = {Simulation-Based Estimation of Mean and Standard Deviation for Meta-Analysis via {{Approximate Bayesian Computation}} ({{ABC}})},
  author = {Kwon, Deukwoo and Reis, Isildinha M.},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {61},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0055-5},
  abstract = {Background: When conducting a meta-analysis of a continuous outcome, estimated means and standard deviations from the selected studies are required in order to obtain an overall estimate of the mean effect and its confidence interval. If these quantities are not directly reported in the publications, they must be estimated from other reported summary statistics, such as the median, the minimum, the maximum, and quartiles. Methods: We propose a simulation-based estimation approach using the Approximate Bayesian Computation (ABC) technique for estimating mean and standard deviation based on various sets of summary statistics found in published studies. We conduct a simulation study to compare the proposed ABC method with the existing methods of Hozo et al. (2005), Bland (2015), and Wan et al. (2014). Results: In the estimation of the standard deviation, our ABC method performs better than the other methods when data are generated from skewed or heavy-tailed distributions. The corresponding average relative error (ARE) approaches zero as sample size increases. In data generated from the normal distribution, our ABC performs well. However, the Wan et al. method is best for estimating standard deviation under normal distribution. In the estimation of the mean, our ABC method is best regardless of assumed distribution. Conclusion: ABC is a flexible method for estimating the study-specific mean and standard deviation for meta-analysis, especially with underlying skewed or heavy-tailed distributions. The ABC method can be applied using other reported summary statistics such as the posterior mean and 95 \% credible interval when Bayesian analysis has been employed.},
  file = {/Users/rritaz/Zotero/storage/XI73TURZ/Kwon and Reis - 2015 - Simulation-based estimation of mean and standard d.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,continuous effect sizes},
  language = {en},
  number = {1}
}

@article{lai_standardized_2014,
  title = {Standardized {{Mean Differences}} in {{Two}}-{{Level Cross}}-{{Classified Random Effects Models}}},
  author = {Lai, Mark H. C. and Kwok, Oi-Man},
  year = {2014},
  month = aug,
  volume = {39},
  pages = {282--302},
  issn = {1076-9986},
  doi = {10.3102/1076998614532950},
  abstract = {Multilevel modeling techniques are becoming more popular in handling data with multilevel structure in educational and behavioral research. Recently, researchers have paid more attention to cross-classified data structure that naturally arises in educational settings. However, unlike traditional single-level research, methodological studies about multilevel effect size have been rare and those that have recently appeared had an emphasis on strictly hierarchical data structure. This article extends the work on multilevel standardized mean differences from strictly hierarchical structure to both fully and partially cross-classified structures. Analytically derived formulae for calculating effect sizes and the corresponding sampling variances (or standard errors) are presented, verified by simulation results, and illustrated with real data examples. Implications for primary research studies and meta-analyses are discussed.},
  file = {/Users/rritaz/Zotero/storage/MFB6KFM9/Lai and Kwok - 2014 - Standardized Mean Differences in Two-Level Cross-C.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {effect size estimation (series)},
  number = {4}
}

@article{lambert_how_2005,
  title = {How Vague Is Vague? {{A}} Simulation Study of the Impact of the Use of Vague Prior Distributions in {{MCMC}} Using {{WinBUGS}}},
  shorttitle = {How Vague Is Vague?},
  author = {Lambert, Paul C. and Sutton, Alex J. and Burton, Paul R. and Abrams, Keith R. and Jones, David R.},
  year = {2005},
  volume = {24},
  pages = {2401--2428},
  issn = {1097-0258},
  doi = {10.1002/sim.2112},
  abstract = {There has been a recent growth in the use of Bayesian methods in medical research. The main reasons for this are the development of computer intensive simulation based methods such as Markov chain Monte Carlo (MCMC), increases in computing power and the introduction of powerful software such as WinBUGS. This has enabled increasingly complex models to be fitted. The ability to fit these complex models has led to MCMC methods being used as a convenient tool by frequentists, who may have no desire to be fully Bayesian. Often researchers want `the data to dominate' when there is no prior information and thus attempt to use vague prior distributions. However, with small amounts of data the use of vague priors can be problematic. The results are potentially sensitive to the choice of prior distribution. In general there are fewer problems with location parameters. The main problem is with scale parameters. With scale parameters, not only does one have to decide the distributional form of the prior distribution, but also whether to put the prior distribution on the variance, standard deviation or precision. We have conducted a simulation study comparing the effects of 13 different prior distributions for the scale parameter on simulated random effects meta-analysis data. We varied the number of studies (5, 10 and 30) and compared three different between-study variances to give nine different simulation scenarios. One thousand data sets were generated for each scenario and each data set was analysed using the 13 different prior distributions. The frequentist properties of bias and coverage were investigated for the between-study variance and the effect size. The choice of prior distribution was crucial when there were just five studies. There was a large variation in the estimates of the between-study variance for the 13 different prior distributions. With a large number of studies the choice of prior distribution was less important. The effect size estimated was not biased, but the precision with which it was estimated varied with the choice of prior distribution leading to varying coverage intervals and, potentially, to different statistical inferences. Again there was less of a problem with a larger number of studies. There is a particular problem if the between-study variance is close to the boundary at zero, as MCMC results tend to produce upwardly biased estimates of the between-study variance, particularly if inferences are based on the posterior mean. The choice of `vague' prior distribution can lead to a marked variation in results, particularly in small studies. Sensitivity to the choice of prior distribution should always be assessed. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/6IBX89U7/Lambert et al. - 2005 - How vague is vague A simulation study of the impa.pdf;/Users/rritaz/Zotero/storage/FA346AI8/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {15}
}

@article{langan_comparison_2019,
  title = {A Comparison of Heterogeneity Variance Estimators in Simulated Random-Effects Meta-Analyses},
  author = {Langan, Dean and Higgins, Julian P. T. and Jackson, Dan and Bowden, Jack and Veroniki, Areti Angeliki and Kontopantelis, Evangelos and Viechtbauer, Wolfgang and Simmonds, Mark},
  year = {2019},
  volume = {10},
  pages = {83--98},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1316},
  abstract = {Studies combined in a meta-analysis often have differences in their design and conduct that can lead to heterogeneous results. A random-effects model accounts for these differences in the underlying study effects, which includes a heterogeneity variance parameter. The DerSimonian-Laird method is often used to estimate the heterogeneity variance, but simulation studies have found the method can be biased and other methods are available. This paper compares the properties of nine different heterogeneity variance estimators using simulated meta-analysis data. Simulated scenarios include studies of equal size and of moderate and large differences in size. Results confirm that the DerSimonian-Laird estimator is negatively biased in scenarios with small studies and in scenarios with a rare binary outcome. Results also show the Paule-Mandel method has considerable positive bias in meta-analyses with large differences in study size. We recommend the method of restricted maximum likelihood (REML) to estimate the heterogeneity variance over other methods. However, considering that meta-analyses of health studies typically contain few studies, the heterogeneity variance estimate should not be used as a reliable gauge for the extent of heterogeneity in a meta-analysis. The estimated summary effect of the meta-analysis and its confidence interval derived from the Hartung-Knapp-Sidik-Jonkman method are more robust to changes in the heterogeneity variance estimate and show minimal deviation from the nominal coverage of 95\% under most of our simulated scenarios.},
  file = {/Users/rritaz/Zotero/storage/IAE5GJVJ/Langan et al. - 2019 - A comparison of heterogeneity variance estimators .pdf;/Users/rritaz/Zotero/storage/7HZ7DJ2U/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {heterogeneity estimators,random-effects},
  language = {en},
  number = {1}
}

@article{law_two_2016,
  title = {Two New Methods to Fit Models for Network Meta-Analysis with Random Inconsistency Effects},
  author = {Law, Martin and Jackson, Dan and Turner, Rebecca and Rhodes, Kirsty and Viechtbauer, Wolfgang},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {87},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0184-5},
  abstract = {Background: Meta-analysis is a valuable tool for combining evidence from multiple studies. Network meta-analysis is becoming more widely used as a means to compare multiple treatments in the same analysis. However, a network meta-analysis may exhibit inconsistency, whereby the treatment effect estimates do not agree across all trial designs, even after taking between-study heterogeneity into account. We propose two new estimation methods for network meta-analysis models with random inconsistency effects. Methods: The model we consider is an extension of the conventional random-effects model for meta-analysis to the network meta-analysis setting and allows for potential inconsistency using random inconsistency effects. Our first new estimation method uses a Bayesian framework with empirically-based prior distributions for both the heterogeneity and the inconsistency variances. We fit the model using importance sampling and thereby avoid some of the difficulties that might be associated with using Markov Chain Monte Carlo (MCMC). However, we confirm the accuracy of our importance sampling method by comparing the results to those obtained using MCMC as the gold standard. The second new estimation method we describe uses a likelihood-based approach, implemented in the metafor package, which can be used to obtain (restricted) maximum-likelihood estimates of the model parameters and profile likelihood confidence intervals of the variance components. Results: We illustrate the application of the methods using two contrasting examples. The first uses all-cause mortality as an outcome, and shows little evidence of between-study heterogeneity or inconsistency. The second uses ``ear discharge" as an outcome, and exhibits substantial between-study heterogeneity and inconsistency. Both new estimation methods give results similar to those obtained using MCMC. Conclusions: The extent of heterogeneity and inconsistency should be assessed and reported in any network meta-analysis. Our two new methods can be used to fit models for network meta-analysis with random inconsistency effects. They are easily implemented using the accompanying R code in the Additional file 1. Using these estimation methods, the extent of inconsistency can be assessed and reported.},
  file = {/Users/rritaz/Zotero/storage/QZQHYYSL/Law et al. - 2016 - Two new methods to fit models for network meta-ana.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,network meta-analysis,random-effects},
  language = {en},
  number = {1}
}

@article{law_two_2019,
  title = {Two New Approaches for the Visualisation of Models for Network Meta-Analysis},
  author = {Law, Martin and Alam, Navid and Veroniki, Areti Angeliki and Yu, Yi and Jackson, Dan},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {61},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0689-9},
  abstract = {Background: Meta-analysis is a useful tool for combining evidence from multiple studies to estimate a pooled treatment effect. An extension of meta-analysis, network meta-analysis, is becoming more commonly used as a way to simultaneously compare multiple treatments in a single analysis. Despite the variety of approaches available for presenting fitted models, ascertaining an intuitive understanding of these models is often difficult. This is especially challenging in large networks with many different treatments. Here we propose two visualisation methods, so that network meta-analysis models can be more easily interpreted. Methods: Our methods can be used irrespective of the statistical model or the estimation method used and are grounded in network analysis. We define three types of distance measures between the treatments that contribute to the network. These three distance measures are based on 1) the estimated treatment effects, 2) their standard errors and 3) the corresponding p-values. Then, by using a suitable threshold, we categorise some treatment pairs as being ``close'' (short distances). Treatments that are close are regarded as ``connected'' in the network analysis theory. Finally, we group the treatments into communities using standard methods for network analysis. We are then able to identify which parts of the network are estimated to have similar (or different) treatment efficacy and which parts of the network are better identified. We also propose a second method using parametric bootstrapping, where a heat map is used in the visualisation. We use the software R and provide the code used. Results: We illustrate our new methods using a challenging dataset containing 22 treatments, and a previously fitted model for this data. Two communities of treatments that appear to have similar efficacy are identified. Furthermore using our methods we can identify parts of the network that are better (and less well) identified. Conclusions: Our new visualisation approaches may be used by network meta-analysts to gain an intuitive understanding of the implications of their fitted models. Our visualisation methods may be used informally, to identify the most salient features of the fitted models that can then be reported, or more formally by presenting the new visualisation devices within published reports.},
  file = {/Users/rritaz/Zotero/storage/HUQU2WG2/Law et al. - 2019 - Two new approaches for the visualisation of models.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,network meta-analysis},
  language = {en},
  number = {1}
}

@article{lazo-langner_comparing_2012,
  title = {Comparing Multiple Competing Interventions in the Absence of Randomized Trials Using Clinical Risk-Benefit Analysis},
  author = {{Lazo-Langner}, Alejandro and Rodger, Marc A and Barrowman, Nicholas J and Ramsay, Tim and Wells, Philip S and Coyle, Douglas A},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {3},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-3},
  abstract = {Background: To demonstrate the use of risk-benefit analysis for comparing multiple competing interventions in the absence of randomized trials, we applied this approach to the evaluation of five anticoagulants to prevent thrombosis in patients undergoing orthopedic surgery. Methods: Using a cost-effectiveness approach from a clinical perspective (i.e. risk benefit analysis) we compared thromboprophylaxis with warfarin, low molecular weight heparin, unfractionated heparin, fondaparinux or ximelagatran in patients undergoing major orthopedic surgery, with sub-analyses according to surgery type. Proportions and variances of events defining risk (major bleeding) and benefit (thrombosis averted) were obtained through a meta-analysis and used to define beta distributions. Monte Carlo simulations were conducted and used to calculate incremental risks, benefits, and risk-benefit ratios. Finally, net clinical benefit was calculated for all replications across a range of risk-benefit acceptability thresholds, with a reference range obtained by estimating the case fatality rate - ratio of thrombosis to bleeding. Results: The analysis showed that compared to placebo ximelagatran was superior to other options but final results were influenced by type of surgery, since ximelagatran was superior in total knee replacement but not in total hip replacement. Conclusions: Using simulation and economic techniques we demonstrate a method that allows comparing multiple competing interventions in the absence of randomized trials with multiple arms by determining the option with the best risk-benefit profile. It can be helpful in clinical decision making since it incorporates risk, benefit, and personal risk acceptance.},
  file = {/Users/rritaz/Zotero/storage/GGRTYENQ/Lazo-Langner et al. - 2012 - Comparing multiple competing interventions in the .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {physical/biological fields},
  language = {en},
  number = {1}
}

@article{le_correcting_2006,
  title = {Correcting for Indirect Range Restriction in Meta-Analysis: Testing a New Meta-Analytic Procedure.},
  shorttitle = {Correcting for Indirect Range Restriction in Meta-Analysis},
  author = {L{\^e}, Huy and Schmidt, Frank L.},
  year = {2006},
  volume = {11},
  pages = {416--438},
  doi = {10.1037/1082-989X.11.4.416},
  abstract = {Using computer simulation, the authors assessed the accuracy of J. E. Hunter, F. L. Schmidt, and H. Le's (2006) procedure for correcting for indirect range restriction, the most common type of range restriction, in comparison with the conventional practice of applying the Thorndike Case II correction for direct range restriction. Hunter et al.'s procedure produced more accurate estimates of both the mean and standard deviation in meta-analysis than the conventional procedure. Even when its key assumption that the effect of selection on a 3rd variable is fully mediated by the independent variable was violated, Hunter et al.'s procedure was still relatively more accurate than the conventional procedure. When applied to data from a previously published meta-analysis, the new procedure yielded results that led to different substantive conclusions.},
  file = {/Users/rritaz/Zotero/storage/FSETL5NP/Lê and Schmidt - 2006 - Correcting for indirect range restriction in meta-.pdf},
  journal = {Psychological methods},
  keywords = {continuous effect sizes,correlation coefficients,diagnostic techniques},
  number = {4}
}

@article{leahy_assessing_2019,
  title = {Assessing the Impact of a Matching-Adjusted Indirect Comparison in a {{Bayesian}} Network Meta-Analysis},
  author = {Leahy, Joy and Walsh, Cathal},
  year = {2019},
  volume = {10},
  pages = {546--568},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1372},
  abstract = {If IPD is available for some or all trials in a network meta-analysis (NMA), then incorporating this IPD into an NMA is routinely considered to be preferable. However, the situation often arises where a researcher has IPD for trials concerning a particular treatment (eg, from a sponsor) but none for other trials. Therefore, one can reweight the IPD so that the covariate characteristics in the IPD trials match that of the aggregate data (AgD) trials, using a matching-adjusted indirect comparison (MAIC). We assess the impact of using the reweighted aggregated data, obtained by the MAIC, in a Bayesian NMA for a connected treatment network. We apply this method to a network of multiple myeloma treatments in newly diagnosed patients (ndMM), where the outcome is progression free survival. We investigate the reliability of the methods and results through a simulation study. The ndMM network consists of three IPD studies comparing lenalidomide to placebo (Len-Placebo), one AgD study comparing Len-Placebo, and one AgD study comparing thalidomide to placebo (Thal-Placebo). We therefore investigate two options of weighting the covariates: (a) All three studies are weighted separately to match the AgD Thal-Placebo trial. (b) Patients are weighted across all three IPD studies to match the AgD Thal-Placebo trial, but the NMA considers each trial separately. We observe limited benefit to MAIC in the full network population. While MAIC can be beneficial as a sensitivity analysis to confirm results across patient populations, we advise that MAIC is used and interpreted with caution.},
  file = {/Users/rritaz/Zotero/storage/UA7L4QYL/Leahy and Walsh - 2019 - Assessing the impact of a matching-adjusted indire.pdf;/Users/rritaz/Zotero/storage/ISJIY4LV/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Individual Patient Data IPD},
  language = {en},
  number = {4}
}

@article{leahy_impact_2018,
  title = {The Impact of Individual Patient Data in a Network Meta-Analysis: {{An}} Investigation into Parameter Estimation and Model Selection},
  shorttitle = {The Impact of Individual Patient Data in a Network Meta-Analysis},
  author = {Leahy, Joy and O'Leary, Aisling and Afdhal, Nezam and Gray, Emma and Milligan, Scott and Wehmeyer, Malte H. and Walsh, Cathal},
  year = {2018},
  volume = {9},
  pages = {441--469},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1305},
  abstract = {The use of individual patient data (IPD) in network meta-analysis (NMA) is becoming increasingly popular. However, as most studies do not report IPD, most NMAs are performed using aggregate data for at least some, if not all, of the studies. We investigate the benefits of including varying proportions of IPD studies in an NMA. Several models have previously been developed for including both aggregate data and IPD in the same NMA. We performed a simulation study based on these models to examine the impact of additional IPD studies on the accuracy and precision of the estimates of both the treatment effect and the covariate effect. We also compared the deviance information criterion (DIC) between models to assess model fit. An increased proportion of IPD resulted in more accurate and precise estimates for most models and datasets. However, the coverage probability sometimes decreased when the model was misspecified. The use of IPD leads to greater differences in DIC, which allows us choose the correct model more often. We analysed a Hepatitis C network consisting of 3 IPD observational studies. The ranking of treatments remained the same for all models and datasets. We observed similar results to the simulation study: The use of IPD leads to differences in DIC and more precise estimates for the covariate effect. However, IPD sometimes increased the posterior SD of the treatment effect estimate, which may indicate between study heterogeneity. We recommend that IPD should be used where possible, especially for assessing model fit.},
  file = {/Users/rritaz/Zotero/storage/UNNXTY2E/Leahy et al. - 2018 - The impact of individual patient data in a network.pdf;/Users/rritaz/Zotero/storage/MHV9VDKE/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Individual Patient Data IPD,network meta-analysis},
  language = {en},
  number = {3}
}

@article{leahy_incorporating_2019,
  title = {Incorporating Single-Arm Evidence into a Network Meta-Analysis Using Aggregate Level Matching: {{Assessing}} the Impact},
  shorttitle = {Incorporating Single-Arm Evidence into a Network Meta-Analysis Using Aggregate Level Matching},
  author = {Leahy, Joy and Thom, Howard and Jansen, Jeroen P. and Gray, Emma and O'Leary, Aisling and White, Arthur and Walsh, Cathal},
  year = {2019},
  volume = {38},
  pages = {2505--2523},
  issn = {1097-0258},
  doi = {10.1002/sim.8139},
  abstract = {Increasingly, single-armed evidence is included in health technology assessment submissions when companies are seeking reimbursement for new drugs. While it is recognized that randomized controlled trials provide a higher standard of evidence, these are not available for many new agents that have been granted licenses in recent years. Therefore, it is important to examine whether alternative strategies for assessing this evidence may be used. In this work, we examine approaches to incorporating single-armed evidence formally in the evaluation process. We consider matching aggregate level covariates to comparator arms or trials and including this evidence in a network meta-analysis. We consider two methods of matching: (i) we include the chosen matched arm in the data set itself as a comparator for the single-arm trial; (ii) we use the baseline odds of an event in a chosen matched trial to use as a plug-in estimator for the single-arm trial. We illustrate that the synthesis of evidence resulting from such a setup is sensitive to the between-study variability, formulation of the prior for the between-design effect, weight given to the single-arm evidence, and extent of the bias in single-armed evidence. We provide a flowchart for the process involved in such a synthesis and highlight additional sensitivity analyses that should be carried out. This work was motivated by a hepatitis C data set, where many agents have only been examined in single-arm studies. We present the results of our methods applied to this data set.},
  file = {/Users/rritaz/Zotero/storage/C3C3BAJU/Leahy et al. - 2019 - Incorporating single-arm evidence into a network m.pdf;/Users/rritaz/Zotero/storage/93F6NGZN/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {14}
}

@article{lee_flexible_2008,
  title = {Flexible Parametric Models for Random-Effects Distributions},
  author = {Lee, Katherine J. and Thompson, Simon G.},
  year = {2008},
  volume = {27},
  pages = {418--434},
  issn = {1097-0258},
  doi = {10.1002/sim.2897},
  abstract = {It is commonly assumed that random effects in hierarchical models follow a normal distribution. This can be extremely restrictive in practice. We explore the use of more flexible alternatives for this assumption, namely the t distribution, and skew extensions to the normal and t distributions, implemented using Markov Chain Monte Carlo methods. Models are compared in terms of parameter estimates, deviance information criteria, and predictive distributions. These methods are applied to examples in meta-analysis and health-professional variation, where the distribution of the random effects is of direct interest. The results highlight the importance of allowing for potential skewing and heavy tails in random-effects distributions, especially when estimating a predictive distribution. We describe the extension of these random-effects models to the bivariate case, with application to a meta-analysis examining the relationship between treatment effect and baseline response. We conclude that inferences regarding the random effects can crucially depend on the assumptions made and recommend using a distribution, such as those suggested here, which is more flexible than the normal. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PNIA8SD4/Lee and Thompson - 2008 - Flexible parametric models for random-effects dist.pdf;/Users/rritaz/Zotero/storage/3BVEU5R3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,random effects models,random-effects},
  language = {en},
  number = {3}
}

@article{legha_individual_2018,
  title = {Individual Participant Data Meta-Analysis of Continuous Outcomes: {{A}} Comparison of Approaches for Specifying and Estimating One-Stage Models},
  shorttitle = {Individual Participant Data Meta-Analysis of Continuous Outcomes},
  author = {Legha, Amardeep and Riley, Richard D. and Ensor, Joie and Snell, Kym I. E. and Morris, Tim P. and Burke, Danielle L.},
  year = {2018},
  volume = {37},
  pages = {4404--4420},
  issn = {1097-0258},
  doi = {10.1002/sim.7930},
  abstract = {One-stage individual participant data meta-analysis models should account for within-trial clustering, but it is currently debated how to do this. For continuous outcomes modeled using a linear regression framework, two competing approaches are a stratified intercept or a random intercept. The stratified approach involves estimating a separate intercept term for each trial, whereas the random intercept approach assumes that trial intercepts are drawn from a normal distribution. Here, through an extensive simulation study for continuous outcomes, we evaluate the impact of using the stratified and random intercept approaches on statistical properties of the summary treatment effect estimate. Further aims are to compare (i) competing estimation options for the one-stage models, including maximum likelihood and restricted maximum likelihood, and (ii) competing options for deriving confidence intervals (CI) for the summary treatment effect, including the standard normal-based 95\% CI, and more conservative approaches of Kenward-Roger and Satterthwaite, which inflate CIs to account for uncertainty in variance estimates. The findings reveal that, for an individual participant data meta-analysis of randomized trials with a 1:1 treatment:control allocation ratio and heterogeneity in the treatment effect, (i) bias and coverage of the summary treatment effect estimate are very similar when using stratified or random intercept models with restricted maximum likelihood, and thus either approach could be taken in practice, (ii) CIs are generally best derived using either a Kenward-Roger or Satterthwaite correction, although occasionally overly conservative, and (iii) if maximum likelihood is required, a random intercept performs better than a stratified intercept model. An illustrative example is provided.},
  file = {/Users/rritaz/Zotero/storage/ZT2HU89I/Legha et al. - 2018 - Individual participant data meta-analysis of conti.pdf;/Users/rritaz/Zotero/storage/LBEAF3P5/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,continuous effect sizes,Individual Patient Data IPD},
  language = {en},
  number = {29}
}

@article{li_bayesian_2018,
  title = {A {{Bayesian}} Latent Variable Approach to Aggregation of Partial and Top-Ranked Lists in Genomic Studies},
  author = {Li, Xue and Choudhary, Pankaj Kumar and Biswas, Swati and Wang, Xinlei},
  year = {2018},
  volume = {37},
  pages = {4266--4278},
  issn = {1097-0258},
  doi = {10.1002/sim.7920},
  abstract = {In genomic research, it is becoming increasingly popular to perform meta-analysis, the practice of combining results from multiple studies that target a common essential biological problem. Rank aggregation, a robust meta-analytic approach, consolidates such studies at the rank level. There exists extensive research on this topic, and various methods have been developed in the past. However, these methods have two major limitations when they are applied in the genomic context. First, they are mainly designed to work with full lists, whereas partial and/or top-ranked lists prevail in genomic studies. Second, the component studies are often clustered, and the existing methods fail to utilize such information. To address the above concerns, a Bayesian latent variable approach, called BiG, is proposed to formally deal with partial and top-ranked lists and incorporate the effect of clustering. Various reasonable prior specifications for variance parameters in hierarchical models are carefully studied and compared. Simulation results demonstrate the superior performance of BiG compared with other popular rank aggregation methods under various practical settings. A non\textendash small-cell lung cancer data example is analyzed for illustration.},
  file = {/Users/rritaz/Zotero/storage/QEGCGV4V/Li et al. - 2018 - A Bayesian latent variable approach to aggregation.pdf;/Users/rritaz/Zotero/storage/DCJC6MDH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,physical/biological fields},
  language = {en},
  number = {28}
}

@article{li_drugdrug_2007,
  title = {Drug\textendash Drug Interaction Prediction: A {{Bayesian}} Meta-Analysis Approach},
  shorttitle = {Drug\textendash Drug Interaction Prediction},
  author = {Li, Lang and Yu, Menggang and Chin, Raymond and Lucksiri, Aroonrut and Flockhart, David A. and Hall, Stephen D.},
  year = {2007},
  volume = {26},
  pages = {3700--3721},
  issn = {1097-0258},
  doi = {10.1002/sim.2837},
  abstract = {In drug\textendash drug interaction (DDI) research, a two drug interaction is usually predicted by individual drug pharmacokinetics (PK). Although subject-specific drug concentration data from clinical PK studies on inhibitor/inducer or substrate's PK are not usually published, sample mean plasma drug concentrations and their standard deviations have been routinely reported. In this paper, an innovative DDI prediction method based on a three-level hierarchical Bayesian meta-analysis model is developed. The first level model is a study-specific sample mean model; the second level model is a random effect model connecting different PK studies; and all priors of PK parameters are specified in the third level model. A Monte Carlo Markov chain (MCMC) PK parameter estimation procedure is developed, and DDI prediction for a future study is conducted based on the PK models of two drugs and posterior distributions of the PK parameters. The performance of Bayesian meta-analysis in DDI prediction is demonstrated through a ketoconazole\textendash midazolam example. The biases of DDI prediction are evaluated through statistical simulation studies. The DDI marker, ratio of area under the concentration curves, is predicted with little bias (less than 5 per cent), and its 90 per cent credible interval coverage rate is close to the nominal level. Sensitivity analysis is conducted to justify prior distribution selections. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/FUS5LMB3/Li et al. - 2007 - Drug–drug interaction prediction a Bayesian meta-.pdf;/Users/rritaz/Zotero/storage/JUESSLTT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {20}
}

@article{li_flexible_2019,
  title = {A Flexible Approach to Identify Interaction Effects between Moderators in Meta-Analysis},
  author = {Li, Xinru and Dusseldorp, Elise and Meulman, Jacqueline J.},
  year = {2019},
  volume = {10},
  pages = {134--152},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1334},
  abstract = {In meta-analytic studies, there are often multiple moderators available (eg, study characteristics). In such cases, traditional meta-analysis methods often lack sufficient power to investigate interaction effects between moderators, especially high-order interactions. To overcome this problem, meta-CART was proposed: an approach that applies classification and regression trees (CART) to identify interactions, and then subgroup meta-analysis to test the significance of moderator effects. The aim of this study is to improve meta-CART upon two aspects: 1) to integrate the two steps of the approach into one and 2) to consistently take into account the fixed-effect or random-effects assumption in both the the interaction identification and testing process. For fixed effect meta-CART, weights are applied, and subgroup analysis is adapted. For random effects meta-CART, a new algorithm has been developed. The performance of the improved meta-CART was investigated via an extensive simulation study on different types of moderator variables (ie, dichotomous, nominal, ordinal, and continuous variables). The simulation results revealed that the new method can achieve satisfactory performance (power greater than 0.80 and Type I error less than 0.05) if appropriate pruning rule is applied and the number of studies is large enough. The required minimum number of studies ranges from 40 to 120 depending on the complexity and strength of the interaction effects, the within-study sample size, the type of moderators, and the residual heterogeneity.},
  file = {/Users/rritaz/Zotero/storage/EQEQVNYC/Li et al. - 2019 - A flexible approach to identify interaction effect.pdf;/Users/rritaz/Zotero/storage/KREVBP3U/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {GLM MA models,modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{lian_bayesian_2019,
  title = {A {{Bayesian}} Approach for Correcting Exposure Misclassification in Meta-Analysis},
  author = {Lian, Qinshu and Hodges, James S. and MacLehose, Richard and Chu, Haitao},
  year = {2019},
  volume = {38},
  pages = {115--130},
  issn = {1097-0258},
  doi = {10.1002/sim.7969},
  abstract = {In observational studies, misclassification of exposure is ubiquitous and can substantially bias the estimated association between an outcome and an exposure. Although misclassification in a single observational study has been well studied, few papers have considered it in a meta-analysis. Meta-analyses of observational studies provide important evidence for health policy decisions, especially when large randomized controlled trials are unethical or unavailable. It is imperative to account properly for misclassification in a meta-analysis to obtain valid point and interval estimates. In this paper, we propose a novel Bayesian approach to filling this methodological gap. We simultaneously synthesize two (or more) meta-analyses, with one on the association between a misclassified exposure and an outcome (main studies), and the other on the association between the misclassified exposure and the true exposure (validation studies). We extend the current scope for using external validation data by relaxing the ``transportability'' assumption by means of random effects models. Our model accounts for heterogeneity between studies and can be extended to allow different studies to have different exposure measurements. The proposed model is evaluated through simulations and illustrated using real data from a meta-analysis of the effect of cigarette smoking on diabetic peripheral neuropathy.},
  file = {/Users/rritaz/Zotero/storage/AWRTGYI5/Lian et al. - 2019 - A Bayesian approach for correcting exposure miscla.pdf;/Users/rritaz/Zotero/storage/R2SKMTTH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance},
  language = {en},
  number = {1}
}

@article{lin_bayesian_2018,
  title = {Bayesian Multivariate Meta-Analysis of Multiple Factors},
  author = {Lin, Lifeng and Chu, Haitao},
  year = {2018},
  volume = {9},
  pages = {261--272},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1293},
  abstract = {In medical sciences, a disease condition is typically associated with multiple risk and protective factors. Although many studies report results of multiple factors, nearly all meta-analyses separately synthesize the association between each factor and the disease condition of interest. The collected studies usually report different subsets of factors, and the results from separate analyses on multiple factors may not be comparable because each analysis may use different subpopulation. This may impact on selecting most important factors to design a multifactor intervention program. This article proposes a new concept, multivariate meta-analysis of multiple factors (MVMA-MF), to synthesize all available factors simultaneously. By borrowing information across factors, MVMA-MF can improve statistical efficiency and reduce biases compared with separate analyses when factors were missing not at random. As within-study correlations between factors are commonly unavailable from published articles, we use a Bayesian hybrid model to perform MVMA-MF, which effectively accounts for both within- and between-study correlations. The performance of MVMA-MF and the conventional methods are compared using simulations and an application to a pterygium dataset consisting of 29 studies on 8 risk factors.},
  file = {/Users/rritaz/Zotero/storage/2FDQNWUX/Lin and Chu - 2018 - Bayesian multivariate meta-analysis of multiple fa.pdf;/Users/rritaz/Zotero/storage/LVIB47NW/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,multivariate},
  language = {en},
  number = {2}
}

@article{lin_graphical_2019,
  title = {Graphical Augmentations to Sample-Size-Based Funnel Plot in Meta-Analysis},
  author = {Lin, Lifeng},
  year = {2019},
  volume = {10},
  pages = {376--388},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1340},
  abstract = {Assessing publication bias is a critical procedure in meta-analyses for rating the synthesized overall evidence. Because statistical tests for publication bias are usually not powerful and only give P values that inform either the presence or absence of the bias, examining the asymmetry of funnel plots has been popular to investigate potentially missing studies and the direction of the bias. Most funnel plots present treatment effects against their standard errors, and the contours depicting studies' significance levels have been used in the plots to distinguish publication bias from other factors (such as heterogeneity and subgroup effects) that may cause the plots' asymmetry. However, treatment effects and their standard errors are frequently associated even if no publication bias exists (eg, both variables depend on the four data cells in a 2 \texttimes{} 2 table for the odds ratio), so standard-error-based funnel plots may lead to false positive conclusions when such association may not be negligible. In addition, the missingness of studies may relate to their sample sizes besides P values (which are partly determined by standard errors); studies with more samples are more likely published. Therefore, funnel plots based on sample sizes can be an alternative tool. However, the contours for standard-error-based funnel plots cannot be directly applied to sample-size-based ones. This article introduces contours for sample-size-based funnel plots of various effect sizes, which may help meta-analysts properly interpret such plots' asymmetry. We provide five examples to illustrate the use of the proposed contours.},
  file = {/Users/rritaz/Zotero/storage/72HDLBXK/Lin - 2019 - Graphical augmentations to sample-size-based funne.pdf;/Users/rritaz/Zotero/storage/249CI2RT/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,publication bias},
  language = {en},
  number = {3}
}

@article{lin_quantifying_2018,
  title = {Quantifying and Presenting Overall Evidence in Network Meta-Analysis},
  author = {Lin, Lifeng},
  year = {2018},
  volume = {37},
  pages = {4114--4125},
  issn = {1097-0258},
  doi = {10.1002/sim.7905},
  abstract = {Network meta-analysis (NMA) has become an increasingly used tool to compare multiple treatments simultaneously by synthesizing direct and indirect evidence in clinical research. However, many existing studies did not properly report the evidence of treatment comparisons and show the comparison structure to audience. In addition, nearly all treatment networks presented only direct evidence, not overall evidence that can reflect the benefit of performing NMAs. This article classifies treatment networks into three types under different assumptions; they include networks with each treatment comparison's edge width proportional to the corresponding number of studies, sample size, and precision. In addition, three new measures (ie, the effective number of studies, the effective sample size, and the effective precision) are proposed to preliminarily quantify overall evidence gained in NMAs. They permit audience to intuitively evaluate the benefit of performing NMAs, compared with pairwise meta-analyses based on only direct evidence. We use four case studies, including one illustrative example, to demonstrate their derivations and interpretations. Treatment networks may look fairly differently when different measures are used to present the evidence. The proposed measures provide clear information about overall evidence of all treatment comparisons, and they also imply the additional number of studies, sample size, and precision obtained from indirect evidence. Some comparisons may benefit little from NMAs. Researchers are encouraged to present overall evidence of all treatment comparisons, so that audience can preliminarily evaluate the quality of NMAs.},
  file = {/Users/rritaz/Zotero/storage/EEGB2S2B/Lin - 2018 - Quantifying and presenting overall evidence in net.pdf;/Users/rritaz/Zotero/storage/ZNBT8LIM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,power},
  language = {en},
  number = {28}
}

@article{liu_can_2017,
  title = {Can Statistic Adjustment of {{OR}} Minimize the Potential Confounding Bias for Meta-Analysis of Case-Control Study? {{A}} Secondary Data Analysis},
  shorttitle = {Can Statistic Adjustment of {{OR}} Minimize the Potential Confounding Bias for Meta-Analysis of Case-Control Study?},
  author = {Liu, Tianyi and Nie, Xiaolu and Wu, Zehao and Zhang, Ying and Feng, Guoshuang and Cai, Siyu and Lv, Yaqi and Peng, Xiaoxia},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {179},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0454-x},
  abstract = {Background: Different confounder adjustment strategies were used to estimate odds ratios (ORs) in case-control study, i.e. how many confounders original studies adjusted and what the variables are. This secondary data analysis is aimed to detect whether there are potential biases caused by difference of confounding factor adjustment strategies in case-control study, and whether such bias would impact the summary effect size of meta-analysis. Methods: We included all meta-analyses that focused on the association between breast cancer and passive smoking among non-smoking women, as well as each original case-control studies included in these meta-analyses. The relative deviations (RDs) of each original study were calculated to detect how magnitude the adjustment would impact the estimation of ORs, compared with crude ORs. At the same time, a scatter diagram was sketched to describe the distribution of adjusted ORs with different number of adjusted confounders. Results: Substantial inconsistency existed in meta-analysis of case-control studies, which would influence the precision of the summary effect size. First, mixed unadjusted and adjusted ORs were used to combine individual OR in majority of meta-analysis. Second, original studies with different adjustment strategies of confounders were combined, i.e. the number of adjusted confounders and different factors being adjusted in each original study. Third, adjustment did not make the effect size of original studies trend to constringency, which suggested that model fitting might have failed to correct the systematic error caused by confounding. Conclusions: The heterogeneity of confounder adjustment strategies in case-control studies may lead to further bias for summary effect size in meta-analyses, especially for weak or medium associations so that the direction of causal inference would be even reversed. Therefore, further methodological researches are needed, referring to the assessment of confounder adjustment strategies, as well as how to take this kind of bias into consideration when drawing conclusion based on summary estimation of meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/DNXVFGSI/Liu et al. - 2017 - Can statistic adjustment of OR minimize the potent.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {meta-meta-analyses},
  language = {en},
  number = {1}
}

@article{longford_efficient_2009,
  title = {Efficient {{Estimation}} of the {{Standardized Value}}},
  author = {Longford, Nicholas T.},
  year = {2009},
  volume = {34},
  pages = {522--529},
  issn = {1076-9986},
  abstract = {We derive an estimator of the standardized value which, under the standard assumptions of normality and homoscedasticity, is more efficient than the established (asymptotically efficient) estimator and discuss its gains for small samples.},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {continuous effect sizes,effect size combination (small sample \& discrete)},
  number = {4}
}

@article{longford_estimation_2010,
  title = {Estimation of the Effect Size in Meta-Analysis with Few Studies},
  author = {Longford, Nicholas T.},
  year = {2010},
  volume = {29},
  pages = {421--430},
  issn = {1097-0258},
  doi = {10.1002/sim.3814},
  abstract = {Meta-analysis is often conducted with only a small number of studies. Adjustments of the (restricted) maximum likelihood estimator of the effect size are derived and their gains in efficiency are explored. The proposed estimators are applied to three sets of studies. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2AQGUHFQ/Longford - 2010 - Estimation of the effect size in meta-analysis wit.pdf;/Users/rritaz/Zotero/storage/SCYUT4G6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size estimation (series),small meta-analysis},
  language = {en},
  number = {4}
}

@article{lopez-lopez_alternatives_2013,
  title = {Alternatives for {{Mixed}}-{{Effects Meta}}-{{Regression Models}} in the {{Reliability Generalization Approach}}: {{A Simulation Study}}},
  shorttitle = {Alternatives for {{Mixed}}-{{Effects Meta}}-{{Regression Models}} in the {{Reliability Generalization Approach}}},
  author = {{L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and Botella, Juan and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2013},
  month = oct,
  volume = {38},
  pages = {443--469},
  issn = {1076-9986},
  doi = {10.3102/1076998612466142},
  abstract = {Since heterogeneity between reliability coefficients is usually found in reliability generalization studies, moderator analyses constitute a crucial step for that meta-analytic approach. In this study, different procedures for conducting mixed-effects meta-regression analyses were compared. Specifically, four transformation methods for the reliability coefficients, two estimators of the residual between-studies variance, and two methods for testing regression coefficients significance were combined in a Monte Carlo simulation study. The different methods were compared in terms of bias and mean square error (MSE) of the slope estimates, and Type I error and statistical power rates for the slope statistical tests. The results of the simulation study did not vary as a function of the residual variance estimator. All transformation methods provided negatively biased estimates, but both bias and MSE were reasonably small in all cases. In contrast, important differences were found regarding statistical tests, with the method proposed by Knapp and Hartung showing a better adjustment to the nominal significance level and higher power rates than the standard method.},
  file = {/Users/rritaz/Zotero/storage/Y9GN5RYD/López-López et al. - 2013 - Alternatives for Mixed-Effects Meta-Regression Mod.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {modeling effect size variation (covariates)},
  number = {5}
}

@article{lopezlopez_assessing_2017,
  title = {Assessing Meta-Regression Methods for Examining Moderator Relationships with Dependent Effect Sizes: {{A Monte Carlo}} Simulation},
  shorttitle = {Assessing Meta-Regression Methods for Examining Moderator Relationships with Dependent Effect Sizes},
  author = {L{\'o}pez-L{\'o}pez, Jos{\'e} Antonio and den Noortgate, Wim Van and Tanner-Smith, Emily E. and Wilson, Sandra Jo and Lipsey, Mark W.},
  year = {2017},
  volume = {8},
  pages = {435--450},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1245},
  abstract = {Dependent effect sizes are ubiquitous in meta-analysis. Using Monte Carlo simulation, we compared the performance of 2 methods for meta-regression with dependent effect sizes\textemdash robust variance estimation (RVE) and 3-level modeling\textemdash with the standard meta-analytic method for independent effect sizes. We further compared bias-reduced linearization and jackknife estimators as small-sample adjustments for RVE and Wald-type and likelihood ratio tests for 3-level models. The bias in the slope estimates, width of the confidence intervals around those estimates, and empirical type I error and statistical power rates of the hypothesis tests from these different methods were compared for mixed-effects meta-regression analysis with one moderator either at the study or at the effect size level. All methods yielded nearly unbiased slope estimates under most scenarios, but as expected, the standard method ignoring dependency provided inflated type I error rates when testing the significance of the moderators. Robust variance estimation methods yielded not only the best results in terms of type I error rate but also the widest confidence intervals and the lowest power rates, especially when using the jackknife adjustments. Three-level models showed a promising performance with a moderate to large number of studies, especially with the likelihood ratio test, and yielded narrower confidence intervals around the slope and higher power rates than those obtained with the RVE approach. All methods performed better when the moderator was at the effect size level, the number of studies was moderate to large, and the between-studies variance was small. Our results can help meta-analysts deal with dependency in their data.},
  file = {/Users/rritaz/Zotero/storage/IFAIRDA6/López‐López et al. - 2017 - Assessing meta-regression methods for examining mo.pdf;/Users/rritaz/Zotero/storage/WQZCJHMK/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {4}
}

@article{lu_combination_2004,
  title = {Combination of Direct and Indirect Evidence in Mixed Treatment Comparisons},
  author = {Lu, G. and Ades, A. E.},
  year = {2004},
  volume = {23},
  pages = {3105--3124},
  issn = {1097-0258},
  doi = {10.1002/sim.1875},
  abstract = {Mixed treatment comparison (MTC) meta-analysis is a generalization of standard pairwise meta-analysis for A vs B trials, to data structures that include, for example, A vs B, B vs C, and A vs C trials. There are two roles for MTC: one is to strengthen inference concerning the relative efficacy of two treatments, by including both `direct' and `indirect' comparisons. The other is to facilitate simultaneous inference regarding all treatments, in order for example to select the best treatment. In this paper, we present a range of Bayesian hierarchical models using the Markov chain Monte Carlo software WinBUGS. These are multivariate random effects models that allow for variation in true treatment effects across trials. We consider models where the between-trials variance is homogeneous across treatment comparisons as well as heterogeneous variance models. We also compare models with fixed (unconstrained) baseline study effects with models with random baselines drawn from a common distribution. These models are applied to an illustrative data set and posterior parameter distributions are compared. We discuss model critique and model selection, illustrating the role of Bayesian deviance analysis, and node-based model criticism. The assumptions underlying the MTC models and their parameterization are also discussed. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/HHES6638/Lu and Ades - 2004 - Combination of direct and indirect evidence in mix.pdf;/Users/rritaz/Zotero/storage/GYBHBMAV/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance,random-effects},
  language = {en},
  number = {20}
}

@article{lu_linear_2011,
  title = {Linear Inference for Mixed Treatment Comparison Meta-Analysis: {{A}} Two-Stage Approach},
  shorttitle = {Linear Inference for Mixed Treatment Comparison Meta-Analysis},
  author = {Lu, Guobing and Welton, Nicky J. and Higgins, Julian P. T. and White, Ian R. and Ades, A. E.},
  year = {2011},
  volume = {2},
  pages = {43--60},
  issn = {1759-2887},
  doi = {10.1002/jrsm.34},
  abstract = {Mixed treatment comparisons (MTC) meta-analysis synthesises comparative evidence on multiple treatments or other interventions from a collection of randomised controlled trials (RCT) available in a research area, while still respecting the randomisation structure in RCTs. This paper sets out to examine the properties of MTC estimates and elucidate the concept of consistency between direct and indirect evidence in MTC networks. We decompose MTC synthesis into two stages. At the first stage, ordinary meta-analysis is performed in each group of trials that have the same treatment comparators\textemdash this provides the `direct' estimates of relative effect parameters. At the second stage, the optimal consistent estimates that minimise the distance between the direct estimates and the consistency hyper-plane can be deduced as the weighted least squares solution to a linear regression model with a specific design matrix that represents the consistency conditions. The consistent MTC estimates can then be represented explicitly as linear combinations of direct estimates, and under normality assumptions the overall evidence consistency can be tested with a likelihood-ratio statistic. This two-stage framework further allows us to use the leverage statistics to diagnose influence of the first-stage evidence and use the regression residuals to assess local inconsistency. The method is illustrated with two examples from medical research. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/P4K8QAU3/Lu et al. - 2011 - Linear inference for mixed treatment comparison me.pdf;/Users/rritaz/Zotero/storage/G9DPFYBL/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis},
  language = {en},
  number = {1}
}

@article{lu_meta-analysis_2007,
  title = {Meta-Analysis of Mixed Treatment Comparisons at Multiple Follow-up Times},
  author = {Lu, G. and Ades, A. E. and Sutton, A. J. and Cooper, N. J. and Briggs, A. H. and Caldwell, D. M.},
  year = {2007},
  volume = {26},
  pages = {3681--3699},
  issn = {1097-0258},
  doi = {10.1002/sim.2831},
  abstract = {Mixed treatment comparisons (MTC) meta-analysis is a methodology for making inferences on relative treatment effects based on a synthesis of both direct and indirect evidence on multiple treatment contrasts. This is particularly useful in the context of cost-effectiveness analysis and medical decision making. Here, we extend these methods to a more complex situation where trials report results at one or more, different yet fixed, follow-up times. These methods are applied to an illustrative data set combining evidence on healing rates under six different treatments for gastro-esophageal reflux disease (GERD). A series of Bayesian hierarchical models based on piece-wise exponential hazards is developed that borrow strength across the MTC networks and also across time points. These include models for absolute and relative treatment effects, models with fixed or random effects over time, random walk models, and models with homogeneous or heterogeneous between-trials variation. The deviance information criterion (DIC) is used to guide model development and selection. Models for absolute treatment effects generate materially different rankings of the treatments than models that separate the trial-specific baselines from the relative treatment effects. The extent of between-trials heterogeneity in treatment effects depends on treatment contrast. In discussion we note that models of this type have a very wide potential application. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JW9AZDDR/Lu et al. - 2007 - Meta-analysis of mixed treatment comparisons at mu.pdf;/Users/rritaz/Zotero/storage/8P45CZXQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,network meta-analysis,random-effects},
  language = {en},
  number = {20}
}

@article{lu_meta-analysis_2018,
  title = {Meta-Analysis Approaches to Combine Multiple Gene Set Enrichment Studies},
  author = {Lu, Wentao and Wang, Xinlei and Zhan, Xiaowei and Gazdar, Adi},
  year = {2018},
  volume = {37},
  pages = {659--672},
  issn = {1097-0258},
  doi = {10.1002/sim.7540},
  abstract = {In the field of gene set enrichment analysis (GSEA), meta-analysis has been used to integrate information from multiple studies to present a reliable summarization of the expanding volume of individual biomedical research, as well as improve the power of detecting essential gene sets involved in complex human diseases. However, existing methods, Meta-Analysis for Pathway Enrichment (MAPE), may be subject to power loss because of (1) using gross summary statistics for combining end results from component studies and (2) using enrichment scores whose distributions depend on the set sizes. In this paper, we adapt meta-analysis approaches recently developed for genome-wide association studies, which are based on fixed effect and random effects (RE) models, to integrate multiple GSEA studies. We further develop a mixed strategy via adaptive testing for choosing RE versus FE models to achieve greater statistical efficiency as well as flexibility. In addition, a size-adjusted enrichment score based on a one-sided Kolmogorov-Smirnov statistic is proposed to formally account for varying set sizes when testing multiple gene sets. Our methods tend to have much better performance than the MAPE methods and can be applied to both discrete and continuous phenotypes. Specifically, the performance of the adaptive testing method seems to be the most stable in general situations.},
  file = {/Users/rritaz/Zotero/storage/MMLI3VGC/Lu et al. - 2018 - Meta-analysis approaches to combine multiple gene .pdf;/Users/rritaz/Zotero/storage/5WARH66R/sim.html},
  journal = {Statistics in Medicine},
  keywords = {physical/biological fields,random-effects},
  language = {en},
  number = {4}
}

@article{lueza_bias_2016,
  title = {Bias and Precision of Methods for Estimating the Difference in Restricted Mean Survival Time from an Individual Patient Data Meta-Analysis},
  author = {Lueza, B{\'e}ranger and Rotolo, Federico and Bonastre, Julia and Pignon, Jean-Pierre and Michiels, Stefan},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {37},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0137-z},
  abstract = {Background: The difference in restricted mean survival time (rmstD\dh t\~A\TH ), the area between two survival curves up to time horizon t\~A, is often used in cost-effectiveness analyses to estimate the treatment effect in randomized controlled trials. A challenge in individual patient data (IPD) meta-analyses is to account for the trial effect. We aimed at comparing different methods to estimate the rmstD\dh t\~A\TH{} from an IPD meta-analysis. Methods: We compared four methods: the area between Kaplan-Meier curves (experimental vs. control arm) ignoring the trial effect (Na\"ive Kaplan-Meier); the area between Peto curves computed at quintiles of event times (Peto-quintile); the weighted average of the areas between either trial-specific Kaplan-Meier curves (Pooled Kaplan-Meier) or trial-specific exponential curves (Pooled Exponential). In a simulation study, we varied the between-trial heterogeneity for the baseline hazard and for the treatment effect (possibly correlated), the overall treatment effect, the time horizon t\~A, the number of trials and of patients, the use of fixed or DerSimonian-Laird random effects model, and the proportionality of hazards. We compared the methods in terms of bias, empirical and average standard errors. We used IPD from the Meta-Analysis of Chemotherapy in Nasopharynx Carcinoma (MAC-NPC) and its updated version MAC-NPC2 for illustration that included respectively 1,975 and 5,028 patients in 11 and 23 comparisons. Results: The Na\"ive Kaplan-Meier method was unbiased, whereas the Pooled Exponential and, to a much lesser extent, the Pooled Kaplan-Meier methods showed a bias with non-proportional hazards. The Peto-quintile method underestimated the rmstD\dh t\~A\TH, except with non-proportional hazards at t\~A= 5 years. In the presence of treatment effect heterogeneity, all methods except the Pooled Kaplan-Meier and the Pooled Exponential with DerSimonian-Laird random effects underestimated the standard error of the rmstD\dh t\~A\TH. Overall, the Pooled Kaplan-Meier method with DerSimonian-Laird random effects formed the best compromise in terms of bias and variance. The rmstD\dh t\~A {$\frac{1}{4}$} 10 years\TH{} estimated with the Pooled Kaplan-Meier method was 0.49 years (95 \% CI: [-0.06;1.03], p = 0.08) when comparing radiotherapy plus chemotherapy vs. radiotherapy alone in the MAC-NPC and 0.59 years (95 \% CI: [0.34;0.84], p {$<$} 0.0001) in the MAC-NPC2. Conclusions: We recommend the Pooled Kaplan-Meier method with DerSimonian-Laird random effects to estimate the difference in restricted mean survival time from an individual-patient data meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/Q6GHQ2B3/Lueza et al. - 2016 - Bias and precision of methods for estimating the d.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {random-effects},
  language = {en},
  number = {1}
}

@article{lui_tests_2000,
  title = {Tests for Homogeneity of the Risk Ratio in a Series of 2\texttimes 2 Tables},
  author = {Lui, Kung-Jong and Kelly, Colleen},
  year = {2000},
  volume = {19},
  pages = {2919--2932},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20001115)19:21<2919::AID-SIM561>3.0.CO;2-D},
  abstract = {We often apply the risk ratio to measure the strength of a causal relationship between a suspected risk factor and a disease of interest. In this paper we consider testing the homogeneity of risk ratio over a series of 2\texttimes 2 tables. In addition to the classical weighted least squares (CWLS) test procedure, we consider two test procedures using simple transformations of the CWLS statistic and develop three other asymptotically weighted test procedures. On the basis of Monte Carlo simulation, we conclude that the commonly-used CWLS test procedure is generally conservative, especially when the number of 2\texttimes 2 tables is large and the mean group size per table is moderate or small. We further find that two of the test procedures discussed here can not only generally outperform the CWLS test procedure, but also perform well in a variety of situations considered in this paper. Finally, we illustrate the use of these testing procedures with an example of six randomized trials that assess the effects of aspirin on the prevention of death in post-myocardial infarction patients. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JYQE89E7/Lui and Kelly - 2000 - Tests for homogeneity of the risk ratio in a serie.pdf;/Users/rritaz/Zotero/storage/2KICIABE/1097-0258(20001115)19212919AID-SIM5613.0.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete)},
  language = {en},
  number = {21}
}

@article{lumley_network_2002,
  title = {Network Meta-Analysis for Indirect Treatment Comparisons},
  author = {Lumley, Thomas},
  year = {2002},
  volume = {21},
  pages = {2313--2324},
  issn = {1097-0258},
  doi = {10.1002/sim.1201},
  abstract = {I present methods for assessing the relative effectiveness of two treatments when they have not been compared directly in a randomized trial but have each been compared to other treatments. These network meta-analysis techniques allow estimation of both heterogeneity in the effect of any given treatment and inconsistency (`incoherence') in the evidence from different pairs of treatments. A simple estimation procedure using linear mixed models is given and used in a meta-analysis of treatments for acute myocardial infarction. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PK6SHW3I/Lumley - 2002 - Network meta-analysis for indirect treatment compa.pdf;/Users/rritaz/Zotero/storage/JX6PG9CS/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis},
  language = {en},
  number = {16}
}

@article{m_detecting_2015,
  title = {Detecting Pleiotropy in {{Mendelian}} Randomisation Studies with Summary Data and a Continuous Outcome},
  author = {M, Fabiola Del Greco and Minelli, Cosetta and Sheehan, Nuala A. and Thompson, John R.},
  year = {2015},
  volume = {34},
  pages = {2926--2940},
  issn = {1097-0258},
  doi = {10.1002/sim.6522},
  abstract = {Mendelian randomisation (MR) estimates causal effects of modifiable phenotypes on an outcome by using genetic variants as instrumental variables, but its validity relies on the assumption of no pleiotropy, that is, genes influence the outcome only through the given phenotype. Excluding pleiotropy is difficult, but the use of multiple instruments can indirectly address the issue: if all genes represent valid instruments, their MR estimates should vary only by chance. The Sargan test detects pleiotropy when individual phenotype, outcome and genotype data are measured in the same subjects. We propose an alternative approach to be used when only summary genetic data are available or data on gene-phenotype and gene-outcome come from different subjects. The presence of pleiotropy is investigated using the between-instrument heterogeneity Q test (together with the I2 index) in a meta-analysis of MR Wald estimates, derived separately from each instrument. For a continuous outcome, we evaluate the approach through simulations and illustrate it using published data. For the scenario where all data come from the same subjects, we compare it with the Sargan test. The Q test tends to be conservative in small samples. Its power increases with the degree of pleiotropy and the sample size, as does the precision of the I2 index, in which case results are similar to those of the Sargan test. In MR studies with large sample sizes based on summary data, the between-instrument Q test represents a useful tool to explore the presence of heterogeneity due to pleiotropy or other causes. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/D6MXJ9XM/M et al. - 2015 - Detecting pleiotropy in Mendelian randomisation st.pdf;/Users/rritaz/Zotero/storage/W8DVT7K3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {continuous effect sizes,physical/biological fields},
  language = {en},
  number = {21}
}

@article{ma_comparison_2009,
  title = {Comparison of {{Bayesian}} and Classical Methods in the Analysis of Cluster Randomized Controlled Trials with a Binary Outcome: {{The Community Hypertension Assessment Trial}} ({{CHAT}})},
  shorttitle = {Comparison of {{Bayesian}} and Classical Methods in the Analysis of Cluster Randomized Controlled Trials with a Binary Outcome},
  author = {Ma, Jinhui and Thabane, Lehana and Kaczorowski, Janusz and Chambers, Larry and Dolovich, Lisa and Karwalajtys, Tina and Levitt, Cheryl},
  year = {2009},
  month = dec,
  volume = {9},
  pages = {37},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-37},
  abstract = {Background: Cluster randomized trials (CRTs) are increasingly used to assess the effectiveness of interventions to improve health outcomes or prevent diseases. However, the efficiency and consistency of using different analytical methods in the analysis of binary outcome have received little attention. We described and compared various statistical approaches in the analysis of CRTs using the Community Hypertension Assessment Trial (CHAT) as an example. The CHAT study was a cluster randomized controlled trial aimed at investigating the effectiveness of pharmacy-based blood pressure clinics led by peer health educators, with feedback to family physicians (CHAT intervention) against Usual Practice model (Control), on the monitoring and management of BP among older adults. Methods: We compared three cluster-level and six individual-level statistical analysis methods in the analysis of binary outcomes from the CHAT study. The three cluster-level analysis methods were: i) un-weighted linear regression, ii) weighted linear regression, and iii) random-effects meta-regression. The six individual level analysis methods were: i) standard logistic regression, ii) robust standard errors approach, iii) generalized estimating equations, iv) random-effects meta-analytic approach, v) random-effects logistic regression, and vi) Bayesian random-effects regression. We also investigated the robustness of the estimates after the adjustment for the cluster and individual level covariates. Results: Among all the statistical methods assessed, the Bayesian random-effects logistic regression method yielded the widest 95\% interval estimate for the odds ratio and consequently led to the most conservative conclusion. However, the results remained robust under all methods \textendash{} showing sufficient evidence in support of the hypothesis of no effect for the CHAT intervention against Usual Practice control model for management of blood pressure among seniors in primary care. The individual-level standard logistic regression is the least appropriate method in the analysis of CRTs because it ignores the correlation of the outcomes for the individuals within the same cluster. Conclusion: We used data from the CHAT trial to compare different methods for analysing data from CRTs. Using different methods to analyse CRTs provides a good approach to assess the sensitivity of the results to enhance interpretation.},
  file = {/Users/rritaz/Zotero/storage/ZQV33LV4/Ma et al. - 2009 - Comparison of Bayesian and classical methods in th.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,discrete effect sizes,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {1}
}

@article{ma_multivariate_2011,
  title = {Multivariate Meta-Analysis: A Robust Approach Based on the Theory of {{U}}-Statistic},
  shorttitle = {Multivariate Meta-Analysis},
  author = {Ma, Yan and Mazumdar, Madhu},
  year = {2011},
  volume = {30},
  pages = {2911--2929},
  issn = {1097-0258},
  doi = {10.1002/sim.4327},
  abstract = {Meta-analysis is the methodology for combining findings from similar research studies asking the same question. When the question of interest involves multiple outcomes, multivariate meta-analysis is used to synthesize the outcomes simultaneously taking into account the correlation between the outcomes. Likelihood-based approaches, in particular restricted maximum likelihood (REML) method, are commonly utilized in this context. REML assumes a multivariate normal distribution for the random-effects model. This assumption is difficult to verify, especially for meta-analysis with small number of component studies. The use of REML also requires iterative estimation between parameters, needing moderately high computation time, especially when the dimension of outcomes is large. A multivariate method of moments (MMM) is available and is shown to perform equally well to REML. However, there is a lack of information on the performance of these two methods when the true data distribution is far from normality. In this paper, we propose a new nonparametric and non-iterative method for multivariate meta-analysis on the basis of the theory of U-statistic and compare the properties of these three procedures under both normal and skewed data through simulation studies. It is shown that the effect on estimates from REML because of non-normal data distribution is marginal and that the estimates from MMM and U-statistic-based approaches are very similar. Therefore, we conclude that for performing multivariate meta-analysis, the U-statistic estimation procedure is a viable alternative to REML and MMM. Easy implementation of all three methods are illustrated by their application to data from two published meta-analysis from the fields of hip fracture and periodontal disease. We discuss ideas for future research based on U-statistic for testing significance of between-study heterogeneity and for extending the work to meta-regression setting. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NC7Y98CN/Ma and Mazumdar - 2011 - Multivariate meta-analysis a robust approach base.pdf;/Users/rritaz/Zotero/storage/2BYDFPYU/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,random-effects},
  language = {en},
  number = {24}
}

@article{ma_performing_2008,
  title = {Performing Meta-Analysis with Incomplete Statistical Information in Clinical Trials},
  author = {Ma, Jianbing and Liu, Weiru and Hunter, Anthony and Zhang, Weiya},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {56},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-56},
  abstract = {Background: Results from clinical trials are usually summarized in the form of sampling distributions. When full information (mean, SEM) about these distributions is given, performing meta-analysis is straightforward. However, when some of the sampling distributions only have mean values, a challenging issue is to decide how to use such distributions in meta-analysis. Currently, the most common approaches are either ignoring such trials or for each trial with a missing SEM, finding a similar trial and taking its SEM value as the missing SEM. Both approaches have drawbacks. As an alternative, this paper develops and tests two new methods, the first being the prognostic method and the second being the interval method, to estimate any missing SEMs from a set of sampling distributions with full information. A merging method is also proposed to handle clinical trials with partial information to simulate meta-analysis. Methods: Both of our methods use the assumption that the samples for which the sampling distributions will be merged are randomly selected from the same population. In the prognostic method, we predict the missing SEMs from the given SEMs. In the interval method, we define intervals that we believe will contain the missing SEMs and then we use these intervals in the merging process. Results: Two sets of clinical trials are used to verify our methods. One family of trials is on comparing different drugs for reduction of low density lipprotein cholesterol (LDL) for Type-2 diabetes, and the other is about the effectiveness of drugs for lowering intraocular pressure (IOP). Both methods are shown to be useful for approximating the conventional meta-analysis including trials with incomplete information. For example, the meta-analysis result of Latanoprost versus Timolol on IOP reduction for six months provided in [1] was 5.05 {$\pm$} 1.15 (Mean {$\pm$} SEM) with full information. If the last trial in this study is assumed to be with partial information, the traditional analysis method for dealing with incomplete information that ignores this trial would give 6.49 {$\pm$} 1.36 while our prognostic method gives 5.02 {$\pm$} 1.15, and our interval method provides two intervals as Mean {$\in$} [4.25, 5.63] and SEM {$\in$} [1.01, 1.24]. Conclusion: Both the prognostic and the interval methods are useful alternatives for dealing with missing data in meta-analysis. We recommend clinicians to use the prognostic method to predict the missing SEMs in order to perform meta-analysis and the interval method for obtaining a more cautious result.},
  file = {/Users/rritaz/Zotero/storage/UQ57EU5D/Ma et al. - 2008 - Performing meta-analysis with incomplete statistic.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,missing data},
  language = {en},
  number = {1}
}

@article{ma_trivariate_2014,
  title = {A Trivariate Meta-Analysis of Diagnostic Studies Accounting for Prevalence and Non-Evaluable Subjects: Re-Evaluation of the Meta-Analysis of Coronary {{CT}} Angiography Studies},
  shorttitle = {A Trivariate Meta-Analysis of Diagnostic Studies Accounting for Prevalence and Non-Evaluable Subjects},
  author = {Ma, Xiaoye and K Suri, Muhammad Fareed and Chu, Haitao},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {128},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-128},
  abstract = {Background: A recent paper proposed an intent-to-diagnose approach to handle non-evaluable index test results and discussed several alternative approaches, with an application to the meta-analysis of coronary CT angiography diagnostic accuracy studies. However, no simulation studies have been conducted to test the performance of the methods. Methods: We propose an extended trivariate generalized linear mixed model (TGLMM) to handle non-evaluable index test results. The performance of the intent-to-diagnose approach, the alternative approaches and the extended TGLMM approach is examined by extensive simulation studies. The meta-analysis of coronary CT angiography diagnostic accuracy studies is re-evaluated by the extended TGLMM. Results: Simulation studies showed that the intent-to-diagnose approach under-estimate sensitivity and specificity. Under the missing at random (MAR) assumption, the TGLMM gives nearly unbiased estimates of test accuracy indices and disease prevalence. After applying the TGLMM approach to re-evaluate the coronary CT angiography meta-analysis, overall median sensitivity is 0.98 (0.967, 0.993), specificity is 0.875 (0.827, 0.923) and disease prevalence is 0.478 (0.379, 0.577). Conclusions: Under MAR assumption, the intent-to-diagnose approach under-estimate both sensitivity and specificity, while the extended TGLMM gives nearly unbiased estimates of sensitivity, specificity and prevalence. We recommend the extended TGLMM to handle non-evaluable index test subjects.},
  file = {/Users/rritaz/Zotero/storage/D6HQCLBN/Ma et al. - 2014 - A trivariate meta-analysis of diagnostic studies a.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {GLM MA models,multivariate,physical/biological fields},
  language = {en},
  number = {1}
}

@article{macaskill_comparison_2001,
  title = {A Comparison of Methods to Detect Publication Bias in Meta-Analysis},
  author = {Macaskill, Petra and Walter, Stephen D. and Irwig, Les},
  year = {2001},
  volume = {20},
  pages = {641--654},
  issn = {1097-0258},
  doi = {10.1002/sim.698},
  abstract = {Meta-analyses are subject to bias for many of reasons, including publication bias. Asymmetry in a funnel plot of study size against treatment effect is often used to identify such bias. We compare the performance of three simple methods of testing for bias: the rank correlation method; a simple linear regression of the standardized estimate of treatment effect on the precision of the estimate; and a regression of the treatment effect on sample size. The tests are applied to simulated meta-analyses in the presence and absence of publication bias. Both one-sided and two-sided censoring of studies based on statistical significance was used. The results indicate that none of the tests performs consistently well. Test performance varied with the magnitude of the true treatment effect, distribution of study size and whether a one- or two-tailed significance test was employed. Overall, the power of the tests was low when the number of studies per meta-analysis was close to that often observed in practice. Tests that showed the highest power also had type I error rates higher than the nominal level. Based on the empirical type I error rates, a regression of treatment effect on sample size, weighted by the inverse of the variance of the logit of the pooled proportion (using the marginal total) is the preferred method. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NVQ8UUH5/Macaskill et al. - 2001 - A comparison of methods to detect publication bias.pdf;/Users/rritaz/Zotero/storage/I7A97MWG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {4}
}

@article{mackinnon_heteroskedasticity-consistent_1985,
  title = {Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties},
  author = {MacKinnon, James G and White, Halbert},
  year = {1985},
  month = sep,
  volume = {29},
  pages = {305--325},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(85)90158-7},
  abstract = {We examine several modified versions of the heteroskedasticity-consistent covariance matrix estimator of Hinkley (1977) and White (1980). On the basis of sampling experiments which compare the performance of quasi t-statistics, we find that one estimator, based on the jackknife, performs better in small samples than the rest. We also examine the finite-sample properties of using modified critical values based on Edgeworth approximations, as proposed by Rothenberg (1984). In addition, we compare the power of several tests for heteroskedasticity, and find that it may be wise to employ the jackknife heteroskedasticity-consistent covariance matrix even in the absence of detected heteroskedasticity.},
  file = {/Users/rritaz/Zotero/storage/UMWK4IKX/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.pdf;/Users/rritaz/Zotero/storage/AALY37YF/0304407685901587.html;/Users/rritaz/Zotero/storage/NHTHENDH/0304407685901587.html},
  journal = {Journal of Econometrics},
  language = {en},
  number = {3}
}

@article{mackinnon_matrix_nodate,
  title = {{{MATRIX ESTIMATORS WITH IMPROVED FINITE SAMPLE PROPERTIES}}},
  author = {MacKINNON, James G},
  pages = {21},
  file = {/Users/rritaz/Zotero/storage/GN6BBTC3/MacKINNON - MATRIX ESTIMATORS WITH IMPROVED FINITE SAMPLE PROP.pdf},
  language = {en}
}

@article{mackinnon_matrix_nodate-1,
  title = {{{MATRIX ESTIMATORS WITH IMPROVED FINITE SAMPLE PROPERTIES}}},
  author = {MacKINNON, James G},
  pages = {21},
  file = {/Users/rritaz/Zotero/storage/62ZKA9C5/MacKINNON - MATRIX ESTIMATORS WITH IMPROVED FINITE SAMPLE PROP.pdf},
  language = {en}
}

@article{makambi_combining_2013,
  title = {Combining Study Outcome Measures Using Dominance Adjusted Weights},
  author = {Makambi, Kepher H. and Lu, Wenxin},
  year = {2013},
  volume = {4},
  pages = {188--197},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1073},
  abstract = {Weighting of studies in meta-analysis is usually implemented by using the estimated inverse variances of treatment effect estimates. However, there is a possibility of one study dominating other studies in the estimation process by taking on a weight that is above some upper limit. We implement an estimator of the heterogeneity variance that takes advantage of dominance adjusted weights. The performance of this estimator is compared with that of the commonly used estimator in meta-analysis, the DerSimonian\textendash Laird estimator. Two test procedures for the overall treatment effect are proposed that are based on the quadratic form associated with the proposed heterogeneity variance estimator. An example is given to illustrate the application of these procedures. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PCXKHKTD/Makambi and Lu - 2013 - Combining study outcome measures using dominance a.pdf;/Users/rritaz/Zotero/storage/UB52BGVE/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,heterogeneity estimators},
  language = {en},
  number = {2}
}

@article{malloy_transforming_2013,
  title = {Transforming the {{Model T}}: Random Effects Meta-Analysis with Stable Weights},
  shorttitle = {Transforming the {{Model T}}},
  author = {Malloy, Michael J. and Prendergast, Luke A. and Staudte, Robert G.},
  year = {2013},
  volume = {32},
  pages = {1842--1864},
  issn = {1097-0258},
  doi = {10.1002/sim.5666},
  abstract = {Standard meta-analytic theory assumes that study outcomes are normally distributed with known variances. However, methods derived from this theory are often applied to effect sizes having skewed distributions with estimated variances. Both shortcomings can be largely overcome by first applying a variance stabilizing transformation. Here we concentrate on study outcomes with Student t-distributions and show that we can better estimate parameters of fixed or random effects models with confidence intervals using stable weights or with profile approximate likelihood intervals following stabilization. We achieve even better coverage with a finite sample bias correction. Further, a simple t-interval provides very good coverage of an overall effect size without estimation of the inter-study variance. We illustrate the methodology on two meta-analytic studies from the medical literature, the effect of salt reduction on systolic blood pressure and the effect of opioids for the relief of breathlessness. Substantial simulation studies compare traditional methods with those newly proposed. We can apply the theoretical results to other study outcomes for which an effective variance stabilizer is available. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/7SDLMBXZ/Malloy et al. - 2013 - Transforming the Model T random effects meta-anal.pdf;/Users/rritaz/Zotero/storage/VSP4LYXV/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random effects models,random-effects,variance-stabilized},
  language = {en},
  number = {11}
}

@article{manolov_assigning_2012,
  title = {Assigning and Combining Probabilities in Single-Case Studies.},
  author = {Manolov, Rumen and Solanas, Antonio},
  year = {2012},
  volume = {17},
  pages = {495--509},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0029248},
  file = {/Users/rritaz/Zotero/storage/FLN28LE4/Manolov and Solanas - 2012 - Assigning and combining probabilities in single-ca.pdf},
  journal = {Psychological Methods},
  keywords = {combined significance},
  language = {en},
  number = {4}
}

@article{mathes_comparison_2018,
  title = {A Comparison of Methods for Meta-Analysis of a Small Number of Studies with Binary Outcomes},
  author = {Mathes, Tim and Kuss, Oliver},
  year = {2018},
  volume = {9},
  pages = {366--381},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1296},
  abstract = {Meta-analyses often include only a small number of studies ({$\leq$}5). Estimating between-study heterogeneity is difficult in this situation. An inaccurate estimation of heterogeneity can result in biased effect estimates and too narrow confidence intervals. The beta-binominal model has shown good statistical properties for meta-analysis of sparse data. We compare the beta-binominal model with different inverse variance random (eg, DerSimonian-Laird, modified Hartung-Knapp, and Paule-Mandel) and fixed effects methods (Mantel-Haenszel and Peto) in a simulation study. The underlying true parameters were obtained from empirical data of actually performed meta-analyses to best mirror real-life situations. We show that valid methods for meta-analysis of a small number of studies are available. In fixed effects situations, the Mantel-Haenszel and Peto methods performed best. In random effects situations, the beta-binominal model performed best for meta-analysis of few studies considering the balance between coverage probability and power. We recommended the beta-binominal model for practical application. If very strong evidence is needed, using the Paule-Mandel heterogeneity variance estimator combined with modified Hartung-Knapp confidence intervals might be useful to confirm the results. Notable most inverse variance random effects models showed unsatisfactory statistical properties also if more studies (10-50) were included in the meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/A777JTSB/Mathes and Kuss - 2018 - A comparison of methods for meta-analysis of a sma.pdf;/Users/rritaz/Zotero/storage/YTF5ZPNT/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes,effect size combination (small sample \& discrete),small meta-analysis},
  language = {en},
  number = {3}
}

@article{mathur_new_2019,
  title = {New Metrics for Meta-Analyses of Heterogeneous Effects},
  author = {Mathur, Maya B. and VanderWeele, Tyler J.},
  year = {2019},
  volume = {38},
  pages = {1336--1342},
  issn = {1097-0258},
  doi = {10.1002/sim.8057},
  abstract = {We provide two simple metrics that could be reported routinely in random-effects meta-analyses to convey evidence strength for scientifically meaningful effects under effect heterogeneity (ie, a nonzero estimated variance of the true effect distribution). First, given a chosen threshold of meaningful effect size, meta-analyses could report the estimated proportion of true effect sizes above this threshold. Second, meta-analyses could estimate the proportion of effect sizes below a second, possibly symmetric, threshold in the opposite direction from the estimated mean. These metrics could help identify if (1) there are few effects of scientifically meaningful size despite a ``statistically significant'' pooled point estimate, (2) there are some large effects despite an apparently null point estimate, or (3) strong effects in the direction opposite the pooled estimate also regularly occur (and thus, potential effect modifiers should be examined). These metrics should be presented with confidence intervals, which can be obtained analytically or, under weaker assumptions, using bias-corrected and accelerated bootstrapping. Additionally, these metrics inform relative comparison of evidence strength across related meta-analyses. We illustrate with applied examples and provide an R function to compute the metrics and confidence intervals.},
  file = {/Users/rritaz/Zotero/storage/W6FGDCKZ/Mathur and VanderWeele - 2019 - New metrics for meta-analyses of heterogeneous eff.pdf;/Users/rritaz/Zotero/storage/EUDY4T2X/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random-effects},
  language = {en},
  number = {8}
}

@article{mavridis_allowing_2015,
  title = {Allowing for Uncertainty Due to Missing Continuous Outcome Data in Pairwise and Network Meta-Analysis},
  author = {Mavridis, Dimitris and White, Ian R. and Higgins, Julian P. T. and Cipriani, Andrea and Salanti, Georgia},
  year = {2015},
  volume = {34},
  pages = {721--741},
  issn = {1097-0258},
  doi = {10.1002/sim.6365},
  abstract = {Missing outcome data are commonly encountered in randomized controlled trials and hence may need to be addressed in a meta-analysis of multiple trials. A common and simple approach to deal with missing data is to restrict analysis to individuals for whom the outcome was obtained (complete case analysis). However, estimated treatment effects from complete case analyses are potentially biased if informative missing data are ignored. We develop methods for estimating meta-analytic summary treatment effects for continuous outcomes in the presence of missing data for some of the individuals within the trials. We build on a method previously developed for binary outcomes, which quantifies the degree of departure from a missing at random assumption via the informative missingness odds ratio. Our new model quantifies the degree of departure from missing at random using either an informative missingness difference of means or an informative missingness ratio of means, both of which relate the mean value of the missing outcome data to that of the observed data. We propose estimating the treatment effects, adjusted for informative missingness, and their standard errors by a Taylor series approximation and by a Monte Carlo method. We apply the methodology to examples of both pairwise and network meta-analysis with multi-arm trials. \textcopyright{} 2014 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/H393HBHT/Mavridis et al. - 2015 - Allowing for uncertainty due to missing continuous.pdf;/Users/rritaz/Zotero/storage/XNH43JUT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,network meta-analysis,random-effects},
  language = {en},
  number = {5}
}

@article{mavridis_allowing_2019,
  title = {Allowing for Uncertainty Due to Missing and {{LOCF}} Imputed Outcomes in Meta-Analysis},
  author = {Mavridis, Dimitris and Salanti, Georgia and Furukawa, Toshi A. and Cipriani, Andrea and Chaimani, Anna and White, Ian R.},
  year = {2019},
  volume = {38},
  pages = {720--737},
  issn = {1097-0258},
  doi = {10.1002/sim.8009},
  abstract = {The use of the last observation carried forward (LOCF) method for imputing missing outcome data in randomized clinical trials has been much criticized and its shortcomings are well understood. However, only recently have published studies widely started using more appropriate imputation methods. Consequently, meta-analyses often include several studies reporting their results according to LOCF. The results from such meta-analyses are potentially biased and overprecise. We develop methods for estimating summary treatment effects for continuous outcomes in the presence of both missing and LOCF-imputed outcome data. Our target is the treatment effect if complete follow-up was obtained even if some participants drop out from the protocol treatment. We extend a previously developed meta-analysis model, which accounts for the uncertainty due to missing outcome data via an informative missingness parameter. The extended model includes an extra parameter that reflects the level of prior confidence in the appropriateness of the LOCF imputation scheme. Neither parameter can be informed by the data and we resort to expert opinion and sensitivity analysis. We illustrate the methodology using two meta-analyses of pharmacological interventions for depression.},
  file = {/Users/rritaz/Zotero/storage/2IIHSLEY/Mavridis et al. - 2019 - Allowing for uncertainty due to missing and LOCF i.pdf;/Users/rritaz/Zotero/storage/DYH6YVTF/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data},
  language = {en},
  number = {5}
}

@article{mavridis_fully_2013,
  title = {A Fully {{Bayesian}} Application of the {{Copas}} Selection Model for Publication Bias Extended to Network Meta-Analysis},
  author = {Mavridis, Dimitris and Sutton, Alex and Cipriani, Andrea and Salanti, Georgia},
  year = {2013},
  volume = {32},
  pages = {51--66},
  issn = {1097-0258},
  doi = {10.1002/sim.5494},
  abstract = {The Copas parametric model is aimed at exploring the potential impact of publication bias via sensitivity analysis, by making assumptions regarding the probability of publication of individual studies related to the standard error of their effect sizes. Reviewers often have prior assumptions about the extent of selection in the set of studies included in a meta-analysis. However, a Bayesian implementation of the Copas model has not been studied yet. We aim to present a Bayesian selection model for publication bias and to extend it to the case of network meta-analysis where each treatment is compared either with placebo or with a reference treatment creating a star-shaped network. We take advantage of the greater flexibility offered in the Bayesian context to incorporate in the model prior information on the extent and strength of selection. To derive prior distributions, we use both external data and an elicitation process of expert opinion. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YGHNEAEY/Mavridis et al. - 2013 - A fully Bayesian application of the Copas selectio.pdf;/Users/rritaz/Zotero/storage/SBPDTRPR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,network meta-analysis,publication bias},
  language = {en},
  number = {1}
}

@article{mavridis_selection_2014,
  title = {A Selection Model for Accounting for Publication Bias in a Full Network Meta-Analysis},
  author = {Mavridis, Dimitris and Welton, Nicky J. and Sutton, Alex and Salanti, Georgia},
  year = {2014},
  volume = {33},
  pages = {5399--5412},
  issn = {1097-0258},
  doi = {10.1002/sim.6321},
  abstract = {Copas and Shi suggested a selection model to explore the potential impact of publication bias via sensitivity analysis based on assumptions for the probability of publication of trials conditional on the precision of their results. Chootrakool et al. extended this model to three-arm trials but did not fully account for the implications of the consistency assumption, and their model is difficult to generalize for complex network structures with more than three treatments. Fitting these selection models within a frequentist setting requires maximization of a complex likelihood function, and identification problems are common. We have previously presented a Bayesian implementation of the selection model when multiple treatments are compared with a common reference treatment. We now present a general model suitable for complex, full network meta-analysis that accounts for consistency when adjusting results for publication bias. We developed a design-by-treatment selection model to describe the mechanism by which studies with different designs (sets of treatments compared in a trial) and precision may be selected for publication. We fit the model in a Bayesian setting because it avoids the numerical problems encountered in the frequentist setting, it is generalizable with respect to the number of treatments and study arms, and it provides a flexible framework for sensitivity analysis using external knowledge. Our model accounts for the additional uncertainty arising from publication bias more successfully compared to the standard Copas model or its previous extensions. We illustrate the methodology using a published triangular network for the failure of vascular graft or arterial patency. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Z3VMK3C6/Mavridis et al. - 2014 - A selection model for accounting for publication b.pdf;/Users/rritaz/Zotero/storage/9YPMWAI4/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,publication bias},
  language = {en},
  number = {30}
}

@article{maxwell_persistence_2004,
  title = {The Persistence of Underpowered Studies in Psychological Research: {{Causes}}, Consequences, and Remedies},
  shorttitle = {The Persistence of Underpowered Studies in Psychological Research},
  author = {Maxwell, Scott E.},
  year = {2004},
  pages = {10--1037},
  abstract = {Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The ``curse of multiplicities '' plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing. The primary purpose of this article is to examine the importance of statistical power for the formulation of a coherent body of scientific literature. The article addresses this goal through consideration of four interrelated subtop-ics: (a) why underpowered studies persist, (b) the undesir-},
  file = {/Users/rritaz/Zotero/storage/RGKHB9AF/Maxwell - 2004 - The persistence of underpowered studies in psychol.pdf;/Users/rritaz/Zotero/storage/JWFQDQLD/summary.html},
  journal = {Psychological Methods},
  keywords = {power}
}

@article{mcgrath_one-sample_2019,
  title = {One-Sample Aggregate Data Meta-Analysis of Medians},
  author = {McGrath, Sean and Zhao, XiaoFei and Qin, Zhi Zhen and Steele, Russell and Benedetti, Andrea},
  year = {2019},
  volume = {38},
  pages = {969--984},
  issn = {1097-0258},
  doi = {10.1002/sim.8013},
  abstract = {An aggregate data meta-analysis is a statistical method that pools the summary statistics of several selected studies to estimate the outcome of interest. When considering a continuous outcome, typically each study must report the same measure of the outcome variable and its spread (eg, the sample mean and its standard error). However, some studies may instead report the median along with various measures of spread. Recently, the task of incorporating medians in meta-analysis has been achieved by estimating the sample mean and its standard error from each study that reports a median in order to meta-analyze the means. In this paper, we propose two alternative approaches to meta-analyze data that instead rely on medians. We systematically compare these approaches via simulation study to each other and to methods that transform the study-specific medians and spread into sample means and their standard errors. We demonstrate that the proposed median-based approaches perform better than the transformation-based approaches, especially when applied to skewed data and data with high inter-study variance. Finally, we illustrate these approaches in a meta-analysis of patient delay in tuberculosis diagnosis.},
  file = {/Users/rritaz/Zotero/storage/282XAK8Y/McGrath et al. - 2019 - One-sample aggregate data meta-analysis of medians.pdf;/Users/rritaz/Zotero/storage/7BMDLQ82/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance},
  language = {en},
  number = {6}
}

@article{mcgrath_when_2006,
  title = {When Effect Sizes Disagree: {{The}} Case of r and d.},
  shorttitle = {When Effect Sizes Disagree},
  author = {McGrath, Robert E. and Meyer, Gregory J.},
  year = {2006},
  volume = {11},
  pages = {386--401},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.4.386},
  file = {/Users/rritaz/Zotero/storage/NZK2RVY4/McGrath and Meyer - 2006 - When effect sizes disagree The case of r and d..pdf},
  journal = {Psychological Methods},
  keywords = {choice of ES,continuous effect sizes,correlation coefficients,discrete effect sizes},
  language = {en},
  number = {4}
}

@article{mckenzie_impact_2016,
  title = {Impact of Analysing Continuous Outcomes Using Final Values, Change Scores and Analysis of Covariance on the Performance of Meta-Analytic Methods: A Simulation Study},
  shorttitle = {Impact of Analysing Continuous Outcomes Using Final Values, Change Scores and Analysis of Covariance on the Performance of Meta-Analytic Methods},
  author = {McKenzie, Joanne E. and Herbison, G. Peter and Deeks, Jonathan J.},
  year = {2016},
  volume = {7},
  pages = {371--386},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1196},
  abstract = {When meta-analysing intervention effects calculated from continuous outcomes, meta-analysts often encounter few trials, with potentially a small number of participants, and a variety of trial analytical methods. It is important to know how these factors affect the performance of inverse-variance fixed and DerSimonian and Laird random effects meta-analytical methods. We examined this performance using a simulation study. Meta-analysing estimates of intervention effect from final values, change scores, ANCOVA or a random mix of the three yielded unbiased estimates of pooled intervention effect. The impact of trial analytical method on the meta-analytic performance measures was important when there was no or little heterogeneity, but was of little relevance as heterogeneity increased. On the basis of larger than nominal type I error rates and poor coverage, the inverse-variance fixed effect method should not be used when there are few small trials. When there are few small trials, random effects meta-analysis is preferable to fixed effect meta-analysis. Meta-analytic estimates need to be cautiously interpreted; type I error rates will be larger than nominal, and confidence intervals will be too narrow. Use of trial analytical methods that are more efficient in these circumstances may have the unintended consequence of further exacerbating these issues. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/D2UWYMHE/McKenzie et al. - 2016 - Impact of analysing continuous outcomes using fina.pdf;/Users/rritaz/Zotero/storage/9JEZCNGF/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,continuous effect sizes,power,small meta-analysis},
  language = {en},
  number = {4}
}

@article{menten_bayesian_2013,
  title = {Bayesian Meta-Analysis of Diagnostic Tests Allowing for Imperfect Reference Standards},
  author = {Menten, J. and Boelaert, M. and Lesaffre, E.},
  year = {2013},
  volume = {32},
  pages = {5398--5413},
  issn = {1097-0258},
  doi = {10.1002/sim.5959},
  abstract = {There is an increasing interest in meta-analyses of rapid diagnostic tests (RDTs) for infectious diseases. To avoid spectrum bias, these meta-analyses should focus on phase IV studies performed in the target population. For many infectious diseases, these target populations attend primary health care centers in resource-constrained settings where it is difficult to perform gold standard diagnostic tests. As a consequence, phase IV diagnostic studies often use imperfect reference standards, which may result in biased meta-analyses of the diagnostic accuracy of novel RDTs. We extend the standard bivariate model for the meta-analysis of diagnostic studies to correct for differing and imperfect reference standards in the primary studies and to accommodate data from studies that try to overcome the absence of a true gold standard through the use of latent class analysis. Using Bayesian methods, improved estimates of sensitivity and specificity are possible, especially when prior information is available on the diagnostic accuracy of the reference test. In this analysis, the deviance information criterion can be used to detect conflicts between the prior information and observed data. When applying the model to a dataset of the diagnostic accuracy of an RDT for visceral leishmaniasis, the standard meta-analytic methods appeared to underestimate the specificity of the RDT. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YAH9VUPF/Menten et al. - 2013 - Bayesian meta-analysis of diagnostic tests allowin.pdf;/Users/rritaz/Zotero/storage/NEEQ86UG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate},
  language = {en},
  number = {30}
}

@article{menten_general_2015,
  title = {A General Framework for Comparative {{Bayesian}} Meta-Analysis of Diagnostic Studies},
  author = {Menten, Joris and Lesaffre, Emmanuel},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {70},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0061-7},
  abstract = {Background: Selecting the most effective diagnostic method is essential for patient management and public health interventions. This requires evidence of the relative performance of alternative tests or diagnostic algorithms. Consequently, there is a need for diagnostic test accuracy meta-analyses allowing the comparison of the accuracy of two or more competing tests. The meta-analyses are however complicated by the paucity of studies that directly compare the performance of diagnostic tests. A second complication is that the diagnostic accuracy of the tests is usually determined through the comparison of the index test results with those of a reference standard. These reference standards are presumed to be perfect, i.e. allowing the classification of diseased and non-diseased subjects without error. In practice, this assumption is however rarely valid and most reference standards show false positive or false negative results. When an imperfect reference standard is used, the estimated accuracy of the tests of interest may be biased, as well as the comparisons between these tests. Methods: We propose a model that allows for the comparison of the accuracy of two diagnostic tests using direct (head-to-head) comparisons as well as indirect comparisons through a third test. In addition, the model allows and corrects for imperfect reference tests. The model is inspired by mixed-treatment comparison meta-analyses that have been developed for the meta-analysis of randomized controlled trials. As the model is estimated using Bayesian methods, it can incorporate prior knowledge on the diagnostic accuracy of the reference tests used. Results: We show the bias that can result from using inappropriate methods in the meta-analysis of diagnostic tests and how our method provides more correct estimates of the difference in diagnostic accuracy between two tests. As an illustration, we apply this model to a dataset on visceral leishmaniasis diagnostic tests, comparing the accuracy of the RK39 dipstick with that of the direct agglutination test. Conclusions: Our proposed meta-analytic model can improve the comparison of the diagnostic accuracy of competing tests in a systematic review. This is however only true if the studies and especially information on the reference tests used are sufficiently detailed. More specifically, the type and exact procedures used as reference tests are needed, including any cut-offs used and the number of subjects excluded from full reference test assessment. If this information is lacking, it may be better to limit the meta-analysis to direct comparisons.},
  file = {/Users/rritaz/Zotero/storage/YQIVJHGT/Menten and Lesaffre - 2015 - A general framework for comparative Bayesian meta-.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,physical/biological fields},
  language = {en},
  number = {1}
}

@article{michel_conclusions_2011,
  title = {Conclusions from Meta-Analytic Structural Equation Models Generally Do Not Change Due to Corrections for Study Artifacts},
  author = {Michel, Jesse S. and Viswesvaran, Chockalingam and Thomas, Jeffrey},
  year = {2011},
  volume = {2},
  pages = {174--187},
  issn = {1759-2887},
  doi = {10.1002/jrsm.47},
  abstract = {Meta-analytic structural equations modeling is increasingly used in theory testing. There has been much debate when meta-analyzed correlation matrices are used in structural equations modeling on whether to use mean observed correlations (i.e., corrected only for sampling error) or correlations corrected for study artifacts such as unreliability in measures. This paper investigates whether the fit indices are affected by the corrections and if the stability of the paths (i.e., changes in significance, magnitude, and relative strengths or rank order) is affected by the corrections. Results suggest that substantive model conclusions are generally unaffected by study artifacts and related statistical corrections as long as the variables included in the path analyses had typical levels of reliability as found in the psychological literature. More specifically, all models examined exhibited similar model fit and pathway stability. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/RP5WQUB7/Michel et al. - 2011 - Conclusions from meta-analytic structural equation.pdf;/Users/rritaz/Zotero/storage/YFCMND3N/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlation coefficients},
  language = {en},
  number = {3}
}

@article{minelli_bayesian_2005,
  title = {Bayesian Implementation of a Genetic Model-Free Approach to the Meta-Analysis of Genetic Association Studies},
  author = {Minelli, Cosetta and Thompson, John R. and Abrams, Keith R. and Lambert, Paul C.},
  year = {2005},
  volume = {24},
  pages = {3845--3861},
  issn = {1097-0258},
  doi = {10.1002/sim.2393},
  abstract = {A genetic model-free method for the meta-analysis of genetic association studies is described that estimates the mode of inheritance from the data rather than assuming that it is known. For a bi-allelic polymorphism, with G as risk allele and g as wild-type, the genetic model depends on the ratio of the two log odds ratios, {$\lambda$} = log ORGg/log ORGG, where ORGG compares GG with gg and ORGg compares Gg with gg. Modelling log ORGG as a random effect creates a hierarchical model that can be implemented within a Bayesian framework. In Bayesian modelling, vague prior distributions have to be specified for all unknown parameters when no external information is available. When the data are sparse even supposedly vague prior distributions may have an influence on the posterior estimates. We investigate the impact of different vague prior distributions for the between-study standard deviation of log ORGG and for {$\lambda$}, by considering three published meta-analyses and associated simulations. Our results show that depending on the characteristics of the meta-analysis the results may indeed be sensitive to the choice of vague prior distribution for either parameter. Genetic association studies usually use a case-control design that should be analysed by the corresponding retrospective likelihood. However, under some circumstances the prospective likelihood has been shown to produce identical results and it is usually preferred for its simplicity. In our meta-analyses the two likelihoods give very similar results. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DYMW7IP3/Minelli et al. - 2005 - Bayesian implementation of a genetic model-free ap.pdf;/Users/rritaz/Zotero/storage/5CBJYFUI/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,physical/biological fields,random-effects},
  language = {en},
  number = {24}
}

@article{mistry_recursive_2018,
  title = {A Recursive Partitioning Approach for Subgroup Identification in Individual Patient Data Meta-Analysis},
  author = {Mistry, Dipesh and Stallard, Nigel and Underwood, Martin},
  year = {2018},
  volume = {37},
  pages = {1550--1561},
  issn = {1097-0258},
  doi = {10.1002/sim.7609},
  abstract = {Background Motivated by the setting of clinical trials in low back pain, this work investigated statistical methods to identify patient subgroups for which there is a large treatment effect (treatment by subgroup interaction). Statistical tests for interaction are often underpowered. Individual patient data (IPD) meta-analyses provide a framework with improved statistical power to investigate subgroups. However, conventional approaches to subgroup analyses applied in both a single trial setting and an IPD setting have a number of issues, one of them being that factors used to define subgroups are investigated one at a time. As individuals have multiple characteristics that may be related to response to treatment, alternative exploratory statistical methods are required. Methods Tree-based methods are a promising alternative that systematically searches the covariate space to identify subgroups defined by multiple characteristics. A tree method in particular, SIDES, is described and extended for application in an IPD meta-analyses setting by incorporating fixed-effects and random-effects models to account for between-trial variation. The performance of the proposed extension was assessed using simulation studies. The proposed method was then applied to an IPD low back pain dataset. Results The simulation studies found that the extended IPD-SIDES method performed well in detecting subgroups especially in the presence of large between-trial variation. The IPD-SIDES method identified subgroups with enhanced treatment effect when applied to the low back pain data. Conclusions This work proposes an exploratory statistical approach for subgroup analyses applicable in any research discipline where subgroup analyses in an IPD meta-analysis setting are of interest.},
  file = {/Users/rritaz/Zotero/storage/CEEKV3IH/Mistry et al. - 2018 - A recursive partitioning approach for subgroup ide.pdf;/Users/rritaz/Zotero/storage/KKA2TMQR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {9}
}

@article{moodie_non-parametric_2004,
  title = {A Non-Parametric Procedure for Evaluating Treatment Effect in the Meta-Analysis of Survival Data},
  author = {Moodie, Patricia F. and Nelson, Norma A. and Koch, Gary G.},
  year = {2004},
  volume = {23},
  pages = {1075--1093},
  issn = {1097-0258},
  doi = {10.1002/sim.1696},
  abstract = {This paper addresses the problem of combining information from independent clinical trials which compare survival distributions of two treatment groups. Current meta-analytic methods which take censoring into account are often not feasible for meta-analyses which synthesize summarized results in published (or unpublished) references, as these methods require information usually not reported. The paper presents methodology which uses the log(-log) survival function difference, (i.e. log(-logS2(t))-log(-logS1(t)), as the contrast index to represent the multiplicative treatment effect on survival in independent trials. This article shows by the second mean value theorem for integrals that this contrast index, denoted as \texttheta, is interpretable as a weighted average on a natural logarithmic scale of hazard ratios within the interval [0,t] in a trial. When the within-trial proportional hazards assumption is true, \texttheta{} is the logarithm of the proportionality constant for the common hazard ratio for the interval considered within the trial. In this situation, an important advantage of using \texttheta{} as a contrast index in the proposed methodology is that the estimation of \texttheta{} is not affected by length of follow-up time. Other commonly used indices such as the odds ratio, risk ratio and risk differences do not have this invariance property under the proportional hazard model, since their estimation may be affected by length of follow-up time as a technical artefact. Thus, the proposed methodology obviates problems which often occur in survival meta-analysis because trials do not report survival at the same length of follow-up time. Even when the within-trial proportional hazards assumption is not realistic, the proposed methodology has the capability of testing a global null hypothesis of no multiplicative treatment effect on the survival distributions of two groups for all studies. A discussion of weighting schemes for meta-analysis is provided, in particular, a weighting scheme based on effective sample sizes is suggested for the meta-analysis of time-to-event data which involves censoring. A medical example illustrating the methodology is given. A simulation investigation suggested that the methodology performs well in the presence of moderate censoring. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2585NL8A/Moodie et al. - 2004 - A non-parametric procedure for evaluating treatmen.pdf;/Users/rritaz/Zotero/storage/EQM3RL35/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random-effects},
  language = {en},
  number = {7}
}

@article{moreno_assessment_2009,
  title = {Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study},
  author = {Moreno, Santiago G and Sutton, Alex J and Ades, Ae and Stanley, Tom D and Abrams, Keith R and Peters, Jaime L and Cooper, Nicola J},
  year = {2009},
  month = dec,
  volume = {9},
  pages = {2},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-2},
  abstract = {Background: In meta-analysis, the presence of funnel plot asymmetry is attributed to publication or other small-study effects, which causes larger effects to be observed in the smaller studies. This issue potentially mean inappropriate conclusions are drawn from a meta-analysis. If meta-analysis is to be used to inform decision-making, a reliable way to adjust pooled estimates for potential funnel plot asymmetry is required. Methods: A comprehensive simulation study is presented to assess the performance of different adjustment methods including the novel application of several regression-based methods (which are commonly applied to detect publication bias rather than adjust for it) and the popular Trim \& Fill algorithm. Meta-analyses with binary outcomes, analysed on the log odds ratio scale, were simulated by considering scenarios with and without i) publication bias and; ii) heterogeneity. Publication bias was induced through two underlying mechanisms assuming the probability of publication depends on i) the study effect size; or ii) the p-value. Results: The performance of all methods tended to worsen as unexplained heterogeneity increased and the number of studies in the meta-analysis decreased. Applying the methods conditional on an initial test for the presence of funnel plot asymmetry generally provided poorer performance than the unconditional use of the adjustment method. Several of the regression based methods consistently outperformed the Trim \& Fill estimators. Conclusion: Regression-based adjustments for publication bias and other small study effects are easy to conduct and outperformed more established methods over a wide range of simulation scenarios.},
  file = {/Users/rritaz/Zotero/storage/FQBRMKMC/Moreno et al. - 2009 - Assessment of regression-based methods to adjust f.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,publication bias,random-effects},
  language = {en},
  number = {1}
}

@article{moreno_generalized_2012,
  title = {A Generalized Weighting Regression-Derived Meta-Analysis Estimator Robust to Small-Study Effects and Heterogeneity},
  author = {Moreno, Santiago G. and Sutton, Alex J. and Thompson, John R. and Ades, A. E. and Abrams, Keith R. and Cooper, Nicola J.},
  year = {2012},
  volume = {31},
  pages = {1407--1417},
  issn = {1097-0258},
  doi = {10.1002/sim.4488},
  abstract = {Heterogeneity and small-study effects are major concerns for the validity of meta-analysis. Although random effects meta-analysis provides a partial solution to heterogeneity, neither takes into account the presence of small-study effects, although they can rarely be ruled out with certainty. In this paper, we facilitate a better understanding of the properties of a recently described regression-based approach to deriving a meta-analysis estimator robust to small-study effects and unexplainable heterogeneity. The weightings of studies in the meta-analysis are derived algebraically for the regression model and compared with the weightings allocated to studies by fixed and random effects models. These weightings are compared in case studies with and without small-study effects. The presence of small-study effects causes pooled estimates from fixed and random effects meta-analyses to differ, potentially markedly, as a result of the different weights allocated to individual studies. Because random effects meta-analysis gives more weight to smaller studies, it becomes more vulnerable to the small-study effects. The regression approach gives heavier weight to the larger studies than either the fixed or random effects models, leading to its dominance in the estimated pooled effect. The weighting properties of the proposed regression-derived meta-analysis estimator are presented and compared with those of the standard meta-analytic estimators. We propose that there is much to recommend the routine use of this model as a reliable way to derive a pooled meta-analysis estimate that is robust to potential small-study effects, while still accommodating heterogeneity, even though uncertainty will often be considerably larger than for standard estimators. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BYXTQZPI/Moreno et al. - 2012 - A generalized weighting regression-derived meta-an.pdf;/Users/rritaz/Zotero/storage/USC6NDLX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),random-effects,small meta-analysis},
  language = {en},
  number = {14}
}

@article{moreno_objective_2014,
  title = {Objective {{Bayesian}} Meta-Analysis for Sparse Discrete Data},
  author = {Moreno, E. and V{\'a}zquez-Polo, F. J. and Negr{\'i}n, M. A.},
  year = {2014},
  volume = {33},
  pages = {3676--3692},
  issn = {1097-0258},
  doi = {10.1002/sim.6163},
  abstract = {This paper presents a Bayesian model for meta-analysis of sparse discrete binomial data, which are out of the scope of the usual hierarchical normal random-effect models. Treatment effectiveness data are often of this type. The crucial linking distribution between the effectiveness conditional on the healthcare center and the unconditional effectiveness is constructed from specific bivariate classes of distributions with given marginals. This assures coherency between the marginal and conditional prior distributions utilized in the analysis. Further, we impose a bivariate class of priors that is able to accommodate a wide range of heterogeneity degrees between the multicenter clinical trials involved. Applications to real multicenter data are given and compared with previous meta-analysis. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/F47N2GY7/Moreno et al. - 2014 - Objective Bayesian meta-analysis for sparse discre.pdf;/Users/rritaz/Zotero/storage/WIBFXK4B/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {21}
}

@article{morris_combining_2002,
  title = {Combining Effect Size Estimates in Meta-Analysis with Repeated Measures and Independent-Groups Designs.},
  author = {Morris, Scott B. and Deshon, Richard P.},
  year = {2002},
  volume = {7},
  pages = {105--125},
  doi = {10.1037//1082-989x.7.1.105},
  abstract = {When a meta-analysis on results from experimental studies is conducted, differences in the study design must be taken into consideration. A method for combining results across independent-groups and repeated measures designs is described, and the conditions under which such an analysis is appropriate are discussed. Combining results across designs requires that (a) all effect sizes be transformed into a common metric, (b) effect sizes from each design estimate the same treatment effect, and (c) meta-analysis procedures use design-specific estimates of sampling variance to reflect the precision of the effect size estimates.},
  file = {/Users/rritaz/Zotero/storage/L3NGVFWI/Morris and Deshon - 2002 - Combining effect size estimates in meta-analysis w.pdf},
  journal = {Psychological methods},
  keywords = {effect size estimation (series),modeling effect size variation (covariates)},
  number = {1}
}

@article{morris_meta-analysis_2018,
  title = {Meta-Analysis of {{Gaussian}} Individual Patient Data: {{Two}}-Stage or Not Two-Stage?},
  shorttitle = {Meta-Analysis of {{Gaussian}} Individual Patient Data},
  author = {Morris, Tim P. and Fisher, David J. and Kenward, Michael G. and Carpenter, James R.},
  year = {2018},
  volume = {37},
  pages = {1419--1438},
  issn = {1097-0258},
  doi = {10.1002/sim.7589},
  abstract = {Quantitative evidence synthesis through meta-analysis is central to evidence-based medicine. For well-documented reasons, the meta-analysis of individual patient data is held in higher regard than aggregate data. With access to individual patient data, the analysis is not restricted to a ``two-stage'' approach (combining estimates and standard errors) but can estimate parameters of interest by fitting a single model to all of the data, a so-called ``one-stage'' analysis. There has been debate about the merits of one- and two-stage analysis. Arguments for one-stage analysis have typically noted that a wider range of models can be fitted and overall estimates may be more precise. The two-stage side has emphasised that the models that can be fitted in two stages are sufficient to answer the relevant questions, with less scope for mistakes because there are fewer modelling choices to be made in the two-stage approach. For Gaussian data, we consider the statistical arguments for flexibility and precision in small-sample settings. Regarding flexibility, several of the models that can be fitted only in one stage may not be of serious interest to most meta-analysis practitioners. Regarding precision, we consider fixed- and random-effects meta-analysis and see that, for a model making certain assumptions, the number of stages used to fit this model is irrelevant; the precision will be approximately equal. Meta-analysts should choose modelling assumptions carefully. Sometimes relevant models can only be fitted in one stage. Otherwise, meta-analysts are free to use whichever procedure is most convenient to fit the identified model.},
  file = {/Users/rritaz/Zotero/storage/6AKJEDMU/Morris et al. - 2018 - Meta-analysis of Gaussian individual patient data.pdf;/Users/rritaz/Zotero/storage/YK7U4SNI/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {9}
}

@article{moses_comparing_2002,
  title = {Comparing Results of Large Clinical Trials to Those of Meta-Analyses},
  author = {Moses, Lincoln E. and Mosteller, Frederick and Buehler, John H.},
  year = {2002},
  volume = {21},
  pages = {793--800},
  issn = {1097-0258},
  doi = {10.1002/sim.1098},
  abstract = {We consider methods for assessing agreement or disagreement between the results of a meta-analysis of small studies addressing a clinical question and the result of a large clinical trial (LCT) addressing the same clinical question. We recommend basing conclusions about agreement upon the difference between the two results (relative risk, log-odds ratio or similar summary statistic), in the light of the estimated standard error of that difference. To estimate the standard error of the meta-analytic result we recommend a random effects analysis, and where a between-studies variance component is found, that component of variance should be used twice: once in the estimated standard error for the meta-analytic result and again in the standard error of the LCT result (augmenting the internal standard error of that statistic). Such broadening of the standard error reduces the appearance of disagreement. We also offer a critique of a different published approach, which is based on consistency of findings of statistical significance, a matter of how the two results regard zero, which is a poor measure of how closely they agree with each other. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ACSZWCIA/Moses et al. - 2002 - Comparing results of large clinical trials to thos.pdf;/Users/rritaz/Zotero/storage/3UFDBAZY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random effects models,random-effects},
  language = {en},
  number = {6}
}

@article{nam_multivariate_2003,
  title = {Multivariate Meta-Analysis},
  author = {Nam, In-Sun and Mengersen, Kerrie and Garthwaite, Paul},
  year = {2003},
  volume = {22},
  pages = {2309--2333},
  issn = {1097-0258},
  doi = {10.1002/sim.1410},
  abstract = {Meta-analysis is now a standard statistical tool for assessing the overall strength and interesting features of a relationship, on the basis of multiple independent studies. There is, however, recent acknowledgement of the fact that in many applications responses are rarely uniquely determined. Hence there has been some change of focus from a single response to the analysis of multiple outcomes. In this paper we propose and evaluate three Bayesian multivariate meta-analysis models: two multivariate analogues of the traditional univariate random effects models which make different assumptions about the relationships between studies and estimates, and a multivariate random effects model which is a Bayesian adaptation of the mixed model approach. Our preferred method is then illustrated through an analysis of a new data set on parental smoking and two health outcomes (asthma and lower respiratory disease) in children. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/867LJICN/Nam et al. - 2003 - Multivariate meta-analysis.pdf;/Users/rritaz/Zotero/storage/48AGDFXD/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate,random-effects},
  language = {en},
  number = {14}
}

@article{nikolakopoulou_planning_2016,
  title = {Planning Future Studies Based on the Precision of Network Meta-Analysis Results},
  author = {Nikolakopoulou, Adriani and Mavridis, Dimitris and Salanti, Georgia},
  year = {2016},
  volume = {35},
  pages = {978--1000},
  issn = {1097-0258},
  doi = {10.1002/sim.6608},
  abstract = {When there are multiple competing interventions for a healthcare problem, the design of new studies could be based on the entire network of evidence as reflected in a network meta-analysis. There is a practical need to answer how many (if any) studies are needed, of which design (i.e., which treatments to compare), and with what sample size to infer conclusively about the relative treatment effects of a set of target or all competing treatments and their relative ranking. We consider the precision in the results obtained from network meta-analysis: the precision of the joint distribution of the estimated basic parameters of the model and the precision in the treatment ranking. We quantify the precision in the estimated effects by considering their variance\textendash covariance matrix and estimate the precision in ranking by quantifying the dissimilarity of the density functions of summary effect estimates. Then, based on a desirable improvement in precision, we calculate the required sample size for each possible study design and number of study arms, and we present visual tools that can help trialists select the optimal study design. We use a published network of interventions for the treatment of hepatocellular carcinoma to illustrate the suggested methodology. The presented methodology can aid investigators making informed and evidence-based decisions about planning new studies. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/6NUE6DQD/Nikolakopoulou et al. - 2016 - Planning future studies based on the precision of .pdf;/Users/rritaz/Zotero/storage/E9G2X8U3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,power,random-effects},
  language = {en},
  number = {7}
}

@article{nikoloulopoulos_mixed_2015,
  title = {A Mixed Effect Model for Bivariate Meta-Analysis of Diagnostic Test Accuracy Studies Using a Copula Representation of the Random Effects Distribution},
  author = {Nikoloulopoulos, Aristidis K.},
  year = {2015},
  volume = {34},
  pages = {3842--3865},
  issn = {1097-0258},
  doi = {10.1002/sim.6595},
  abstract = {Diagnostic test accuracy studies typically report the number of true positives, false positives, true negatives and false negatives. There usually exists a negative association between the number of true positives and true negatives, because studies that adopt less stringent criterion for declaring a test positive invoke higher sensitivities and lower specificities. A generalized linear mixed model (GLMM) is currently recommended to synthesize diagnostic test accuracy studies. We propose a copula mixed model for bivariate meta-analysis of diagnostic test accuracy studies. Our general model includes the GLMM as a special case and can also operate on the original scale of sensitivity and specificity. Summary receiver operating characteristic curves are deduced for the proposed model through quantile regression techniques and different characterizations of the bivariate random effects distribution. Our general methodology is demonstrated with an extensive simulation study and illustrated by re-analysing the data of two published meta-analyses. Our study suggests that there can be an improvement on GLMM in fit to data and makes the argument for moving to copula random effects models. Our modelling framework is implemented in the package CopulaREMADA within the open source statistical environment R. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YR3IXU5E/Nikoloulopoulos - 2015 - A mixed effect model for bivariate meta-analysis o.pdf;/Users/rritaz/Zotero/storage/3IQACWPE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,multivariate},
  language = {en},
  number = {29}
}

@article{nixon_using_2007,
  title = {Using Mixed Treatment Comparisons and Meta-Regression to Perform Indirect Comparisons to Estimate the Efficacy of Biologic Treatments in Rheumatoid Arthritis},
  author = {Nixon, R. M. and Bansback, N. and Brennan, A.},
  year = {2007},
  volume = {26},
  pages = {1237--1254},
  issn = {1097-0258},
  doi = {10.1002/sim.2624},
  abstract = {Mixed treatment comparison (MTC) is a generalization of meta-analysis. Instead of the same treatment for a disease being tested in a number of studies, a number of different interventions are considered. Meta-regression is also a generalization of meta-analysis where an attempt is made to explain the heterogeneity between the treatment effects in the studies by regressing on study-level covariables. Our focus is where there are several different treatments considered in a number of randomized controlled trials in a specific disease, the same treatment can be applied in several arms within a study, and where differences in efficacy can be explained by differences in the study settings. We develop methods for simultaneously comparing several treatments and adjusting for study-level covariables by combining ideas from MTC and meta-regression. We use a case study from rheumatoid arthritis. We identified relevant trials of biologic verses standard therapy or placebo and extracted the doses, comparators and patient baseline characteristics. Efficacy is measured using the log odds ratio of achieving six-month ACR50 responder status. A random-effects meta-regression model is fitted which adjusts the log odds ratio for study-level prognostic factors. A different random-effect distribution on the log odds ratios is allowed for each different treatment. The odds ratio is found as a function of the prognostic factors for each treatment. The apparent differences in the randomized trials between tumour necrosis factor alpha (TNF-{$\propto$}) antagonists are explained by differences in prognostic factors and the analysis suggests that these drugs as a class are not different from each other. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EEFH2NT8/Nixon et al. - 2007 - Using mixed treatment comparisons and meta-regress.pdf;/Users/rritaz/Zotero/storage/JEWC4WES/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,modeling effect size variation (covariates)},
  language = {en},
  number = {6}
}

@article{noauthor_correcting_2009,
  title = {Correcting for Multivariate Measurement Error by Regression Calibration in Meta-Analyses of Epidemiological Studies},
  year = {2009},
  volume = {28},
  pages = {1067--1092},
  issn = {1097-0258},
  doi = {10.1002/sim.3530},
  abstract = {Within-person variability in measured values of multiple risk factors can bias their associations with disease. The multivariate regression calibration (RC) approach can correct for such measurement error and has been applied to studies in which true values or independent repeat measurements of the risk factors are observed on a subsample. We extend the multivariate RC techniques to a meta-analysis framework where multiple studies provide independent repeat measurements and information on disease outcome. We consider the cases where some or all studies have repeat measurements, and compare study-specific, averaged and empirical Bayes estimates of RC parameters. Additionally, we allow for binary covariates (e.g. smoking status) and for uncertainty and time trends in the measurement error corrections. Our methods are illustrated using a subset of individual participant data from prospective long-term studies in the Fibrinogen Studies Collaboration to assess the relationship between usual levels of plasma fibrinogen and the risk of coronary heart disease, allowing for measurement error in plasma fibrinogen and several confounders. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/49XWLYR7/2009 - Correcting for multivariate measurement error by r.pdf;/Users/rritaz/Zotero/storage/EEZKBDUP/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),multivariate,random-effects},
  language = {en},
  number = {7}
}

@misc{noauthor_estimating_nodate,
  title = {Estimating Heterogeneity Variance in Meta-analysis - {{Rukhin}} - 2013 - {{Journal}} of the {{Royal Statistical Society}}: {{Series B}} ({{Statistical Methodology}}) - {{Wiley Online Library}}},
  file = {/Users/rritaz/Zotero/storage/ZN4R83PU/j.1467-9868.2012.01047.html},
  howpublished = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2012.01047.x?casa\_token=CEEyv-JpZ3AAAAAA\%3AJy-bU-DOWpXfKeu63jG0KCsHd-3kNN4aW\_HO-Vm7CIslcXK-4nrCicLnlhe8HtsYkigSXZP8aCL-}
}

@article{noauthor_phonological_2006,
  title = {Phonological {{Awareness Training}}},
  year = {2006},
  pages = {30},
  file = {/Users/rritaz/Zotero/storage/YQHA9EQR/2006 - Phonological Awareness Training.pdf},
  language = {en}
}

@article{noauthor_phonological_2006-1,
  title = {Phonological {{Awareness Training}}},
  year = {2006},
  pages = {30},
  file = {/Users/rritaz/Zotero/storage/BNBVMPAJ/2006 - Phonological Awareness Training.pdf},
  language = {en}
}

@book{noauthor_statistical_1985,
  title = {Statistical {{Methods}} for {{Meta}}-{{Analysis}}},
  year = {1985},
  month = aug,
  edition = {1 edition},
  publisher = {{Academic Press}},
  address = {{Orlando}},
  abstract = {The main purpose of this book is to address the statistical issues for integrating independent studies. There exist a number of papers and books that discuss the mechanics of collecting, coding, and preparing data for a meta-analysis , and we do not deal with these. Because this book concerns methodology, the content necessarily is statistical, and at times mathematical. In order to make the material accessible to a wider audience, we have not provided proofs in the text. Where proofs are given, they are placed as commentary at the end of a chapter. These can be omitted at the discretion of the reader.Throughout the book we describe computational procedures whenever required. Many computations can be completed on a hand calculator, whereas some require the use of a standard statistical package such as SAS, SPSS, or BMD. Readers with experience using a statistical package or who conduct analyses such as multiple regression or analysis of variance should be able to carry out the analyses described with the aid of a statistical package.},
  isbn = {978-0-12-336380-0},
  language = {English}
}

@book{noauthor_statistical_1985-1,
  title = {Statistical {{Methods}} for {{Meta}}-{{Analysis}}},
  year = {1985},
  month = aug,
  edition = {1 edition},
  publisher = {{Academic Press}},
  address = {{Orlando}},
  abstract = {The main purpose of this book is to address the statistical issues for integrating independent studies. There exist a number of papers and books that discuss the mechanics of collecting, coding, and preparing data for a meta-analysis , and we do not deal with these. Because this book concerns methodology, the content necessarily is statistical, and at times mathematical. In order to make the material accessible to a wider audience, we have not provided proofs in the text. Where proofs are given, they are placed as commentary at the end of a chapter. These can be omitted at the discretion of the reader.Throughout the book we describe computational procedures whenever required. Many computations can be completed on a hand calculator, whereas some require the use of a standard statistical package such as SAS, SPSS, or BMD. Readers with experience using a statistical package or who conduct analyses such as multiple regression or analysis of variance should be able to carry out the analyses described with the aid of a statistical package.},
  isbn = {978-0-12-336380-0},
  language = {English}
}

@article{noauthor_systematically_2009,
  title = {Systematically Missing Confounders in Individual Participant Data Meta-Analysis of Observational Cohort Studies},
  year = {2009},
  volume = {28},
  pages = {1218--1237},
  issn = {1097-0258},
  doi = {10.1002/sim.3540},
  abstract = {One difficulty in performing meta-analyses of observational cohort studies is that the availability of confounders may vary between cohorts, so that some cohorts provide fully adjusted analyses while others only provide partially adjusted analyses. Commonly, analyses of the association between an exposure and disease either are restricted to cohorts with full confounder information, or use all cohorts but do not fully adjust for confounding. We propose using a bivariate random-effects meta-analysis model to use information from all available cohorts while still adjusting for all the potential confounders. Our method uses both the fully adjusted and the partially adjusted estimated effects in the cohorts with full confounder information, together with an estimate of their within-cohort correlation. The method is applied to estimate the association between fibrinogen level and coronary heart disease incidence using data from 154 012 participants in 31 cohorts.\textdagger{} One hundred and ninety-nine participants from the original 154 211 withdrew their consent and have been removed from this analysis. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/QJM8RBC4/2009 - Systematically missing confounders in individual p.pdf;/Users/rritaz/Zotero/storage/2LTI38AQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,correlated effects,Individual Patient Data IPD,modeling effect size variation (covariates),multivariate},
  language = {en},
  number = {8}
}

@misc{noauthor_wwc_nodate,
  title = {{{WWC}} | {{Phonological Awareness Training}}},
  file = {/Users/rritaz/Zotero/storage/24BUZ26T/274.html},
  howpublished = {https://ies.ed.gov/ncee/wwc/Intervention/274}
}

@misc{noauthor_wwc_nodate-1,
  title = {{{WWC}} | {{Phonological Awareness Training}}},
  file = {/Users/rritaz/Zotero/storage/L5JSZG7A/274.html},
  howpublished = {https://ies.ed.gov/ncee/wwc/Intervention/274}
}

@article{noma_bartlett-type_2018,
  title = {Bartlett-Type Corrections and Bootstrap Adjustments of Likelihood-Based Inference Methods for Network Meta-Analysis},
  author = {Noma, Hisashi and Nagashima, Kengo and Maruo, Kazushi and Gosho, Masahiko and Furukawa, Toshi A.},
  year = {2018},
  volume = {37},
  pages = {1178--1190},
  issn = {1097-0258},
  doi = {10.1002/sim.7578},
  abstract = {In network meta-analyses that synthesize direct and indirect comparison evidence concerning multiple treatments, multivariate random effects models have been routinely used for addressing between-studies heterogeneities. Although their standard inference methods depend on large sample approximations (eg, restricted maximum likelihood estimation) for the number of trials synthesized, the numbers of trials are often moderate or small. In these situations, standard estimators cannot be expected to behave in accordance with asymptotic theory; in particular, confidence intervals cannot be assumed to exhibit their nominal coverage probabilities (also, the type I error probabilities of the corresponding tests cannot be retained). The invalidity issue may seriously influence the overall conclusions of network meta-analyses. In this article, we develop several improved inference methods for network meta-analyses to resolve these problems. We first introduce 2 efficient likelihood-based inference methods, the likelihood ratio test\textendash based and efficient score test\textendash based methods, in a general framework of network meta-analysis. Then, to improve the small-sample inferences, we developed improved higher-order asymptotic methods using Bartlett-type corrections and bootstrap adjustment methods. The proposed methods adopt Monte Carlo approaches using parametric bootstraps to effectively circumvent complicated analytical calculations of case-by-case analyses and to permit flexible application to various statistical models network meta-analyses. These methods can also be straightforwardly applied to multivariate meta-regression analyses and to tests for the evaluation of inconsistency. In numerical evaluations via simulations, the proposed methods generally performed well compared with the ordinary restricted maximum likelihood\textendash based inference method. Applications to 2 network meta-analysis datasets are provided.},
  file = {/Users/rritaz/Zotero/storage/3AELGQZS/Noma et al. - 2018 - Bartlett-type corrections and bootstrap adjustment.pdf;/Users/rritaz/Zotero/storage/WH5C6RVN/sim.html},
  journal = {Statistics in Medicine},
  keywords = {small network meta-analysis (useful for tests)},
  language = {en},
  number = {7}
}

@article{noma_confidence_2011,
  title = {Confidence Intervals for a Random-Effects Meta-Analysis Based on {{Bartlett}}-Type Corrections},
  author = {Noma, Hisashi},
  year = {2011},
  volume = {30},
  pages = {3304--3312},
  issn = {1097-0258},
  doi = {10.1002/sim.4350},
  abstract = {In medical meta-analysis, the DerSimonian-Laird confidence interval for the average treatment effect has been widely adopted in practice. However, it is well known that its coverage probability (the probability that the interval actually includes the true value) can be substantially below the target level. One particular reason is that the validity of the confidence interval depends on the assumption that the number of synthesized studies is sufficiently large. In typical medical meta-analyses, the number of studies is fewer than 20. In this article, we developed three confidence intervals for improving coverage properties, based on (i) the Bartlett corrected likelihood ratio statistic, (ii) the efficient score statistic, and (iii) the Bartlett-type adjusted efficient score statistic. The Bartlett and Bartlett-type corrections improve the large sample approximations for the likelihood ratio and efficient score statistics. Through numerical evaluations by simulations, these confidence intervals demonstrated better coverage properties than the existing methods. In particular, with a moderate number of synthesized studies, the Bartlett and Bartlett-type corrected confidence intervals performed well. An application to a meta-analysis of the treatment for myocardial infarction with intravenous magnesium is presented. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/FAKG5BWZ/Noma - 2011 - Confidence intervals for a random-effects meta-ana.pdf;/Users/rritaz/Zotero/storage/DHKB2BE3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Confidence Intervals,effect size combination (small sample \& discrete),random-effects},
  language = {en},
  number = {28}
}

@article{noma_efficient_2019,
  title = {Efficient Two-Step Multivariate Random Effects Meta-Analysis of Individual Participant Data for Longitudinal Clinical Trials Using Mixed Effects Models},
  author = {Noma, Hisashi and Maruo, Kazushi and Gosho, Masahiko and Levine, Stephen Z. and Goldberg, Yair and Leucht, Stefan and Furukawa, Toshi A.},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {33},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0676-1},
  abstract = {Background: Mixed effects models have been widely applied in clinical trials that involve longitudinal repeated measurements, which possibly contain missing outcome data. In meta-analysis of individual participant data (IPD) based on these longitudinal studies, joint synthesis of the regression coefficient parameters can improve efficiency, especially for explorations of effect modifiers that are useful to predict the response or lack of response to particular treatments. Methods: In this article, we provide a valid and efficient two-step method for IPD meta-analyses using the mixed effects models that adequately addresses the between-studies heterogeneity using random effects models. The two-step method overcomes the practical difficulties of computations and modellings of the heterogeneity in the one-step method, and enables valid inference without loss of efficiency. We also show the two-step method can effectively circumvent the modellings of the between-studies heterogeneity of the variance-covariance parameters and provide valid and efficient estimators for the regression coefficient parameters, which are the primary objects of interests in the longitudinal studies. In addition, these methods can be easily implemented using standard statistical packages, and enable synthesis of IPD from different sources (e.g., from different platforms of clinical trial data sharing systems). Results: To assess the proposed method, we conducted simulation studies and also applied the method to an IPD meta-analysis of clinical trials for new generation antidepressants. Through the numerical studies, the validity and efficiency of the proposed method were demonstrated. Conclusions: The two-step approach is an effective method for IPD meta-analyses of longitudinal clinical trials using mixed effects models. It can also effectively circumvent the modellings of the between-studies heterogeneity of the variance-covariance parameters, and enable efficient inferences for the regression coefficient parameters.},
  file = {/Users/rritaz/Zotero/storage/EXYYYQKF/Noma et al. - 2019 - Efficient two-step multivariate random effects met.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Individual Patient Data IPD,multivariate,random-effects},
  language = {en},
  number = {1}
}

@article{noma_quantifying_2017,
  title = {Quantifying Indirect Evidence in Network Meta-Analysis},
  author = {Noma, Hisashi and Tanaka, Shiro and Matsui, Shigeyuki and Cipriani, Andrea and Furukawa, Toshi A.},
  year = {2017},
  volume = {36},
  pages = {917--927},
  issn = {1097-0258},
  doi = {10.1002/sim.7187},
  abstract = {Network meta-analysis enables comprehensive synthesis of evidence concerning multiple treatments and their simultaneous comparisons based on both direct and indirect evidence. A fundamental pre-requisite of network meta-analysis is the consistency of evidence that is obtained from different sources, particularly whether direct and indirect evidence are in accordance with each other or not, and how they may influence the overall estimates. We have developed an efficient method to quantify indirect evidence, as well as a testing procedure to evaluate their inconsistency using Lindsay's composite likelihood method. We also show that this estimator has complete information for the indirect evidence. Using this method, we can assess the degree of consistency between direct and indirect evidence and their contribution rates to the overall estimate. Sensitivity analyses can be also conducted with this method to assess the influences of potentially inconsistent treatment contrasts on the overall results. These methods can provide useful information for overall comparative results that might be biased from specific inconsistent treatment contrasts. We also provide some fundamental requirements for valid inference on these methods concerning consistency restrictions on multi-arm trials. In addition, the efficiency of the developed method is demonstrated based on simulation studies. Applications to a network meta-analysis of 12 new-generation antidepressants are presented. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/AIRKTRDY/Noma et al. - 2017 - Quantifying indirect evidence in network meta-anal.pdf;/Users/rritaz/Zotero/storage/84SCD4AG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis},
  language = {en},
  number = {6}
}

@article{novielli_bayesian_2010,
  title = {Bayesian Model Selection for Meta-Analysis of Diagnostic Test Accuracy Data: {{Application}} to {{Ddimer}} for Deep Vein Thrombosis},
  shorttitle = {Bayesian Model Selection for Meta-Analysis of Diagnostic Test Accuracy Data},
  author = {Novielli, Nicola and Cooper, Nicola J. and Sutton, Alexander J. and Abrams, Keith R.},
  year = {2010},
  volume = {1},
  pages = {226--238},
  issn = {1759-2887},
  doi = {10.1002/jrsm.15},
  abstract = {A number of statistical models have been developed for meta-analysis (MA) of diagnostic test (DT) accuracy data. Here we consider these alternative MA models, explore the relationships between them, and consider the use of the deviance information criteria (DIC) to decide which is the most appropriate model for a given dataset. A Bayesian statistical approach is adopted throughout. The alternative MA models are applied to a dataset of 198 assays of Ddimer to diagnose deep vein thrombosis. In this example, based on the DIC, a bivariate random effects model for sensitivity and specificity fitted the data best. When considering the inclusion of study level covariates, allowing sensitivity to vary by study setting further improved the fit of the model. The model fitting approach is then repeated for a subset of the data, which highlights the less decisive results obtained when using the DIC with a more limited dataset. Formal approaches to model selection are often overlooked in an MA context; however, they offer sensible rationale to the analysis, particularly for complex models such as those proposed for DT accuracy. Specifically, the use of the DIC statistic appears to be well suited for deciding between potentially complex mixed-effect MA models, possibly including covariates. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BJ3VNSMQ/Novielli et al. - 2010 - Bayesian model selection for meta-analysis of diag.pdf;/Users/rritaz/Zotero/storage/MWFH2VVN/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,GLM MA models,modeling effect size variation (covariates)},
  language = {en},
  number = {3-4}
}

@article{ogilvie_harvest_2008,
  title = {The Harvest Plot: {{A}} Method for Synthesising Evidence about the Differential Effects of Interventions},
  shorttitle = {The Harvest Plot},
  author = {Ogilvie, David and Fayter, Debra and Petticrew, Mark and Sowden, Amanda and Thomas, Sian and Whitehead, Margaret and Worthy, Gill},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {8},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-8},
  abstract = {Background: One attraction of meta-analysis is the forest plot, a compact overview of the essential data included in a systematic review and the overall 'result'. However, meta-analysis is not always suitable for synthesising evidence about the effects of interventions which may influence the wider determinants of health. As part of a systematic review of the effects of population-level tobacco control interventions on social inequalities in smoking, we designed a novel approach to synthesis intended to bring aspects of the graphical directness of a forest plot to bear on the problem of synthesising evidence from a complex and diverse group of studies. Methods: We coded the included studies (n = 85) on two methodological dimensions (suitability of study design and quality of execution) and extracted data on effects stratified by up to six different dimensions of inequality (income, occupation, education, gender, race or ethnicity, and age), distinguishing between 'hard' (behavioural) and 'intermediate' (process or attitudinal) outcomes. Adopting a hypothesis-testing approach, we then assessed which of three competing hypotheses (positive social gradient, negative social gradient, or no gradient) was best supported by each study for each dimension of inequality. Results: We plotted the results on a matrix ('harvest plot') for each category of intervention, weighting studies by the methodological criteria and distributing them between the competing hypotheses. These matrices formed part of the analytical process and helped to encapsulate the output, for example by drawing attention to the finding that increasing the price of tobacco products may be more effective in discouraging smoking among people with lower incomes and in lower occupational groups. Conclusion: The harvest plot is a novel and useful method for synthesising evidence about the differential effects of population-level interventions. It contributes to the challenge of making best use of all available evidence by incorporating all relevant data. The visual display assists both the process of synthesis and the assimilation of the findings. The method is suitable for adaptation to a variety of questions in evidence synthesis and may be particularly useful for systematic reviews addressing the broader type of research question which may be most relevant to policymakers.},
  file = {/Users/rritaz/Zotero/storage/3TSGSQXE/Ogilvie et al. - 2008 - The harvest plot A method for synthesising eviden.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {1}
}

@article{okada_bayesian_2015,
  title = {Bayesian Meta-Analysis of {{Cronbach}}'s Coefficient Alpha to Evaluate Informative Hypotheses},
  author = {Okada, Kensuke},
  year = {2015},
  volume = {6},
  pages = {333--346},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1155},
  abstract = {This paper proposes a new method to evaluate informative hypotheses for meta-analysis of Cronbach's coefficient alpha using a Bayesian approach. The coefficient alpha is one of the most widely used reliability indices. In meta-analyses of reliability, researchers typically form specific informative hypotheses beforehand, such as `alpha of this test is greater than 0.8' or `alpha of one form of a test is greater than the others.' The proposed method enables direct evaluation of these informative hypotheses. To this end, a Bayes factor is calculated to evaluate the informative hypothesis against its complement. It allows researchers to summarize the evidence provided by previous studies in favor of their informative hypothesis. The proposed approach can be seen as a natural extension of the Bayesian meta-analysis of coefficient alpha recently proposed in this journal (Brannick and Zhang, ). The proposed method is illustrated through two meta-analyses of real data that evaluate different kinds of informative hypotheses on superpopulation: one is that alpha of a particular test is above the criterion value, and the other is that alphas among different test versions have ordered relationships. Informative hypotheses are supported from the data in both cases, suggesting that the proposed approach is promising for application. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/N2PDL76E/Okada - 2015 - Bayesian meta-analysis of Cronbach's coefficient a.pdf;/Users/rritaz/Zotero/storage/G73VAIES/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian},
  language = {en},
  number = {4}
}

@article{olejnik_generalized_2003,
  title = {Generalized {{Eta}} and {{Omega Squared Statistics}}: {{Measures}} of {{Effect Size}} for {{Some Common Research Designs}}},
  shorttitle = {Generalized {{Eta}} and {{Omega Squared Statistics}}},
  author = {Olejnik, Stephen and Algina, James},
  year = {2003},
  month = dec,
  volume = {8},
  pages = {434--447},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.8.4.434},
  abstract = {The editorial policies of several prominent educational and psychological journals require that researchers report some measure of effect size along with tests for statistical significance. In analysis of variance contexts, this requirement might be met by using eta squared or omega squared statistics. Current procedures for computing these measures of effect often do not consider the effect that design features of the study have on the size of these statistics. Because research-design features can have a large effect on the estimated proportion of explained variance, the use of partial eta or omega squared can be misleading. The present article provides formulas for computing generalized eta and omega squared statistics, which provide estimates of effect size that are comparable across a variety of research designs. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/3EGQD9V6/Olejnik and Algina - 2003 - Generalized Eta and Omega Squared Statistics Meas.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes},
  number = {4},
  series = {Metric in {{Meta}}-{{Analysis}}}
}

@article{olkin_gosh_2012,
  title = {{{GOSH}} \textendash{} a Graphical Display of Study Heterogeneity},
  author = {Olkin, Ingram and Dahabreh, Issa J. and Trikalinos, Thomas A.},
  year = {2012},
  volume = {3},
  pages = {214--223},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1053},
  abstract = {Estimates from individual studies included in a meta-analysis often are not in agreement, giving rise to statistical heterogeneity. In such cases, exploration of the causes of heterogeneity can advance knowledge by formulating novel hypotheses. We present a new method for visualizing between-study heterogeneity using combinatorial meta-analysis. The method is based on performing separate meta-analyses on all possible subsets of studies in a meta-analysis. We use the summary effect sizes and other statistics produced by the all-subsets meta-analyses to generate graphs that can be used to investigate heterogeneity, identify influential studies, and explore subgroup effects. This graphical approach complements alternative graphical explorations of data. We apply the method to numerous biomedical examples, to allow readers to develop intuition on the interpretation of the all-subsets graphical display. The proposed graphical approach may be useful for exploratory data analysis in systematic reviews. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/TFJRIXM8/Olkin et al. - 2012 - GOSH – a graphical display of study heterogeneity.pdf;/Users/rritaz/Zotero/storage/DC7NEPSU/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,graphical approach for heterogeneity,random-effects},
  language = {en},
  number = {3}
}

@article{olkin_retrieving_2012,
  title = {Retrieving Treatment and Control Proportions from Incomplete Summary Data in Meta-Analysis},
  author = {Olkin, Ingram},
  year = {2012},
  volume = {3},
  pages = {250--254},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1043},
  abstract = {One of the vexing problems often encountered when combining the results of independent studies in a meta-analysis is that the data provided in individual studies are incomplete. Some studies may provide only a risk difference and others only an odds ratio. Of course, if the proportions for treatment and control are reported, then the meta-analyst can carry out a variety of analyses, such as fixed or random effect estimates. Excluding studies with incomplete data carries a risk and should be avoided if it is possible to retrieve the original proportions. Inclusion criteria in some meta-analyses may require that studies contain full data. This requirement can be relaxed if the original data can be retrieved. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WQ2XW6LL/Olkin - 2012 - Retrieving treatment and control proportions from .pdf;/Users/rritaz/Zotero/storage/NUR3EBL2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {missing data},
  language = {en},
  number = {3}
}

@article{oort_maximum_2016,
  title = {Maximum Likelihood Estimation in Meta-Analytic Structural Equation Modeling},
  author = {Oort, Frans J. and Jak, Suzanne},
  year = {2016},
  volume = {7},
  pages = {156--167},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1203},
  abstract = {Meta-analytic structural equation modeling (MASEM) involves fitting models to a common population correlation matrix that is estimated on the basis of correlation coefficients that are reported by a number of independent studies. MASEM typically consist of two stages. The method that has been found to perform best in terms of statistical properties is the two-stage structural equation modeling, in which maximum likelihood analysis is used to estimate the common correlation matrix in the first stage, and weighted least squares analysis is used to fit structural equation models to the common correlation matrix in the second stage. In the present paper, we propose an alternative method, ML MASEM, that uses ML estimation throughout. In a simulation study, we use both methods and compare chi-square distributions, bias in parameter estimates, false positive rates, and true positive rates. Both methods appear to yield unbiased parameter estimates and false and true positive rates that are close to the expected values. ML MASEM parameter estimates are found to be significantly less bias than two-stage structural equation modeling estimates, but the differences are very small. The choice between the two methods may therefore be based on other fundamental or practical arguments. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/U3Z55IM2/Oort and Jak - 2016 - Maximum likelihood estimation in meta-analytic str.pdf;/Users/rritaz/Zotero/storage/WUCZ5ZMN/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlation coefficients,random-effects},
  language = {en},
  number = {2}
}

@article{palmer_meta-analysis_2008,
  title = {Meta-Analysis of {{Mendelian}} Randomization Studies Incorporating All Three Genotypes},
  author = {Palmer, Tom M. and Thompson, John R. and Tobin, Martin D.},
  year = {2008},
  volume = {27},
  pages = {6570--6582},
  issn = {1097-0258},
  doi = {10.1002/sim.3423},
  abstract = {In Mendelian randomization a carefully selected gene is used as an instrumental variable in the estimation of the association between a biological phenotype and a disease. A study using Mendelian randomization will have information on an individual's disease status, the genotype and the phenotype. The phenotype must be on the causal pathway between gene and disease for the instrumental-variable analysis to be valid. For a biallelic polymorphism there are three possible genotypes with which to compare disease risk. Existing methods select two of the three possible genotypes for use in a Mendelian randomization analysis. Multivariate meta-analysis models for Mendelian randomization case\textendash control studies are proposed, which extend previous methods by estimating the pooled phenotype\textendash disease association across both genotype comparisons by using the gene\textendash disease log odds ratios and differences in mean phenotypes. The methods are illustrated using a meta-analysis of the effect of a gene related to collagen production on bone mineral density and osteoporotic fracture. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/Z5GRPEY2/Palmer et al. - 2008 - Meta-analysis of Mendelian randomization studies i.pdf;/Users/rritaz/Zotero/storage/D6KJ9AUA/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,physical/biological fields},
  language = {en},
  number = {30}
}

@article{papadimitropoulou_one-stage_2019,
  title = {One-Stage Random Effects Meta-Analysis Using Linear Mixed Models for Aggregate Continuous Outcome Data},
  author = {Papadimitropoulou, Katerina and Stijnen, Theo and Dekkers, Olaf M. and le Cessie, Saskia},
  year = {2019},
  volume = {10},
  pages = {360--375},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1331},
  abstract = {The vast majority of meta-analyses uses summary/aggregate data retrieved from published studies in contrast to meta-analysis of individual participant data (IPD). When the outcome is continuous and IPD are available, linear mixed modelling methods can be employed in a one-stage approach. This allows for flexible modelling of within-study variability and between-study effects and accounts for the uncertainty in the estimates of between-study and within-study residual variances. However, IPD are seldom available. For the normal outcome case, we present a method to generate pseudo IPD from aggregate data using group mean, standard deviation, and sample sizes within each study, ie, the sufficient statistics. Analyzing the pseudo IPD with likelihood-based methods yields identical results as the analysis of the unknown true IPD. The advantage of this method is that we can employ the mixed modelling framework, implemented in many statistical software packages, and explore modelling options suitable for IPD, such as fixed study-specific intercepts and fixed treatment effect model, fixed study-specific intercepts and random treatment effects, and both random study and treatment effects and different options to model the within-study residual variance. This allows choosing the most realistic (or potentially complex) residual variance structures across studies, instead of using an overly simple structure. We demonstrate these methods in two empirical datasets in Alzheimer disease, where an extensive model, assuming all within-study variances to be free, fitted considerably better. In simulations, the pseudo IPD approach showed adequate coverage probability, because it accounted for small sample effects.},
  file = {/Users/rritaz/Zotero/storage/S3MHDPC9/Papadimitropoulou et al. - 2019 - One-stage random effects meta-analysis using linea.pdf;/Users/rritaz/Zotero/storage/QHHFENAK/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,continuous effect sizes,Individual Patient Data IPD,random-effects},
  language = {en},
  number = {3}
}

@article{partlett_random_2017,
  title = {Random Effects Meta-Analysis: {{Coverage}} Performance of 95\% Confidence and Prediction Intervals Following {{REML}} Estimation},
  shorttitle = {Random Effects Meta-Analysis},
  author = {Partlett, Christopher and Riley, Richard D.},
  year = {2017},
  volume = {36},
  pages = {301--317},
  issn = {1097-0258},
  doi = {10.1002/sim.7140},
  abstract = {A random effects meta-analysis combines the results of several independent studies to summarise the evidence about a particular measure of interest, such as a treatment effect. The approach allows for unexplained between-study heterogeneity in the true treatment effect by incorporating random study effects about the overall mean. The variance of the mean effect estimate is conventionally calculated by assuming that the between study variance is known; however, it has been demonstrated that this approach may be inappropriate, especially when there are few studies. Alternative methods that aim to account for this uncertainty, such as Hartung\textendash Knapp, Sidik\textendash Jonkman and Kenward\textendash Roger, have been proposed and shown to improve upon the conventional approach in some situations. In this paper, we use a simulation study to examine the performance of several of these methods in terms of the coverage of the 95\% confidence and prediction intervals derived from a random effects meta-analysis estimated using restricted maximum likelihood. We show that, in terms of the confidence intervals, the Hartung\textendash Knapp correction performs well across a wide-range of scenarios and outperforms other methods when heterogeneity was large and/or study sizes were similar. However, the coverage of the Hartung\textendash Knapp method is slightly too low when the heterogeneity is low (I2 {$<$} 30\%) and the study sizes are quite varied. In terms of prediction intervals, the conventional approach is only valid when heterogeneity is large (I2 {$>$} 30\%) and study sizes are similar. In other situations, especially when heterogeneity is small and the study sizes are quite varied, the coverage is far too low and could not be consistently improved by either increasing the number of studies, altering the degrees of freedom or using variance inflation methods. Therefore, researchers should be cautious in deriving 95\% prediction intervals following a frequentist random-effects meta-analysis until a more reliable solution is identified. \textcopyright{} 2016 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/9P5RWM8A/Partlett and Riley - 2017 - Random effects meta-analysis Coverage performance.pdf;/Users/rritaz/Zotero/storage/KD5WVAQ2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random effects models,random-effects},
  language = {en},
  number = {2}
}

@article{pateras_data-generating_2018,
  title = {Data-Generating Models of Dichotomous Outcomes: {{Heterogeneity}} in Simulation Studies for a Random-Effects Meta-Analysis},
  shorttitle = {Data-Generating Models of Dichotomous Outcomes},
  author = {Pateras, Konstantinos and Nikolakopoulos, Stavros and Roes, Kit},
  year = {2018},
  volume = {37},
  pages = {1115--1124},
  issn = {1097-0258},
  doi = {10.1002/sim.7569},
  abstract = {Simulation studies to evaluate performance of statistical methods require a well-specified data-generating model. Details of these models are essential to interpret the results and arrive at proper conclusions. A case in point is random-effects meta-analysis of dichotomous outcomes. We reviewed a number of simulation studies that evaluated approximate normal models for meta-analysis of dichotomous outcomes, and we assessed the data-generating models that were used to generate events for a series of (heterogeneous) trials. We demonstrate that the performance of the statistical methods, as assessed by simulation, differs between these 3 alternative data-generating models, with larger differences apparent in the small population setting. Our findings are relevant to multilevel binomial models in general.},
  file = {/Users/rritaz/Zotero/storage/P9T9RYNW/Pateras et al. - 2018 - Data-generating models of dichotomous outcomes He.pdf;/Users/rritaz/Zotero/storage/BMFNSRHX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random-effects},
  language = {en},
  number = {7}
}

@article{paul_bayesian_2010,
  title = {Bayesian Bivariate Meta-Analysis of Diagnostic Test Studies Using Integrated Nested {{Laplace}} Approximations},
  author = {Paul, M. and Riebler, A. and Bachmann, L. M. and Rue, H. and Held, L.},
  year = {2010},
  volume = {29},
  pages = {1325--1339},
  issn = {1097-0258},
  doi = {10.1002/sim.3858},
  abstract = {For bivariate meta-analysis of diagnostic studies, likelihood approaches are very popular. However, they often run into numerical problems with possible non-convergence. In addition, the construction of confidence intervals is controversial. Bayesian methods based on Markov chain Monte Carlo (MCMC) sampling could be used, but are often difficult to implement, and require long running times and diagnostic convergence checks. Recently, a new Bayesian deterministic inference approach for latent Gaussian models using integrated nested Laplace approximations (INLA) has been proposed. With this approach MCMC sampling becomes redundant as the posterior marginal distributions are directly and accurately approximated. By means of a real data set we investigate the influence of the prior information provided and compare the results obtained by INLA, MCMC, and the maximum likelihood procedure SAS PROC NLMIXED. Using a simulation study we further extend the comparison of INLA and SAS PROC NLMIXED by assessing their performance in terms of bias, mean-squared error, coverage probability, and convergence rate. The results indicate that INLA is more stable and gives generally better coverage probabilities for the pooled estimates and less biased estimates of variance parameters. The user-friendliness of INLA is demonstrated by documented R-code. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/HERWIPMI/Paul et al. - 2010 - Bayesian bivariate meta-analysis of diagnostic tes.pdf;/Users/rritaz/Zotero/storage/VPMXJBQG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate},
  language = {en},
  number = {12}
}

@article{pedder_modelling_2019,
  title = {Modelling Time-Course Relationships with Multiple Treatments: {{Model}}-Based Network Meta-Analysis for Continuous Summary Outcomes},
  shorttitle = {Modelling Time-Course Relationships with Multiple Treatments},
  author = {Pedder, Hugo and Dias, Sofia and Bennetts, Margherita and Boucher, Martin and Welton, Nicky J.},
  year = {2019},
  volume = {10},
  pages = {267--286},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1351},
  abstract = {Background Model-based meta-analysis (MBMA) is increasingly used to inform drug-development decisions by synthesising results from multiple studies to estimate treatment, dose-response, and time-course characteristics. Network meta-analysis (NMA) is used in Health Technology Appraisals for simultaneously comparing effects of multiple treatments, to inform reimbursement decisions. Recently, a framework for dose-response model-based network meta-analysis (MBNMA) has been proposed that combines, often nonlinear, MBMA modelling with the statistically robust properties of NMA. Here, we aim to extend this framework to time-course models. Methods We propose a Bayesian time-course MBNMA modelling framework for continuous summary outcomes that allows for nonlinear modelling of multiparameter time-course functions, accounts for residual correlation between observations, preserves randomisation by modelling relative effects, and allows for testing of inconsistency between direct and indirect evidence on the time-course parameters. We demonstrate our modelling framework using an illustrative dataset of 23 trials investigating treatments for pain in osteoarthritis. Results Of the time-course functions that we explored, the Emax model gave the best fit to the data and has biological plausibility. Some simplifying assumptions were needed to identify the ET50, due to few observations at early follow-up times. Treatment estimates were robust to the inclusion of correlations in the likelihood. Conclusions Time-course MBNMA provides a statistically robust framework for synthesising evidence on multiple treatments at multiple time points. The use of placebo-controlled studies in drug-development means there is limited potential for inconsistency. The methods can inform drug-development decisions and provide the rigour needed in the reimbursement decision-making process.},
  file = {/Users/rritaz/Zotero/storage/692236XH/Pedder et al. - 2019 - Modelling time-course relationships with multiple .pdf;/Users/rritaz/Zotero/storage/ZZ5N97E7/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {2}
}

@article{pereira_critical_2010,
  title = {Critical Interpretation of {{Cochran}}'s {{Q}} Test Depends on Power and Prior Assumptions about Heterogeneity},
  author = {Pereira, Tiago V. and Patsopoulos, Nikolaos A. and Salanti, Georgia and Ioannidis, John P. A.},
  year = {2010},
  volume = {1},
  pages = {149--161},
  issn = {1759-2887},
  doi = {10.1002/jrsm.13},
  abstract = {We describe how an appropriate interpretation of the Q-test depends on its power to detect a given typical amount of between-study variance ({$\tau$}2) as well as prior beliefs on heterogeneity. We illustrate these concepts in an evaluation of 1011 meta-analyses of clinical trials with {$\geqslant$}4 studies and binary outcomes. These concepts can be seen as an application of the Bayes theorem. Across the 1011 meta-analyses, power to detect typical heterogeneity was low in most situations. Thus, usually a non-significant Q test did not change perceptibly prior convictions on heterogeneity. Conversely, significant results for the Q test typically augmented considerably the probability of heterogeneity. The posterior probability of heterogeneity depends on what {$\tau$}2 we want to detect. With the same approach, one may also estimate the posterior probability for the presence of heterogeneity that is large enough to annul statistically significant summary effects; that is half the average within-study variance of the combined studies; and that is able to change the summary effect estimate of the meta-analysis by 20\%. The discussed analyses are exploratory, and may depend heavily on prior assumptions when power for the Q-test is low. Statistical heterogeneity in meta-analyses should be cautiously interpreted considering the power to detect a specific {$\tau$}2 and prior assumptions about the presence of heterogeneity. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CYIYVJ7L/Pereira et al. - 2010 - Critical interpretation of Cochran's Q test depend.pdf;/Users/rritaz/Zotero/storage/29M8P874/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {power,random-effects},
  language = {en},
  number = {2}
}

@article{peters_performance_2007,
  title = {Performance of the Trim and Fill Method in the Presence of Publication Bias and Between-Study Heterogeneity},
  author = {Peters, Jaime L. and Sutton, Alex J. and Jones, David R. and Abrams, Keith R. and Rushton, Lesley},
  year = {2007},
  volume = {26},
  pages = {4544--4562},
  issn = {1097-0258},
  doi = {10.1002/sim.2889},
  abstract = {The trim and fill method allows estimation of an adjusted meta-analysis estimate in the presence of publication bias. To date, the performance of the trim and fill method has had little assessment. In this paper, we provide a more comprehensive examination of different versions of the trim and fill method in a number of simulated meta-analysis scenarios, comparing results with those from usual unadjusted meta-analysis models and two simple alternatives, namely use of the estimate from: (i) the largest; or (ii) the most precise study in the meta-analysis. Findings suggest a great deal of variability in the performance of the different approaches. When there is large between-study heterogeneity the trim and fill method can underestimate the true positive effect when there is no publication bias. However, when publication bias is present the trim and fill method can give estimates that are less biased than the usual meta-analysis models. Although results suggest that the use of the estimate from the largest or most precise study seems a reasonable approach in the presence of publication bias, when between-study heterogeneity exists our simulations show that these estimates are quite biased. We conclude that in the presence of publication bias use of the trim and fill method can help to reduce the bias in pooled estimates, even though the performance of this method is not ideal. However, because we do not know whether funnel plot asymmetry is truly caused by publication bias, and because there is great variability in the performance of different trim and fill estimators and models in various meta-analysis scenarios, we recommend use of the trim and fill method as a form of sensitivity analysis as intended by the authors of the method. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5UIFT9X4/Peters et al. - 2007 - Performance of the trim and fill method in the pre.pdf;/Users/rritaz/Zotero/storage/JSNW7Q7P/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,publication bias},
  language = {en},
  number = {25}
}

@article{petkova_interpreting_2013,
  title = {Interpreting Meta-Regression: Application to Recent Controversies in Antidepressants' Efficacy},
  shorttitle = {Interpreting Meta-Regression},
  author = {Petkova, Eva and Tarpey, Thaddeus and Huang, Lei and Deng, Liping},
  year = {2013},
  volume = {32},
  pages = {2875--2892},
  issn = {1097-0258},
  doi = {10.1002/sim.5766},
  abstract = {A recent meta-regression of antidepressant efficacy on baseline depression severity has caused considerable controversy in the popular media. A central source of the controversy is a lack of clarity about the relation of meta-regression parameters to corresponding parameters in models for subject-level data. This paper focuses on a linear regression with continuous outcome and predictor, a case that is often considered less problematic. We frame meta-regression in a general mixture setting that encompasses both finite and infinite mixture models. In many applications of meta-analysis, the goal is to evaluate the efficacy of a treatment from several studies, and authors use meta-regression on grouped data to explain variations in the treatment efficacy by study features. When the study feature is a characteristic that has been averaged over subjects, it is difficult not to interpret the meta-regression results on a subject level, a practice that is still widespread in medical research. Although much of the attention in the literature is on methods of estimating meta-regression model parameters, our results illustrate that estimation methods cannot protect against erroneous interpretations of meta-regression on grouped data. We derive relations between meta-regression parameters and within-study model parameters and show that the conditions under which slopes from these models are equal cannot be verified on the basis of group-level information only. The effects of these model violations cannot be known without subject-level data. We conclude that interpretations of meta-regression results are highly problematic when the predictor is a subject-level characteristic that has been averaged over study subjects. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/52T9RSK2/Petkova et al. - 2013 - Interpreting meta-regression application to recen.pdf;/Users/rritaz/Zotero/storage/BD2C3G7E/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {17}
}

@article{petropoulou_comparison_2017,
  title = {A Comparison of 20 Heterogeneity Variance Estimators in Statistical Synthesis of Results from Studies: A Simulation Study},
  shorttitle = {A Comparison of 20 Heterogeneity Variance Estimators in Statistical Synthesis of Results from Studies},
  author = {Petropoulou, Maria and Mavridis, Dimitris},
  year = {2017},
  volume = {36},
  pages = {4266--4280},
  issn = {1097-0258},
  doi = {10.1002/sim.7431},
  abstract = {When we synthesize research findings via meta-analysis, it is common to assume that the true underlying effect differs across studies. Total variability consists of the within-study and between-study variances (heterogeneity). There have been established measures, such as I2, to quantify the proportion of the total variation attributed to heterogeneity. There is a plethora of estimation methods available for estimating heterogeneity. The widely used DerSimonian and Laird estimation method has been challenged, but knowledge of the overall performance of heterogeneity estimators is incomplete. We identified 20 heterogeneity estimators in the literature and evaluated their performance in terms of mean absolute estimation error, coverage probability, and length of the confidence interval for the summary effect via a simulation study. Although previous simulation studies have suggested the Paule-Mandel estimator, it has not been compared with all the available estimators. For dichotomous outcomes, estimating heterogeneity through Markov chain Monte Carlo is a good choice if an informative prior distribution for heterogeneity is employed (eg, by published Cochrane reviews). Nonparametric bootstrap and positive DerSimonian and Laird perform well for all assessment criteria for both dichotomous and continuous outcomes. Hartung-Makambi estimator can be the best choice when the heterogeneity values are close to 0.07 for dichotomous outcomes and medium heterogeneity values (0.01 , 0.05) for continuous outcomes. Hence, there are heterogeneity estimators (nonparametric bootstrap DerSimonian and Laird and positive DerSimonian and Laird) that perform better than the suggested Paule-Mandel. Maximum likelihood provides the best performance for both types of outcome in the absence of heterogeneity.},
  file = {/Users/rritaz/Zotero/storage/89KU4DIK/Petropoulou and Mavridis - 2017 - A comparison of 20 heterogeneity variance estimato.pdf;/Users/rritaz/Zotero/storage/ZFAW2YAU/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity estimators,random-effects},
  language = {en},
  number = {27}
}

@article{phillips_cross_2010,
  title = {`{{Cross}} Hairs' Plots for Diagnostic Meta-Analysis},
  author = {Phillips, Bob and Stewart, Lesley A. and Sutton, Alex J.},
  year = {2010},
  volume = {1},
  pages = {308--315},
  issn = {1759-2887},
  doi = {10.1002/jrsm.26},
  abstract = {Understanding diagnostic test accuracy (DTA) data, especially meta-analysis of this information, can be challenging for researchers and clinicians. The use of plots of receiver-operator curve (ROC) space showing individual studies and summary estimates of diagnostic accuracy has become common but can be difficult to interpret. The assessment of heterogeneity in sensitivity and specificity across studies can be particularly difficult. In this paper, we review the key concepts of assessing DTA, starting at the level of individual studies and progressing to the setting of research synthesis. We explore the standard displays of this information and then propose and explain an alternative approach to summarizing key data. These `cross-hairs' plots display the individual studies in ROC space with paired confidence intervals representing sensitivity and specificity, and allow for the results of meta-analysis to be overlaid on the plot. We suggest that these plots are more easily interpreted, and are a more informative graphical form than common approaches. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EH4DFLFR/Phillips et al. - 2010 - ‘Cross hairs’ plots for diagnostic meta-analysis.pdf;/Users/rritaz/Zotero/storage/DIEUKDRZ/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {3-4}
}

@article{phillips_publication_2004,
  title = {Publication Bias in Situ},
  author = {Phillips, Carl V},
  year = {2004},
  month = dec,
  volume = {4},
  pages = {20},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-4-20},
  abstract = {Background: Publication bias, as typically defined, refers to the decreased likelihood of studies' results being published when they are near the null, not statistically significant, or otherwise "less interesting." But choices about how to analyze the data and which results to report create a publication bias within the published results, a bias I label "publication bias in situ" (PBIS).},
  file = {/Users/rritaz/Zotero/storage/YF543796/Phillips - 2004 - Publication bias in situ.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {publication bias},
  language = {en},
  number = {1}
}

@article{piepho_multiplicative_2015,
  title = {Multiplicative Interaction in Network Meta-Analysis},
  author = {Piepho, Hans-Peter and Madden, Laurence V. and Williams, Emlyn R.},
  year = {2015},
  volume = {34},
  pages = {582--594},
  issn = {1097-0258},
  doi = {10.1002/sim.6372},
  abstract = {Meta-analysis of a set of clinical trials is usually conducted using a linear predictor with additive effects representing treatments and trials. Additivity is a strong assumption. In this paper, we consider models for two or more treatments that involve multiplicative terms for interaction between treatment and trial. Multiplicative models provide information on the sensitivity of each treatment effect relative to the trial effect. In developing these models, we make use of a two-way analysis-of-variance approach to meta-analysis and consider fixed or random trial effects. It is shown using two examples that models with multiplicative terms may fit better than purely additive models and provide insight into the nature of the trial effect. We also show how to model inconsistency using multiplicative terms. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GSURVQCJ/Piepho et al. - 2015 - Multiplicative interaction in network meta-analysi.pdf;/Users/rritaz/Zotero/storage/8WT98VEX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {4}
}

@article{piepho_network-meta_2014,
  title = {Network-Meta Analysis Made Easy: Detection of Inconsistency Using Factorial Analysis-of-Variance Models},
  shorttitle = {Network-Meta Analysis Made Easy},
  author = {Piepho, Hans-Peter},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {61},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-61},
  abstract = {Background: Network meta-analysis can be used to combine results from several randomized trials involving more than two treatments. Potential inconsistency among different types of trial (designs) differing in the set of treatments tested is a major challenge, and application of procedures for detecting and locating inconsistency in trial networks is a key step in the conduct of such analyses. Methods: Network meta-analysis can be very conveniently performed using factorial analysis-of-variance methods. Inconsistency can be scrutinized by inspecting the design \texttimes{} treatment interaction. This approach is in many ways simpler to implement than the more common approach of using treatment-versus-control contrasts. Results: We show that standard regression diagnostics available in common linear mixed model packages can be used to detect and locate inconsistency in trial networks. Moreover, a suitable definition of factors and effects allows devising significance tests for inconsistency. Conclusion: Factorial analysis of variance provides a convenient framework for conducting network meta-analysis, including diagnostic checks for inconsistency.},
  file = {/Users/rritaz/Zotero/storage/GGGL7ZNZ/Piepho - 2014 - Network-meta analysis made easy detection of inco.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {categorical MA models,diagnostic techniques,network meta-analysis},
  language = {en},
  number = {1}
}

@article{pigott_combining_2012,
  title = {Combining Individual Participant and Aggregated Data in a Meta-Analysis with Correlational Studies},
  author = {Pigott, Terri and Williams, Ryan and Polanin, Joshua},
  year = {2012},
  volume = {3},
  pages = {257--268},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1051},
  abstract = {This paper presents methods for combining individual participant data (IPD) with aggregated study level data (AD) in a meta-analysis of correlational studies. Although medical researchers have employed IPD in a wide range of studies, only a single example exists in the social sciences. New policies at the National Science Foundation requiring grantees to submit data archiving plans may increase social scientists' access to individual level data that could be combined with traditional meta-analysis. The methods presented here extend prior work on IPD to meta-analyses using correlational studies. The examples presented illustrate the synthesis of publicly available national datasets in education with aggregated study data from a meta-analysis examining the correlation of socioeconomic status measures and academic achievement. The major benefit of the inclusion of the individual level is that both within-study and between-study interactions among moderators of effect size can be estimated. Given the potential growth in data archives in the social sciences, we should see a corresponding increase in the ability to synthesize IPD and AD in a single meta-analysis, leading to a more complete understanding of how within-study and between-study moderators relate to effect size. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WIGGP6CC/Pigott et al. - 2012 - Combining individual participant and aggregated da.pdf;/Users/rritaz/Zotero/storage/CLNGX4XL/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,Individual Patient Data IPD},
  language = {en},
  number = {4}
}

@article{polanin_use_2015,
  title = {The Use of Meta-Analytic Statistical Significance Testing},
  author = {Polanin, Joshua R. and Pigott, Terri D.},
  year = {2015},
  volume = {6},
  pages = {63--73},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1124},
  abstract = {Meta-analysis multiplicity, the concept of conducting multiple tests of statistical significance within one review, is an underdeveloped literature. We address this issue by considering how Type I errors can impact meta-analytic results, suggest how statistical power may be affected through the use of multiplicity corrections, and propose how meta-analysts should analyze multiple tests of statistical significance. The context for this study is a meta-review of meta-analyses published in two leading review journals in education and psychology. Our review of 130 meta-analyses revealed a strong reliance on statistical significance testing without consideration of Type I errors or the use of multiplicity corrections. In order to provide valid conclusions, meta-analysts must consider these issues prior to conducting the review. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/K6AYS5FW/Polanin and Pigott - 2015 - The use of meta-analytic statistical significance .pdf;/Users/rritaz/Zotero/storage/KJC22N89/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {power},
  language = {en},
  number = {1}
}

@article{preacher_effect_2011,
  title = {Effect Size Measures for Mediation Models: {{Quantitative}} Strategies for Communicating Indirect Effects.},
  shorttitle = {Effect Size Measures for Mediation Models},
  author = {Preacher, Kristopher J. and Kelley, Ken},
  year = {2011},
  volume = {16},
  pages = {93--115},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0022658},
  abstract = {The statistical analysis of mediation effects has become an indispensable tool for helping scientists investigate processes thought to be causal. Yet, in spite of many recent advances in the estimation and testing of mediation effects, little attention has been given to methods for communicating effect size and the practical importance of those effect sizes. Our goals in this article are to (a) outline some general desiderata for effect size measures, (b) describe current methods of expressing effect size and practical importance for mediation, (c) use the desiderata to evaluate these methods, and (d) develop new methods to communicate effect size in the context of mediation analysis. The first new effect size index we describe is a residual-based index that quantifies the amount of variance explained in both the mediator and the outcome. The second new effect size index quantifies the indirect effect as the proportion of the maximum possible indirect effect that could have been obtained, given the scales of the variables involved. We supplement our discussion by offering easy-to-use R tools for the numerical and visual communication of effect size for mediation effects.},
  file = {/Users/rritaz/Zotero/storage/8ZH2F37T/Preacher and Kelley - 2011 - Effect size measures for mediation models Quantit.pdf},
  journal = {Psychological Methods},
  keywords = {communicating ES,continuous effect sizes},
  language = {en},
  number = {2}
}

@article{prendergast_meta-analysis_2016,
  title = {Meta-Analysis of Ratios of Sample Variances},
  author = {Prendergast, Luke A. and Staudte, Robert G.},
  year = {2016},
  volume = {35},
  pages = {1780--1799},
  issn = {1097-0258},
  doi = {10.1002/sim.6838},
  abstract = {When conducting a meta-analysis of standardized mean differences (SMDs), it is common to use Cohen's d, or its variants, that require equal variances in the two arms of each study. While interpretation of these SMDs is simple, this alone should not be used as a justification for assuming equal variances. Until now, researchers have either used an F-test for each individual study or perhaps even conveniently ignored such tools altogether. In this paper, we propose a meta-analysis of ratios of sample variances to assess whether the equality of variances assumptions is justified prior to a meta-analysis of SMDs. Quantile\textendash quantile plots, an omnibus test for equal variances or an overall meta-estimate of the ratio of variances can all be used to formally justify the use of less common methods when evidence of unequal variances is found. The methods in this paper are simple to implement and the validity of the approaches are reinforced by simulation studies and an application to a real data set. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/F2MVZ8R4/Prendergast and Staudte - 2016 - Meta-analysis of ratios of sample variances.pdf;/Users/rritaz/Zotero/storage/ZVHRIQYX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,random-effects},
  language = {en},
  number = {11}
}

@article{prevost_allowing_2007,
  title = {Allowing for Correlations between Correlations in Random-Effects Meta-Analysis of Correlation Matrices.},
  author = {Prevost, A. Toby and Mason, Dan and Griffin, Simon and Kinmonth, Ann-Louise and Sutton, Stephen and Spiegelhalter, David},
  year = {2007},
  month = dec,
  volume = {12},
  pages = {434--450},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.12.4.434},
  abstract = {Practical meta-analysis of correlation matrices generally ignores covariances (and hence correlations) between correlation estimates. The authors consider various methods for allowing for covariances, including generalized least squares, maximum marginal likelihood, and Bayesian approaches, illustrated using a 6-dimensional response in a series of psychological studies concerning prediction of exercise behavior change. Quantities of interest include the overall population mean correlation matrix, the contrast between the mean correlations, the predicted correlation matrix in a new study, and the conflict between the existing studies and a new correlation matrix. The authors conclude that accounting for correlations between correlations is unnecessary when interested in individual correlations but potentially important if concerned with a composite measure involving 2 or more correlations. A simulation study indicates the asymptotic normal assumption appears reasonable. Because of potential instability in the generalized least squares methods, they recommend a model-based approach, either the maximum marginal likelihood approach or a full Bayesian analysis.},
  file = {/Users/rritaz/Zotero/storage/BR3MXN87/Prevost et al. - 2007 - Allowing for correlations between correlations in .pdf},
  journal = {Psychological Methods},
  keywords = {bayesian,continuous effect sizes,correlated effects,correlation coefficients,multivariate,random-effects},
  language = {en},
  number = {4}
}

@article{prevost_hierarchical_2000,
  title = {Hierarchical Models in Generalized Synthesis of Evidence: An Example Based on Studies of Breast Cancer Screening},
  shorttitle = {Hierarchical Models in Generalized Synthesis of Evidence},
  author = {Prevost, Teresa C. and Abrams, Keith R. and Jones, David R.},
  year = {2000},
  volume = {19},
  pages = {3359--3376},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20001230)19:24<3359::AID-SIM710>3.0.CO;2-N},
  abstract = {Evidence regarding the potential benefits of a particular health care intervention is often available from a variety of disparate sources. However, formal synthesis of such evidence has traditionally concentrated almost exclusively on that derived from randomized studies, although for a range of conditions the randomized evidence will be less than adequate due to economic, organizational or ethical considerations. In such situations a formal synthesis of the evidence that is available from observational studies can be valuable whilst awaiting higher quality evidence from randomized trials. Consideration of randomized studies alone may be appropriate when assessing the efficacy of an intervention, but assessment of the effectiveness of such an intervention within a more general target population may be improved by consideration of evidence from non-randomized studies as well. Standard meta-analysis methods may allow for both within- and between-study heterogeneity; however when multiple sources of evidence are considered an extra level of complexity is introduced, namely study type. One possible solution to the problem of making inferences, particularly regarding an overall population effect, in such situations is to model the heterogeneity, both quantitative and qualitative, using a Bayesian hierarchical model. The hierarchical nature of such models specifically allows for the quantitative within and between sources of heterogeneity, whilst the Bayesian approach can accommodate a priori beliefs regarding qualitative differences between the various sources of evidence. The use of such methods in practice is illustrated in the context of screening for breast cancer; in this example evidence is available from both randomized clinical trials and observational studies. A particular appeal of a Bayesian approach for this type of problem lies in the prediction of future benefits likely to be observed in a target population. This approach to health service monitoring in general is discussed. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JN3NHR2X/Prevost et al. - 2000 - Hierarchical models in generalized synthesis of ev.pdf;/Users/rritaz/Zotero/storage/GULQKZS7/1097-0258(20001230)19243359AID-SIM7103.0.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {24}
}

@article{price_parameterization_2011,
  title = {Parameterization of Treatment Effects for Meta-Analysis in Multi-State {{Markov}} Models},
  author = {Price, Malcolm J. and Welton, Nicky J. and Ades, A. E.},
  year = {2011},
  volume = {30},
  pages = {140--151},
  issn = {1097-0258},
  doi = {10.1002/sim.4059},
  abstract = {Standard approaches to analysis of randomized controlled trials (RCTs) using Markov models make it difficult to generalize treatment effects to new patient groups and synthesize evidence across trials. This paper demonstrates how pair-wise and mixed treatment comparison meta-analysis can be applied to event history data for disease progression reported by RCTs. The data, in the form of aggregated discrete time transitions, have a multi-nomial likelihood. In order for evidence synthesis to be performed a structured approach to modelling the differences in the effects of the different treatments must be taken. A multi-state continuous-time Markov model similar to others used in published economic evaluations of asthma treatments is developed, with transition rates related to the likelihood via Kolmogorov's forward equations. The formulation in terms of rates allows a flexible characterization of summary treatment effects. These ideas are applied to an illustrative data set consisting of a set of five trials comparing eight different treatments for asthma. A range of models is developed in which the relative treatment effects act on forward, backward transitions, or both, and models are compared using the DIC. Bayesian inferential techniques are used and the parameters are estimated using MCMC simulation in WinBUGS. An intuitively appealing mechanism of action involving a single parameter acting on all backward transitions was identified for the relative effects of the treatments, which allowed the estimation of a pooled treatment effect, allowing us to rank the different treatment options within each connected evidence network to ascertain which were the most clinically effective. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YK3U43P4/Price et al. - 2011 - Parameterization of treatment effects for meta-ana.pdf;/Users/rritaz/Zotero/storage/PW9ULL6I/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,effect size combination (small sample \& discrete)},
  language = {en},
  number = {2}
}

@article{pustejovsky_converting_2014,
  title = {Converting from d to r to z When the Design Uses Extreme Groups, Dichotomization, or Experimental Control},
  author = {Pustejovsky, James E.},
  year = {2014},
  month = mar,
  volume = {19},
  pages = {92--112},
  issn = {1082-989X},
  doi = {10.1037/a0033788},
  abstract = {Meta-analyses of the relationship between 2 continuous variables sometimes involves conversions between different effect sizes, but methodological literature offers conflicting guidance about how to make such conversions. This article provides methods for converting from a standardized mean difference to a correlation coefficient (and from there to Fisher's z) under 3 types of study designs: extreme groups, dichotomization of a continuous variable, and controlled experiments. Also provided are formulas and recommendations regarding how the sampling variance of effect size statistics should be estimated in each of these cases. The conversion formula for extreme groups designs, originally due to Feldt (1961), can be viewed as a generalization of Hunter and Schmidt's (1990) method for dichotomization designs. A simulation study examines the finite-sample properties of the proposed methods. The conclusion highlights areas where current guidance in the literature should be amended or clarified. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/RA446GNB/Pustejovsky - 2014 - Converting from d to r to z when the design uses e.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlation coefficients},
  number = {1}
}

@article{pustejovsky_measurement-comparable_2015,
  title = {Measurement-Comparable Effect Sizes for Single-Case Studies of Free-Operant Behavior.},
  author = {Pustejovsky, James E.},
  year = {2015},
  volume = {20},
  pages = {342--359},
  doi = {10.1037/met0000019},
  abstract = {Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic techniques for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by 2 examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.},
  file = {/Users/rritaz/Zotero/storage/8VPRX6LI/Pustejovsky - 2015 - Measurement-comparable effect sizes for single-cas.pdf},
  journal = {Psychological methods},
  keywords = {correlated effects,discrete effect sizes},
  number = {3}
}

@article{pustejovsky_procedural_2019,
  title = {Procedural Sensitivities of Effect Sizes for Single-Case Designs with Directly Observed Behavioral Outcome Measures},
  author = {Pustejovsky, James E.},
  year = {2019},
  month = apr,
  volume = {24},
  pages = {217--235},
  issn = {1082-989X},
  doi = {10.1037/met0000179},
  abstract = {A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference as well as nonoverlap measures such as the percentage of nonoverlapping data, improvement rate difference, and nonoverlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the nonoverlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study's design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  file = {/Users/rritaz/Zotero/storage/8RIEASCF/Pustejovsky - 2019 - Procedural sensitivities of effect sizes for singl.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,effect size estimation (series)},
  number = {2}
}

@article{pustejovsky_testing_2019,
  title = {Testing for Funnel Plot Asymmetry of Standardized Mean Differences},
  author = {Pustejovsky, James E. and Rodgers, Melissa A.},
  year = {2019},
  volume = {10},
  pages = {57--71},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1332},
  abstract = {Publication bias and other forms of outcome reporting bias are critical threats to the validity of findings from research syntheses. A variety of methods have been proposed for detecting selective outcome reporting in a collection of effect size estimates, including several methods based on assessment of asymmetry of funnel plots, such as the Egger's regression test, the rank correlation test, and the Trim-and-Fill test. Previous research has demonstated that the Egger's regression test is miscalibrated when applied to log-odds ratio effect size estimates, because of artifactual correlation between the effect size estimate and its standard error. This study examines similar problems that occur in meta-analyses of the standardized mean difference, a ubiquitous effect size measure in educational and psychological research. In a simulation study of standardized mean difference effect sizes, we assess the Type I error rates of conventional tests of funnel plot asymmetry, as well as the likelihood ratio test from a three-parameter selection model. Results demonstrate that the conventional tests have inflated Type I error due to the correlation between the effect size estimate and its standard error, while tests based on either a simple modification to the conventional standard error formula or a variance-stabilizing transformation both maintain close-to-nominal Type I error.},
  file = {/Users/rritaz/Zotero/storage/4KIJ9ZFQ/Pustejovsky and Rodgers - 2019 - Testing for funnel plot asymmetry of standardized .pdf;/Users/rritaz/Zotero/storage/SPHQKRZY/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {publication bias},
  language = {en},
  number = {1}
}

@article{quartagno_multiple_2016,
  title = {Multiple Imputation for {{IPD}} Meta-Analysis: Allowing for Heterogeneity and Studies with Missing Covariates},
  shorttitle = {Multiple Imputation for {{IPD}} Meta-Analysis},
  author = {Quartagno, M. and Carpenter, J. R.},
  year = {2016},
  volume = {35},
  pages = {2938--2954},
  issn = {1097-0258},
  doi = {10.1002/sim.6837},
  abstract = {Recently, multiple imputation has been proposed as a tool for individual patient data meta-analysis with sporadically missing observations, and it has been suggested that within-study imputation is usually preferable. However, such within study imputation cannot handle variables that are completely missing within studies. Further, if some of the contributing studies are relatively small, it may be appropriate to share information across studies when imputing. In this paper, we develop and evaluate a joint modelling approach to multiple imputation of individual patient data in meta-analysis, with an across-study probability distribution for the study specific covariance matrices. This retains the flexibility to allow for between-study heterogeneity when imputing while allowing (i) sharing information on the covariance matrix across studies when this is appropriate, and (ii) imputing variables that are wholly missing from studies. Simulation results show both equivalent performance to the within-study imputation approach where this is valid, and good results in more general, practically relevant, scenarios with studies of very different sizes, non-negligible between-study heterogeneity and wholly missing variables. We illustrate our approach using data from an individual patient data meta-analysis of hypertension trials. \textcopyright{} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/GQJL2MTI/Quartagno and Carpenter - 2016 - Multiple imputation for IPD meta-analysis allowin.pdf;/Users/rritaz/Zotero/storage/IJEKVKQY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,missing data,random-effects},
  language = {en},
  number = {17}
}

@article{raudenbush_statistical_2000,
  title = {Statistical Power and Optimal Design for Multisite Randomized Trials.},
  author = {Raudenbush, Stephen W. and Liu, Xianan},
  year = {2000},
  volume = {5},
  pages = {199--213},
  doi = {10.1037/1082-989X.5.2.199},
  abstract = {The multisite trial, widely used in mental health research and education, enables experimenters to assess the average impact of a treatment across sites, the variance of treatment impact across sites, and the moderating effect of site characteristics on treatment efficacy. Key design decisions include the sample size per site and the number of sites. To consider power implications, this article proposes a standardized hierarchical linear model and uses rules of thumb similar to those proposed by J. Cohen (1988) for small, medium, and large effect sizes and for small, medium, and large treatment-by-site variance. Optimal allocation of resources within and between sites as a function of variance components and costs at each level are also considered. The approach generalizes to quasiexperiments with a similar structure. These ideas are illustrated with newly developed software.},
  journal = {Psychological methods},
  keywords = {effect size estimation (series),GLM MA models,power},
  number = {2}
}

@article{ravva_linearization_2014,
  title = {A Linearization Approach for the Model-Based Analysis of Combined Aggregate and Individual Patient Data},
  author = {Ravva, Patanjali and Karlsson, Mats O. and French, Jonathan L.},
  year = {2014},
  volume = {33},
  pages = {1460--1476},
  issn = {1097-0258},
  doi = {10.1002/sim.6045},
  abstract = {AbstractThe application of model-based meta-analysis in drug development has gained prominence recently, particularly for characterizing dose-response relationships and quantifying treatment effect sizes of competitor drugs. The models are typically nonlinear in nature and involve covariates to explain the heterogeneity in summary-level literature (or aggregate data (AD)). Inferring individual patient-level relationships from these nonlinear meta-analysis models leads to aggregation bias. Individual patient-level data (IPD) are indeed required to characterize patient-level relationships but too often this information is limited. Since combined analyses of AD and IPD allow advantage of the information they share to be taken, the models developed for AD must be derived from IPD models; in the case of linear models, the solution is a closed form, while for nonlinear models, closed form solutions do not exist.Here, we propose a linearization method based on a second order Taylor series approximation for fitting models to AD alone or combined AD and IPD. The application of this method is illustrated by an analysis of a continuous landmark endpoint, i.e., change from baseline in HbA1c at week 12, from 18 clinical trials evaluating the effects of DPP-4 inhibitors on hyperglycemia in diabetic patients. The performance of this method is demonstrated by a simulation study where the effects of varying the degree of nonlinearity and of heterogeneity in covariates (as assessed by the ratio of between-trial to within-trial variability) were studied. A dose-response relationship using an Emax model with linear and nonlinear effects of covariates on the emax parameter was used to simulate data. The simulation results showed that when an IPD model is simply used for modeling AD, the bias in the emax parameter estimate increased noticeably with an increasing degree of nonlinearity in the model, with respect to covariates. When using an appropriately derived AD model, the linearization method adequately corrected for bias. It was also noted that the bias in the model parameter estimates decreased as the ratio of between-trial to within-trial variability in covariate distribution increased. Taken together, the proposed linearization approach allows addressing the issue of aggregation bias in the particular case of nonlinear models of aggregate data. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LU8WJAPF/Ravva et al. - 2014 - A linearization approach for the model-based analy.pdf;/Users/rritaz/Zotero/storage/JVC48TPE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {9}
}

@article{renfro_bayesian_2012,
  title = {Bayesian Adjusted {{R2}} for the Meta-Analytic Evaluation of Surrogate Time-to-Event Endpoints in Clinical Trials},
  author = {Renfro, Lindsay A. and Shi, Qian and Sargent, Daniel J. and Carlin, Bradley P.},
  year = {2012},
  volume = {31},
  pages = {743--761},
  issn = {1097-0258},
  doi = {10.1002/sim.4416},
  abstract = {A two-stage model for evaluating both trial-level and patient-level surrogacy of correlated time-to-event endpoints has been introduced, using patient-level data when multiple clinical trials are available. However, the associated maximum likelihood approach often suffers from numerical problems when different baseline hazards among trials and imperfect estimation of treatment effects are assumed. To address this issue, we propose performing the second-stage, trial-level evaluation of potential surrogates within a Bayesian framework, where we may naturally borrow information across trials while maintaining these realistic assumptions. Posterior distributions on surrogacy measures of interest may then be used to compare measures or make decisions regarding the candidacy of a specific endpoint. We perform a simulation study to investigate differences in estimation performance between traditional maximum likelihood and new Bayesian representations of common meta-analytic surrogacy measures, while assessing sensitivity to data characteristics such as number of trials, trial size, and amount of censoring. Furthermore, we present both frequentist and Bayesian trial-level surrogacy evaluations of time to recurrence for overall survival in two meta-analyses of adjuvant therapy trials in colon cancer. With these results, we recommend Bayesian evaluation as an attractive and numerically stable alternative in the multitrial assessment of potential surrogate endpoints. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/4CHKHPC6/Renfro et al. - 2012 - Bayesian adjusted R2 for the meta-analytic evaluat.pdf;/Users/rritaz/Zotero/storage/MFJB6ZAW/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance},
  language = {en},
  number = {8}
}

@article{rescherigon_multiple_2013,
  title = {Multiple Imputation for Handling Systematically Missing Confounders in Meta-Analysis of Individual Participant Data},
  author = {Resche-Rigon, Matthieu and White, Ian R. and W.  Bartlett, Jonathan and Peters, Sanne A. E. and Thompson, Simon G.},
  year = {2013},
  volume = {32},
  pages = {4890--4905},
  issn = {1097-0258},
  doi = {10.1002/sim.5894},
  abstract = {A variable is `systematically missing' if it is missing for all individuals within particular studies in an individual participant data meta-analysis. When a systematically missing variable is a potential confounder in observational epidemiology, standard methods either fail to adjust the exposure\textendash disease association for the potential confounder or exclude studies where it is missing. We propose a new approach to adjust for systematically missing confounders based on multiple imputation by chained equations. Systematically missing data are imputed via multilevel regression models that allow for heterogeneity between studies. A simulation study compares various choices of imputation model. An illustration is given using data from eight studies estimating the association between carotid intima media thickness and subsequent risk of cardiovascular events. Results are compared with standard methods and also with an extension of a published method that exploits the relationship between fully adjusted and partially adjusted estimated effects through a multivariate random effects meta-analysis model. We conclude that multiple imputation provides a practicable approach that can handle arbitrary patterns of systematic missingness. Bias is reduced by including sufficient between-study random effects in the imputation model. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WGQJ3524/Resche‐Rigon et al. - 2013 - Multiple imputation for handling systematically mi.pdf;/Users/rritaz/Zotero/storage/SCJH2IKT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data},
  language = {en},
  number = {28}
}

@article{rhodes_implementing_2016,
  title = {Implementing Informative Priors for Heterogeneity in Meta-Analysis Using Meta-Regression and Pseudo Data},
  author = {Rhodes, Kirsty M. and Turner, Rebecca M. and White, Ian R. and Jackson, Dan and Spiegelhalter, David J. and Higgins, Julian P. T.},
  year = {2016},
  volume = {35},
  pages = {5495--5511},
  issn = {1097-0258},
  doi = {10.1002/sim.7090},
  abstract = {Many meta-analyses combine results from only a small number of studies, a situation in which the between-study variance is imprecisely estimated when standard methods are applied. Bayesian meta-analysis allows incorporation of external evidence on heterogeneity, providing the potential for more robust inference on the effect size of interest. We present a method for performing Bayesian meta-analysis using data augmentation, in which we represent an informative conjugate prior for between-study variance by pseudo data and use meta-regression for estimation. To assist in this, we derive predictive inverse-gamma distributions for the between-study variance expected in future meta-analyses. These may serve as priors for heterogeneity in new meta-analyses. In a simulation study, we compare approximate Bayesian methods using meta-regression and pseudo data against fully Bayesian approaches based on importance sampling techniques and Markov chain Monte Carlo (MCMC). We compare the frequentist properties of these Bayesian methods with those of the commonly used frequentist DerSimonian and Laird procedure. The method is implemented in standard statistical software and provides a less complex alternative to standard MCMC approaches. An importance sampling approach produces almost identical results to standard MCMC approaches, and results obtained through meta-regression and pseudo data are very similar. On average, data augmentation provides closer results to MCMC, if implemented using restricted maximum likelihood estimation rather than DerSimonian and Laird or maximum likelihood estimation. The methods are applied to real datasets, and an extension to network meta-analysis is described. The proposed method facilitates Bayesian meta-analysis in a way that is accessible to applied researchers. \textcopyright{} 2016 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/CKIWZ4DQ/Rhodes et al. - 2016 - Implementing informative priors for heterogeneity .pdf;/Users/rritaz/Zotero/storage/F8YYNX2I/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,effect size combination (small sample \& discrete),modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {29}
}

@article{rhodes_label-invariant_2018,
  title = {Label-Invariant Models for the Analysis of Meta-Epidemiological Data},
  author = {Rhodes, K. M. and Mawdsley, D. and Turner, R. M. and Jones, H. E. and Savovi{\'c}, J. and Higgins, J. P. T.},
  year = {2018},
  volume = {37},
  pages = {60--70},
  issn = {1097-0258},
  doi = {10.1002/sim.7491},
  abstract = {Rich meta-epidemiological data sets have been collected to explore associations between intervention effect estimates and study-level characteristics. Welton et al proposed models for the analysis of meta-epidemiological data, but these models are restrictive because they force heterogeneity among studies with a particular characteristic to be at least as large as that among studies without the characteristic. In this paper we present alternative models that are invariant to the labels defining the 2 categories of studies. To exemplify the methods, we use a collection of meta-analyses in which the Cochrane Risk of Bias tool has been implemented. We first investigate the influence of small trial sample sizes (less than 100 participants), before investigating the influence of multiple methodological flaws (inadequate or unclear sequence generation, allocation concealment, and blinding). We fit both the Welton et al model and our proposed label-invariant model and compare the results. Estimates of mean bias associated with the trial characteristics and of between-trial variances are not very sensitive to the choice of model. Results from fitting a univariable model show that heterogeneity variance is, on average, 88\% greater among trials with less than 100 participants. On the basis of a multivariable model, heterogeneity variance is, on average, 25\% greater among trials with inadequate/unclear sequence generation, 51\% greater among trials with inadequate/unclear blinding, and 23\% lower among trials with inadequate/unclear allocation concealment, although the 95\% intervals for these ratios are very wide. Our proposed label-invariant models for meta-epidemiological data analysis facilitate investigations of between-study heterogeneity attributable to certain study characteristics.},
  file = {/Users/rritaz/Zotero/storage/MK7ID7FM/Rhodes et al. - 2018 - Label-invariant models for the analysis of meta-ep.pdf;/Users/rritaz/Zotero/storage/7NIG6RDP/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{riepe_additive_2011,
  title = {Additive Scales in Degenerative Disease - Calculation of Effect Sizes and Clinical Judgment},
  author = {Riepe, Matthias W and Wilkinson, David and F{\"o}rstl, Hans and Brieden, Andreas},
  year = {2011},
  month = dec,
  volume = {11},
  pages = {169},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-169},
  abstract = {Background: The therapeutic efficacy of an intervention is often assessed in clinical trials by scales measuring multiple diverse activities that are added to produce a cumulative global score. Medical communities and health care systems subsequently use these data to calculate pooled effect sizes to compare treatments. This is done because major doubt has been cast over the clinical relevance of statistically significant findings relying on p values with the potential to report chance findings. Hence in an aim to overcome this pooling the results of clinical studies into a meta-analyses with a statistical calculus has been assumed to be a more definitive way of deciding of efficacy. Methods: We simulate the therapeutic effects as measured with additive scales in patient cohorts with different disease severity and assess the limitations of an effect size calculation of additive scales which are proven mathematically. Results: We demonstrate that the major problem, which cannot be overcome by current numerical methods, is the complex nature and neurobiological foundation of clinical psychiatric endpoints in particular and additive scales in general. This is particularly relevant for endpoints used in dementia research. `Cognition' is composed of functions such as memory, attention, orientation and many more. These individual functions decline in varied and non-linear ways. Here we demonstrate that with progressive diseases cumulative values from multidimensional scales are subject to distortion by the limitations of the additive scale. The non-linearity of the decline of function impedes the calculation of effect sizes based on cumulative values from these multidimensional scales. Conclusions: Statistical analysis needs to be guided by boundaries of the biological condition. Alternatively, we suggest a different approach avoiding the error imposed by over-analysis of cumulative global scores from additive scales.},
  file = {/Users/rritaz/Zotero/storage/AFQHDBNR/Riepe et al. - 2011 - Additive scales in degenerative disease - calculat.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,physical/biological fields},
  language = {en},
  number = {1}
}

@article{riley_bivariate_2007,
  title = {Bivariate Random-Effects Meta-Analysis and the Estimation of between-Study Correlation},
  author = {Riley, Richard D and Abrams, Keith R and Sutton, Alexander J and Lambert, Paul C and Thompson, John R},
  year = {2007},
  month = dec,
  volume = {7},
  pages = {3},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-7-3},
  abstract = {Background: When multiple endpoints are of interest in evidence synthesis, a multivariate meta-analysis can jointly synthesise those endpoints and utilise their correlation. A multivariate random-effects metaanalysis must incorporate and estimate the between-study correlation ({$\rho$}B). Methods: In this paper we assess maximum likelihood estimation of a general normal model and a generalised model for bivariate random-effects meta-analysis (BRMA). We consider two applied examples, one involving a diagnostic marker and the other a surrogate outcome. These motivate a simulation study where estimation properties from BRMA are compared with those from two separate univariate randomeffects meta-analyses (URMAs), the traditional approach. Results: The normal BRMA model estimates {$\rho$}B as -1 in both applied examples. Analytically we show this is due to the maximum likelihood estimator sensibly truncating the between-study covariance matrix on the boundary of its parameter space. Our simulations reveal this commonly occurs when the number of studies is small or the within-study variation is relatively large; it also causes upwardly biased betweenstudy variance estimates, which are inflated to compensate for the restriction on {$\rho$}\textasciicircum{} B. Importantly, this does not induce any systematic bias in the pooled estimates and produces conservative standard errors and mean-square errors. Furthermore, the normal BRMA is preferable to two normal URMAs; the meansquare error and standard error of pooled estimates is generally smaller in the BRMA, especially given data missing at random. For meta-analysis of proportions we then show that a generalised BRMA model is better still. This correctly uses a binomial rather than normal distribution, and produces better estimates than the normal BRMA and also two generalised URMAs; however the model may sometimes not converge due to difficulties estimating {$\rho$}B. Conclusion: A BRMA model offers numerous advantages over separate univariate synthesises; this paper highlights some of these benefits in both a normal and generalised modelling framework, and examines the estimation of between-study correlation to aid practitioners.},
  file = {/Users/rritaz/Zotero/storage/JH8DDK3F/Riley et al. - 2007 - Bivariate random-effects meta-analysis and the est.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {1}
}

@article{riley_evaluation_2007,
  title = {An Evaluation of Bivariate Random-Effects Meta-Analysis for the Joint Synthesis of Two Correlated Outcomes},
  author = {Riley, R. D. and Abrams, K. R. and Lambert, P. C. and Sutton, A. J. and Thompson, J. R.},
  year = {2007},
  volume = {26},
  pages = {78--97},
  issn = {1097-0258},
  doi = {10.1002/sim.2524},
  abstract = {Often multiple outcomes are of interest in each study identified by a systematic review, and in this situation a separate univariate meta-analysis is usually applied to synthesize the evidence for each outcome independently; an alternative approach is a single multivariate meta-analysis model that utilizes any correlation between outcomes and obtains all the pooled estimates jointly. Surprisingly, multivariate meta-analysis is rarely considered in practice, so in this paper we illustrate the benefits and limitations of the approach to provide helpful insight for practitioners. We compare a bivariate random-effects meta-analysis (BRMA) to two independent univariate random-effects meta-analyses (URMA), and show how and why a BRMA is able to `borrow strength' across outcomes. Then, on application to two examples in healthcare, we show: (i) given complete data for both outcomes in each study, BRMA is likely to produce individual pooled estimates with very similar standard errors to those from URMA; (ii) given some studies where one of the outcomes is missing at random, the `borrowing of strength' is likely to allow BRMA to produce individual pooled estimates with noticeably smaller standard errors than those from URMA; (iii) for either complete data or missing data, BRMA will produce a more appropriate standard error of the pooled difference between outcomes as it incorporates their correlation, which is not possible using URMA; and (iv) despite its advantages, BRMA may often not be possible due to the difficulty in obtaining the within-study correlations required to fit the model. Bivariate meta-regression and further research priorities are also discussed. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/9924FN28/Riley et al. - 2007 - An evaluation of bivariate random-effects meta-ana.pdf;/Users/rritaz/Zotero/storage/TKN2IBW8/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects},
  language = {en},
  number = {1}
}

@article{riley_meta-analysis_2008,
  title = {Meta-Analysis of Continuous Outcomes Combining Individual Patient Data and Aggregate Data},
  author = {Riley, Richard D. and Lambert, Paul C. and Staessen, Jan A. and Wang, Jiguang and Gueyffier, Francois and Thijs, Lutgarde and Boutitie, Florent},
  year = {2008},
  volume = {27},
  pages = {1870--1893},
  issn = {1097-0258},
  doi = {10.1002/sim.3165},
  abstract = {Meta-analysis of individual patient data (IPD) is the gold-standard for synthesizing evidence across clinical studies. However, for some studies IPD may not be available and only aggregate data (AD), such as a treatment effect estimate and its standard error, may be obtained. In this situation, methods for combining IPD and AD are important to utilize all the available evidence. In this paper, we develop and assess a range of statistical methods for combining IPD and AD in meta-analysis of continuous outcomes from randomized controlled trials. The methods take either a one-step or a two-step approach. The latter is simple, with IPD reduced to AD so that standard AD meta-analysis techniques can be employed. The one-step approach is more complex but offers a flexible framework to include both patient-level and trial-level parameters. It uses a dummy variable to distinguish IPD trials from AD trials and to constrain which parameters the AD trials estimate. We show that this is important when assessing how patient-level covariates modify treatment effect, as aggregate-level relationships across trials are subject to ecological bias and confounding. We thus develop models to separate within-trial and across-trials treatment\textendash covariate interactions; this ensures that only IPD trials estimate the former, whilst both IPD and AD trials estimate the latter in addition to the pooled treatment effect and any between-study heterogeneity. Extension to multiple correlated outcomes is also considered. Ten IPD trials in hypertension, with blood pressure the continuous outcome of interest, are used to assess the models and identify the benefits of utilizing AD alongside IPD. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VA8MR76V/Riley et al. - 2008 - Meta-analysis of continuous outcomes combining ind.pdf;/Users/rritaz/Zotero/storage/KJR3EK7N/sim.html},
  journal = {Statistics in Medicine},
  keywords = {continuous effect sizes,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {11}
}

@article{riley_meta-analysis_2008-1,
  title = {Meta-Analysis of Diagnostic Test Studies Using Individual Patient Data and Aggregate Data},
  author = {Riley, Richard D. and Dodd, Susanna R. and Craig, Jean V. and Thompson, John R. and Williamson, Paula R.},
  year = {2008},
  volume = {27},
  pages = {6111--6136},
  issn = {1097-0258},
  doi = {10.1002/sim.3441},
  abstract = {A meta-analysis of diagnostic test studies provides evidence-based results regarding the accuracy of a particular test, and usually involves synthesizing aggregate data (AD) from each study, such as the 2 by 2 tables of diagnostic accuracy. A bivariate random-effects meta-analysis (BRMA) can appropriately synthesize these tables, and leads to clinical results, such as the summary sensitivity and specificity across studies. However, translating such results into practice may be limited by between-study heterogeneity and that they relate to some `average' patient across studies. In this paper we describe how the meta-analysis of individual patient data (IPD) from diagnostic studies can lead to clinical results more tailored to the individual patient. We develop IPD models that extend the BRMA framework to include study-level covariates, which help explain the between-study heterogeneity, and also patient-level covariates, which allow one to assess the effect of patient characteristics on test accuracy. We show how the inclusion of patient-level covariates requires a careful separation of within-study and across-study accuracy-covariate effects, as the latter are particularly prone to confounding. Our models are assessed through simulation and extended to allow IPD studies to be combined with AD studies, as IPD are not always available for all studies. Application is made to 23 studies assessing the accuracy of ear thermometers for diagnosing fever in children, with 16 IPD and 7 AD studies. The models reveal that between-study heterogeneity is partly explained by the use of different measurement devices, but there is no evidence that being an infant modifies diagnostic accuracy. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NYRKK5XM/Riley et al. - 2008 - Meta-analysis of diagnostic test studies using ind.pdf;/Users/rritaz/Zotero/storage/9FX5VWU7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),multivariate},
  language = {en},
  number = {29}
}

@article{riley_meta-analysis_2010,
  title = {Meta-Analysis of a Binary Outcome Using Individual Participant Data and Aggregate Data},
  author = {Riley, Richard D. and Steyerberg, Ewout W.},
  year = {2010},
  volume = {1},
  pages = {2--19},
  issn = {1759-2887},
  doi = {10.1002/jrsm.4},
  abstract = {In this paper, we develop meta-analysis models that synthesize a binary outcome from health-care studies while accounting for participant-level covariates. In particular, we show how to synthesize the observed event-risk across studies while accounting for the within-study association between participant-level covariates and individual event probability. The models are adapted for situations where studies provide individual participant data (IPD), or a mixture of IPD and aggregate data. We show that the availability of IPD is crucial in at least some studies; this allows one to model potentially complex within-study associations and separate them from across-study associations, so as to account for potential ecological bias and study-level confounding. The models can produce pertinent population-level and individual-level results, such as the pooled event-risk and the covariate-specific event probability for an individual. Application is made to 14 studies of traumatic brain injury, where IPD are available for four studies and the six-month mortality risk is synthesized in relation to individual age. The results show that as individual age increases the probability of six-month mortality also increases; further, the models reveal clear evidence of ecological bias, with the mean age in each study additionally influencing an individual's mortality probability. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/XWRNI9S3/Riley and Steyerberg - 2010 - Meta-analysis of a binary outcome using individual.pdf;/Users/rritaz/Zotero/storage/YFUCHJA4/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,discrete effect sizes,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {1}
}

@article{riley_meta-analysis_2013,
  title = {Meta-Analysis of Randomised Trials with a Continuous Outcome According to Baseline Imbalance and Availability of Individual Participant Data},
  author = {Riley, Richard D. and Kauser, Iram and Bland, Martin and Thijs, Lutgarde and Staessen, Jan A. and Wang, Jiguang and Gueyffier, Francois and Deeks, Jonathan J.},
  year = {2013},
  volume = {32},
  pages = {2747--2766},
  issn = {1097-0258},
  doi = {10.1002/sim.5726},
  abstract = {We describe methods for meta-analysis of randomised trials where a continuous outcome is of interest, such as blood pressure, recorded at both baseline (pre treatment) and follow-up (post treatment). We used four examples for illustration, covering situations with and without individual participant data (IPD) and with and without baseline imbalance between treatment groups in each trial.Given IPD, meta-analysts can choose to synthesise treatment effect estimates derived using analysis of covariance (ANCOVA), a regression of just final scores, or a regression of the change scores. When there is baseline balance in each trial, treatment effect estimates derived using ANCOVA are more precise and thus preferred. However, we show that meta-analysis results for the summary treatment effect are similar regardless of the approach taken. Thus, without IPD, if trials are balanced, reviewers can happily utilise treatment effect estimates derived from any of the approaches.However, when some trials have baseline imbalance, meta-analysts should use treatment effect estimates derived from ANCOVA, as this adjusts for imbalance and accounts for the correlation between baseline and follow-up; we show that the other approaches can give substantially different meta-analysis results. Without IPD and with unavailable ANCOVA estimates, reviewers should limit meta-analyses to those trials with baseline balance. Trowman's method to adjust for baseline imbalance without IPD performs poorly in our examples and so is not recommended.Finally, we extend the ANCOVA model to estimate the interaction between treatment effect and baseline values and compare options for estimating this interaction given only aggregate data. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CT45PTVW/Riley et al. - 2013 - Meta-analysis of randomised trials with a continuo.pdf;/Users/rritaz/Zotero/storage/ST2SDPUM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {continuous effect sizes,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {16}
}

@article{riley_multivariate_2015,
  title = {Multivariate Meta-Analysis of Prognostic Factor Studies with Multiple Cut-Points and/or Methods of Measurement},
  author = {Riley, Richard D. and Elia, Eleni G. and Malin, Gemma and Hemming, Karla and Price, Malcolm P.},
  year = {2015},
  volume = {34},
  pages = {2481--2496},
  issn = {1097-0258},
  doi = {10.1002/sim.6493},
  abstract = {A prognostic factor is any measure that is associated with the risk of future health outcomes in those with existing disease. Often, the prognostic ability of a factor is evaluated in multiple studies. However, meta-analysis is difficult because primary studies often use different methods of measurement and/or different cut-points to dichotomise continuous factors into `high' and `low' groups; selective reporting is also common. We illustrate how multivariate random effects meta-analysis models can accommodate multiple prognostic effect estimates from the same study, relating to multiple cut-points and/or methods of measurement. The models account for within-study and between-study correlations, which utilises more information and reduces the impact of unreported cut-points and/or measurement methods in some studies. The applicability of the approach is improved with individual participant data and by assuming a functional relationship between prognostic effect and cut-point to reduce the number of unknown parameters. The models provide important inferential results for each cut-point and method of measurement, including the summary prognostic effect, the between-study variance and a 95\% prediction interval for the prognostic effect in new populations. Two applications are presented. The first reveals that, in a multivariate meta-analysis using published results, the Apgar score is prognostic of neonatal mortality but effect sizes are smaller at most cut-points than previously thought. In the second, a multivariate meta-analysis of two methods of measurement provides weak evidence that microvessel density is prognostic of mortality in lung cancer, even when individual participant data are available so that a continuous prognostic trend is examined (rather than cut-points). \textcopyright{} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/FCAPZBJZ/Riley et al. - 2015 - Multivariate meta-analysis of prognostic factor st.pdf;/Users/rritaz/Zotero/storage/KHVZGBFI/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate},
  language = {en},
  number = {17}
}

@article{riley_multivariate_2015-1,
  title = {Multivariate Meta-Analysis Using Individual Participant Data},
  author = {Riley, R. D. and Price, M. J. and Jackson, D. and Wardle, M. and Gueyffier, F. and Wang, J. and Staessen, J. A. and White, I. R.},
  year = {2015},
  volume = {6},
  pages = {157--174},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1129},
  abstract = {When combining results across related studies, a multivariate meta-analysis allows the joint synthesis of correlated effect estimates from multiple outcomes. Joint synthesis can improve efficiency over separate univariate syntheses, may reduce selective outcome reporting biases, and enables joint inferences across the outcomes. A common issue is that within-study correlations needed to fit the multivariate model are unknown from published reports. However, provision of individual participant data (IPD) allows them to be calculated directly. Here, we illustrate how to use IPD to estimate within-study correlations, using a joint linear regression for multiple continuous outcomes and bootstrapping methods for binary, survival and mixed outcomes. In a meta-analysis of 10 hypertension trials, we then show how these methods enable multivariate meta-analysis to address novel clinical questions about continuous, survival and binary outcomes; treatment\textendash covariate interactions; adjusted risk/prognostic factor effects; longitudinal data; prognostic and multiparameter models; and multiple treatment comparisons. Both frequentist and Bayesian approaches are applied, with example software code provided to derive within-study correlations and to fit the models. \textcopyright{} 2014 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2ICZTI4U/Riley et al. - 2015 - Multivariate meta-analysis using individual partic.pdf;/Users/rritaz/Zotero/storage/9Y4X3ZYD/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,correlated effects,Individual Patient Data IPD,multivariate},
  language = {en},
  number = {2}
}

@article{riley_summarising_2015,
  title = {Summarising and Validating Test Accuracy Results across Multiple Studies for Use in Clinical Practice},
  author = {Riley, Richard D. and Ahmed, Ikhlaaq and Debray, Thomas P. A. and Willis, Brian H. and Noordzij, J. Pieter and Higgins, Julian P. T. and Deeks, Jonathan J.},
  year = {2015},
  volume = {34},
  pages = {2081--2103},
  issn = {1097-0258},
  doi = {10.1002/sim.6471},
  abstract = {Following a meta-analysis of test accuracy studies, the translation of summary results into clinical practice is potentially problematic. The sensitivity, specificity and positive (PPV) and negative (NPV) predictive values of a test may differ substantially from the average meta-analysis findings, because of heterogeneity. Clinicians thus need more guidance: given the meta-analysis, is a test likely to be useful in new populations, and if so, how should test results inform the probability of existing disease (for a diagnostic test) or future adverse outcome (for a prognostic test)? We propose ways to address this. Firstly, following a meta-analysis, we suggest deriving prediction intervals and probability statements about the potential accuracy of a test in a new population. Secondly, we suggest strategies on how clinicians should derive post-test probabilities (PPV and NPV) in a new population based on existing meta-analysis results and propose a cross-validation approach for examining and comparing their calibration performance. Application is made to two clinical examples. In the first example, the joint probability that both sensitivity and specificity will be {$>$}80\% in a new population is just 0.19, because of a low sensitivity. However, the summary PPV of 0.97 is high and calibrates well in new populations, with a probability of 0.78 that the true PPV will be at least 0.95. In the second example, post-test probabilities calibrate better when tailored to the prevalence in the new population, with cross-validation revealing a probability of 0.97 that the observed NPV will be within 10\% of the predicted NPV. \textcopyright{} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/DSFDW29E/Riley et al. - 2015 - Summarising and validating test accuracy results a.pdf;/Users/rritaz/Zotero/storage/3XSCFWYG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,prediction intervals,random-effects},
  language = {en},
  number = {13}
}

@article{rodriguez_meta-analysis_2006,
  title = {Meta-Analysis of Coefficient Alpha.},
  author = {Rodriguez, Michael C. and Maeda, Yukiko},
  year = {2006},
  volume = {11},
  pages = {306--322},
  doi = {10.1037/1082-989X.11.3.306},
  abstract = {The meta-analysis of coefficient alpha across many studies is becoming more common in psychology by a methodology labeled reliability generalization. Existing reliability generalization studies have not used the sampling distribution of coefficient alpha for precision weighting and other common meta-analytic procedures. A framework is provided for a statistically grounded meta-analysis of coefficient alpha using its sampling distribution. Two empirical examples are offered to illustrate these methods, and limitations of reliability generalization are described.},
  journal = {Psychological methods},
  keywords = {continuous effect sizes,correlation coefficients,GLM MA models,random-effects},
  number = {3}
}

@article{rodriguez-barranco_standardizing_2017,
  title = {Standardizing Effect Size from Linear Regression Models with Log-Transformed Variables for Meta-Analysis},
  author = {{Rodr{\'i}guez-Barranco}, Miguel and Tob{\'i}as, Aurelio and Redondo, Daniel and {Molina-Portillo}, Elena and S{\'a}nchez, Mar{\'i}a Jos{\'e}},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {44},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0322-8},
  abstract = {Background: Meta-analysis is very useful to summarize the effect of a treatment or a risk factor for a given disease. Often studies report results based on log-transformed variables in order to achieve the principal assumptions of a linear regression model. If this is the case for some, but not all studies, the effects need to be homogenized. Methods: We derived a set of formulae to transform absolute changes into relative ones, and vice versa, to allow including all results in a meta-analysis. We applied our procedure to all possible combinations of log-transformed independent or dependent variables. We also evaluated it in a simulation based on two variables either normally or asymmetrically distributed. Results: In all the scenarios, and based on different change criteria, the effect size estimated by the derived set of formulae was equivalent to the real effect size. To avoid biased estimates of the effect, this procedure should be used with caution in the case of independent variables with asymmetric distributions that significantly differ from the normal distribution. We illustrate an application of this procedure by an application to a meta-analysis on the potential effects on neurodevelopment in children exposed to arsenic and manganese. Conclusions: The procedure proposed has been shown to be valid and capable of expressing the effect size of a linear regression model based on different change criteria in the variables. Homogenizing the results from different studies beforehand allows them to be combined in a meta-analysis, independently of whether the transformations had been performed on the dependent and/or independent variables.},
  file = {/Users/rritaz/Zotero/storage/RVMF4CST/Rodríguez-Barranco et al. - 2017 - Standardizing effect size from linear regression m.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes},
  language = {en},
  number = {1}
}

@article{roloff_planning_2013,
  title = {Planning Future Studies Based on the Conditional Power of a Meta-Analysis},
  author = {Roloff, Verena and Higgins, Julian P. T. and Sutton, Alex J.},
  year = {2013},
  volume = {32},
  pages = {11--24},
  issn = {1097-0258},
  doi = {10.1002/sim.5524},
  abstract = {Systematic reviews often provide recommendations for further research. When meta-analyses are inconclusive, such recommendations typically argue for further studies to be conducted. However, the nature and amount of future research should depend on the nature and amount of the existing research. We propose a method based on conditional power to make these recommendations more specific. Assuming a random-effects meta-analysis model, we evaluate the influence of the number of additional studies, of their information sizes and of the heterogeneity anticipated among them on the ability of an updated meta-analysis to detect a prespecified effect size. The conditional powers of possible design alternatives can be summarized in a simple graph which can also be the basis for decision making. We use three examples from the Cochrane Database of Systematic Reviews to demonstrate our strategy. We demonstrate that if heterogeneity is anticipated, it might not be possible for a single study to reach the desirable power no matter how large it is. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/B2I944Y9/Roloff et al. - 2013 - Planning future studies based on the conditional p.pdf;/Users/rritaz/Zotero/storage/YKYGBPBY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {power},
  language = {en},
  number = {1}
}

@article{rondeau_investigating_2008,
  title = {Investigating Trial and Treatment Heterogeneity in an Individual Patient Data Meta-Analysis of Survival Data by Means of the Penalized Maximum Likelihood Approach},
  author = {Rondeau, V. and Michiels, S. and Liquet, B. and Pignon, J. P.},
  year = {2008},
  volume = {27},
  pages = {1894--1910},
  issn = {1097-0258},
  doi = {10.1002/sim.3161},
  abstract = {In a meta-analysis combining survival data from different clinical trials, an important issue is the possible heterogeneity between trials. Such intertrial variation can not only be explained by heterogeneity of treatment effects across trials but also by heterogeneity of their baseline risk. In addition, one might examine the relationship between magnitude of the treatment effect and the underlying risk of the patients in the different trials. Such a scenario can be accounted for by using additive random effects in the Cox model, with a random trial effect and a random treatment-by-trial interaction. We propose to use this kind of model with a general correlation structure for the random effects and to estimate parameters and hazard function using a semi-parametric penalized marginal likelihood method (maximum penalized likelihood estimators). This approach gives smoothed estimates of the hazard function, which represents incidence in epidemiology. The idea for the approach in this paper comes from the study of heterogeneity in a large meta-analysis of randomized trials in patients with head and neck cancers (meta-analysis of chemotherapy in head and neck cancers) and the effect of adding chemotherapy to locoregional treatment. The simulation study and the application demonstrate that the proposed approach yields satisfactory results and they illustrate the need to use a flexible variance\textendash covariance structure for the random effects. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VCJY6CVP/Rondeau et al. - 2008 - Investigating trial and treatment heterogeneity in.pdf;/Users/rritaz/Zotero/storage/RSX83234/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {11}
}

@article{rosenthal_requivalent_2003,
  title = {Requivalent: {{A}} Simple Effect Size Indicator.},
  shorttitle = {Requivalent},
  author = {Rosenthal, Robert and Rubin, Donald B.},
  year = {2003},
  month = dec,
  volume = {8},
  pages = {492--496},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.8.4.492},
  file = {/Users/rritaz/Zotero/storage/WIZU9YC3/Rosenthal and Rubin - 2003 - requivalent A simple effect size indicator..pdf},
  journal = {Psychological Methods},
  keywords = {combined significance,correlation coefficients},
  language = {en},
  number = {4}
}

@article{rota_random-effects_2010,
  title = {Random-Effects Meta-Regression Models for Studying Nonlinear Dose\textendash Response Relationship, with an Application to Alcohol and Esophageal Squamous Cell Carcinoma},
  author = {Rota, Matteo and Bellocco, Rino and Scotti, Lorenza and Tramacere, Irene and Jenab, Mazda and Corrao, Giovanni and Vecchia, Carlo La and Boffetta, Paolo and Bagnardi, Vincenzo},
  year = {2010},
  volume = {29},
  pages = {2679--2687},
  issn = {1097-0258},
  doi = {10.1002/sim.4041},
  abstract = {A fundamental challenge in meta-analyses of published epidemiological dose\textendash response data is the estimate of the function describing how the risk of disease varies across different levels of a given exposure. Issues in trend estimate include within studies variability, between studies heterogeneity, and nonlinear trend components. We present a method, based on a two-step process, that addresses simultaneously these issues. First, two-term fractional polynomial models are fitted within each study included in the meta-analysis, taking into account the correlation between the reported estimates for different exposure levels. Second, the pooled dose\textendash response relationship is estimated considering the between studies heterogeneity, using a bivariate random-effects model. This method is illustrated by a meta-analysis aimed to estimate the shape of the dose\textendash response curve between alcohol consumption and esophageal squamous cell carcinoma (SCC). Overall, 14 case\textendash control studies and one cohort study, including 3000 cases of esophageal SCC, were included. The meta-analysis provided evidence that ethanol intake was related to esophageal SCC risk in a nonlinear fashion. High levels of alcohol consumption resulted in a substantial risk of esophageal SCC as compared to nondrinkers. However, a statistically significant excess risk for moderate and intermediate doses of alcohol was also observed, with no evidence of a threshold effect. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EE58L6FT/Rota et al. - 2010 - Random-effects meta-regression models for studying.pdf;/Users/rritaz/Zotero/storage/CYQQZKPB/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),multivariate,random-effects},
  language = {en},
  number = {26}
}

@article{rotondi_evidence-based_2012,
  title = {Evidence-Based Sample Size Estimation Based upon an Updated Meta-Regression Analysis},
  author = {Rotondi, Michael A. and Donner, Allan and Koval, John J.},
  year = {2012},
  volume = {3},
  pages = {269--284},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1055},
  abstract = {A traditional meta-analysis examines the overall effectiveness of an intervention by producing a pooled estimate of treatment efficacy. In contrast to this, a meta-regression model seeks to determine whether a study-level covariate (X) is a plausible source of heterogeneity in a set of treatment effects. Upon performing such an analysis, the results may suggest the presence of a meaningful amount of variation in the treatment effects because of the covariate; however, the current set of trials may not provide sufficient statistical power for such a conclusion. The proposed approach provides quantitative insight into the amount of support that a new trial may provide to the hypothesis that X is a meaningful source of variation in an updated meta-regression model, which includes both the previously completed and the proposed trial. This empirical algorithm allows examination of the potential feasibility of a planned study of various sizes to further support or refute the hypothesis that X is a statistically significant source of variation. A detailed example illustrates the sample size estimation algorithm for both a planned individually or cluster randomized trial to investigate the now commonly accepted impact of geographical latitude on the observed effectiveness of the Bacillus Calmette-Gu\'erin vaccine in the prevention of tuberculosis. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NLLCUNLA/Rotondi et al. - 2012 - Evidence-based sample size estimation based upon a.pdf;/Users/rritaz/Zotero/storage/NBYZQQIV/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {modeling effect size variation (covariates),power},
  language = {en},
  number = {4}
}

@article{rouder_beyond_2019,
  title = {Beyond Overall Effects: {{A Bayesian}} Approach to Finding Constraints in Meta-Analysis},
  shorttitle = {Beyond Overall Effects},
  author = {Rouder, Jeffrey N. and Haaf, Julia M. and {Davis-Stober}, Clintin P. and Hilgard, Joseph},
  year = {2019},
  month = oct,
  volume = {24},
  pages = {606--621},
  issn = {1082-989X},
  doi = {10.1037/met0000216},
  abstract = {Most meta-analyses focus on the behavior of meta-analytic means. In many cases, however, this mean is difficult to defend as a construct because the underlying distribution of studies reflects many factors, including how we as researchers choose to design studies. We present an alternative goal for meta-analysis. The analyst may ask about relations that are stable across all the studies. In a typical meta-analysis, there is a hypothesized direction (e.g., that violent video games increase, rather than decrease, aggressive behavior). We ask whether all studies in a meta-analysis have true effects in the hypothesized direction. If so, this is an example of a stable relation across all the studies. We propose 4 models: (a) all studies are truly null; (b) all studies share a single true nonzero effect; (c) studies differ, but all true effects are in the same direction; and (d) some study effects are truly positive, whereas others are truly negative. We develop Bayes factor model comparison for these models and apply them to 4 extant meta-analyses to show their usefulness. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {bayesian,GLM MA models,random-effects},
  number = {5}
}

@article{rover_hartung-knapp-sidik-jonkman_2015,
  title = {Hartung-{{Knapp}}-{{Sidik}}-{{Jonkman}} Approach and Its Modification for Random-Effects Meta-Analysis with Few Studies},
  author = {R{\"o}ver, Christian and Knapp, Guido and Friede, Tim},
  year = {2015},
  month = nov,
  volume = {15},
  pages = {99},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0091-1},
  abstract = {Random-effects meta-analysis is commonly performed by first deriving an estimate of the between-study variation, the heterogeneity, and subsequently using this as the basis for combining results, i.e., for estimating the effect, the figure of primary interest. The heterogeneity variance estimate however is commonly associated with substantial uncertainty, especially in contexts where there are only few studies available, such as in small populations and rare diseases.},
  file = {/Users/rritaz/Zotero/storage/DI9GFD65/Röver et al. - 2015 - Hartung-Knapp-Sidik-Jonkman approach and its modif.pdf;/Users/rritaz/Zotero/storage/YTVA3Y7V/s12874-015-0091-1.html},
  journal = {BMC Medical Research Methodology},
  number = {1}
}

@article{rover_hartung-knapp-sidik-jonkman_2015-1,
  title = {Hartung-{{Knapp}}-{{Sidik}}-{{Jonkman}} Approach and Its Modification for Random-Effects Meta-Analysis with Few Studies},
  author = {R{\"o}ver, Christian and Knapp, Guido and Friede, Tim},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {99},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0091-1},
  abstract = {Background: Random-effects meta-analysis is commonly performed by first deriving an estimate of the between-study variation, the heterogeneity, and subsequently using this as the basis for combining results, i.e., for estimating the effect, the figure of primary interest. The heterogeneity variance estimate however is commonly associated with substantial uncertainty, especially in contexts where there are only few studies available, such as in small populations and rare diseases. Methods: Confidence intervals and tests for the effect may be constructed via a simple normal approximation, or via a Student-t distribution, using the Hartung-Knapp-Sidik-Jonkman (HKSJ) approach, which additionally uses a refined estimator of variance of the effect estimator. The modified Knapp-Hartung method (mKH) applies an ad hoc correction and has been proposed to prevent counterintuitive effects and to yield more conservative inference. We performed a simulation study to investigate the behaviour of the standard HKSJ and modified mKH procedures in a range of circumstances, with a focus on the common case of meta-analysis based on only a few studies. Results: The standard HKSJ procedure works well when the treatment effect estimates to be combined are of comparable precision, but nominal error levels are exceeded when standard errors vary considerably between studies (e.g. due to variations in study size). Application of the modification on the other hand yields more conservative results with error rates closer to the nominal level. Differences are most pronounced in the common case of few studies of varying size or precision. Conclusions: Use of the modified mKH procedure is recommended, especially when only a few studies contribute to the meta-analysis and the involved studies' precisions (standard errors) vary.},
  file = {/Users/rritaz/Zotero/storage/MZRFKKFZ/Röver et al. - 2015 - Hartung-Knapp-Sidik-Jonkman approach and its modif.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Confidence Intervals,effect size combination (small sample \& discrete),random effects models,random-effects,small meta-analysis},
  language = {en},
  number = {1}
}

@article{rover_model_2019,
  title = {Model Averaging for Robust Extrapolation in Evidence Synthesis},
  author = {R{\"o}ver, Christian and Wandel, Simon and Friede, Tim},
  year = {2019},
  volume = {38},
  pages = {674--694},
  issn = {1097-0258},
  doi = {10.1002/sim.7991},
  abstract = {Extrapolation from a source to a target, eg, from adults to children, is a promising approach to utilize external information when data are sparse. In the context of meta-analyses, one is commonly faced with a small number of studies, whereas potentially relevant additional information may also be available. Here, we describe a simple extrapolation strategy using heavy-tailed mixture priors for effect estimation in meta-analysis, which effectively results in a model-averaging technique. The described method is robust in the sense that a potential prior-data conflict, ie, a discrepancy between source and target data, is explicitly anticipated. The aim of this paper is to develop a solution for this particular application to showcase the ease of implementation by providing R code, and to demonstrate the robustness of the general approach in simulations.},
  file = {/Users/rritaz/Zotero/storage/IX9FHGVR/Röver et al. - 2019 - Model averaging for robust extrapolation in eviden.pdf;/Users/rritaz/Zotero/storage/UWW9C2U3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,effect size combination (small sample \& discrete)},
  language = {en},
  number = {4}
}

@article{rua_choice_2015,
  title = {The Choice of Prior Distribution for a Covariance Matrix in Multivariate Meta-Analysis: A Simulation Study},
  shorttitle = {The Choice of Prior Distribution for a Covariance Matrix in Multivariate Meta-Analysis},
  author = {R{\'u}a, Sandra M. Hurtado and Mazumdar, Madhu and Strawderman, Robert L.},
  year = {2015},
  volume = {34},
  pages = {4083--4104},
  issn = {1097-0258},
  doi = {10.1002/sim.6631},
  abstract = {Bayesian meta-analysis is an increasingly important component of clinical research, with multivariate meta-analysis a promising tool for studies with multiple endpoints. Model assumptions, including the choice of priors, are crucial aspects of multivariate Bayesian meta-analysis (MBMA) models. In a given model, two different prior distributions can lead to different inferences about a particular parameter. A simulation study was performed in which the impact of families of prior distributions for the covariance matrix of a multivariate normal random effects MBMA model was analyzed. Inferences about effect sizes were not particularly sensitive to prior choice, but the related covariance estimates were. A few families of prior distributions with small relative biases, tight mean squared errors, and close to nominal coverage for the effect size estimates were identified. Our results demonstrate the need for sensitivity analysis and suggest some guidelines for choosing prior distributions in this class of problems. The MBMA models proposed here are illustrated in a small meta-analysis example from the periodontal field and a medium meta-analysis from the study of stroke. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/97TQCVES/Rúa et al. - 2015 - The choice of prior distribution for a covariance .pdf;/Users/rritaz/Zotero/storage/343T56CD/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,correlated effects,multivariate,random-effects},
  language = {en},
  number = {30}
}

@article{rubioaparicio_estimation_2018,
  title = {Estimation of an Overall Standardized Mean Difference in Random-Effects Meta-Analysis If the Distribution of Random Effects Departs from Normal},
  author = {Rubio-Aparicio, Mar{\'i}a and L{\'o}pez-L{\'o}pez, Jos{\'e} Antonio and S{\'a}nchez-Meca, Julio and Mar{\'i}n-Mart{\'i}nez, Fulgencio and Viechtbauer, Wolfgang and den Noortgate, Wim Van},
  year = {2018},
  volume = {9},
  pages = {489--503},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1312},
  abstract = {The random-effects model, applied in most meta-analyses nowadays, typically assumes normality of the distribution of the effect parameters. The purpose of this study was to examine the performance of various random-effects methods (standard method, Hartung's method, profile likelihood method, and bootstrapping) for computing an average effect size estimate and a confidence interval (CI) around it, when the normality assumption is not met. For comparison purposes, we also included the fixed-effect model. We manipulated a wide range of conditions, including conditions with some degree of departure from the normality assumption, using Monte Carlo simulation. To simulate realistic scenarios, we chose the manipulated conditions from a systematic review of meta-analyses on the effectiveness of psychological treatments. We compared the performance of the different methods in terms of bias and mean squared error of the average effect estimators, empirical coverage probability and width of the CIs, and variability of the standard errors. Our results suggest that random-effects methods are largely robust to departures from normality, with Hartung's profile likelihood methods yielding the best performance under suboptimal conditions.},
  file = {/Users/rritaz/Zotero/storage/ZSTYLCCV/Rubio‐Aparicio et al. - 2018 - Estimation of an overall standardized mean differe.pdf;/Users/rritaz/Zotero/storage/CMFA8BDB/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,Confidence Intervals,random effects models,random-effects},
  language = {en},
  number = {3}
}

@article{rucker_arcsine_2008,
  title = {Arcsine Test for Publication Bias in Meta-Analyses with Binary Outcomes},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James},
  year = {2008},
  volume = {27},
  pages = {746--763},
  issn = {1097-0258},
  doi = {10.1002/sim.2971},
  abstract = {In meta-analyses, it sometimes happens that smaller trials show different, often larger, treatment effects. One possible reason for such `small study effects' is publication bias. This is said to occur when the chance of a smaller study being published is increased if it shows a stronger effect. Assuming no other small study effects, under the null hypothesis of no publication bias, there should be no association between effect size and effect precision (e.g. inverse standard error) among the trials in a meta-analysis. A number of tests for small study effects/publication bias have been developed. These use either a non-parametric test or a regression test for association between effect size and precision. However, when the outcome is binary, the effect is summarized by the log-risk ratio or log-odds ratio (log OR). Unfortunately, these measures are not independent of their estimated standard error. Consequently, established tests reject the null hypothesis too frequently. We propose new tests based on the arcsine transformation, which stabilizes the variance of binomial random variables. We report results of a simulation study under the Copas model (on the log OR scale) for publication bias, which evaluates tests so far proposed in the literature. This shows that: (i) the size of one of the new tests is comparable to those of the best existing tests, including those recently published; and (ii) among such tests it has slightly greater power, especially when the effect size is small and heterogeneity is present. Arcsine tests have additional advantages that they can include trials with zero events in both arms and that they can be very easily performed using the existing software for regression tests. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BYAQES4H/Rücker et al. - 2008 - Arcsine test for publication bias in meta-analyses.pdf;/Users/rritaz/Zotero/storage/SMKPXCNH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlation between variance and effect size,discrete effect sizes,publication bias},
  language = {en},
  number = {5}
}

@article{rucker_network_2012,
  title = {Network Meta-Analysis, Electrical Networks and Graph Theory},
  author = {R{\"u}cker, Gerta},
  year = {2012},
  volume = {3},
  pages = {312--324},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1058},
  abstract = {Network meta-analysis is an active field of research in clinical biostatistics. It aims to combine information from all randomized comparisons among a set of treatments for a given medical condition. We show how graph-theoretical methods can be applied to network meta-analysis. A meta-analytic graph consists of vertices (treatments) and edges (randomized comparisons). We illustrate the correspondence between meta-analytic networks and electrical networks, where variance corresponds to resistance, treatment effects to voltage, and weighted treatment effects to current flows. Based thereon, we then show that graph-theoretical methods that have been routinely applied to electrical networks also work well in network meta-analysis. In more detail, the resulting consistent treatment effects induced in the edges can be estimated via the Moore\textendash Penrose pseudoinverse of the Laplacian matrix. Moreover, the variances of the treatment effects are estimated in analogy to electrical effective resistances. It is shown that this method, being computationally simple, leads to the usual fixed effect model estimate when applied to pairwise meta-analysis and is consistent with published results when applied to network meta-analysis examples from the literature. Moreover, problems of heterogeneity and inconsistency, random effects modeling and including multi-armed trials are addressed. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZHITJV6W/Rücker - 2012 - Network meta-analysis, electrical networks and gra.pdf;/Users/rritaz/Zotero/storage/5LQQQI2K/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis},
  language = {en},
  number = {4}
}

@article{rucker_presenting_2014,
  title = {Presenting Simulation Results in a Nested Loop Plot},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {129},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-129},
  abstract = {Background: Statisticians investigate new methods in simulations to evaluate their properties for future real data applications. Results are often presented in a number of figures, e.g., Trellis plots. We had conducted a simulation study on six statistical methods for estimating the treatment effect in binary outcome meta-analyses, where selection bias (e.g., publication bias) was suspected because of apparent funnel plot asymmetry. We varied five simulation parameters: true treatment effect, extent of selection, event proportion in control group, heterogeneity parameter, and number of studies in meta-analysis. In combination, this yielded a total number of 768 scenarios. To present all results using Trellis plots, 12 figures were needed. Methods: Choosing bias as criterion of interest, we present a `nested loop plot', a diagram type that aims to have all simulation results in one plot. The idea was to bring all scenarios into a lexicographical order and arrange them consecutively on the horizontal axis of a plot, whereas the treatment effect estimate is presented on the vertical axis. Results: The plot illustrates how parameters simultaneously influenced the estimate. It can be combined with a Trellis plot in a so-called hybrid plot. Nested loop plots may also be applied to other criteria such as the variance of estimation. Conclusion: The nested loop plot, similar to a time series graph, summarizes all information about the results of a simulation study with respect to a chosen criterion in one picture and provides a suitable alternative or an addition to Trellis plots.},
  file = {/Users/rritaz/Zotero/storage/4MTL52YK/Rücker and Schwarzer - 2014 - Presenting simulation results in a nested loop plo.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques},
  language = {en},
  number = {1}
}

@article{rucker_ranking_2015,
  title = {Ranking Treatments in Frequentist Network Meta-Analysis Works without Resampling Methods},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {58},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0060-8},
  abstract = {Background: Network meta-analysis is used to compare three or more treatments for the same condition. Within a Bayesian framework, for each treatment the probability of being best, or, more general, the probability that it has a certain rank can be derived from the posterior distributions of all treatments. The treatments can then be ranked by the surface under the cumulative ranking curve (SUCRA). For comparing treatments in a network meta-analysis, we propose a frequentist analogue to SUCRA which we call P-score that works without resampling. Methods: P-scores are based solely on the point estimates and standard errors of the frequentist network meta-analysis estimates under normality assumption and can easily be calculated as means of one-sided p-values. They measure the mean extent of certainty that a treatment is better than the competing treatments. Results: Using case studies of network meta-analysis in diabetes and depression, we demonstrate that the numerical values of SUCRA and P-Score are nearly identical. Conclusions: Ranking treatments in frequentist network meta-analysis works without resampling. Like the SUCRA values, P-scores induce a ranking of all treatments that mostly follows that of the point estimates, but takes precision into account. However, neither SUCRA nor P-score offer a major advantage compared to looking at credible or confidence intervals.},
  file = {/Users/rritaz/Zotero/storage/QGH987W3/Rücker and Schwarzer - 2015 - Ranking treatments in frequentist network meta-ana.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {network meta-analysis},
  language = {en},
  number = {1}
}

@article{rucker_reduce_2014,
  title = {Reduce Dimension or Reduce Weights? {{Comparing}} Two Approaches to Multi-Arm Studies in Network Meta-Analysis},
  shorttitle = {Reduce Dimension or Reduce Weights?},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2014},
  volume = {33},
  pages = {4353--4369},
  issn = {1097-0258},
  doi = {10.1002/sim.6236},
  abstract = {Network meta-analysis is a statistical method combining information from randomised trials that compare two or more treatments for a given medical condition. Consistent treatment effects are estimated for all possible treatment comparisons. For estimation, weighted least squares regression that in a natural way generalises standard pairwise meta-analysis can be used. Typically, as part of the network, multi-arm studies are found. In a multi-arm study, observed pairwise comparisons are correlated, which must be accounted for. To this aim, two methods have been proposed, a standard regression approach and a new approach coming from graph theory and based on contrast-based data (R\"ucker 2012). In the standard approach, the dimension of the design matrix is appropriately reduced until it is invertible (`reduce dimension'). In the alternative approach, the weights of comparisons coming from multi-arm studies are appropriately reduced (`reduce weights'). As it was unclear, to date, how these approaches are related to each other, we give a mathematical proof that both approaches lead to identical estimates. The `reduce weights' approach can be interpreted as the construction of a network of independent two-arm studies, which is basically equivalent to the given network with multi-arm studies. Thus, a simple random-effects model is obtained, with one additional parameter for a common heterogeneity variance. This is applied to a systematic review in depression. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/QQR87WIT/Rücker and Schwarzer - 2014 - Reduce dimension or reduce weights Comparing two .pdf;/Users/rritaz/Zotero/storage/7GBF369W/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {25}
}

@article{rucker_simpsons_2008,
  title = {Simpson's Paradox Visualized: {{The}} Example of the {{Rosiglitazone}} Meta-Analysis},
  shorttitle = {Simpson's Paradox Visualized},
  author = {R{\"u}cker, Gerta and Schumacher, Martin},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {34},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-34},
  abstract = {Background: Simpson's paradox is sometimes referred to in the areas of epidemiology and clinical research. It can also be found in meta-analysis of randomized clinical trials. However, though readers are able to recalculate examples from hypothetical as well as real data, they may have problems to easily figure where it emerges from. Method: First, two kinds of plots are proposed to illustrate the phenomenon graphically, a scatter plot and a line graph. Subsequently, these can be overlaid, resulting in a overlay plot. The plots are applied to the recent large meta-analysis of adverse effects of rosiglitazone on myocardial infarction and to an example from the literature. A large set of meta-analyses is screened for further examples. Results: As noted earlier by others, occurrence of Simpson's paradox in the meta-analytic setting, if present, is associated with imbalance of treatment arm size. This is well illustrated by the proposed plots. The rosiglitazone meta-analysis shows an effect reversion if all trials are pooled. In a sample of 157 meta-analyses, nine showed an effect reversion after pooling, though non-significant in all cases. Conclusion: The plots give insight on how the imbalance of trial arm size works as a confounder, thus producing Simpson's paradox. Readers can see why meta-analytic methods must be used and what is wrong with simple pooling.},
  file = {/Users/rritaz/Zotero/storage/C73LFE8J/Rücker and Schumacher - 2008 - Simpson's paradox visualized The example of the R.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques},
  language = {en},
  number = {1}
}

@article{rucker_summary_2010,
  title = {Summary {{ROC}} Curve Based on a Weighted {{Youden}} Index for Selecting an Optimal Cutpoint in Meta-Analysis of Diagnostic Accuracy},
  author = {R{\"u}cker, Gerta and Schumacher, Martin},
  year = {2010},
  volume = {29},
  pages = {3069--3078},
  issn = {1097-0258},
  doi = {10.1002/sim.3937},
  abstract = {Established approaches for analyzing meta-analyses of diagnostic accuracy model the bivariate distribution of the observed pairs of specificity Sp and sensitivity Se, thus accounting for across-study correlation. However, it is still a matter of debate how to define a summary ROC (SROC) curve. It was recently pointed out that the SROC curve is in principle unidentifiable if only one (Sp, Se) pair per study is known. We evaluate an alternative approach, modeling the study-specific ROC curves based on the assumption of linearity in logit space. A setting is considered in which the pair (Sp, Se) that is selected for publication in a particular study maximizes a weighted Youden index {$\lambda$}Se+(1-{$\lambda$})Sp with a given weight {$\lambda$}.This leads to a fixed slope (1-{$\lambda$})/{$\lambda$} of the ROC curve in (1-Sp, Se), equivalent to a slope of (1-{$\lambda$})Sp(1-Sp)/({$\lambda$}Se(1-Se)) for the corresponding straight line in logit space. While the slope depends on the variance ratio of the underlying distributions, the intercept is a function of the mean difference. Our approach leads in a natural way to a new, model-based proposal for a summary ROC curve. It is illustrated using an example from the literature. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SKLHQAPH/Rücker and Schumacher - 2010 - Summary ROC curve based on a weighted Youden index.pdf;/Users/rritaz/Zotero/storage/BVG9WPM2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size estimation (series)},
  language = {en},
  number = {30}
}

@article{rucker_undue_2008,
  title = {Undue Reliance on {{I2}} in Assessing Heterogeneity May Mislead},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James R and Schumacher, Martin},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {79},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-79},
  abstract = {Background: The heterogeneity statistic I2, interpreted as the percentage of variability due to heterogeneity between studies rather than sampling error, depends on precision, that is, the size of the studies included. Methods: Based on a real meta-analysis, we simulate artificially 'inflating' the sample size under the random effects model. For a given inflation factor M = 1, 2, 3,... and for each trial i, we create a Minflated trial by drawing a treatment effect estimate from the random effects model, using s 2 i /M as within-trial sampling variance. Results: As precision increases, while estimates of the heterogeneity variance {$\tau$}2 remain unchanged on average, estimates of I2 increase rapidly to nearly 100\%. A similar phenomenon is apparent in a sample of 157 meta-analyses. Conclusion: When deciding whether or not to pool treatment estimates in a meta-analysis, the yard-stick should be the clinical relevance of any heterogeneity present. {$\tau$}2, rather than I2, is the appropriate measure for this purpose.},
  file = {/Users/rritaz/Zotero/storage/ZHVC79M9/Rücker et al. - 2008 - Undue reliance on I2 in assessing heterogeneity ma.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,quantifying heterogeneity,random-effects},
  language = {en},
  number = {1}
}

@article{rucker_why_2009,
  title = {Why Add Anything to Nothing? {{The}} Arcsine Difference as a Measure of Treatment Effect in Meta-Analysis with Zero Cells},
  shorttitle = {Why Add Anything to Nothing?},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James and Olkin, Ingram},
  year = {2009},
  volume = {28},
  pages = {721--738},
  issn = {1097-0258},
  doi = {10.1002/sim.3511},
  abstract = {For clinical trials with binary endpoints there are a variety of effect measures, for example risk difference, risk ratio and odds ratio (OR). The choice of metric is not always straightforward and should reflect the clinical question. Additional issues arise if the event of interest is rare. In systematic reviews, trials with zero events in both arms are encountered and often excluded from the meta-analysis. The arcsine difference (AS) is a measure which is rarely considered in the medical literature. It appears to have considerable promise, because it handles zeros naturally, and its asymptotic variance does not depend on the event probability. This paper investigates the pros and cons of using the AS as a measure of intervention effect. We give a pictorial representation of its meaning and explore its properties in relation to other measures. Based on analytical calculation of the variance of the arcsine transformation, a more conservative variance estimate for the rare event setting is proposed. Motivated by a published meta-analysis in cardiac surgery, we examine the statistical properties of the various metrics in the rare event setting. We find the variance estimate of the AS to be more stable than that of the log-OR, even if events are rare. However, parameter estimation is biased if the groups are markedly unbalanced. Though, from a theoretical viewpoint, the AS is a natural choice, its practical use is likely to continue to be limited by its less direct interpretation. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5I2Z3XRW/Rücker et al. - 2009 - Why add anything to nothing The arcsine differenc.pdf;/Users/rritaz/Zotero/storage/5I64KG35/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size estimation (series)},
  language = {en},
  number = {5}
}

@article{rukhin_estimating_2013,
  title = {Estimating Heterogeneity Variance in Meta-Analysis},
  author = {Rukhin, Andrew L.},
  year = {2013},
  volume = {75},
  pages = {451--469},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2012.01047.x},
  abstract = {Summary. Several new estimators of the between-study variability in a heterogeneous random effects meta-analysis model are derived. One is the unbiased statistic which is locally optimal for small values of the parameter. Others are Bayes procedures within a class of quadratic statistics for a diffuse prior with different choices of the prior mean. These estimators are compared with the DerSimonian\textendash Laird procedure and the Hedges statistic in particular via the quadratic risk of the treatment effect estimator. A Monte Carlo study supports the usage of confidence intervals derived from the new estimators.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2012.01047.x},
  copyright = {Published 2012. This article is a US Government work and is in the public domain in the USA.},
  file = {/Users/rritaz/Zotero/storage/J9J2AS3R/Rukhin - 2013 - Estimating heterogeneity variance in meta-analysis.pdf;/Users/rritaz/Zotero/storage/CEHX8IFE/j.1467-9868.2012.01047.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Bayes estimators,Diffuse prior,Heteroscedasticity,Interlaboratory study,Random effects model,Unbiased estimators},
  language = {en},
  number = {3}
}

@article{ruscio_probability-based_2008,
  title = {A Probability-Based Measure of Effect Size: {{Robustness}} to Base Rates and Other Factors.},
  shorttitle = {A Probability-Based Measure of Effect Size},
  author = {Ruscio, John},
  year = {2008},
  volume = {13},
  pages = {19--30},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.13.1.19},
  abstract = {Calculating and reporting appropriate measures of effect size are becoming standard practice in psychological research. One of the most common scenarios encountered involves the comparison of 2 groups, which includes research designs that are experimental (e.g., random assignment to treatment vs. placebo conditions) and nonexperimental (e.g., testing for gender differences). Familiar measures such as the standardized mean difference (d) or the pointbiserial correlation (rpb) characterize the magnitude of the difference between groups, but these effect size measures are sensitive to a number of additional influences. For example, R. E. McGrath and G. J. Meyer (2006) showed that rpb is sensitive to sample base rates, and extending their analysis to situations of unequal variances reveals that d is, too. The probability-based measure A, the nonparametric generalization of what K. O. McGraw and S. P. Wong (1992) called the common language effect size statistic, is insensitive to base rates and more robust to several other factors (e.g., extreme scores, nonlinear transformations). In addition to its excellent generalizability across contexts, A is easy to understand and can be obtained from standard computer output or through simple hand calculations.},
  file = {/Users/rritaz/Zotero/storage/8CSFXQYT/Ruscio - 2008 - A probability-based measure of effect size Robust.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,correlation coefficients,standardized mean difference},
  language = {en},
  number = {1}
}

@article{rutter_hierarchical_2001,
  title = {A Hierarchical Regression Approach to Meta-Analysis of Diagnostic Test Accuracy Evaluations},
  author = {Rutter, Carolyn M. and Gatsonis, Constantine A.},
  year = {2001},
  volume = {20},
  pages = {2865--2884},
  issn = {1097-0258},
  doi = {10.1002/sim.942},
  abstract = {An important quality of meta-analytic models for research synthesis is their ability to account for both within- and between-study variability. Currently available meta-analytic approaches for studies of diagnostic test accuracy work primarily within a fixed-effects framework. In this paper we describe a hierarchical regression model for meta-analysis of studies reporting estimates of test sensitivity and specificity. The model allows more between- and within-study variability than fixed-effect approaches, by allowing both test stringency and test accuracy to vary across studies. It is also possible to examine the effects of study specific covariates. Estimates are computed using Markov Chain Monte Carlo simulation with publicly available software (BUGS). This estimation method allows flexibility in the choice of summary statistics. We demonstrate the advantages of this modelling approach using a recently published meta-analysis comparing three tests used to detect nodal metastasis of cervical cancer. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/J279WMTQ/Rutter and Gatsonis - 2001 - A hierarchical regression approach to meta-analysi.pdf;/Users/rritaz/Zotero/storage/ISS2SIHX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,modeling effect size variation (covariates)},
  language = {en},
  number = {19}
}

@article{sadashima_meta-analysis_2016,
  title = {Meta-Analysis of Prognostic Studies for a Biomarker with a Study-Specific Cutoff Value},
  author = {Sadashima, Eiji and Hattori, Satoshi and Takahashi, Kunihiko},
  year = {2016},
  volume = {7},
  pages = {402--419},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1201},
  abstract = {In prognostic studies, a summary statistic such as a hazard ratio is often reported between low-expression and high-expression groups of a biomarker with a study-specific cutoff value. Recently, several meta-analyses of prognostic studies have been reported, but these studies simply combined hazard ratios provided by the individual studies, overlooking the fact that the cutoff values are study-specific. We propose a method to summarize hazard ratios with study-specific cutoff values by estimating the hazard ratio for a 1-unit change of the biomarker in the underlying individual-level model. To this end, we introduce a model for a relationship between a reported log-hazard ratio for a 1-unit expected difference in the mean biomarker value between the low-expression and high-expression groups, which approximates the individual-level model, and propose to make an inference of the model by using the method for trend estimation based on grouped exposure data. Our combined estimator provides a valid interpretation if the biomarker distribution is correctly specified. We applied our proposed method to a dataset that examined the association between the biomarker Ki-67 and disease-free survival in breast cancer patients. We conducted simulation studies to examine the performance of our method. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UBFFECN9/Sadashima et al. - 2016 - Meta-analysis of prognostic studies for a biomarke.pdf;/Users/rritaz/Zotero/storage/SUYRDZ9Y/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {effect size estimation (series),physical/biological fields},
  language = {en},
  number = {4}
}

@article{salanti_bayesian_2006,
  title = {Bayesian Synthesis of Epidemiological Evidence with Different Combinations of Exposure Groups: Application to a Gene\textendash Gene\textendash Environment Interaction},
  shorttitle = {Bayesian Synthesis of Epidemiological Evidence with Different Combinations of Exposure Groups},
  author = {Salanti, Georgia and Higgins, Julian P. T. and White, Ian R.},
  year = {2006},
  volume = {25},
  pages = {4147--4163},
  issn = {1097-0258},
  doi = {10.1002/sim.2689},
  abstract = {Meta-analysis to investigate the joint effect of multiple factors in the aetiology of a disease is of increasing importance in epidemiology. This task is often challenging in practice, because studies typically concentrate on studying the effect of only one exposure, sometimes may report the interaction between two exposures, but rarely address more complex interactions that involve more than two exposures. In this paper, we develop a meta-analysis framework that combines estimates from studies of multiple exposures. A key development is an approach to combining results from studies that report information on any subset or combination of the full set of exposures. The model requires assumptions to be made about the prevalence of the specific exposures. We discuss several possible model specifications and prior distributions, including information internal and external to the meta-analysis data set, and using fixed-effect and random-effects meta-analysis assumptions. The methodology is implemented in an original meta-analysis of studies relating the risk of bladder cancer to two N-acetyltransferase genes, NAT1 and NAT2, and smoking status. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NC5BM7TD/Salanti et al. - 2006 - Bayesian synthesis of epidemiological evidence wit.pdf;/Users/rritaz/Zotero/storage/757DTPTE/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,combined significance,multivariate},
  language = {en},
  number = {24}
}

@article{salanti_bayesian_2007,
  title = {Bayesian Meta-Analysis and Meta-Regression for Gene\textendash Disease Associations and Deviations from {{Hardy}}\textendash{{Weinberg}} Equilibrium},
  author = {Salanti, Georgia and Higgins, Julian P. T. and Trikalinos, Thomas A. and Ioannidis, John P. A.},
  year = {2007},
  volume = {26},
  pages = {553--567},
  issn = {1097-0258},
  doi = {10.1002/sim.2575},
  abstract = {Violation of Hardy\textendash Weinberg equilibrium (HWE) can raise doubts about the validity of the conclusions from genetic association studies. However, for most currently performed gene\textendash disease association studies, the available tests have low power to detect deviations from HWE. We consider this issue from a meta-analysis perspective, and suggest an approach to estimate the deviation and investigate its relationship with the observed genetic effects. Different degrees of deviation from HWE have previously been proposed as a potential source of heterogeneity across studies. We present a hierarchical meta-regression model that can be applied to test this assumption, using the concept of the fixation coefficient. We re-analyse seven meta-analyses to illustrate these methods. The uncertainty in the genetic effect estimate tended to increase once the fixation coefficient was taken into account. Dependence of the genetic effect size on the deviation from HWE was found in one meta-analysis, while in the other six examples, deviations from HWE did not clearly explain between-study heterogeneity in the genetic effects. The proposed hierarchical models allow the synthesis of data across gene\textendash disease association studies with appropriate consideration of HWE issues. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZRFRQ4A9/Salanti et al. - 2007 - Bayesian meta-analysis and meta-regression for gen.pdf;/Users/rritaz/Zotero/storage/PS3AQWUC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,modeling effect size variation (covariates),physical/biological fields},
  language = {en},
  number = {3}
}

@article{salanti_evaluating_2010,
  title = {Evaluating Novel Agent Effects in Multiple-Treatments Meta-Regression},
  author = {Salanti, Georgia and Dias, Sofia and Welton, Nicky J. and Ades, A. E. and Golfinopoulos, Vassilis and Kyrgiou, Maria and Mauri, Davide and Ioannidis, John P. A.},
  year = {2010},
  volume = {29},
  pages = {2369--2383},
  issn = {1097-0258},
  doi = {10.1002/sim.4001},
  abstract = {Multiple-treatments meta-analyses are increasingly used to evaluate the relative effectiveness of several competing regimens. In some fields which evolve with the continuous introduction of new agents over time, it is possible that in trials comparing older with newer regimens the effectiveness of the latter is exaggerated. Optimism bias, conflicts of interest and other forces may be responsible for this exaggeration, but its magnitude and impact, if any, needs to be formally assessed in each case. Whereas such novelty bias is not identifiable in a pair-wise meta-analysis, it is possible to explore it in a network of trials involving several treatments. To evaluate the hypothesis of novel agent effects and adjust for them, we developed a multiple-treatments meta-regression model fitted within a Bayesian framework. When there are several multiple-treatments meta-analyses for diverse conditions within the same field/specialty with similar agents involved, one may consider either different novel agent effects in each meta-analysis or may consider the effects to be exchangeable across the different conditions and outcomes. As an application, we evaluate the impact of modelling and adjusting for novel agent effects for chemotherapy and other non-hormonal systemic treatments for three malignancies. We present the results and the impact of different model assumptions to the relative ranking of the various regimens in each network. We established that multiple-treatments meta-regression is a good method for examining whether novel agent effects are present and estimation of their magnitude in the three worked examples suggests an exaggeration of the hazard ratio by 6 per cent (2\textendash 11 per cent). Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UQUYJM5R/Salanti et al. - 2010 - Evaluating novel agent effects in multiple-treatme.pdf;/Users/rritaz/Zotero/storage/WTHT5W7U/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {23}
}

@article{salanti_meta-analysis_2008,
  title = {Meta-Analysis of Genetic Association Studies under Different Inheritance Models Using Data Reported as Merged Genotypes},
  author = {Salanti, Georgia and Higgins, Julian P. T.},
  year = {2008},
  volume = {27},
  pages = {764--777},
  issn = {1097-0258},
  doi = {10.1002/sim.2919},
  abstract = {Meta-analysis of population-based genetic association studies is often challenged by obstacles associated with the underlying inheritance model. For a simple genetic variant with two alleles, a recessive, dominant or co-dominant model is typically assumed. In the absence of a strong biological rationale for a particular inheritance model, a recently suggested inheritance-model-free approach can be implemented. To enable a flexible choice among these models, summary results from each of the three genotypes are required. Incompatibility of the data across studies because of different inheritance models is a common problem. For instance, if the underlying model is dominant, studies that have assumed the recessive model and presented the results accordingly, have so far been excluded from the meta-analysis. We show how to combine data and make inferences under any inheritance model, irrespective of the models assumed within each study and the way that data are presented. Within a Bayesian framework we describe prospective models for binary and continuous outcomes, and retrospective models for binary outcomes. The methods exploit an assumption of Hardy\textendash Weinberg equilibrium, prior information about genotype prevalence or assumption of a specific inheritance model. On application to meta-analyses of the associations between a polymorphism in the lipoprotein lipase gene and coronary heart disease or high-density lipoprotein cholesterol, we observe substantial gains in precision when there is a large proportion of studies in which different inheritance models have been assumed. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/J4HYFVI5/Salanti and Higgins - 2008 - Meta-analysis of genetic association studies under.pdf;/Users/rritaz/Zotero/storage/QN3DMRS3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,continuous effect sizes,discrete effect sizes,physical/biological fields},
  language = {en},
  number = {5}
}

@article{saltaji_influence_2018,
  title = {Influence of Blinding on Treatment Effect Size Estimate in Randomized Controlled Trials of Oral Health Interventions},
  author = {Saltaji, Humam and {Armijo-Olivo}, Susan and Cummings, Greta G. and Amin, Maryam and {da Costa}, Bruno R. and {Flores-Mir}, Carlos},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {42},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0491-0},
  abstract = {Background: Recent methodologic evidence suggests that lack of blinding in randomized trials can result in under- or overestimation of the treatment effect size. The objective of this study is to quantify the extent of bias associated with blinding in randomized controlled trials of oral health interventions. Methods: We selected all oral health meta-analyses that included a minimum of five randomized controlled trials. We extracted data, in duplicate, related to nine blinding-related criteria, namely: patient blinding, assessor blinding, care-provider blinding, investigator blinding, statistician blinding, blinding of both patients and assessors, study described as ``double blind'', blinding of patients, assessors, and care providers concurrently, and the appropriateness of blinding. We quantified the impact of bias associated with blinding on the magnitude of effect size using a two-level meta-meta-analytic approach with a random effects model to allow for intra- and inter-metaanalysis heterogeneity. Results: We identified 540 randomized controlled trials, included in 64 meta-analyses, analyzing data from 137,957 patients. We identified significantly larger treatment effect size estimates in trials that had inadequate patient blinding (difference in treatment effect size = 0.12; 95\% CI: 0.00 to 0.23), lack of blinding of both patients and assessors (difference = 0.19; 95\% CI: 0.06 to 0.32), and lack of blinding of patients, assessors, and care-providers concurrently (difference = 0.14; 95\% CI: 0.03 to 0.25). In contrast, assessor blinding (difference = 0.06; 95\% CI: -0.06 to 0.18), caregiver blinding (difference = 0.02; 95\% CI: -0.04 to 0.09), principal-investigator blinding (difference = - 0.02; 95\% CI: -0.10 to 0.06), describing a trial as ``double-blind'' (difference = 0.09; 95\% CI: -0.05 to 0.22), and lack of an appropriate method of blinding (difference = 0.06; 95\% CI: -0.06 to 0.18) were not associated with over- or underestimated treatment effect size. Conclusions: We found significant differences in treatment effect size estimates between oral health trials based on lack of patient and assessor blinding. Treatment effect size estimates were 0.19 and 0.14 larger in trials with lack of blinding of both patients and assessors and blinding of patients, assessors, and care-providers concurrently. No significant differences were identified in other blinding criteria. Investigators of oral health systematic reviews should perform sensitivity analyses based on the adequacy of blinding in included trials.},
  file = {/Users/rritaz/Zotero/storage/TX7XZ9B9/Saltaji et al. - 2018 - Influence of blinding on treatment effect size est.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {meta-meta-analyses,physical/biological fields},
  language = {en},
  number = {1}
}

@article{sanchez-meca_confidence_2008,
  title = {Confidence Intervals for the Overall Effect Size in Random-Effects Meta-Analysis.},
  author = {{S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2008},
  volume = {13},
  pages = {31--48},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.13.1.31},
  file = {/Users/rritaz/Zotero/storage/ERTRIV2Q/Sánchez-Meca and Marín-Martínez - 2008 - Confidence intervals for the overall effect size i.pdf},
  journal = {Psychological Methods},
  keywords = {Confidence Intervals,continuous effect sizes,correlation coefficients},
  language = {en},
  number = {1}
}

@article{sanchez-meca_effect-size_2003,
  title = {Effect-{{Size Indices}} for {{Dichotomized Outcomes}} in {{Meta}}-{{Analysis}}.},
  author = {{S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and {Chac{\'o}n-Moscoso}, Salvador},
  year = {2003},
  volume = {8},
  pages = {448--467},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.8.4.448},
  file = {/Users/rritaz/Zotero/storage/DGICHQ3I/Sánchez-Meca et al. - 2003 - Effect-Size Indices for Dichotomized Outcomes in M.pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,discrete effect sizes,rrita-future},
  language = {en},
  number = {4}
}

@article{sangnawakij_statistical_2017,
  title = {Statistical Methodology for Estimating the Mean Difference in a Meta-Analysis without Study-Specific Variance Information},
  author = {Sangnawakij, Patarawan and B{\"o}hning, Dankmar and Adams, Stephen and Stanton, Michael and Holling, Heinz},
  year = {2017},
  volume = {36},
  pages = {1395--1413},
  issn = {1097-0258},
  doi = {10.1002/sim.7232},
  abstract = {Statistical inference for analyzing the results from several independent studies on the same quantity of interest has been investigated frequently in recent decades. Typically, any meta-analytic inference requires that the quantity of interest is available from each study together with an estimate of its variability. The current work is motivated by a meta-analysis on comparing two treatments (thoracoscopic and open) of congenital lung malformations in young children. Quantities of interest include continuous end-points such as length of operation or number of chest tube days. As studies only report mean values (and no standard errors or confidence intervals), the question arises how meta-analytic inference can be developed. We suggest two methods to estimate study-specific variances in such a meta-analysis, where only sample means and sample sizes are available in the treatment arms. A general likelihood ratio test is derived for testing equality of variances in two groups. By means of simulation studies, the bias and estimated standard error of the overall mean difference from both methodologies are evaluated and compared with two existing approaches: complete study analysis only and partial variance information. The performance of the test is evaluated in terms of type I error. Additionally, we illustrate these methods in the meta-analysis on comparing thoracoscopic and open surgery for congenital lung malformations and in a meta-analysis on the change in renal function after kidney donation. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LCNL3EM9/Sangnawakij et al. - 2017 - Statistical methodology for estimating the mean di.pdf;/Users/rritaz/Zotero/storage/UTTIGKS4/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,random-effects},
  language = {en},
  number = {9}
}

@article{saramago_methods_2016,
  title = {Methods for Network Meta-Analysis of Continuous Outcomes Using Individual Patient Data: A Case Study in Acupuncture for Chronic Pain},
  shorttitle = {Methods for Network Meta-Analysis of Continuous Outcomes Using Individual Patient Data},
  author = {Saramago, Pedro and Woods, Beth and Weatherly, Helen and Manca, Andrea and Sculpher, Mark and Khan, Kamran and Vickers, Andrew J. and MacPherson, Hugh},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {131},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0224-1},
  abstract = {Background: Network meta-analysis methods, which are an extension of the standard pair-wise synthesis framework, allow for the simultaneous comparison of multiple interventions and consideration of the entire body of evidence in a single statistical model. There are well-established advantages to using individual patient data to perform network meta-analysis and methods for network meta-analysis of individual patient data have already been developed for dichotomous and time-to-event data. This paper describes appropriate methods for the network meta-analysis of individual patient data on continuous outcomes. Methods: This paper introduces and describes network meta-analysis of individual patient data models for continuous outcomes using the analysis of covariance framework. Comparisons are made between this approach and change score and final score only approaches, which are frequently used and have been proposed in the methodological literature. A motivating example on the effectiveness of acupuncture for chronic pain is used to demonstrate the methods. Individual patient data on 28 randomised controlled trials were synthesised. Consistency of endpoints across the evidence base was obtained through standardisation and mapping exercises. Results: Individual patient data availability avoided the use of non-baseline-adjusted models, allowing instead for analysis of covariance models to be applied and thus improving the precision of treatment effect estimates while adjusting for baseline imbalance. Conclusions: The network meta-analysis of individual patient data using the analysis of covariance approach is advocated to be the most appropriate modelling approach for network meta-analysis of continuous outcomes, particularly in the presence of baseline imbalance. Further methods developments are required to address the challenge of analysing aggregate level data in the presence of baseline imbalance.},
  file = {/Users/rritaz/Zotero/storage/27P92N78/Saramago et al. - 2016 - Methods for network meta-analysis of continuous ou.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {categorical MA models,Individual Patient Data IPD,network meta-analysis,physical/biological fields,random-effects},
  language = {en},
  number = {1}
}

@article{saramago_mixed_2012,
  title = {Mixed Treatment Comparisons Using Aggregate and Individual Participant Level Data},
  author = {Saramago, Pedro and Sutton, Alex J. and Cooper, Nicola J. and Manca, Andrea},
  year = {2012},
  volume = {31},
  pages = {3516--3536},
  issn = {1097-0258},
  doi = {10.1002/sim.5442},
  abstract = {Mixed treatment comparisons (MTC) extend the traditional pair-wise meta-analytic framework to synthesize information on more than two interventions. Although most MTCs use aggregate data (AD), a proportion of the evidence base might be available at the individual level (IPD). We develop a series of novel Bayesian statistical MTC models to allow for the simultaneous synthesis of IPD and AD, potentially incorporating study and individual level covariates. The effectiveness of different interventions to increase the provision of functioning smoke alarms in households with children was used as a motivating dataset. This included 20 studies (11 AD and 9 IPD), including 11 500 participants. Incorporating the IPD into the network allowed the inclusion of information on subject level covariates, which produced markedly more accurate treatment\textendash covariate interaction estimates than an analysis solely on the AD from all studies. Including evidence at the IPD level in the MTC is desirable when exploring participant level covariates; even when IPD is available only for a fraction of the studies. Such modelling may not only reduce inconsistencies within networks of trials but also assist the estimation of intervention subgroup effects to guide more individualised treatment decisions. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/XW3MEYUH/Saramago et al. - 2012 - Mixed treatment comparisons using aggregate and in.pdf;/Users/rritaz/Zotero/storage/2UPF69VL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,Individual Patient Data IPD,modeling effect size variation (covariates),network meta-analysis},
  language = {en},
  number = {28}
}

@article{saramago_network_2014,
  title = {Network Meta-Analysis of (Individual Patient) Time to Event Data alongside (Aggregate) Count Data},
  author = {Saramago, Pedro and Chuang, Ling-Hsiang and Soares, Marta O},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {105},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-105},
  abstract = {Background: Network meta-analysis methods extend the standard pair-wise framework to allow simultaneous comparison of multiple interventions in a single statistical model. Despite published work on network meta-analysis mainly focussing on the synthesis of aggregate data, methods have been developed that allow the use of individual patient-level data specifically when outcomes are dichotomous or continuous. This paper focuses on the synthesis of individual patient-level and summary time to event data, motivated by a real data example looking at the effectiveness of high compression treatments on the healing of venous leg ulcers. Methods: This paper introduces a novel network meta-analysis modelling approach that allows individual patient-level (time to event with censoring) and summary-level data (event count for a given follow-up time) to be synthesised jointly by assuming an underlying, common, distribution of time to healing. Alternative model assumptions were tested within the motivating example. Model fit and adequacy measures were used to compare and select models. Results: Due to the availability of individual patient-level data in our example we were able to use a Weibull distribution to describe time to healing; otherwise, we would have been limited to specifying a uniparametric distribution. Absolute effectiveness estimates were more sensitive than relative effectiveness estimates to a range of alternative specifications for the model. Conclusions: The synthesis of time to event data considering individual patient-level data provides modelling flexibility, and can be particularly important when absolute effectiveness estimates, and not just relative effect estimates, are of interest.},
  file = {/Users/rritaz/Zotero/storage/Y9LZF559/Saramago et al. - 2014 - Network meta-analysis of (individual patient) time.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {Individual Patient Data IPD,network meta-analysis},
  language = {en},
  number = {1}
}

@article{sauerbrei_new_2011,
  title = {A New Strategy for Meta-Analysis of Continuous Covariates in Observational Studies},
  author = {Sauerbrei, Willi and Royston, Patrick},
  year = {2011},
  volume = {30},
  pages = {3341--3360},
  issn = {1097-0258},
  doi = {10.1002/sim.4333},
  abstract = {When several studies are available, a meta-analytic assessment of the effect of a risk or prognostic factor on an outcome is often required. We propose a new strategy, requiring individual participant data, to provide a summary estimate of the functional relationship between a continuous covariate and the outcome in a regression model, adjusting for confounding factors. Our procedure comprises three steps. First, we determine a confounder model. Ideally, the latter should include the same variables across studies, but this may be impossible. Next, we estimate the functional form for the continuous variable of interest in each study, adjusted for the confounder model. Finally, we combine the individual functions by weighted averaging to obtain a summary estimate of the function. Fractional polynomial methodology and pointwise weighted averaging of functions are the key components. In contrast to a pooled analysis, our approach can reflect more variability between functions from different studies and more flexibility with respect to confounders. We illustrate the procedure by using data from breast cancer patients in the Surveillance, Epidemiology, and End Results Program database, where we consider data from nine individual registries as separate studies. We estimate the functional forms for the number of positive lymph nodes and age. The former is an example where a strong prognostic effect has long been recognized, whereas the prognostic effect of the latter is weak or even controversial. We further discuss some general issues that are found in meta-analyses of observational studies. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PQMJKL7U/Sauerbrei and Royston - 2011 - A new strategy for meta-analysis of continuous cov.pdf;/Users/rritaz/Zotero/storage/5UZYURS3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {28}
}

@article{schimmack_ironic_2012,
  title = {The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles},
  author = {Schimmack, Ulrich},
  year = {2012},
  pages = {551--566},
  abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power of studies has not increased, while the number of studies in a single article has increased. It has been overlooked that multiple studies with modest power have a high probability of producing nonsig-nificant results because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple-study articles undermines the credibility of the reported results, and it is likely that questionable research practices have contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple-study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with nonsignificant results if these studies have high power to replicate a published finding.},
  file = {/Users/rritaz/Zotero/storage/IJZS82SI/Schimmack - 2012 - The ironic effect of significant results on the cr.pdf;/Users/rritaz/Zotero/storage/U5IA33XD/summary.html},
  journal = {Psychological Methods},
  keywords = {meta-meta-analyses,power,publication bias}
}

@article{schmid_bayesian_2014,
  title = {Bayesian Network Meta-Analysis for Unordered Categorical Outcomes with Incomplete Data},
  author = {Schmid, Christopher H. and Trikalinos, Thomas A. and Olkin, Ingram},
  year = {2014},
  volume = {5},
  pages = {162--185},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1103},
  abstract = {We develop a Bayesian multinomial network meta-analysis model for unordered (nominal) categorical outcomes that allows for partially observed data in which exact event counts may not be known for each category. This model properly accounts for correlations of counts in mutually exclusive categories and enables proper comparison and ranking of treatment effects across multiple treatments and multiple outcome categories. We apply the model to analyze 17 trials, each of which compares two of three treatments (high and low dose statins and standard care/control) for three outcomes for which data are complete: cardiovascular death, non-cardiovascular death and no death. We also analyze the cardiovascular death category divided into the three subcategories (coronary heart disease, stroke and other cardiovascular diseases) that are not completely observed. The multinomial and network representations show that high dose statins are effective in reducing the risk of cardiovascular disease. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2X9T8CNT/Schmid et al. - 2014 - Bayesian network meta-analysis for unordered categ.pdf;/Users/rritaz/Zotero/storage/PMAT8EM5/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bayesian,discrete effect sizes,missing data,network meta-analysis},
  language = {en},
  number = {2}
}

@article{schmitz_incorporating_2013,
  title = {Incorporating Data from Various Trial Designs into a Mixed Treatment Comparison Model},
  author = {Schmitz, Susanne and Adams, Roisin and Walsh, Cathal},
  year = {2013},
  volume = {32},
  pages = {2935--2949},
  issn = {1097-0258},
  doi = {10.1002/sim.5764},
  abstract = {Estimates of relative efficacy between alternative treatments are crucial for decision making in health care. Bayesian mixed treatment comparison models provide a powerful methodology to obtain such estimates when head-to-head evidence is not available or insufficient. In recent years, this methodology has become widely accepted and applied in economic modelling of healthcare interventions. Most evaluations only consider evidence from randomized controlled trials, while information from other trial designs is ignored. In this paper, we propose three alternative methods of combining data from different trial designs in a mixed treatment comparison model. Naive pooling is the simplest approach and does not differentiate between-trial designs. Utilizing observational data as prior information allows adjusting for bias due to trial design. The most flexible technique is a three-level hierarchical model. Such a model allows for bias adjustment while also accounting for heterogeneity between-trial designs. These techniques are illustrated using an application in rheumatoid arthritis. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/73TRU5SM/Schmitz et al. - 2013 - Incorporating data from various trial designs into.pdf;/Users/rritaz/Zotero/storage/Z9MR72L2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {17}
}

@article{schmitz_use_2018,
  title = {The Use of Single Armed Observational Data to Closing the Gap in Otherwise Disconnected Evidence Networks: A Network Meta-Analysis in Multiple Myeloma},
  shorttitle = {The Use of Single Armed Observational Data to Closing the Gap in Otherwise Disconnected Evidence Networks},
  author = {Schmitz, Susanne and Maguire, {\'A}ine and Morris, James and Ruggeri, Kai and Haller, Elisa and Kuhn, Isla and Leahy, Joy and Homer, Natalia and Khan, Ayesha and Bowden, Jack and Buchanan, Vanessa and O'Dwyer, Michael and Cook, Gordon and Walsh, Cathal},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {66},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0509-7},
  abstract = {Background: Network meta-analysis (NMA) allows for the estimation of comparative effectiveness of treatments that have not been studied in head-to-head trials; however, relative treatment effects for all interventions can only be derived where available evidence forms a connected network. Head-to-head evidence is limited in many disease areas, regularly resulting in disconnected evidence structures where a large number of treatments are available. This is also the case in the evidence of treatments for relapsed or refractory multiple myeloma. Methods: Randomised controlled trials (RCTs) identified in a systematic literature review form two disconnected evidence networks. Standard Bayesian NMA models are fitted to obtain estimates of relative effects within each network. Observational evidence was identified to fill the evidence gap. Single armed trials are matched to act as each other's control group based on a distance metric derived from covariate information. Uncertainty resulting from including this evidence is incorporated by analysing the space of possible matches. Results: Twenty five randomised controlled trials form two disconnected evidence networks; 12 single armed observational studies are considered for bridging between the networks. Five matches are selected to bridge between the networks. While significant variation in the ranking is observed, daratumumab in combination with dexamethasone and either lenalidomide or bortezomib, as well as triple therapy of carfilzomib, ixazomib and elozumatab, in combination with lenalidomide and dexamethasone, show the highest effects on progression free survival, on average. Conclusions: The analysis shows how observational data can be used to fill gaps in the existing networks of RCT evidence; allowing for the indirect comparison of a large number of treatments, which could not be compared otherwise. Additional uncertainty is accounted for by scenario analyses reducing the risk of over confidence in interpretation of results.},
  file = {/Users/rritaz/Zotero/storage/EDKB5AEC/Schmitz et al. - 2018 - The use of single armed observational data to clos.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {network meta-analysis,physical/biological fields},
  language = {en},
  number = {1}
}

@article{schou_meta-analysis_2013,
  title = {Meta-Analysis of Clinical Trials with Early Stopping: An Investigation of Potential Bias},
  shorttitle = {Meta-Analysis of Clinical Trials with Early Stopping},
  author = {Schou, I. Manjula and Marschner, Ian C.},
  year = {2013},
  volume = {32},
  pages = {4859--4874},
  issn = {1097-0258},
  doi = {10.1002/sim.5893},
  abstract = {Clinical trials that stop early for benefit have a treatment difference that overestimates the true effect. The consequences of this fact have been extensively debated in the literature. Some researchers argue that early stopping, or truncation, is an important source of bias in treatment effect estimates, particularly when truncated studies are incorporated into meta-analyses. Such claims are bound to lead some systematic reviewers to consider excluding truncated studies from evidence synthesis. We therefore investigated the implications of this strategy by examining the properties of sequentially monitored studies conditional on reaching the final analysis. As well as estimation bias, we studied information bias measured by the difference between standard measures of statistical information, such as sample size, and the actual information based on the conditional sampling distribution. We found that excluding truncated studies leads to underestimation of treatment effects and overestimation of information. Importantly, the information bias increases with the estimation bias, meaning that greater estimation bias is accompanied by greater overweighting in a meta-analysis. Simulations of meta-analyses confirmed that the bias from excluding truncated studies can be substantial. In contrast, when meta-analyses included truncated studies, treatment effect estimates were essentially unbiased. Previous analyses comparing treatment effects in truncated and non-truncated studies are shown not to be indicative of bias in truncated studies. We conclude that early stopping of clinical trials is not a substantive source of bias in meta-analyses and recommend that all studies, both truncated and non-truncated, be included in evidence synthesis. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VMLT32YD/Schou and Marschner - 2013 - Meta-analysis of clinical trials with early stoppi.pdf;/Users/rritaz/Zotero/storage/LD55N2D3/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,missing data},
  language = {en},
  number = {28}
}

@article{schurmann_differences_2016,
  title = {Differences in Surrogate Threshold Effect Estimates between Original and Simplified Correlation-Based Validation Approaches},
  author = {Sch{\"u}rmann, Christoph and Sieben, Wiebke},
  year = {2016},
  volume = {35},
  pages = {1049--1062},
  issn = {1097-0258},
  doi = {10.1002/sim.6778},
  abstract = {Surrogate endpoint validation has been well established by the meta-analytical correlation-based approach as outlined in the seminal work of Buyse et al. (Biostatistics, 2000). Surrogacy can be assumed if strong associations on individual and study levels can be demonstrated. Alternatively, if an effect on a true endpoint is to be predicted from a surrogate endpoint in a new study, the surrogate threshold effect (STE, Burzykowski and Buyse, Pharmaceutical Statistics, 2006) can be used. In practice, as individual patient data (IPD) are hard to obtain, some authors use only aggregate data and perform simplified regression analyses. We are interested in to what extent such simplified analyses are biased compared with the ones from a full model with IPD. To this end, we conduct a simulation study with IPD and compute STEs from full and simplified analyses for varying data situations in terms of number of studies, correlations, variances and so on. In the scenarios considered, we show that, for normally distributed patient data, STEs derived from ordinary (weighted) linear regression generally underestimate STEs derived from the original model, whereas meta-regression often results in overestimation. Therefore, if individual data cannot be obtained, STEs from meta-regression may be used as conservative alternatives, but ordinary (weighted) linear regression should not be used for surrogate endpoint validation. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/X4A5BKEI/Schürmann and Sieben - 2016 - Differences in surrogate threshold effect estimate.pdf;/Users/rritaz/Zotero/storage/HQT8UR27/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlation coefficients,Individual Patient Data IPD,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {7}
}

@article{schwarzer_inflation_2002,
  title = {Inflation of Type {{I}} Error Rate in Two Statistical Tests for the Detection of Publication Bias in Meta-Analyses with Binary Outcomes},
  author = {Schwarzer, Guido and Antes, Gerd and Schumacher, Martin},
  year = {2002},
  volume = {21},
  pages = {2465--2477},
  issn = {1097-0258},
  doi = {10.1002/sim.1224},
  abstract = {The use of meta-analysis to combine results of several trials is still increasing in the medical field. The validity of a meta-analysis may be affected by various sources of bias (for example, publication bias, language bias). Therefore, an analysis of bias should be an integral part of any systematic review. Statistical tests and graphical methods have been developed for this purpose. In this paper, two statistical tests for the detection of bias in meta-analysis were investigated in a simulation study. Binary outcome data, which are very common in medical applications, were considered and relative effect measures (odds ratios, relative risk) were used for pooling. Sample sizes were generated according to findings in a survey of eight German medical journals. Simulation results indicate an inflation of type I error rates for both tests when the data are sparse. Results get worse with increasing treatment effect and number of trials combined. Valid statistical tests for the detection of bias in meta-analysis with sparse data need to be developed. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/AHRW7Q23/Schwarzer et al. - 2002 - Inflation of type I error rate in two statistical .pdf;/Users/rritaz/Zotero/storage/74G7AFVW/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {17}
}

@article{schwarzer_test_2007,
  title = {A Test for Publication Bias in Meta-Analysis with Sparse Binary Data},
  author = {Schwarzer, Guido and Antes, Gerd and Schumacher, Martin},
  year = {2007},
  volume = {26},
  pages = {721--733},
  issn = {1097-0258},
  doi = {10.1002/sim.2588},
  abstract = {A new test for the detection of publication bias in meta-analysis with sparse binary data is proposed. The test statistic is based on observed and expected cell frequencies, and the variance of the observed cell frequencies. These quantities are utilized in a rank correlation test. Type I error rate and power of the test are evaluated in simulations; results are compared to those of two other commonly used test procedures. Sample sizes were generated according to findings in a survey of eight German medical journals. Simulation results indicate that, in contrast to existing test procedures, the new test holds the prescribed significance level when data are sparse. However, the power of all tests is low in many situations of practical importance. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/A2LQPTY9/Schwarzer et al. - 2007 - A test for publication bias in meta-analysis with .pdf;/Users/rritaz/Zotero/storage/BPLU98JK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {power,publication bias},
  language = {en},
  number = {4}
}

@book{searle_linear_1971,
  title = {Linear Models},
  author = {Searle, S. R.},
  year = {1971},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-76950-7},
  keywords = {Analysis of variance; Estimation theory; Statistical hypothesis testing; Linear models (Statistics)},
  language = {eng},
  lccn = {519.5 S434l},
  series = {A {{Wiley}} Publication in Mathematical Statistics}
}

@book{searle_linear_2016,
  title = {Linear {{Models}}},
  author = {Searle, Shayle R. and Gruber, Marvin H. J.},
  year = {2016},
  month = oct,
  publisher = {{John Wiley \& Sons}},
  abstract = {Provides an easy-to-understand guide to statistical linear models and its uses in data analysis This book defines a broad spectrum of statistical linear models that is useful in the analysis of data. Considerable rewriting was done to make the book more reader friendly than the first edition. Linear Models, Second Edition is written in such a way as to be self-contained for a person with a background in basic statistics, calculus and linear algebra. The text includes numerous applied illustrations, numerical examples, and exercises, now augmented with computer outputs in SAS and R. Also new to this edition is: \textbullet{} A greatly improved internal design and format \textbullet{} A short introductory chapter to ease understanding of the order in which topics are taken up \textbullet{} Discussion of additional topics including multiple comparisons and shrinkage estimators \textbullet{} Enhanced discussions of generalized inverses, the MINQUE, Bayes and Maximum Likelihood estimators for estimating variance components Furthermore, in this edition, the second author adds many pedagogical elements throughout the book. These include numbered examples, end-of-example and end-of-proof symbols, selected hints and solutions to exercises available on the book's website, and references to ``big data'' in everyday life. Featuring a thorough update, Linear Models, Second Edition includes: \textbullet{} A new internal format, additional instructional pedagogy, selected hints and solutions to exercises, and several more real-life applications \textbullet{} Many examples using SAS and R with timely data sets \textbullet{} Over 400 examples and exercises throughout the book to reinforce understanding Linear Models, Second Edition is a textbook and a reference for upper-level undergraduate and beginning graduate-level courses on linear models, statisticians, engineers, and scientists who use multiple regression or analysis of variance in their work. SHAYLE R. SEARLE, PhD, was Professor Emeritus of Biometry at Cornell University. He was the author of the first edition of Linear Models, Linear Models for Unbalanced Data, and Generalized, Linear, and Mixed Models (with Charles E. McCulloch), all from Wiley. The first edition of Linear Models appears in the Wiley Classics Library. MARVIN H. J. GRUBER, PhD, is Professor Emeritus at Rochester Institute of Technology, School of Mathematical Sciences. Dr. Gruber has written a number of papers and has given numerous presentations at professional meetings during his tenure as a professor at RIT. His fields of interest include regression estimators and the improvement of their efficiency using shrinkage estimators. He has written and published two books on this topic. Another of his books, Matrix Algebra for Linear Models, also published by Wiley, provides good preparation for studying Linear Models. He is a member of the American Mathematical Society, the Institute of Mathematical Statistics and the American Statistical Association.},
  googlebooks = {53MXDQAAQBAJ},
  isbn = {978-1-118-95283-2},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  language = {en}
}

@article{seide_likelihood-based_2019,
  title = {Likelihood-Based Random-Effects Meta-Analysis with Few Studies: Empirical and Simulation Studies},
  shorttitle = {Likelihood-Based Random-Effects Meta-Analysis with Few Studies},
  author = {Seide, Svenja E. and R{\"o}ver, Christian and Friede, Tim},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {16},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0618-3},
  abstract = {Background: Standard random-effects meta-analysis methods perform poorly when applied to few studies only. Such settings however are commonly encountered in practice. It is unclear, whether or to what extent small-sample-size behaviour can be improved by more sophisticated modeling. Methods: We consider likelihood-based methods, the DerSimonian-Laird approach, Empirical Bayes, several adjustment methods and a fully Bayesian approach. Confidence intervals are based on a normal approximation, or on adjustments based on the Student-t-distribution. In addition, a linear mixed model and two generalized linear mixed models (GLMMs) assuming binomial or Poisson distributed numbers of events per study arm are considered for pairwise binary meta-analyses. We extract an empirical data set of 40 meta-analyses from recent reviews published by the German Institute for Quality and Efficiency in Health Care (IQWiG). Methods are then compared empirically as well as in a simulation study, based on few studies, imbalanced study sizes, and considering odds-ratio (OR) and risk ratio (RR) effect sizes. Coverage probabilities and interval widths for the combined effect estimate are evaluated to compare the different approaches. Results: Empirically, a majority of the identified meta-analyses include only 2 studies. Variation of methods or effect measures affects the estimation results. In the simulation study, coverage probability is, in the presence of heterogeneity and few studies, mostly below the nominal level for all frequentist methods based on normal approximation, in particular when sizes in meta-analyses are not balanced, but improve when confidence intervals are adjusted. Bayesian methods result in better coverage than the frequentist methods with normal approximation in all scenarios, except for some cases of very large heterogeneity where the coverage is slightly lower. Credible intervals are empirically and in the simulation study wider than unadjusted confidence intervals, but considerably narrower than adjusted ones, with some exceptions when considering RRs and small numbers of patients per trial-arm. Confidence intervals based on the GLMMs are, in general, slightly narrower than those from other frequentist methods. Some methods turned out impractical due to frequent numerical problems. Conclusions: In the presence of between-study heterogeneity, especially with unbalanced study sizes, caution is needed in applying meta-analytical methods to few studies, as either coverage probabilities might be compromised, or intervals are inconclusively wide. Bayesian estimation with a sensibly chosen prior for between-trial heterogeneity may offer a promising compromise.},
  file = {/Users/rritaz/Zotero/storage/EPXBYKEB/Seide et al. - 2019 - Likelihood-based random-effects meta-analysis with.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {effect size combination (small sample \& discrete),random-effects,small meta-analysis},
  language = {en},
  number = {1}
}

@article{seide_simulation_2019,
  title = {Simulation and Data-Generation for Random-Effects Network Meta-Analysis of Binary Outcome},
  author = {Seide, Svenja E. and Jensen, Katrin and Kieser, Meinhard},
  year = {2019},
  volume = {38},
  pages = {3288--3303},
  issn = {1097-0258},
  doi = {10.1002/sim.8193},
  abstract = {The performance of statistical methods is frequently evaluated by means of simulation studies. In case of network meta-analysis of binary data, however, available data-generating models (DGMs) are restricted to either inclusion of two-armed trials or the fixed-effect model. Based on data-generation in the pairwise case, we propose a framework for the simulation of random-effect network meta-analyses including multiarm trials with binary outcome. The only one of the common DGMs used in the pairwise case, which is directly applicable to a random-effects network setting uses strongly restrictive assumptions. To overcome these limitations, we modify this approach and derive a related simulation procedure using odds ratios as effect measure. The performance of this procedure is evaluated with synthetic data and in an empirical example.},
  file = {/Users/rritaz/Zotero/storage/67BR7HWD/Seide et al. - 2019 - Simulation and data-generation for random-effects .pdf;/Users/rritaz/Zotero/storage/I4UT29R8/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),network meta-analysis,random-effects},
  language = {en},
  number = {17}
}

@article{senn_trying_2007,
  title = {{Trying to be precise about vagueness}},
  author = {Senn, Stephen},
  year = {2007},
  volume = {26},
  pages = {1417--1430},
  issn = {1097-0258},
  doi = {10.1002/sim.2639},
  abstract = {A previous investigation by Lambert et al., which used computer simulation to examine the influence of choice of prior distribution on inferences from Bayesian random effects meta-analysis, is critically examined from a number of viewpoints. The practical example used is shown to be problematic. The various prior distributions are shown to be unreasonable in terms of what they imply about the joint distribution of the overall treatment effect and the random effects variance. An alternative form of prior distribution is tentatively proposed. Finally, some practical recommendations are made that stress the value both of fixed effect analyses and of frequentist approaches as well as various diagnostic investigations. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YTVPZ5PY/Senn - 2007 - Trying to be precise about vagueness.pdf;/Users/rritaz/Zotero/storage/GY9XMSUX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {fr},
  number = {7}
}

@article{sera_extended_2019,
  title = {An Extended Mixed-Effects Framework for Meta-Analysis},
  author = {Sera, Francesco and Armstrong, Benedict and Blangiardo, Marta and Gasparrini, Antonio},
  year = {2019},
  volume = {38},
  pages = {5429--5444},
  issn = {1097-0258},
  doi = {10.1002/sim.8362},
  abstract = {Standard methods for meta-analysis are limited to pooling tasks in which a single effect size is estimated from a set of independent studies. However, this setting can be too restrictive for modern meta-analytical applications. In this contribution, we illustrate a general framework for meta-analysis based on linear mixed-effects models, where potentially complex patterns of effect sizes are modeled through an extended and flexible structure of fixed and random terms. This definition includes, as special cases, a variety of meta-analytical models that have been separately proposed in the literature, such as multivariate, network, multilevel, dose-response, and longitudinal meta-analysis and meta-regression. The availability of a unified framework for meta-analysis, complemented with the implementation in a freely available and fully documented software, will provide researchers with a flexible tool for addressing nonstandard pooling problems.},
  file = {/Users/rritaz/Zotero/storage/SJGRPVKX/Sera et al. - 2019 - An extended mixed-effects framework for meta-analy.pdf;/Users/rritaz/Zotero/storage/N85UTBAH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models},
  language = {en},
  number = {29}
}

@article{shadish_campbell_2010,
  title = {Campbell and {{Rubin}}: {{A}} Primer and Comparison of Their Approaches to Causal Inference in Field Settings.},
  shorttitle = {Campbell and {{Rubin}}},
  author = {Shadish, William R.},
  year = {2010},
  volume = {15},
  pages = {3--17},
  doi = {10.1037/a0015916},
  abstract = {This article compares Donald Campbell's and Donald Rubin's work on causal inference in field settings on issues of epistemology, theories of cause and effect, methodology, statistics, generalization, and terminology. The two approaches are quite different but compatible, differing mostly in matters of bandwidth versus fidelity. Campbell's work demonstrates broad narrative scope that covers a wide array of concepts related to causation, with a powerful appreciation for human fallibility in making causal judgments, with a more elaborate theory of cause and generalization, and with a preference for design over analysis. Rubin's approach is a more narrow and formal quantitative analysis of effect estimation, sharing a preference for design but best known for analysis, with compelling quantitative approaches to obtaining unbiased quantitative effect estimates from nonrandomized designs and with comparatively little to say about generalization. Much could be gained by joining the emphasis on design in Campbell with the emphasis on analysis in Rubin. However, the 2 approaches also speak modestly different languages that leave some questions about their total commensurability that only continued dialogue can fully clarify.},
  file = {/Users/rritaz/Zotero/storage/UJC6YEPR/Shadish - 2010 - Campbell and Rubin A primer and comparison of the.pdf},
  journal = {Psychological methods},
  keywords = {causality},
  number = {1}
}

@article{shadish_revisiting_2002,
  title = {Revisiting Field Experimentation: {{Field}} Notes for the Future},
  shorttitle = {Revisiting Field Experimentation},
  author = {Shadish, William R.},
  year = {2002},
  month = mar,
  volume = {7},
  pages = {3--18},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.7.1.3},
  abstract = {Field experiments in the social sciences were increasingly used in the 20th century. This article briefly reviews some important lessons in design, analysis, and theory of field experiments emerging from that experience. Topics include the importance of ensuring that selection into experiments and assignment to conditions occurs properly, how to prevent and analyze attrition, the need to attend to power and effect size, how to measure and take partial treatment implementation into account in analyses, modern analyses of quasi-experimental and multilevel data, Rubin's model, and the role of internal and external validity. The article ends with observations on the computer revolution in methodology and statistics, convergences in theory and methods across disciplines, the need for an empirical program of methodological research, the key problem of selection bias, and the inevitability of increased specialization in field experimentation in the years to come. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {causality},
  number = {1}
}

@article{shi_influence_2017,
  title = {Influence Diagnostics in Meta-Regression Model},
  author = {Shi, Lei and Zuo, ShanShan and Yu, Dalei and Zhou, Xiaohua},
  year = {2017},
  volume = {8},
  pages = {343--354},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1247},
  abstract = {This paper studies the influence diagnostics in meta-regression model including case deletion diagnostic and local influence analysis. We derive the subset deletion formulae for the estimation of regression coefficient and heterogeneity variance and obtain the corresponding influence measures. The DerSimonian and Laird estimation and maximum likelihood estimation methods in meta-regression are considered, respectively, to derive the results. Internal and external residual and leverage measure are defined. The local influence analysis based on case-weights perturbation scheme, responses perturbation scheme, covariate perturbation scheme, and within-variance perturbation scheme are explored. We introduce a method by simultaneous perturbing responses, covariate, and within-variance to obtain the local influence measure, which has an advantage of capable to compare the influence magnitude of influential studies from different perturbations. An example is used to illustrate the proposed methodology.},
  file = {/Users/rritaz/Zotero/storage/GPFNR64E/Shi et al. - 2017 - Influence diagnostics in meta-regression model.pdf;/Users/rritaz/Zotero/storage/8Q7R2RVV/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,GLM MA models,modeling effect size variation (covariates)},
  language = {en},
  number = {3}
}

@article{shi_meta-analysis_2004,
  title = {Meta-Analysis for Trend Estimation},
  author = {Shi, Jian Qing and Copas, J. B.},
  year = {2004},
  volume = {23},
  pages = {3--19},
  issn = {1097-0258},
  doi = {10.1002/sim.1595},
  abstract = {Grouped dose measures, heterogeneity and publication bias are three major problems for meta-analysis in trend estimation. In this paper, we propose a model that allows for arbitrarily aggregated dose levels, and show that the resulting estimates and standard errors can be quite different from those given by the usual assigned value method. Based on fitting a model to the funnel plot, we discuss a method for random-effects sensitivity analysis that deals with the problems of heterogeneity and publication bias. A meta-analysis of epidemiological studies on the effect of alcohol on the risk of breast cancer is used to illustrate the method. Our analysis suggests that the rate of increase in risk with alcohol consumption is substantially less than has been previously suggested. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/BEIRYZW7/Shi and Copas - 2004 - Meta-analysis for trend estimation.pdf;/Users/rritaz/Zotero/storage/SPAFYH4P/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete),random-effects},
  language = {en},
  number = {1}
}

@article{shih_evaluating_2019,
  title = {Evaluating Network Meta-Analysis and Inconsistency Using Arm-Parameterized Model in Structural Equation Modeling},
  author = {Shih, Ming-Chieh and Tu, Yu-Kang},
  year = {2019},
  volume = {10},
  pages = {240--254},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1344},
  abstract = {Network meta-analysis (NMA) uses both direct and indirect evidence to compare the efficacy and harm between several treatments. Structural equation modeling (SEM) is a statistical method that investigates relations among observed and latent variables. Previous studies have shown that the contrast-based Lu-Ades model for NMA can be implemented in the SEM framework. However, the Lu-Ades model uses the difference between treatments as the unit of analysis, thereby introducing correlations between observations. The main objective of this study is to demonstrate how to undertake NMA in SEM using the outcome of treatment arms as the unit of analysis (arm-parameterized model) and to evaluate direct-indirect evidence inconsistency under this framework. We then showed that our models can include trials of within-person designs without the need for complex data manipulation. Moreover, we showed that a novel approach to meta-analysis, the unrestricted weighted least squares, can be readily extended to NMA under our framework. Finally, we demonstrated that the direct-indirect evidence inconsistency can be evaluated by using multiple group analysis in SEM. We then proposed a novel arm-parameterized inconsistency model for inconsistency evaluation. We applied the proposed models to two NMA datasets and showed that our approach yielded results identical to the Lu-Ades model. We also showed that relaxing variance assumptions can reduce the confidence intervals for certain treatment contrasts, thereby yielding greater statistical power. The arm-parameterized inconsistency model unifies current approaches to inconsistency evaluation.},
  file = {/Users/rritaz/Zotero/storage/YYPIRVNZ/Shih and Tu - 2019 - Evaluating network meta-analysis and inconsistency.pdf;/Users/rritaz/Zotero/storage/KFF9M9E6/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {2}
}

@article{shuster_empirical_2010,
  title = {Empirical vs Natural Weighting in Random Effects Meta-Analysis},
  author = {Shuster, Jonathan J.},
  year = {2010},
  volume = {29},
  pages = {1259--1265},
  issn = {1097-0258},
  doi = {10.1002/sim.3607},
  abstract = {This article brings into serious question the validity of empirically based weighting in random effects meta-analysis. These methods treat sample sizes as non-random, whereas they need to be part of the random effects analysis. It will be demonstrated that empirical weighting risks substantial bias. Two alternate methods are proposed. The first estimates the arithmetic mean of the population of study effect sizes per the classical model for random effects meta-analysis. We show that anything other than an unweighted mean of study effect sizes will risk serious bias for this targeted parameter. The second method estimates a patient level effect size, something quite different from the first. To prevent inconsistent estimation for this population parameter, the study effect sizes must be weighted in proportion to their total sample sizes for the trial. The two approaches will be presented for a meta-analysis of a nasal decongestant, while at the same time will produce counter-intuitive results for the DerSimonian\textendash Laird approach, the most popular empirically based weighted method. It is concluded that all past publications based on empirically weighted random effects meta-analysis should be revisited to see if the qualitative conclusions hold up under the methods proposed herein. It is also recommended that empirically based weighted random effects meta-analysis not be used in the future, unless strong cautions about the assumptions underlying these analyses are stated, and at a minimum, some form of secondary analysis based on the principles set forth in this article be provided to supplement the primary analysis. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/UHACGRDE/Shuster - 2010 - Empirical vs natural weighting in random effects m.pdf;/Users/rritaz/Zotero/storage/ALDQZZHK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random effects models,random-effects},
  language = {en},
  number = {12}
}

@article{shuster_fixed_2007,
  title = {Fixed vs Random Effects Meta-Analysis in Rare Event Studies: {{The Rosiglitazone}} Link with Myocardial Infarction and Cardiac Death},
  shorttitle = {Fixed vs Random Effects Meta-Analysis in Rare Event Studies},
  author = {Shuster, Jonathan J. and Jones, Lynn S. and Salmon, Daniel A.},
  year = {2007},
  volume = {26},
  pages = {4375--4385},
  issn = {1097-0258},
  doi = {10.1002/sim.3060},
  abstract = {Meta-analyses can be powerful tools to combine the results of randomized clinical trials and observational studies to make consensus inferences about a medical issue. It will be demonstrated that a common practice of testing for homogeneity of effect size, and acting upon the inference to decide between fixed vs random effects, can lead to potentially misleading results. A by-product of this paper is a new ratio estimator approach to random effects meta-analysis of a large set of studies with low event rates. As a case study, we shall use the recent Rosiglitazone example, where diagnostic testing failed to reject homogeneity, leading the investigators to use fixed effects. The results for the fixed and random effects analyses are discordant. In the fixed (random) effects analysis, the p-values for myocardial infarction were 0.03 (0.11) while those for cardiac death were 0.06 (0.0017). Had the fixed effects analysis controlled the study error for multiple testing via a Bonferonni correction, the joint 95+ per cent confidence rectangle for the two outcomes would have included odds ratios of (1.0, 1.0). For the Rosiglitazone example, random effects analysis, where all studies receive the same weight, is the superior choice over fixed effects, where two large studies dominate. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CSEYSTRN/Shuster et al. - 2007 - Fixed vs random effects meta-analysis in rare even.pdf;/Users/rritaz/Zotero/storage/C5EZHGWZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,discrete effect sizes,fixed vs random effects,GLM MA models},
  language = {en},
  number = {24}
}

@article{shuster_pocock_2013,
  title = {A {{Pocock}} Approach to Sequential Meta-Analysis of Clinical Trials},
  author = {Shuster, Jonathan J. and Neu, Josef},
  year = {2013},
  volume = {4},
  pages = {269--279},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1088},
  abstract = {Three recent papers have provided sequential methods for meta-analysis of two-treatment randomized clinical trials. This paper provides an alternate approach that has three desirable features. First, when carried out prospectively (i.e., we only have the results up to the time of our current analysis), we do not require knowledge of the information fraction (the fraction of the total information that is available at each analysis). Second, the methods work even if the expected values of the effect sizes vary from study to study. Finally, our methods have easily interpretable metrics that make sense under changing effect sizes. Although the other published methods can be adapted to be ``group sequential'' (recommended), meaning that a set number and timing of looks are specified, rather than looking after every trial, ours can be used in both a continuous or group sequential manner. We provide an example on the role of probiotics in preventing necrotizing enterocolitis in preterm infants. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LLFUTSZK/Shuster and Neu - 2013 - A Pocock approach to sequential meta-analysis of c.pdf;/Users/rritaz/Zotero/storage/CSKH3PD2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance},
  language = {en},
  number = {3}
}

@article{siadaty_proportional_2004,
  title = {Proportional Odds Ratio Model for Comparison of Diagnostic Tests in Meta-Analysis},
  author = {Siadaty, Mir Said and Shu, Jianfen},
  year = {2004},
  month = dec,
  volume = {4},
  pages = {27},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-4-27},
  abstract = {Background: Consider a meta-analysis where a 'head-to-head' comparison of diagnostic tests for a disease of interest is intended. Assume there are two or more tests available for the disease, where each test has been studied in one or more papers. Some of the papers may have studied more than one test, hence the results are not independent. Also the collection of tests studied may change from one paper to the other, hence incomplete matched groups. Methods: We propose a model, the proportional odds ratio (POR) model, which makes no assumptions about the shape of ORp, a baseline function capturing the way OR changes across papers. The POR model does not assume homogeneity of ORs, but merely specifies a relationship between the ORs of the two tests. One may expand the domain of the POR model to cover dependent studies, multiple outcomes, multiple thresholds, multi-category or continuous tests, and individual-level data. Results: In the paper we demonstrate how to formulate the model for a few real examples, and how to use widely available or popular statistical software (like SAS, R or S-Plus, and Stata) to fit the models, and estimate the discrimination accuracy of tests. Furthermore, we provide code for converting ORs into other measures of test performance like predictive values, post-test probabilities, and likelihood ratios, under mild conditions. Also we provide code to convert numerical results into graphical ones, like forest plots, heterogeneous ROC curves, and post test probability difference graphs. Conclusions: The flexibility of POR model, coupled with ease with which it can be estimated in familiar software, suits the daily practice of meta-analysis and improves clinical decision-making.},
  file = {/Users/rritaz/Zotero/storage/IZFLPTMG/Siadaty and Shu - 2004 - Proportional odds ratio model for comparison of di.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,physical/biological fields},
  language = {en},
  number = {1}
}

@article{siannis_one-stage_2010,
  title = {One-Stage Parametric Meta-Analysis of Time-to-Event Outcomes},
  author = {Siannis, F. and Barrett, J. K. and Farewell, V. T. and Tierney, J. F.},
  year = {2010},
  volume = {29},
  pages = {3030--3045},
  issn = {1097-0258},
  doi = {10.1002/sim.4086},
  abstract = {Methodology for the meta-analysis of individual patient data with survival end-points is proposed. Motivated by questions about the reliance on hazard ratios as summary measures of treatment effects, a parametric approach is considered and percentile ratios are introduced as an alternative to hazard ratios. The generalized log-gamma model, which includes many common time-to-event distributions as special cases, is discussed in detail. Likelihood inference for percentile ratios is outlined. The proposed methodology is used for a meta-analysis of glioma data that was one of the studies which motivated this work. A simulation study exploring the validity of the proposed methodology is available electronically. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/7WY3WTXB/Siannis et al. - 2010 - One-stage parametric meta-analysis of time-to-even.pdf;/Users/rritaz/Zotero/storage/JV5GFGUM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {continuous effect sizes,Individual Patient Data IPD},
  language = {en},
  number = {29}
}

@article{siddique_multiple_2015,
  title = {Multiple Imputation for Harmonizing Longitudinal Non-Commensurate Measures in Individual Participant Data Meta-Analysis},
  author = {Siddique, Juned and Reiter, Jerome P. and Brincks, Ahnalee and Gibbons, Robert D. and Crespi, Catherine M. and Brown, C. Hendricks},
  year = {2015},
  volume = {34},
  pages = {3399--3414},
  issn = {1097-0258},
  doi = {10.1002/sim.6562},
  abstract = {There are many advantages to individual participant data meta-analysis for combining data from multiple studies. These advantages include greater power to detect effects, increased sample heterogeneity, and the ability to perform more sophisticated analyses than meta-analyses that rely on published results. However, a fundamental challenge is that it is unlikely that variables of interest are measured the same way in all of the studies to be combined. We propose that this situation can be viewed as a missing data problem in which some outcomes are entirely missing within some trials and use multiple imputation to fill in missing measurements. We apply our method to five longitudinal adolescent depression trials where four studies used one depression measure and the fifth study used a different depression measure. None of the five studies contained both depression measures. We describe a multiple imputation approach for filling in missing depression measures that makes use of external calibration studies in which both depression measures were used. We discuss some practical issues in developing the imputation model including taking into account treatment group and study. We present diagnostics for checking the fit of the imputation model and investigate whether external information is appropriately incorporated into the imputed values. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/9XJY8APV/Siddique et al. - 2015 - Multiple imputation for harmonizing longitudinal n.pdf;/Users/rritaz/Zotero/storage/AID52VDH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,missing data,multivariate},
  language = {en},
  number = {26}
}

@article{sidik_comparison_2007,
  title = {A Comparison of Heterogeneity Variance Estimators in Combining Results of Studies},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2007},
  volume = {26},
  pages = {1964--1981},
  issn = {1097-0258},
  doi = {10.1002/sim.2688},
  abstract = {For random effects meta-analysis, seven different estimators of the heterogeneity variance are compared and assessed using a simulation study. The seven estimators are the variance component type estimator (VC), the method of moments estimator (MM), the maximum likelihood estimator (ML), the restricted maximum likelihood estimator (REML), the empirical Bayes estimator (EB), the model error variance type estimator (MV), and a variation of the MV estimator (MVvc). The performance of the estimators is compared in terms of both bias and mean squared error, using Monte Carlo simulation. The results show that the REML and especially the ML and MM estimators are not accurate, having large biases unless the true heterogeneity variance is small. The VC estimator tends to overestimate the heterogeneity variance in general, but is quite accurate when the number of studies is large. The MV estimator is not a good estimator when the heterogeneity variance is small to moderate, but it is reasonably accurate when the heterogeneity variance is large. The MVvc estimator is an improved estimator compared to the MV estimator, especially for small to moderate values of the heterogeneity variance. The two estimators MVvc and EB are found to be the most accurate in general, particularly when the heterogeneity variance is moderate to large. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CMRWGYSV/Sidik and Jonkman - 2007 - A comparison of heterogeneity variance estimators .pdf;/Users/rritaz/Zotero/storage/R36CCNP4/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity estimators,random-effects},
  language = {en},
  number = {9}
}

@article{sidik_comparison_2007-1,
  title = {A Comparison of Heterogeneity Variance Estimators in Combining Results of Studies},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2007},
  volume = {26},
  pages = {1964--1981},
  issn = {1097-0258},
  doi = {10.1002/sim.2688},
  abstract = {For random effects meta-analysis, seven different estimators of the heterogeneity variance are compared and assessed using a simulation study. The seven estimators are the variance component type estimator (VC), the method of moments estimator (MM), the maximum likelihood estimator (ML), the restricted maximum likelihood estimator (REML), the empirical Bayes estimator (EB), the model error variance type estimator (MV), and a variation of the MV estimator (MVvc). The performance of the estimators is compared in terms of both bias and mean squared error, using Monte Carlo simulation. The results show that the REML and especially the ML and MM estimators are not accurate, having large biases unless the true heterogeneity variance is small. The VC estimator tends to overestimate the heterogeneity variance in general, but is quite accurate when the number of studies is large. The MV estimator is not a good estimator when the heterogeneity variance is small to moderate, but it is reasonably accurate when the heterogeneity variance is large. The MVvc estimator is an improved estimator compared to the MV estimator, especially for small to moderate values of the heterogeneity variance. The two estimators MVvc and EB are found to be the most accurate in general, particularly when the heterogeneity variance is moderate to large. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2688},
  file = {/Users/rritaz/Zotero/storage/G86UN9B8/Sidik and Jonkman - 2007 - A comparison of heterogeneity variance estimators .pdf;/Users/rritaz/Zotero/storage/NEDFPKUD/sim.html},
  journal = {Statistics in Medicine},
  keywords = {across-study variance,effect size estimation,meta-analysis,random effects model,simulation},
  language = {en},
  number = {9}
}

@article{sidik_note_2019,
  title = {A Note on the Empirical {{Bayes}} Heterogeneity Variance Estimator in Meta-Analysis},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2019},
  volume = {38},
  pages = {3804--3816},
  issn = {1097-0258},
  doi = {10.1002/sim.8197},
  abstract = {This paper focuses on the empirical Bayes (EB) or Mandel-Paule estimator of the heterogeneity variance in meta-analysis, which was discussed by Morris and proposed in earlier publications by Mandel and Paule in an inter-laboratory context. The relationship of the EB estimator to other heterogeneity variance estimators typically used in meta-analysis is explored, and approximate variance estimators for the EB estimate of the heterogeneity variance are proposed based on the M-estimation method. Statistical inference for the overall treatment effect using the EB estimator and the proposed standard errors is discussed using two example data sets from meta-analysis applications.},
  file = {/Users/rritaz/Zotero/storage/SBLUZUQN/Sidik and Jonkman - 2019 - A note on the empirical Bayes heterogeneity varian.pdf;/Users/rritaz/Zotero/storage/SR5HN3UK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random-effects},
  language = {en},
  number = {20}
}

@article{sidik_robust_2006,
  title = {Robust Variance Estimation for Random Effects Meta-Analysis},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2006},
  month = aug,
  volume = {50},
  pages = {3681--3701},
  issn = {01679473},
  doi = {10.1016/j.csda.2005.07.019},
  abstract = {In random effects meta-analysis, an overall effect is estimated using a weighted mean, with weights based on estimated marginal variances. The variance of the overall effect is often estimated using the inverse of the sum of the estimated weights, and inference about the overall effect is typically conducted using this `usual' variance estimator, which is not robust to errors in the estimated marginal variances. In this paper, robust estimation for the asymptotic variance of a weighted overall effect estimate is explored by considering a robust variance estimator in comparison with the usual variance estimator and another less frequently used estimator, a weighted version of the sample variance. Three illustrative examples are presented to demonstrate and compare the three estimation methods. Furthermore, a simulation study is conducted to assess the robustness of the three variance estimators using estimated weights. The simulation results show that the robust variance estimator and the weighted sample variance estimator both estimate the variance of an overall effect more accurately than the usual variance estimator when the weights are imprecise due to the use of estimated marginal variances, as is typically the case in practice.Therefore, we argue that inference about an overall effect should be based on the robust variance estimator or the weighted sample variance, which provide protection against the practice of using estimated weights in meta-analytical inference.},
  file = {/Users/rritaz/Zotero/storage/2H65JU3A/Sidik and Jonkman - 2006 - Robust variance estimation for random effects meta.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {12}
}

@article{sidik_simple_2002,
  title = {A Simple Confidence Interval for Meta-Analysis},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2002},
  volume = {21},
  pages = {3153--3159},
  issn = {1097-0258},
  doi = {10.1002/sim.1262},
  abstract = {In the context of a random effects model for meta-analysis, a number of methods are available to estimate confidence limits for the overall mean effect. A simple and commonly used method is the DerSimonian and Laird approach. This paper discusses an alternative simple approach for constructing the confidence interval, based on the t-distribution. This approach has improved coverage probability compared to the DerSimonian and Laird method. Moreover, it is easy to calculate, and unlike some methods suggested in the statistical literature, no iterative computation is required. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/IRXWLA93/Sidik and Jonkman - 2002 - A simple confidence interval for meta-analysis.pdf;/Users/rritaz/Zotero/storage/MPTB6Z3R/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,Confidence Intervals,random-effects},
  language = {en},
  number = {21}
}

@article{sidik_simple_2002-1,
  title = {A Simple Confidence Interval for Meta-Analysis},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2002},
  volume = {21},
  pages = {3153--3159},
  issn = {1097-0258},
  doi = {10.1002/sim.1262},
  abstract = {In the context of a random effects model for meta-analysis, a number of methods are available to estimate confidence limits for the overall mean effect. A simple and commonly used method is the DerSimonian and Laird approach. This paper discusses an alternative simple approach for constructing the confidence interval, based on the t-distribution. This approach has improved coverage probability compared to the DerSimonian and Laird method. Moreover, it is easy to calculate, and unlike some methods suggested in the statistical literature, no iterative computation is required. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1262},
  copyright = {Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DEQI2WQE/Sidik and Jonkman - 2002 - A simple confidence interval for meta-analysis.pdf;/Users/rritaz/Zotero/storage/9EVU8RWV/sim.html},
  journal = {Statistics in Medicine},
  keywords = {coverage probability,random effects model,simulation study,t-distribution,weighted estimation},
  language = {en},
  number = {21}
}

@article{sidik_simple_2002-2,
  title = {A Simple Confidence Interval for Meta-Analysis},
  author = {Sidik, Kurex and Jonkman, Jeffrey N.},
  year = {2002},
  volume = {21},
  pages = {3153--3159},
  issn = {1097-0258},
  doi = {10.1002/sim.1262},
  abstract = {In the context of a random effects model for meta-analysis, a number of methods are available to estimate confidence limits for the overall mean effect. A simple and commonly used method is the DerSimonian and Laird approach. This paper discusses an alternative simple approach for constructing the confidence interval, based on the t-distribution. This approach has improved coverage probability compared to the DerSimonian and Laird method. Moreover, it is easy to calculate, and unlike some methods suggested in the statistical literature, no iterative computation is required. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1262},
  file = {/Users/rritaz/Zotero/storage/RMVCPH92/Sidik and Jonkman - 2002 - A simple confidence interval for meta-analysis.pdf;/Users/rritaz/Zotero/storage/MCLL5MX6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {coverage probability,random effects model,simulation study,t-distribution,weighted estimation},
  language = {en},
  number = {21}
}

@article{siersma_multivariable_2007,
  title = {Multivariable Modelling for Meta-Epidemiological Assessment of the Association between Trial Quality and Treatment Effects Estimated in Randomized Clinical Trials},
  author = {Siersma, V. and Als-Nielsen, B. and Chen, W. and Hilden, J. and Gluud, L. L. and Gluud, C.},
  year = {2007},
  volume = {26},
  pages = {2745--2758},
  issn = {1097-0258},
  doi = {10.1002/sim.2752},
  abstract = {Methodological deficiencies are known to affect the results of randomized trials. There are several components of trial quality, which, when inadequately attended to, may bias the treatment effect under study. The extent of this bias, so far only vaguely known, is currently being investigated by `meta-epidemiological' re-analysis of data collected as part of systematic reviews. As inadequate quality components often co-occur, we maintain that the suspected biases must be evaluated simultaneously. Furthermore, the biases cannot safely be assumed to be homogeneous across systematic reviews. Therefore, a stable multivariable method that allows for heterogeneity is needed for assessing the `bias coefficients'. We present two general statistical models for analysis of a study of 523 randomized trials from 48 meta-analyses in a random sample of Cochrane reviews: a logistic regression model uses the design of the trials as such to give estimates; a weighted regression model incorporates between-trial variation and thus gives wider confidence intervals, but is computationally lighter and can be used with trials of more general design. In both models, heterogeneity in the bias coefficients can be incorporated in two ways. A stratification approach pools the estimates from models estimated on subgroups of the data. We explore stratification by reviews and by broad trial types, the latter of which gives larger subgroups of the data, circumventing instabilities. A multilevel approach also avoids instabilities and addresses the more fundamental problem of interpretation of the pooled multivariable effect in the presence of heterogeneity. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/6WVIHEG8/Siersma et al. - 2007 - Multivariable modelling for meta-epidemiological a.pdf;/Users/rritaz/Zotero/storage/ENU5S7IR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates),multivariate,random-effects},
  language = {en},
  number = {14}
}

@article{simmonds_covariate_2007,
  title = {Covariate Heterogeneity in Meta-Analysis: {{Criteria}} for Deciding between Meta-Regression and Individual Patient Data},
  shorttitle = {Covariate Heterogeneity in Meta-Analysis},
  author = {Simmonds, M. C. and Higgins, J. P. T.},
  year = {2007},
  volume = {26},
  pages = {2982--2999},
  issn = {1097-0258},
  doi = {10.1002/sim.2768},
  abstract = {Meta-analyses of clinical trials are increasingly seeking to go beyond estimating the effect of a treatment and may also aim to investigate the effect of other covariates and how they alter treatment effectiveness. This requires the estimation of treatment-covariate interactions. Meta-regression can be used to estimate such interactions using published data, but it is known to lack statistical power, and is prone to bias. The use of individual patient data can improve estimation of such interactions, among other benefits, but it can be difficult and time-consuming to collect and analyse. This paper derives, under certain conditions, the power of meta-regression and IPD methods to detect treatment\textendash covariate interactions. These power formulae are shown to depend on heterogeneity in the covariate distributions across studies. This allows the derivation of simple tests, based on heterogeneity statistics, for comparing the statistical power of the analysis methods. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2BVUMJJ4/Simmonds and Higgins - 2007 - Covariate heterogeneity in meta-analysis Criteria.pdf;/Users/rritaz/Zotero/storage/SGGEFURM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),power},
  language = {en},
  number = {15}
}

@article{simmonds_meta-analysis_2011,
  title = {Meta-Analysis of Time-to-Event Data: A Comparison of Two-Stage Methods},
  shorttitle = {Meta-Analysis of Time-to-Event Data},
  author = {Simmonds, Mark C. and Tierney, Jayne and Bowden, Jack and Higgins, Julian PT},
  year = {2011},
  volume = {2},
  pages = {139--149},
  issn = {1759-2887},
  doi = {10.1002/jrsm.44},
  abstract = {Meta-analysis is widely used to synthesise results from randomised trials. When the relevant trials collected time-to-event data, individual participant data are commonly sought from all trials. Meta-analyses of time-to-event data are typically performed using variants of the log-rank test, but modern statistical software allows for the use of maximum likelihood methods such as Cox proportional hazards models or interval-censored logistic regression. In this paper, the different approaches to the analysis of time-to-event data are examined and compared with show that log-rank test approaches are in fact first-order approximations to the maximum likelihood methods and that some methods assume proportional hazards, whereas others assume proportional odds. A simulation study is performed to compare the different methods, which shows that log-rank test approaches give biased estimates when the underlying hazard ratio or odds ratio is far from unity. It also shows that proportional hazards methods give biased results when hazards are not proportional, and proportional odds methods give biased results when odds are not proportional. Maximum likelihood models should, therefore, be preferred to log-rank test based methods for the meta-analysis of time-to-event data and any such meta-analysis should investigate whether proportional hazards or proportional odds assumptions are valid. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/P8757IW9/Simmonds et al. - 2011 - Meta-analysis of time-to-event data a comparison .pdf;/Users/rritaz/Zotero/storage/L7P46DTN/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {effect size combination (small sample \& discrete)},
  language = {en},
  number = {3}
}

@article{simmonds_random-effects_2013,
  title = {Random-Effects Meta-Analysis of Time-to-Event Data Using the Expectation\textendash Maximisation Algorithm and Shrinkage Estimators},
  author = {Simmonds, Mark C. and Higgins, Julian PT and Stewart, Lesley A.},
  year = {2013},
  volume = {4},
  pages = {144--155},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1067},
  abstract = {Meta-analysis of time-to-event data has proved difficult in the past because consistent summary statistics often cannot be extracted from published results. The use of individual patient data allows for the re-analysis of each study in a consistent fashion and thus makes meta-analysis of time-to-event data feasible. Time-to-event data can be analysed using proportional hazards models, but incorporating random effects into these models is not straightforward in standard software. This paper fits random-effects proportional hazards models by treating the random effects as missing data and applying the expectation\textendash maximisation algorithm. This approach has been used before by using Markov chain Monte Carlo methods to perform the expectation step of the algorithm. In this paper, the expectation step is simplified, without sacrificing accuracy, by approximating the expected values of the random effects using simple shrinkage estimators. This provides a robust method for fitting random-effects models that can be implemented in standard statistical packages. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/FVKYD226/Simmonds et al. - 2013 - Random-effects meta-analysis of time-to-event data.pdf;/Users/rritaz/Zotero/storage/RCRUAGQE/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes,Individual Patient Data IPD,missing data,random-effects},
  language = {en},
  number = {2}
}

@article{smail-faugeron_comparison_2014,
  title = {Comparison of Intervention Effects in Split-Mouth and Parallel-Arm Randomized Controlled Trials: A Meta-Epidemiological Study},
  shorttitle = {Comparison of Intervention Effects in Split-Mouth and Parallel-Arm Randomized Controlled Trials},
  author = {{Sma{\"i}l-Faugeron}, Violaine and {Fron-Chabouis}, H{\'e}l{\`e}ne and Courson, Fr{\'e}d{\'e}ric and Durieux, Pierre},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {64},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-64},
  abstract = {Background: Split-mouth randomized controlled trials (RCTs) are popular in oral health research. Meta-analyses frequently include trials of both split-mouth and parallel-arm designs to derive combined intervention effects. However, carry-over effects may induce bias in split- mouth RCTs. We aimed to assess whether intervention effect estimates differ between split- mouth and parallel-arm RCTs investigating the same questions. Methods: We performed a meta-epidemiological study. We systematically reviewed meta- analyses including both split-mouth and parallel-arm RCTs with binary or continuous outcomes published up to February 2013. Two independent authors selected studies and extracted data. We used a two-step approach to quantify the differences between split-mouth and parallel-arm RCTs: for each meta-analysis. First, we derived ratios of odds ratios (ROR) for dichotomous data and differences in standardized mean differences ({$\Delta$}SMD) for continuous data; second, we pooled RORs or {$\Delta$}SMDs across meta-analyses by random-effects meta-analysis models. Results: We selected 18 systematic reviews, for 15 meta-analyses with binary outcomes (28 split-mouth and 28 parallel-arm RCTs) and 19 meta-analyses with continuous outcomes (28 split-mouth and 28 parallel-arm RCTs). Effect estimates did not differ between split-mouth and parallel-arm RCTs (mean ROR, 0.96, 95\% confidence interval 0.52\textendash 1.80; mean {$\Delta$}SMD, 0.08, -0.14\textendash 0.30). Conclusions: Our study did not provide sufficient evidence for a difference in intervention effect estimates derived from split-mouth and parallel-arm RCTs. Authors should consider including split-mouth RCTs in their meta-analyses with suitable and appropriate analysis.},
  file = {/Users/rritaz/Zotero/storage/AKPHVP54/Smaïl-Faugeron et al. - 2014 - Comparison of intervention effects in split-mouth .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {meta-meta-analyses,physical/biological fields},
  language = {en},
  number = {1}
}

@article{smith_investigating_2005,
  title = {Investigating Heterogeneity in an Individual Patient Data Meta-Analysis of Time to Event Outcomes},
  author = {Smith, Catrin Tudur and Williamson, Paula R. and Marson, Anthony G.},
  year = {2005},
  volume = {24},
  pages = {1307--1319},
  issn = {1097-0258},
  doi = {10.1002/sim.2050},
  abstract = {Differences across studies in terms of design features and methodology, clinical procedures, and patient characteristics, are factors that can contribute to variability in the treatment effect between studies in a meta-analysis (statistical heterogeneity). Regression modelling can be used to examine relationships between treatment effect and covariates with the aim of explaining the variability in terms of clinical, methodological, or other factors. Such an investigation can be undertaken using aggregate data or individual patient data. An aggregate data approach can be problematic as sufficient data are rarely available and translating aggregate effects to individual patients can often be misleading. An individual patient data approach, although usually more resource demanding, allows a more thorough investigation of potential sources of heterogeneity and enables a fuller analysis of time to event outcomes in meta-analysis. Hierarchical Cox regression models are used to identify and explore the evidence for heterogeneity in meta-analysis and examine the relationship between covariates and censored failure time data in this context. Alternative formulations of the model are possible and illustrated using individual patient data from a meta-analysis of five randomized controlled trials which compare two drugs for the treatment of epilepsy. The models are further applied to simulated data examples in which the degree of heterogeneity and magnitude of treatment effect are varied. The behaviour of each model in each situation is explored and compared. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/A4YS8LNK/Smith et al. - 2005 - Investigating heterogeneity in an individual patie.pdf;/Users/rritaz/Zotero/storage/3D9622AG/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {9}
}

@article{song_extent_2009,
  title = {Extent of Publication Bias in Different Categories of Research Cohorts: A Meta-Analysis of Empirical Studies},
  shorttitle = {Extent of Publication Bias in Different Categories of Research Cohorts},
  author = {Song, Fujian and {Parekh-Bhurke}, Sheetal and Hooper, Lee and Loke, Yoon K and Ryder, Jon J and Sutton, Alex J and Hing, Caroline B and Harvey, Ian},
  year = {2009},
  month = dec,
  volume = {9},
  pages = {79},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-79},
  abstract = {Background: The validity of research synthesis is threatened if published studies comprise a biased selection of all studies that have been conducted. We conducted a meta-analysis to ascertain the strength and consistency of the association between study results and formal publication. Methods: The Cochrane Methodology Register Database, MEDLINE and other electronic bibliographic databases were searched (to May 2009) to identify empirical studies that tracked a cohort of studies and reported the odds of formal publication by study results. Reference lists of retrieved articles were also examined for relevant studies. Odds ratios were used to measure the association between formal publication and significant or positive results. Included studies were separated into subgroups according to starting time of follow-up, and results from individual cohort studies within the subgroups were quantitatively pooled. Results: We identified 12 cohort studies that followed up research from inception, four that included trials submitted to a regulatory authority, 28 that assessed the fate of studies presented as conference abstracts, and four cohort studies that followed manuscripts submitted to journals. The pooled odds ratio of publication of studies with positive results, compared to those without positive results (publication bias) was 2.78 (95\% CI: 2.10 to 3.69) in cohorts that followed from inception, 5.00 (95\% CI: 2.01 to 12.45) in trials submitted to regulatory authority, 1.70 (95\% CI: 1.44 to 2.02) in abstract cohorts, and 1.06 (95\% CI: 0.80 to 1.39) in cohorts of manuscripts. Conclusion: Dissemination of research findings is likely to be a biased process. Publication bias appears to occur early, mainly before the presentation of findings at conferences or submission of manuscripts to journals.},
  file = {/Users/rritaz/Zotero/storage/HH7A2ZAJ/Song et al. - 2009 - Extent of publication bias in different categories.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {meta-meta-analyses,publication bias},
  language = {en},
  number = {1}
}

@article{song_simulation_2012,
  title = {Simulation Evaluation of Statistical Properties of Methods for Indirect and Mixed Treatment Comparisons},
  author = {Song, Fujian and Clark, Allan and Bachmann, Max O and Maas, Jim},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {138},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-138},
  abstract = {Background: Indirect treatment comparison (ITC) and mixed treatment comparisons (MTC) have been increasingly used in network meta-analyses. This simulation study comprehensively investigated statistical properties and performances of commonly used ITC and MTC methods, including simple ITC (the Bucher method), frequentist and Bayesian MTC methods. Methods: A simple network of three sets of two-arm trials with a closed loop was simulated. Different simulation scenarios were based on different number of trials, assumed treatment effects, extent of heterogeneity, bias and inconsistency. The performance of the ITC and MTC methods was measured by the type I error, statistical power, observed bias and mean squared error (MSE). Results: When there are no biases in primary studies, all ITC and MTC methods investigated are on average unbiased. Depending on the extent and direction of biases in different sets of studies, ITC and MTC methods may be more or less biased than direct treatment comparisons (DTC). Of the methods investigated, the simple ITC method has the largest mean squared error (MSE). The DTC is superior to the ITC in terms of statistical power and MSE. Under the simulated circumstances in which there are no systematic biases and inconsistencies, the performances of MTC methods are generally better than the performance of the corresponding DTC methods. For inconsistency detection in network meta-analysis, the methods evaluated are on average unbiased. The statistical power of commonly used methods for detecting inconsistency is very low. Conclusions: The available methods for indirect and mixed treatment comparisons have different advantages and limitations, depending on whether data analysed satisfies underlying assumptions. To choose the most valid statistical methods for research synthesis, an appropriate assessment of primary studies included in evidence network is required.},
  file = {/Users/rritaz/Zotero/storage/SGY2F9BC/Song et al. - 2012 - Simulation evaluation of statistical properties of.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {1}
}

@article{souverein_transformations_2012,
  title = {Transformations of Summary Statistics as Input in Meta-Analysis for Linear Dose-Response Models on a Logarithmic Scale: A Methodology Developed within {{EURRECA}}},
  shorttitle = {Transformations of Summary Statistics as Input in Meta-Analysis for Linear Dose-Response Models on a Logarithmic Scale},
  author = {Souverein, Olga W and Dullemeijer, Carla and {van `t Veer}, Pieter and {van der Voet}, Hilko},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {57},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-57},
  abstract = {Background: To derive micronutrient recommendations in a scientifically sound way, it is important to obtain and analyse all published information on the association between micronutrient intake and biochemical proxies for micronutrient status using a systematic approach. Therefore, it is important to incorporate information from randomized controlled trials as well as observational studies as both of these provide information on the association. However, original research papers present their data in various ways. Methods: This paper presents a methodology to obtain an estimate of the dose\textendash response curve, assuming a bivariate normal linear model on the logarithmic scale, incorporating a range of transformations of the original reported data. Results: The simulation study, conducted to validate the methodology, shows that there is no bias in the transformations. Furthermore, it is shown that when the original studies report the mean and standard deviation or the geometric mean and confidence interval the results are less variable compared to when the median with IQR or range is reported in the original study. Conclusions: The presented methodology with transformations for various reported data provides a valid way to estimate the dose\textendash response curve for micronutrient intake and status using both randomized controlled trials and observational studies.},
  file = {/Users/rritaz/Zotero/storage/4SQKMB73/Souverein et al. - 2012 - Transformations of summary statistics as input in .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,physical/biological fields},
  language = {en},
  number = {1}
}

@article{spence_bayesian_2016,
  title = {A {{Bayesian}} Approach to Sequential Meta-Analysis},
  author = {Spence, Graeme T. and Steinsaltz, David and Fanshawe, Thomas R.},
  year = {2016},
  volume = {35},
  pages = {5356--5375},
  issn = {1097-0258},
  doi = {10.1002/sim.7052},
  abstract = {As evidence accumulates within a meta-analysis, it is desirable to determine when the results could be considered conclusive to guide systematic review updates and future trial designs. Adapting sequential testing methodology from clinical trials for application to pooled meta-analytic effect size estimates appears well suited for this objective. In this paper, we describe a Bayesian sequential meta-analysis method, in which an informative heterogeneity prior is employed and stopping rule criteria are applied directly to the posterior distribution for the treatment effect parameter. Using simulation studies, we examine how well this approach performs under different parameter combinations by monitoring the proportion of sequential meta-analyses that reach incorrect conclusions (to yield error rates), the number of studies required to reach conclusion, and the resulting parameter estimates. By adjusting the stopping rule thresholds, the overall error rates can be controlled within the target levels and are no higher than those of alternative frequentist and semi-Bayes methods for the majority of the simulation scenarios. To illustrate the potential application of this method, we consider two contrasting meta-analyses using data from the Cochrane Library and compare the results of employing different sequential methods while examining the effect of the heterogeneity prior in the proposed Bayesian approach. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/TB6XCFH7/Spence et al. - 2016 - A Bayesian approach to sequential meta-analysis.pdf;/Users/rritaz/Zotero/storage/3ERWWDX6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {29}
}

@article{spineli_empirical_2019,
  title = {An Empirical Comparison of {{Bayesian}} Modelling Strategies for Missing Binary Outcome Data in Network Meta-Analysis},
  author = {Spineli, Loukia M.},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {86},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0731-y},
  abstract = {Background: A number of strategies have been proposed to handle missing binary outcome data (MOD) in systematic reviews. However, none of these have been evaluated empirically in a series of published systematic reviews. Methods: Using published systematic reviews with network meta-analysis (NMA) from a wide range of health-related fields, we evaluated comparatively the most frequently described Bayesian modelling strategies for MOD in terms of log odds ratio (log OR), between-trial variance, inconsistency factor (i.e. difference between direct and indirect estimates for a comparison), surface under the cumulative ranking (SUCRA) and rankings. We extended the Bayesian random-effects NMA model to incorporate the informative missingness odds ratio (IMOR) parameter, and applied the node-splitting approach to investigate inconsistency locally. We considered both pattern-mixture and selection models, different structures for prior distribution of log IMOR, and different scenarios for MOD. To illustrate level of agreement between different strategies and scenarios, we used Bland-Altman plots. Results: Addressing MOD using extreme scenarios and ignoring the uncertainty about the scenarios led to systematically different and more precise log ORs compared to modelling MOD under the missing at random (MAR) assumption. Hierarchical structure of log IMORs led to lower between-trial variance, especially in the case of substantial MOD. Assuming common-within-network or trial-specific log IMORs yielded similar posterior results for all NMA estimates, whereas intervention-specific structure systematically inflated uncertainty around log ORs and SUCRAs. Pattern-mixture model agreed with selection model, particularly under the trial-specific structure; however, selection model systematically reduced precision around log IMORs. Overall, different strategies and scenarios mostly had good agreement in the case of low MOD. Conclusions: Addressing MOD using extreme scenarios and/or ignoring the uncertainty about the scenarios may negatively affect NMA estimates. Modelling MOD via the IMOR parameter can ensure bias-adjusted estimates and offer valuable insights into missingness mechanisms. The researcher should seek an expert opinion in order to decide on the structure of log IMOR that best aligns to the condition and interventions studied and to define a proper prior distribution for log IMOR. Our findings also apply to pairwise meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/4VVPA8MS/Spineli - 2019 - An empirical comparison of Bayesian modelling stra.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,missing data,network meta-analysis},
  language = {en},
  number = {1}
}

@article{spineli_participants_2019,
  title = {Participants' Outcomes Gone Missing within a Network of Interventions: {{Bayesian}} Modeling Strategies},
  shorttitle = {Participants' Outcomes Gone Missing within a Network of Interventions},
  author = {Spineli, Loukia M. and Kalyvas, Chrysostomos and Pateras, Konstantinos},
  year = {2019},
  volume = {38},
  pages = {3861--3879},
  issn = {1097-0258},
  doi = {10.1002/sim.8207},
  abstract = {Objectives: To investigate the implications of addressing informative missing binary outcome data (MOD) on network meta-analysis (NMA) estimates while applying the missing at random (MAR) assumption under different prior structures of the missingness parameter. Methods: In three motivating examples, we compared six different prior structures of the informative missingness odds ratio (IMOR) parameter in logarithmic scale under pattern-mixture and selection models. Then, we simulated 1000 triangle networks of two-arm trials assuming informative MOD related to interventions. We extended the Bayesian random-effects NMA model for binary outcomes and node-splitting approach to incorporate these 12 models in total. With interval plots, we illustrated the posterior distribution of log OR, common between-trial variance ({$\tau$}2), inconsistency factor and probability of being best per intervention under each model. Results: All models gave similar point estimates for all NMA estimates regardless of simulation scenario. For moderate and large MOD, intervention-specific prior structure of log IMOR led to larger posterior standard deviation of log ORs compared to trial-specific and common-within-network prior structures. Hierarchical prior structure led to slightly more precise {$\tau$}2 compared to identical prior structure, particularly for moderate inconsistency and large MOD. Pattern-mixture and selection models agreed for all NMA estimates. Conclusions: Analyzing informative MOD assuming MAR with different prior structures of log IMOR affected mainly the precision of NMA estimates. Reviewers should decide in advance on the prior structure of log IMOR that best aligns with the condition and interventions investigated.},
  file = {/Users/rritaz/Zotero/storage/WJVYC97X/Spineli et al. - 2019 - Participants' outcomes gone missing within a netwo.pdf;/Users/rritaz/Zotero/storage/4DE6UAUC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,missing data,network meta-analysis},
  language = {en},
  number = {20}
}

@article{spittal_meta-analysis_2015,
  title = {Meta-Analysis of Incidence Rate Data in the Presence of Zero Events},
  author = {Spittal, Matthew J and Pirkis, Jane and Gurrin, Lyle C},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {42},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0031-0},
  abstract = {Background: When summary results from studies of counts of events in time contain zeros, the study-specific incidence rate ratio (IRR) and its standard error cannot be calculated because the log of zero is undefined. This poses problems for the widely used inverse-variance method that weights the study-specific IRRs to generate a pooled estimate. Methods: We conducted a simulation study to compare the inverse-variance method of conducting a meta-analysis (with and without the continuity correction) with alternative methods based on either Poisson regression with fixed interventions effects or Poisson regression with random intervention effects. We manipulated the percentage of zeros in the intervention group (from no zeros to approximately 80 percent zeros), the levels of baseline variability and heterogeneity in the intervention effect, and the number of studies that comprise each meta-analysis. We applied these methods to an example from our own work in suicide prevention and to a recent meta-analysis of the effectiveness of condoms in preventing HIV transmission. Results: As the percentage of zeros in the data increased, the inverse-variance method of pooling data shows increased bias and reduced coverage. Estimates from Poisson regression with fixed interventions effects also display evidence of bias and poor coverage, due to their inability to account for heterogeneity. Pooled IRRs from Poisson regression with random intervention effects were unaffected by the percentage of zeros in the data or the amount of heterogeneity. Conclusion: Inverse-variance methods perform poorly when the data contains zeros in either the control or intervention arms. Methods based on Poisson regression with random effect terms for the variance components are very flexible offer substantial improvement.},
  file = {/Users/rritaz/Zotero/storage/MQCK6Y4L/Spittal et al. - 2015 - Meta-analysis of incidence rate data in the presen.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {effect size combination (small sample \& discrete)},
  language = {en},
  number = {1}
}

@article{stanley_finding_2017,
  title = {Finding the Power to Reduce Publication Bias},
  author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A.},
  year = {2017},
  volume = {36},
  pages = {1580--1598},
  issn = {1097-0258},
  doi = {10.1002/sim.7228},
  abstract = {The central purpose of this study is to document how a sharper focus upon statistical power may reduce the impact of selective reporting bias in meta-analyses. We introduce the weighted average of the adequately powered (WAAP) as an alternative to the conventional random-effects (RE) estimator. When the results of some of the studies have been selected to be positive and statistically significant (i.e. selective reporting), our simulations show that WAAP will have smaller bias than RE at no loss to its other statistical properties. When there is no selective reporting, the difference between RE's and WAAP's statistical properties is practically negligible. Nonetheless, when selective reporting is especially severe or heterogeneity is very large, notable bias can remain in all weighted averages. The main limitation of this approach is that the majority of meta-analyses of medical research do not contain any studies with adequate power (i.e. {$>$}80\%). For such areas of medical research, it remains important to document their low power, and, as we demonstrate, an alternative unrestricted weighted least squares weighted average can be used instead of WAAP. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3XLC8KGZ/Stanley et al. - 2017 - Finding the power to reduce publication bias.pdf;/Users/rritaz/Zotero/storage/9HRM7IB7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {power,publication bias},
  language = {en},
  number = {10}
}

@article{stanley_meta-regression_2014,
  title = {Meta-Regression Approximations to Reduce Publication Selection Bias},
  author = {Stanley, T. D. and Doucouliagos, Hristos},
  year = {2014},
  volume = {5},
  pages = {60--78},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1095},
  abstract = {Publication selection bias is a serious challenge to the integrity of all empirical sciences. We derive meta-regression approximations to reduce this bias. Our approach employs Taylor polynomial approximations to the conditional mean of a truncated distribution. A quadratic approximation without a linear term, precision-effect estimate with standard error (PEESE), is shown to have the smallest bias and mean squared error in most cases and to outperform conventional meta-analysis estimators, often by a great deal. Monte Carlo simulations also demonstrate how a new hybrid estimator that conditionally combines PEESE and the Egger regression intercept can provide a practical solution to publication selection bias. PEESE is easily expanded to accommodate systematic heterogeneity along with complex and differential publication selection bias that is related to moderator variables. By providing an intuitive reason for these approximations, we can also explain why the Egger regression works so well and when it does not. These meta-regression methods are applied to several policy-relevant areas of research including antidepressant effectiveness, the value of a statistical life, the minimum wage, and nicotine replacement therapy. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/85EYFLXB/Stanley and Doucouliagos - 2014 - Meta-regression approximations to reduce publicati.pdf;/Users/rritaz/Zotero/storage/DQLLYBPV/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {modeling effect size variation (covariates),publication bias},
  language = {en},
  number = {1}
}

@article{stanley_neither_2015,
  title = {Neither Fixed nor Random: Weighted Least Squares Meta-Analysis},
  shorttitle = {Neither Fixed nor Random},
  author = {Stanley, T. D. and Doucouliagos, Hristos},
  year = {2015},
  volume = {34},
  pages = {2116--2127},
  issn = {1097-0258},
  doi = {10.1002/sim.6481},
  abstract = {This study challenges two core conventional meta-analysis methods: fixed effect and random effects. We show how and explain why an unrestricted weighted least squares estimator is superior to conventional random-effects meta-analysis when there is publication (or small-sample) bias and better than a fixed-effect weighted average if there is heterogeneity. Statistical theory and simulations of effect sizes, log odds ratios and regression coefficients demonstrate that this unrestricted weighted least squares estimator provides satisfactory estimates and confidence intervals that are comparable to random effects when there is no publication (or small-sample) bias and identical to fixed-effect meta-analysis when there is no heterogeneity. When there is publication selection bias, the unrestricted weighted least squares approach dominates random effects; when there is excess heterogeneity, it is clearly superior to fixed-effect meta-analysis. In practical applications, an unrestricted weighted least squares weighted average will often provide superior estimates to both conventional fixed and random effects. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/YD9Z52ME/Stanley and Doucouliagos - 2015 - Neither fixed nor random weighted least squares m.pdf;/Users/rritaz/Zotero/storage/MPYMMVZM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size estimation (series),random effects models,random-effects},
  language = {en},
  number = {13}
}

@article{stanley_neither_2017,
  title = {Neither Fixed nor Random: Weighted Least Squares Meta-Regression},
  shorttitle = {Neither Fixed nor Random},
  author = {Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  volume = {8},
  pages = {19--42},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1211},
  abstract = {Our study revisits and challenges two core conventional meta-regression estimators: the prevalent use of `mixed-effects' or random-effects meta-regression analysis and the correction of standard errors that defines fixed-effects meta-regression analysis (FE-MRA). We show how and explain why an unrestricted weighted least squares MRA (WLS-MRA) estimator is superior to conventional random-effects (or mixed-effects) meta-regression when there is publication (or small-sample) bias that is as good as FE-MRA in all cases and better than fixed effects in most practical applications. Simulations and statistical theory show that WLS-MRA provides satisfactory estimates of meta-regression coefficients that are practically equivalent to mixed effects or random effects when there is no publication bias. When there is publication selection bias, WLS-MRA always has smaller bias than mixed effects or random effects. In practical applications, an unrestricted WLS meta-regression is likely to give practically equivalent or superior estimates to fixed-effects, random-effects, and mixed-effects meta-regression approaches. However, random-effects meta-regression remains viable and perhaps somewhat preferable if selection for statistical significance (publication bias) can be ruled out and when random, additive normal heterogeneity is known to directly affect the `true' regression coefficient. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SPH96WYE/Stanley and Doucouliagos - 2017 - Neither fixed nor random weighted least squares m.pdf;/Users/rritaz/Zotero/storage/QT3JBXC2/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {GLM MA models,publication bias,random-effects},
  language = {en},
  number = {1}
}

@article{steinhauser_modelling_2016,
  title = {Modelling Multiple Thresholds in Meta-Analysis of Diagnostic Test Accuracy Studies},
  author = {Steinhauser, Susanne and Schumacher, Martin and R{\"u}cker, Gerta},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {97},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0196-1},
  abstract = {Background: In meta-analyses of diagnostic test accuracy, routinely only one pair of sensitivity and specificity per study is used. However, for tests based on a biomarker or a questionnaire often more than one threshold and the corresponding values of true positives, true negatives, false positives and false negatives are known. Methods: We present a new meta-analysis approach using this additional information. It is based on the idea of estimating the distribution functions of the underlying biomarker or questionnaire within the non-diseased and diseased individuals. Assuming a normal or logistic distribution, we estimate the distribution parameters in both groups applying a linear mixed effects model to the transformed data. The model accounts for across-study heterogeneity and dependence of sensitivity and specificity. In addition, a simulation study is presented. Results: We obtain a summary receiver operating characteristic (SROC) curve as well as the pooled sensitivity and specificity at every specific threshold. Furthermore, the determination of an optimal threshold across studies is possible through maximization of the Youden index. We demonstrate our approach using two meta-analyses of B type natriuretic peptide in heart failure and procalcitonin as a marker for sepsis. Conclusions: Our approach uses all the available information and results in an estimation not only of the performance of the biomarker but also of the threshold at which the optimal performance can be expected.},
  file = {/Users/rritaz/Zotero/storage/NU3Y2F8R/Steinhauser et al. - 2016 - Modelling multiple thresholds in meta-analysis of .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {physical/biological fields},
  language = {en},
  number = {1}
}

@article{sterne_statistical_2002,
  title = {Statistical Methods for Assessing the Influence of Study Characteristics on Treatment Effects in `Meta-Epidemiological' Research},
  author = {Sterne, Jonathan A. C. and J{\"u}ni, Peter and Schulz, Kenneth F. and Altman, Douglas G. and Bartlett, Christopher and Egger, Matthias},
  year = {2002},
  volume = {21},
  pages = {1513--1524},
  issn = {1097-0258},
  doi = {10.1002/sim.1184},
  abstract = {Biases in systematic reviews and meta-analyses may be examined in `meta-epidemiological' studies, in which the influence of trial characteristics such as measures of study quality on treatment effect estimates is explored. Published studies to date have analysed data from collections of meta-analyses with binary outcomes, using logistic regression models that assume that there is no between- or within-meta-analysis heterogeneity. Using data from a study of publication bias (39 meta-analyses, 394 published and 88 unpublished trials) and language bias (29 meta-analyses, 297 English language trials and 52 non-English language trials), we compare results from logistic regression models, with and without robust standard errors to allow for clustering on meta-analysis, with results using a `meta-meta-analytic' approach that can allow for between- and within-meta-analysis heterogeneity. We also consider how to allow for the confounding effects of different trial characteristics. We show that both within- and between meta-analysis heterogeneity may be of importance in the analysis of meta-epidemiological studies, and that confounding exists between the effects of publication status and trial quality. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/7NU3BPN7/Sterne et al. - 2002 - Statistical methods for assessing the influence of.pdf;/Users/rritaz/Zotero/storage/LGJZVYCQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {meta-meta-analyses,publication bias},
  language = {en},
  number = {11}
}

@article{stevens_hierarchical_2009,
  title = {Hierarchical {{Dependence}} in {{Meta}}-{{Analysis}}},
  author = {Stevens, John R. and Taylor, Alan M.},
  year = {2009},
  month = mar,
  volume = {34},
  pages = {46--73},
  issn = {1076-9986},
  doi = {10.3102/1076998607309080},
  abstract = {Meta-analysis is a frequent tool among education and behavioral researchers to combine results from multiple experiments to arrive at a clear understanding of some effect of interest. One of the traditional assumptions in a meta-analysis is the independence of the effect sizes from the studies under consideration. This article presents a meta-analytic review of 13 experiments with 18 study reports all involving the effect of native-language (L1) vocabulary aids on second-language (L2) reading comprehension. Some experiments produced multiple study reports, creating a dependence structure among the resulting effect size estimates. The covariance among these effect size estimates is estimated and incorporated into a proposed meta-analysis model that accounts for the dependence at a hierarchical level. The overall effect size estimate (g =.63) indicates that L1 vocabulary aids can be an effective L2 reading comprehension aid in the short term. An interpretation of the hierarchical components is discussed.},
  file = {/Users/rritaz/Zotero/storage/K38NEETC/Stevens and Taylor - 2009 - Hierarchical Dependence in Meta-Analysis.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {correlated effects},
  number = {1}
}

@article{steyerberg_prognostic_2000,
  title = {Prognostic Models Based on Literature and Individual Patient Data in Logistic Regression Analysis},
  author = {Steyerberg, E. W. and Eijkemans, M. J. C. and Houwelingen, J. C. Van and Lee, K. L. and Habbema, J. D. F.},
  year = {2000},
  volume = {19},
  pages = {141--160},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(20000130)19:2<141::AID-SIM334>3.0.CO;2-O},
  abstract = {Prognostic models can be developed with multiple regression analysis of a data set containing individual patient data. Often this data set is relatively small, while previously published studies present results for larger numbers of patients. We describe a method to combine univariable regression results from the medical literature with univariable and multivariable results from the data set containing individual patient data. This `adaptation method' exploits the generally strong correlation between univariable and multivariable regression coefficients. The method is illustrated with several logistic regression models to predict 30-day mortality in patients with acute myocardial infarction. The regression coefficients showed considerably less variability when estimated with the adaptation method, compared to standard maximum likelihood estimates. Also, model performance, as distinguished in calibration and discrimination, improved clearly when compared to models including shrunk or penalized estimates. We conclude that prognostic models may benefit substantially from explicit incorporation of literature data. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/X5V6L695/Steyerberg et al. - 2000 - Prognostic models based on literature and individu.pdf;/Users/rritaz/Zotero/storage/KB7BIQAZ/(SICI)1097-0258(20000130)192141AID-SIM3343.0.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {2}
}

@article{stijnen_random_2010,
  title = {Random Effects Meta-Analysis of Event Outcome in the Framework of the Generalized Linear Mixed Model with Applications in Sparse Data},
  author = {Stijnen, Theo and Hamza, Taye H. and {\"O}zdemir, Pinar},
  year = {2010},
  volume = {29},
  pages = {3046--3067},
  issn = {1097-0258},
  doi = {10.1002/sim.4040},
  abstract = {We consider random effects meta-analysis where the outcome variable is the occurrence of some event of interest. The data structures handled are where one has one or more groups in each study, and in each group either the number of subjects with and without the event, or the number of events and the total duration of follow-up is available. Traditionally, the meta-analysis follows the summary measures approach based on the estimates of the outcome measure(s) and the corresponding standard error(s). This approach assumes an approximate normal within-study likelihood and treats the standard errors as known. This approach has several potential disadvantages, such as not accounting for the standard errors being estimated, not accounting for correlation between the estimate and the standard error, the use of an (arbitrary) continuity correction in case of zero events, and the normal approximation being bad in studies with few events. We show that these problems can be overcome in most cases occurring in practice by replacing the approximate normal within-study likelihood by the appropriate exact likelihood. This leads to a generalized linear mixed model that can be fitted in standard statistical software. For instance, in the case of odds ratio meta-analysis, one can use the non-central hypergeometric distribution likelihood leading to mixed-effects conditional logistic regression. For incidence rate ratio meta-analysis, it leads to random effects logistic regression with an offset variable. We also present bivariate and multivariate extensions. We present a number of examples, especially with rare events, among which an example of network meta-analysis. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/RUYWQR7D/Stijnen et al. - 2010 - Random effects meta-analysis of event outcome in t.pdf;/Users/rritaz/Zotero/storage/UYCRZ7NC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {discrete effect sizes,GLM MA models,random-effects},
  language = {en},
  number = {29}
}

@article{stukel_two-stage_2001,
  title = {Two-Stage Methods for the Analysis of Pooled Data},
  author = {Stukel, Therese A. and Demidenko, Eugene and Dykes, James and Karagas, Margaret R.},
  year = {2001},
  volume = {20},
  pages = {2115--2130},
  issn = {1097-0258},
  doi = {10.1002/sim.852},
  abstract = {Epidemiologic studies of disease often produce inconclusive or contradictory results due to small sample sizes or regional variations in the disease incidence or the exposures. To clarify these issues, researchers occasionally pool and reanalyse original data from several large studies. In this paper we explore the use of a two-stage random-effects model for analysing pooled case-control studies and undertake a thorough examination of bias in the pooled estimator under various conditions. The two-stage model analyses each study using the model appropriate to the design with study-specific confounders, and combines the individual study-specific adjusted log-odds ratios using a linear mixed-effects model; it is computationally simple and can incorporate study-level covariates and random effects. Simulations indicate that when the individual studies are large, two-stage methods produce nearly unbiased exposure estimates and standard errors of the exposure estimates from a generalized linear mixed model. By contrast, joint fixed-effects logistic regression produces attenuated exposure estimates and underestimates the standard error when heterogeneity is present. While bias in the pooled regression coefficient increases with interstudy heterogeneity for both models, it is much smaller using the two-stage model. In pooled analyses, where covariates may not be uniformly defined and coded across studies, and occasionally not measured in all studies, a joint model is often not feasible. The two-stage method is shown to be a simple, valid and practical method for the analysis of pooled binary data. The results are applied to a study of reproductive history and cutaneous melanoma risk in women using data from ten large case-control studies. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/MPI5ZSEX/Stukel et al. - 2001 - Two-stage methods for the analysis of pooled data.pdf;/Users/rritaz/Zotero/storage/J9Z95KLW/sim.html},
  journal = {Statistics in Medicine},
  keywords = {GLM MA models,modeling effect size variation (covariates)},
  language = {en},
  number = {14}
}

@article{su_statistical_2018,
  title = {Statistical Approaches to Adjusting Weights for Dependent Arms in Network Meta-Analysis},
  author = {Su, Yu-Xuan and Tu, Yu-Kang},
  year = {2018},
  volume = {9},
  pages = {431--440},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1304},
  abstract = {Network meta-analysis compares multiple treatments in terms of their efficacy and harm by including evidence from randomized controlled trials. Most clinical trials use parallel design, where patients are randomly allocated to different treatments and receive only 1 treatment. However, some trials use within person designs such as split-body, split-mouth, and crossover designs, where each patient may receive more than one treatment. Data from treatment arms within these trials are no longer independent, so the correlations between dependent arms need to be accounted for within the statistical analyses. Ignoring these correlations may result in incorrect conclusions. The main objective of this study is to develop statistical approaches to adjusting weights for dependent arms within special design trials. In this study, we demonstrate the following 3 approaches: the data augmentation approach, the adjusting variance approach, and the reducing weight approach. These 3 methods could be perfectly applied in current statistical tools such as R and STATA. An example of periodontal regeneration was used to demonstrate how these approaches could be undertaken and implemented within statistical software packages and to compare results from different approaches. The adjusting variance approach can be implemented within the network package in STATA, while reducing weight approach requires computer software programming to set up the within-study variance-covariance matrix.},
  file = {/Users/rritaz/Zotero/storage/98LZ4TNS/Su and Tu - 2018 - Statistical approaches to adjusting weights for de.pdf;/Users/rritaz/Zotero/storage/5Y37LX2A/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis},
  language = {en},
  number = {3}
}

@article{sutton_evidence-based_2007,
  title = {Evidence-Based Sample Size Calculations Based upon Updated Meta-Analysis},
  author = {Sutton, Alexander J. and Cooper, Nicola J. and Jones, David R. and Lambert, Paul C. and Thompson, John R. and Abrams, Keith R.},
  year = {2007},
  volume = {26},
  pages = {2479--2500},
  issn = {1097-0258},
  doi = {10.1002/sim.2704},
  abstract = {Meta-analyses of randomized controlled trials (RCTs) provide the highest level of evidence regarding the effectiveness of interventions and as such underpin much of evidence-based medicine. Despite this, meta-analyses are usually produced as observational by-products of the existing literature, with no formal consideration of future meta-analyses when individual trials are being designed. Basing the sample size of a new trial on the results of an updated meta-analysis which will include it, may sometimes make more sense than powering the trial in isolation. A framework for sample size calculation for a future RCT based on the results of a meta-analysis of the existing evidence is presented. Both fixed and random effect approaches are explored through an example. Bayesian Markov Chain Monte Carlo simulation modelling is used for the random effects model since it has computational advantages over the classical approach. Several criteria on which to base inference and hence power are considered. The prior expectation of the power is averaged over the prior distribution for the unknown true treatment effect. An extension to the framework allowing for consideration of the design for a series of new trials is also presented. Results suggest that power can be highly dependent on the statistical model used to meta-analyse the data and even very large studies may have little impact on a meta-analysis when there is considerable between study heterogeneity. This raises issues regarding the appropriateness of the use of random effect models when designing and drawing inferences across a series of studies. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5B9Z2SKJ/Sutton et al. - 2007 - Evidence-based sample size calculations based upon.pdf;/Users/rritaz/Zotero/storage/G4EXHQFC/sim.html},
  journal = {Statistics in Medicine},
  keywords = {causality,power,random-effects},
  language = {en},
  number = {12}
}

@article{sutton_meta-analysis_2008,
  title = {Meta-Analysis of Individual- and Aggregate-Level Data},
  author = {Sutton, A. J. and Kendrick, D. and Coupland, C. a. C.},
  year = {2008},
  volume = {27},
  pages = {651--669},
  issn = {1097-0258},
  doi = {10.1002/sim.2916},
  abstract = {The methodology described here was developed for a systematic review and individual participant-level meta-analysis of home safety education and the provision of safety equipment for the prevention of childhood accidents. This review had a particular emphasis on exploring whether effectiveness was related to socio-demographic characteristics previously shown to be associated with injury risk. Individual participant data were only made available to us for a proportion of the included studies. This resulted in the need for developing a new methodology to combine the available data most efficiently. Our objective was to develop a (random effects) meta-analysis model that could synthesize both individual-level and aggregate-level binary outcome data while exploring the effects of binary covariates also available in a combination of individual participant and aggregate level data. To add further complication, the studies to be combined were a mixture of cluster and individual participant-allocated designs. A Bayesian model using Markov chain Monte Carlo methods to estimate parameters is described which efficiently synthesizes the data by allowing different models to be fitted to the different study design and data format combinations available. Initially we describe a model to estimate mean effects ignoring the influence of the covariates, and then extend it to include a binary covariate. The application of the method is illustrated by application to one outcome from the motivating home safety meta-analysis for illustration. Using the same general approach, it would be possible to develop further `tailor made' evidence synthesis models to synthesize all available evidence most effectively. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/PXSZQIZN/Sutton et al. - 2008 - Meta-analysis of individual- and aggregate-level d.pdf;/Users/rritaz/Zotero/storage/2XQI6G7F/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {5}
}

@article{sutton_recent_2008,
  title = {Recent Developments in Meta-Analysis},
  author = {Sutton, Alexander J. and Higgins, Julian P. T.},
  year = {2008},
  volume = {27},
  pages = {625--650},
  issn = {1097-0258},
  doi = {10.1002/sim.2934},
  abstract = {The art and science of meta-analysis, the combination of results from multiple independent studies, is now more than a century old. In the last 30 years, however, as the need for medical research and clinical practice to be based on the totality of relevant and sound evidence has been increasingly recognized, the impact of meta-analysis has grown enormously. In this paper, we review highlights of recent developments in meta-analysis in medical research. We outline in particular how emphasis has been placed on (i) heterogeneity and random-effects analyses; (ii) special consideration in different areas of application; (iii) assessing bias within and across studies; and (iv) extension of ideas to complex evidence synthesis. We conclude the paper with some remarks on ongoing challenges and possible directions for the future. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2934},
  copyright = {Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/KHSGA8IW/Sutton and Higgins - 2008 - Recent developments in meta-analysis.pdf;/Users/rritaz/Zotero/storage/96MHI8F2/sim.html},
  journal = {Statistics in Medicine},
  keywords = {evidence synthesis,meta-analysis,review,systematic review},
  language = {en},
  number = {5}
}

@article{sweeting_what_2004,
  title = {What to Add to Nothing? {{Use}} and Avoidance of Continuity Corrections in Meta-Analysis of Sparse Data},
  shorttitle = {What to Add to Nothing?},
  author = {Sweeting, Michael J. and Sutton, Alexander J. and Lambert, Paul C.},
  year = {2004},
  volume = {23},
  pages = {1351--1375},
  issn = {1097-0258},
  doi = {10.1002/sim.1761},
  abstract = {Objectives: To compare the performance of different meta-analysis methods for pooling odds ratios when applied to sparse event data with emphasis on the use of continuity corrections. Background: Meta-analysis of side effects from RCTs or risk factors for rare diseases in epidemiological studies frequently requires the synthesis of data with sparse event rates. Combining such data can be problematic when zero events exist in one or both arms of a study as continuity corrections are often needed, but, these can influence results and conclusions. Methods: A simulation study was undertaken comparing several meta-analysis methods for combining odds ratios (using various classical and Bayesian methods of estimation) on sparse event data. Where required, the routine use of a constant and two alternative continuity corrections; one based on a function of the reciprocal of the opposite group arm size; and the other an empirical estimate of the pooled effect size from the remaining studies in the meta-analysis, were also compared. A number of meta-analysis scenarios were simulated and replicated 1000 times, varying the ratio of the study arm sizes. Results: Mantel\textendash Haenszel summary estimates using the alternative continuity correction factors gave the least biased results for all group size imbalances. Logistic regression was virtually unbiased for all scenarios and gave good coverage properties. The Peto method provided unbiased results for balanced treatment groups but bias increased with the ratio of the study arm sizes. The Bayesian fixed effect model provided good coverage for all group size imbalances. The two alternative continuity corrections outperformed the constant correction factor in nearly all situations. The inverse variance method performed consistently badly, irrespective of the continuity correction used. Conclusions: Many routinely used summary methods provide widely ranging estimates when applied to sparse data with high imbalance between the size of the studies' arms. A sensitivity analysis using several methods and continuity correction factors is advocated for routine practice. Copyright 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/X64IBQGS/Sweeting et al. - 2004 - What to add to nothing Use and avoidance of conti.pdf;/Users/rritaz/Zotero/storage/A6CR893G/sim.html},
  journal = {Statistics in Medicine},
  keywords = {effect size combination (small sample \& discrete)},
  language = {en},
  number = {9}
}

@article{takeshima_which_2014,
  title = {Which Is More Generalizable, Powerful and Interpretable in Meta-Analyses, Mean Difference or Standardized Mean Difference?},
  author = {Takeshima, Nozomi and Sozu, Takashi and Tajika, Aran and Ogawa, Yusuke and Hayasaka, Yu and Furukawa, Toshiaki A},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {30},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-30},
  abstract = {Background: To examine empirically whether the mean difference (MD) or the standardised mean difference (SMD) is more generalizable and statistically powerful in meta-analyses of continuous outcomes when the same unit is used. Methods: From all the Cochrane Database (March 2013), we identified systematic reviews that combined 3 or more randomised controlled trials (RCT) using the same continuous outcome. Generalizability was assessed using the I-squared (I2) and the percentage agreement. The percentage agreement was calculated by comparing the MD or SMD of each RCT with the corresponding MD or SMD from the meta-analysis of all the other RCTs. The statistical power was estimated using Z-scores. Meta-analyses were conducted using both random-effects and fixed-effect models. Results: 1068 meta-analyses were included. The I2 index was significantly smaller for the SMD than for the MD (P {$<$} 0.0001, sign test). For continuous outcomes, the current Cochrane reviews pooled some extremely heterogeneous results. When all these or less heterogeneous subsets of the reviews were examined, the SMD always showed a greater percentage agreement than the MD. When the I2 index was less than 30\%, the percentage agreement was 55.3\% for MD and 59.8\% for SMD in the random-effects model and 53.0\% and 59.8\%, respectively, in the fixed effect model (both P {$<$} 0.0001, sign test). Although the Z-scores were larger for MD than for SMD, there were no differences in the percentage of statistical significance between MD and SMD in either model. Conclusions: The SMD was more generalizable than the MD. The MD had a greater statistical power than the SMD but did not result in material differences.},
  file = {/Users/rritaz/Zotero/storage/K8DS6JVY/Takeshima et al. - 2014 - Which is more generalizable, powerful and interpre.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,random-effects,standardized mean difference},
  language = {en},
  number = {1}
}

@article{talbot_meta-analysis_nodate,
  title = {{{META}}-{{ANALYSIS OF CORRELATION COEFFICIENTS}}\_{{A MONTE CARLO COMPARISON OF FIXED}}- {{AND RANDOM}}-{{EFFECTS METHODS}}},
  author = {Talbot, Jade},
  pages = {51},
  file = {/Users/rritaz/Zotero/storage/W8E7YQK2/Talbot - META-ANALYSIS OF CORRELATION COEFFICIENTS_A MONTE .pdf},
  keywords = {correlation coefficients,random-effects},
  language = {en}
}

@article{terrin_adjusting_2003,
  title = {Adjusting for Publication Bias in the Presence of Heterogeneity},
  author = {Terrin, Norma and Schmid, Christopher H. and Lau, Joseph and Olkin, Ingram},
  year = {2003},
  volume = {22},
  pages = {2113--2126},
  issn = {1097-0258},
  doi = {10.1002/sim.1461},
  abstract = {It is known that the existence of publication bias can influence the conclusions of a meta-analysis. Some methods have been developed to deal with publication bias, but issues remain. One particular method called `trim and fill' is designed to adjust for publication bias. The method, which is intuitively appealing and comprehensible by non-statisticians, is based on a simple and popular graphical tool called the funnel plot. We present a simulation study designed to evaluate the behaviour of this method. Our results indicate that when the studies are heterogeneous (that is, when they estimate different effects), trim and fill may inappropriately adjust for publication bias where none exists. We found that trim and fill may spuriously adjust for non-existent bias if (i) the variability among studies causes some precisely estimated studies to have effects far from the global mean or (ii) an inverse relationship between treatment efficacy and sample size is introduced by the studies' a priori power calculations. The results suggest that the funnel plot itself is inappropriate for heterogeneous meta-analyses. Selection modelling is an alternative method warranting further study. It performed better than trim and fill in our simulations, although its frequency of convergence varied, depending on the simulation parameters. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/MN4RC4PT/Terrin et al. - 2003 - Adjusting for publication bias in the presence of .pdf;/Users/rritaz/Zotero/storage/C2S7FKV7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {publication bias},
  language = {en},
  number = {13}
}

@article{thakkinstian_method_2005,
  title = {A Method for Meta-Analysis of Molecular Association Studies},
  author = {Thakkinstian, Ammarin and McElduff, Patrick and D'Este, Catherine and Duffy, David and Attia, John},
  year = {2005},
  volume = {24},
  pages = {1291--1306},
  issn = {1097-0258},
  doi = {10.1002/sim.2010},
  abstract = {Although population-based molecular association studies are becoming increasingly popular, methodology for the meta-analysis of these studies has been neglected, particularly with regard to two issues: testing Hardy\textendash Weinberg equilibrium (HWE), and pooling results in a manner that reflects a biological model of gene effect. We propose a process for pooling results from population-based molecular association studies which consists of the following steps: (1) checking HWE using chi-square goodness of fit; we suggest performing sensitivity analysis with and without studies that are in HWE. (2) Heterogeneity is then checked, and if present, possible causes are explored. (3) If no heterogeneity is present, regression analysis is used to pool data and to determine the gene effect. (4) If there is a significant gene effect, pairwise group differences are analysed and these data are allowed to `dictate' the best genetic model. (5) Data may then be pooled using this model. This method is easily performed using standard software, and has the advantage of not assuming an a priori genetic model. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/DJZLKJJM/Thakkinstian et al. - 2005 - A method for meta-analysis of molecular associatio.pdf;/Users/rritaz/Zotero/storage/EA7QGXFM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {physical/biological fields,random-effects},
  language = {en},
  number = {9}
}

@article{thom_automated_2019,
  title = {Automated Methods to Test Connectedness and Quantify Indirectness of Evidence in Network Meta-Analysis},
  author = {Thom, Howard and White, Ian R. and Welton, Nicky J. and Lu, Guobing},
  year = {2019},
  volume = {10},
  pages = {113--124},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1329},
  abstract = {Network meta-analysis compares multiple treatments from studies that form a connected network of evidence. However, for complex networks, it is not easy to see if the network is connected. We use simple techniques from graph theory to test the connectedness of evidence networks in network meta-analysis. The method is to build the adjacency matrix for a network, with rows and columns corresponding to the treatments in the network and entries being one or zero depending on whether the treatments have been compared or not, and with zeros along the diagonal. Manipulation of this matrix gives the indirect connection matrix. The entries of this matrix determine whether two treatments can be compared, directly or indirectly. We also describe the distance matrix, which gives the minimum number of steps in the network required to compare a pair of treatments. This is a useful assessment of an indirect comparison as each additional step requires further assumptions of homogeneity in, for example, design and target populations of included trials. If there are no loops in the network, the distance is a measure of the degree of assumptions needed; it is approximately this with loops. We illustrate our methods using several constructed examples and giving R code for computation. We have also implemented the techniques in the Stata package ``network.'' The methods provide a fast way to ensure comparisons are only made between connected treatments and to assess the degree of indirectness of a comparison.},
  file = {/Users/rritaz/Zotero/storage/K3AE3C6L/Thom et al. - 2019 - Automated methods to test connectedness and quanti.pdf;/Users/rritaz/Zotero/storage/BTA8FF3G/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {1}
}

@article{thom_network_2015,
  title = {Network Meta-Analysis Combining Individual Patient and Aggregate Data from a Mixture of Study Designs with an Application to Pulmonary Arterial Hypertension},
  author = {Thom, Howard HZ and Capkun, Gorana and Cerulli, Annamaria and Nixon, Richard M and Howard, Luke S},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {34},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0007-0},
  abstract = {Background: Network meta-analysis (NMA) is a methodology for indirectly comparing, and strengthening direct comparisons of two or more treatments for the management of disease by combining evidence from multiple studies. It is sometimes not possible to perform treatment comparisons as evidence networks restricted to randomized controlled trials (RCTs) may be disconnected. We propose a Bayesian NMA model that allows to include single-arm, before-and-after, observational studies to complete these disconnected networks. We illustrate the method with an indirect comparison of treatments for pulmonary arterial hypertension (PAH). Methods: Our method uses a random effects model for placebo improvements to include single-arm observational studies into a general NMA. Building on recent research for binary outcomes, we develop a covariate-adjusted continuous-outcome NMA model that combines individual patient data (IPD) and aggregate data from two-arm RCTs with the single-arm observational studies. We apply this model to a complex comparison of therapies for PAH combining IPD from a phase-III RCT of imatinib as add-on therapy for PAH and aggregate data from RCTs and single-arm observational studies, both identified by a systematic review. Results: Through the inclusion of observational studies, our method allowed the comparison of imatinib as add-on therapy for PAH with other treatments. This comparison had not been previously possible due to the limited RCT evidence available. However, the credible intervals of our posterior estimates were wide so the overall results were inconclusive. The comparison should be treated as exploratory and should not be used to guide clinical practice. Conclusions: Our method for the inclusion of single-arm observational studies allows the performance of indirect comparisons that had previously not been possible due to incomplete networks composed solely of available RCTs. We also built on many recent innovations to enable researchers to use both aggregate data and IPD. This method could be used in similar situations where treatment comparisons have not been possible due to restrictions to RCT evidence and where a mixture of aggregate data and IPD are available.},
  file = {/Users/rritaz/Zotero/storage/29FGT249/Thom et al. - 2015 - Network meta-analysis combining individual patient.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,Individual Patient Data IPD,network meta-analysis},
  language = {en},
  number = {1}
}

@article{thomas_comparison_2017,
  title = {A Comparison of Analytic Approaches for Individual Patient Data Meta-Analyses with Binary Outcomes},
  author = {Thomas, Doneal and Platt, Robert and Benedetti, Andrea},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {28},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0307-7},
  abstract = {Background: Individual patient data meta-analyses (IPD-MA) are often performed using a one-stage approach\textendash{} a form of generalized linear mixed model (GLMM) for binary outcomes. We compare (i) one-stage to two-stage approaches (ii) the performance of two estimation procedures (Penalized Quasi-likelihood-PQL and Adaptive Gaussian Hermite Quadrature-AGHQ) for GLMMs with binary outcomes within the one-stage approach and (iii) using stratified study-effect or random study-effects. Methods: We compare the different approaches via a simulation study, in terms of bias, mean-squared error (MSE), coverage and numerical convergence, of the pooled treatment effect ({$\beta$}1) and between-study heterogeneity of the treatment effect ({$\tau$}12). We varied the prevalence of the outcome, sample size, number of studies and variances and correlation of the random effects. Results: The two-stage and one-stage methods produced approximately unbiased {$\beta$}1 estimates. PQL performed better than AGHQ for estimating {$\tau$}12 with respect to MSE, but performed comparably with AGHQ in estimating the bias of {$\beta$}1 and of {$\tau$}12. The random study-effects model outperformed the stratified study-effects model in small size MA. Conclusion: The one-stage approach is recommended over the two-stage method for small size MA. There was no meaningful difference between the PQL and AGHQ procedures. Though the random-intercept and stratified-intercept approaches can suffer from their underlining assumptions, fitting GLMM with a random-intercept are less prone to misfit and has good convergence rate.},
  file = {/Users/rritaz/Zotero/storage/HPGBY9D7/Thomas et al. - 2017 - A comparison of analytic approaches for individual.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {GLM MA models,Individual Patient Data IPD},
  language = {en},
  number = {1}
}

@article{thompson_impact_2014,
  title = {The Impact of Multiple Endpoint Dependency on {{Q}} and {{I2}} in Meta-Analysis},
  author = {Thompson, Christopher Glen and Becker, Betsy Jane},
  year = {2014},
  volume = {5},
  pages = {235--253},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1110},
  abstract = {A common assumption in meta-analysis is that effect sizes are independent. When correlated effect sizes are analyzed using traditional univariate techniques, this assumption is violated. This research assesses the impact of dependence arising from treatment-control studies with multiple endpoints on homogeneity measures Q and I2 in scenarios using the unbiased standardized-mean-difference effect size. Univariate and multivariate meta-analysis methods are examined. Conditions included different overall outcome effects, study sample sizes, numbers of studies, between-outcomes correlations, dependency structures, and ways of computing the correlation. The univariate approach used typical fixed-effects analyses whereas the multivariate approach used generalized least-squares (GLS) estimates of a fixed-effects model, weighted by the inverse variance\textendash covariance matrix. Increased dependence among effect sizes led to increased Type I error rates from univariate models. When effect sizes were strongly dependent, error rates were drastically higher than nominal levels regardless of study sample size and number of studies. In contrast, using GLS estimation to account for multiple-endpoint dependency maintained error rates within nominal levels. Conversely, mean I2 values were not greatly affected by increased amounts of dependency. Last, we point out that the between-outcomes correlation should be estimated as a pooled within-groups correlation rather than using a full-sample estimator that does not consider treatment/control group membership. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/7ZJ6AYAP/Thompson and Becker - 2014 - The impact of multiple endpoint dependency on Q an.pdf;/Users/rritaz/Zotero/storage/RYSGZL3T/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {correlated effects,GLM MA models,power,random-effects},
  language = {en},
  number = {3}
}

@article{thompson_mendelian_2017,
  title = {Mendelian Randomization Incorporating Uncertainty about Pleiotropy},
  author = {Thompson, John R. and Minelli, Cosetta and Bowden, Jack and Greco, Fabiola M. Del and Gill, Dipender and Jones, Elinor M. and Shapland, Chin Yang and Sheehan, Nuala A.},
  year = {2017},
  volume = {36},
  pages = {4627--4645},
  issn = {1097-0258},
  doi = {10.1002/sim.7442},
  abstract = {Mendelian randomization (MR) requires strong assumptions about the genetic instruments, of which the most difficult to justify relate to pleiotropy. In a two-sample MR, different methods of analysis are available if we are able to assume, M1: no pleiotropy (fixed effects meta-analysis), M2: that there may be pleiotropy but that the average pleiotropic effect is zero (random effects meta-analysis), and M3: that the average pleiotropic effect is nonzero (MR-Egger). In the latter 2 cases, we also require that the size of the pleiotropy is independent of the size of the effect on the exposure. Selecting one of these models without good reason would run the risk of misrepresenting the evidence for causality. The most conservative strategy would be to use M3 in all analyses as this makes the weakest assumptions, but such an analysis gives much less precise estimates and so should be avoided whenever stronger assumptions are credible. We consider the situation of a two-sample design when we are unsure which of these 3 pleiotropy models is appropriate. The analysis is placed within a Bayesian framework and Bayesian model averaging is used. We demonstrate that even large samples of the scale used in genome-wide meta-analysis may be insufficient to distinguish the pleiotropy models based on the data alone. Our simulations show that Bayesian model averaging provides a reasonable trade-off between bias and precision. Bayesian model averaging is recommended whenever there is uncertainty about the nature of the pleiotropy.},
  file = {/Users/rritaz/Zotero/storage/2GKE67D4/Thompson et al. - 2017 - Mendelian randomization incorporating uncertainty .pdf;/Users/rritaz/Zotero/storage/994N9LQL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,physical/biological fields},
  language = {en},
  number = {29}
}

@article{thompson_meta-analysis_2005,
  title = {Meta-Analysis of Genetic Studies Using {{Mendelian}} Randomization\textemdash a Multivariate Approach},
  author = {Thompson, John R. and Minelli, Cosetta and Abrams, Keith R. and Tobin, Martin D. and Riley, Richard D.},
  year = {2005},
  volume = {24},
  pages = {2241--2254},
  issn = {1097-0258},
  doi = {10.1002/sim.2100},
  abstract = {In traditional epidemiological studies the association between phenotype (risk factor) and disease is often biased by confounding and reverse causation. As a person's genotype is assigned by a seemingly random process, genes are potentially useful instrumental variables for adjusting for such bias. This type of adjustment combines information on the genotype\textendash disease association and the genotype\textendash phenotype association to estimate the phenotype\textendash disease association and has become known as Mendelian randomization. The information on genotype\textendash disease and genotype\textendash phenotype may well come from a meta-analysis. In such a synthesis, a multivariate approach needs to be used whenever some studies provide evidence on both the genotype\textendash phenotype and genotype\textendash disease associations. This paper presents two multivariate meta-analytical models, which differ in their treatment of the heterogeneities (between-study variances). Heterogeneities on the genotype\textendash phenotype and genotype\textendash disease associations may be highly correlated, but a multivariate model that parameterizes the heterogeneity directly is difficult to fit because that correlation is poorly estimated. We advocate an alternative model that treats the heterogeneities on genotype\textendash phenotype and phenotype\textendash disease as being independent. This model fits readily and implicitly defines the correlation between the heterogeneities on genotype\textendash phenotype and genotype\textendash disease. We show how either maximum likelihood or a Bayesian approach with vague prior distributions can be used to fit the alternative model. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/QWHU7DAM/Thompson et al. - 2005 - Meta-analysis of genetic studies using Mendelian r.pdf;/Users/rritaz/Zotero/storage/8653NDXN/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,physical/biological fields,random-effects},
  language = {en},
  number = {14}
}

@article{thorlund_modelling_2013,
  title = {Modelling Heterogeneity Variances in Multiple Treatment Comparison Meta-Analysis \textendash{} {{Are}} Informative Priors the Better Solution?},
  author = {Thorlund, Kristian and Thabane, Lehana and Mills, Edward J},
  year = {2013},
  month = dec,
  volume = {13},
  pages = {2},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-2},
  abstract = {Background: Multiple treatment comparison (MTC) meta-analyses are commonly modeled in a Bayesian framework, and weakly informative priors are typically preferred to mirror familiar data driven frequentist approaches. Random-effects MTCs have commonly modeled heterogeneity under the assumption that the between-trial variance for all involved treatment comparisons are equal (i.e., the `common variance' assumption). This approach `borrows strength' for heterogeneity estimation across treatment comparisons, and thus, ads valuable precision when data is sparse. The homogeneous variance assumption, however, is unrealistic and can severely bias variance estimates. Consequently 95\% credible intervals may not retain nominal coverage, and treatment rank probabilities may become distorted. Relaxing the homogeneous variance assumption may be equally problematic due to reduced precision. To regain good precision, moderately informative variance priors or additional mathematical assumptions may be necessary. Methods: In this paper we describe four novel approaches to modeling heterogeneity variance - two novel model structures, and two approaches for use of moderately informative variance priors. We examine the relative performance of all approaches in two illustrative MTC data sets. We particularly compare between-study heterogeneity estimates and model fits, treatment effect estimates and 95\% credible intervals, and treatment rank probabilities. Results: In both data sets, use of moderately informative variance priors constructed from the pair wise meta-analysis data yielded the best model fit and narrower credible intervals. Imposing consistency equations on variance estimates, assuming variances to be exchangeable, or using empirically informed variance priors also yielded good model fits and narrow credible intervals. The homogeneous variance model yielded high precision at all times, but overall inadequate estimates of between-trial variances. Lastly, treatment rankings were similar among the novel approaches, but considerably different when compared with the homogenous variance approach. Conclusions: MTC models using a homogenous variance structure appear to perform sub-optimally when between-trial variances vary between comparisons. Using informative variance priors, assuming exchangeability or imposing consistency between heterogeneity variances can all ensure sufficiently reliable and realistic heterogeneity estimation, and thus more reliable MTC inferences. All four approaches should be viable candidates for replacing or supplementing the conventional homogeneous variance MTC model, which is currently the most widely used in practice.},
  file = {/Users/rritaz/Zotero/storage/6UCC9RNP/Thorlund et al. - 2013 - Modelling heterogeneity variances in multiple trea.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,random-effects},
  language = {en},
  number = {1}
}

@article{timm_testing_2002,
  title = {Testing {{Non}}-Nested {{Multivariate Effect Size Models}} in {{Meta}}-{{Analysis}}},
  author = {Timm, Neil H.},
  year = {2002},
  month = dec,
  volume = {27},
  pages = {321--333},
  issn = {1076-9986},
  doi = {10.3102/10769986027004321},
  abstract = {In modeling multiple effect sizes in meta-analysis, one often wants to evaluate two linear models for multivariate outcomes by including different sets of non-nested predictors for each outcome. This results in two non-nested multivariate effect size models. In this article, we show how to test the hypothesis that a non-nested model fits a set of predictors.},
  file = {/Users/rritaz/Zotero/storage/GMIW8DLB/Timm - 2002 - Testing Non-nested Multivariate Effect Size Models.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {modeling effect size variation (covariates),multivariate},
  number = {4}
}

@article{timmer_publication_2002,
  title = {Publication Bias in Gastroenterological Research \textendash{} a Retrospective Cohort Study Based on Abstracts Submitted to a Scientific Meeting},
  author = {Timmer, Antje and Hilsden, Robert J and Cole, John and Hailey, David and Sutherland, Lloyd R},
  year = {2002},
  month = dec,
  volume = {2},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-2-7},
  abstract = {Background: The aim of this study was to examine the determinants of publication and whether publication bias occurred in gastroenterological research. Methods: A random sample of abstracts submitted to DDW, the major GI meeting (1992\textendash 1995) was evaluated. The publication status was determined by database searches, complemented by a mailed survey to abstract authors. Determinants of publication were examined by Cox proportional hazards model and multiple logistic regression. Results: The sample included abstracts on 326 controlled clinical trials (CCT), 336 other clinical research reports (OCR), and 174 basic science studies (BSS). 392 abstracts (47\%) were published as full papers. Acceptance for presentation at the meeting was a strong predictor of subsequent publication for all research types (overall, 54\% vs. 34\%, OR 2.3, 95\% CI 1.7 to 3.1). In the multivariate analysis, multi-center status was found to predict publication (OR 2.8, 95\% CI 1.6\textendash 4.9). There was no significant association between direction of study results and subsequent publication. Studies were less likely to be published in high impact journals if the results were not statistically significant (OR 0.5, 95 CI 95\% 0.3\textendash 0.6). The author survey identified lack of time or interest as the main reason for failure to publish. Conclusions: Abstracts which were selected for presentation at the DDW are more likely to be followed by full publications. The statistical significance of the study results was not found to be a predictor of publication but influences the chances for high impact publication.},
  file = {/Users/rritaz/Zotero/storage/YMUXXF62/Timmer et al. - 2002 - Publication bias in gastroenterological research –.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {publication bias},
  language = {en},
  number = {1}
}

@article{tipton_framework_2017,
  title = {A Framework for the Meta-Analysis of {{Bland}}\textendash{{Altman}} Studies Based on a Limits of Agreement Approach},
  author = {Tipton, Elizabeth and Shuster, Jonathan},
  year = {2017},
  volume = {36},
  pages = {3621--3635},
  issn = {1097-0258},
  doi = {10.1002/sim.7352},
  abstract = {Bland\textendash Altman method comparison studies are common in the medical sciences and are used to compare a new measure to a gold-standard (often costlier or more invasive) measure. The distribution of these differences is summarized by two statistics, the `bias' and standard deviation, and these measures are combined to provide estimates of the limits of agreement (LoA). When these LoA are within the bounds of clinically insignificant differences, the new non-invasive measure is preferred. Very often, multiple Bland\textendash Altman studies have been conducted comparing the same two measures, and random-effects meta-analysis provides a means to pool these estimates. We provide a framework for the meta-analysis of Bland\textendash Altman studies, including methods for estimating the LoA and measures of uncertainty (i.e., confidence intervals). Importantly, these LoA are likely to be wider than those typically reported in Bland\textendash Altman meta-analyses. Frequently, Bland\textendash Altman studies report results based on repeated measures designs but do not properly adjust for this design in the analysis. Meta-analyses of Bland\textendash Altman studies frequently exclude these studies for this reason. We provide a meta-analytic approach that allows inclusion of estimates from these studies. This includes adjustments to the estimate of the standard deviation and a method for pooling the estimates based upon robust variance estimation. An example is included based on a previously published meta-analysis. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5QX32GDZ/Tipton and Shuster - 2017 - A framework for the meta-analysis of Bland–Altman .pdf;/Users/rritaz/Zotero/storage/8K8M6UHM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random-effects,robust variance estimation},
  language = {en},
  number = {23}
}

@article{tipton_robust_2013,
  title = {Robust Variance Estimation in Meta-Regression with Binary Dependent Effects},
  author = {Tipton, Elizabeth},
  year = {2013},
  volume = {4},
  pages = {169--187},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1070},
  abstract = {Dependent effect size estimates are a common problem in meta-analysis. Recently, a robust variance estimation method was introduced that can be used whenever effect sizes in a meta-analysis are not independent. This problem arises, for example, when effect sizes are nested or when multiple measures are collected on the same individuals. In this paper, we investigate the robustness of this method in small samples when the effect size of interest is the risk difference, log risk ratio, or log odds ratio. This simulation study examines the accuracy of 95\% confidence intervals constructed using the robust variance estimator across a large variety of parameter values. We report results for both estimations of the mean effect (intercept) and of a slope. The results indicate that the robust variance estimator performs well even when the number of studies is as small as 10, although coverage is generally less than nominal in the slope estimation case. Throughout, an example based on a meta-analysis of cognitive behavior therapy is used for motivation. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/MT5XP4RS/Tipton - 2013 - Robust variance estimation in meta-regression with.pdf;/Users/rritaz/Zotero/storage/52FR3WTS/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {robust variance estimation},
  language = {en},
  number = {2}
}

@article{tipton_small_2015,
  title = {Small Sample Adjustments for Robust Variance Estimation with Meta-Regression},
  author = {Tipton, Elizabeth},
  year = {2015},
  month = sep,
  volume = {20},
  pages = {375--393},
  issn = {1082-989X},
  doi = {10.1037/met0000011},
  abstract = {Although primary studies often report multiple outcomes, the covariances between these outcomes are rarely reported. This leads to difficulties when combining studies in a meta-analysis. This problem was recently addressed with the introduction of robust variance estimation. This new method enables the estimation of meta-regression models with dependent effect sizes, even when the dependence structure is unknown. Although robust variance estimation has been shown to perform well when the number of studies in the meta-analysis is large, previous simulation studies suggest that the associated tests often have Type I error rates that are much larger than nominal. In this article, I introduce 6 estimators with better small sample properties and study the effectiveness of these estimators via 2 simulation studies. The results of these simulations suggest that the best estimator involves correcting both the residuals and degrees of freedom used in the robust variance estimator. These studies also suggest that the degrees of freedom depend on not only the number of studies but also the type of covariates in the meta-regression. The fact that the degrees of freedom can be small, even when the number of studies is large, suggests that these small-sample corrections should be used more generally. I conclude with an example comparing the results of a meta-regression with robust variance estimation with the results from the corrected estimator. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {correlated effects,GLM MA models,robust variance estimation},
  number = {3},
  series = {Meta-{{Analysis Topics}}}
}

@article{tipton_small-sample_2015,
  title = {Small-{{Sample Adjustments}} for {{Tests}} of {{Moderators}} and {{Model Fit Using Robust Variance Estimation}} in {{Meta}}-{{Regression}}},
  author = {Tipton, Elizabeth and Pustejovsky, James E.},
  year = {2015},
  month = dec,
  volume = {40},
  pages = {604--634},
  issn = {1076-9986},
  doi = {10.3102/1076998615606099},
  abstract = {Meta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling?s T2 distribution, is level-{$\alpha$} and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large-sample F-test.},
  file = {/Users/rritaz/Zotero/storage/IY6XAHVM/Tipton and Pustejovsky - 2015 - Small-Sample Adjustments for Tests of Moderators a.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {correlated effects,modeling effect size variation (covariates),robust variance estimation},
  number = {6}
}

@article{trikalinos_empirical_2014,
  title = {An Empirical Comparison of Univariate and Multivariate Meta-Analyses for Categorical Outcomes},
  author = {Trikalinos, Thomas A. and Hoaglin, David C. and Schmid, Christopher H.},
  year = {2014},
  volume = {33},
  pages = {1441--1459},
  issn = {1097-0258},
  doi = {10.1002/sim.6044},
  abstract = {Treatment effects for multiple outcomes can be meta-analyzed separately or jointly, but no systematic empirical comparison of the two approaches exists. From the Cochrane Library of Systematic Reviews, we identified 45 reviews, including 1473 trials and 258,675 patients, that contained two or three univariate meta-analyses of categorical outcomes for the same interventions that could also be analyzed jointly. Eligible were meta-analyses with at least seven trials reporting all outcomes for which the cross-classification tables were exactly recoverable (e.g., outcomes were mutually exclusive, or one was a subset of the other). This ensured known correlation structures. Outcomes in 40 reviews had an is-subset-of relationship, and those in 5 were mutually exclusive. We analyzed these data with univariate and multivariate models based on discrete and approximate likelihoods. Discrete models were fit in the Bayesian framework using slightly informative priors. The summary effects for each outcome were similar with univariate and multivariate meta-analyses (both using the approximate and discrete likelihoods); however, the multivariate model with the discrete likelihood gave smaller between-study variance estimates, and narrower predictive intervals for new studies. When differences in the summary treatment effects were examined, the multivariate models gave similar summary estimates but considerably longer (shorter) uncertainty intervals because of positive (negative) correlation between outcome treatment effects. It is unclear whether any of the examined reviews would change their overall conclusions based on multivariate versus univariate meta-analyses, because extra-analytical and context-specific considerations contribute to conclusions and, secondarily, because numerical differences were often modest. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3QL5E5NM/Trikalinos et al. - 2014 - An empirical comparison of univariate and multivar.pdf;/Users/rritaz/Zotero/storage/44WUDUEZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,discrete effect sizes,multivariate},
  language = {en},
  number = {9}
}

@article{trikalinos_method_2008,
  title = {A Method for the Meta-Analysis of Mutually Exclusive Binary Outcomes},
  author = {Trikalinos, Thomas A. and Olkin, Ingram},
  year = {2008},
  volume = {27},
  pages = {4279--4300},
  issn = {1097-0258},
  doi = {10.1002/sim.3299},
  abstract = {Meta-analyses of multiple outcomes need to take into account the within-study correlation across the different outcomes. Here we focus on the meta-analysis of dichotomous outcomes that are mutually exclusive and exhaustive. Correlations between effect sizes for mutually exclusive outcomes are negative and can be obtained from data already available. We present both fixed-effects and random-effects methods that account for the negative correlations and yield correct simultaneous confidence intervals for both the marginal outcome-specific effect sizes and the relative effect sizes between outcomes. Formulae for the odds ratio, risk ratio, risk difference, and the differences in the arcsin-transformed risks are provided. An example of a meta-analysis of randomized trials of radiotherapy and mastectomy with axillary lymph node clearance versus only mastectomy with axillary clearance for early breast cancer is presented. The mutually exclusive outcomes of breast cancer deaths and deaths secondary to other causes are examined in separate meta-analyses, and also by taking the between-outcome correlation into account. We argue that mutually exclusive outcomes in the meta-analyses of binary data are optimally analyzed in a multinomial setting. This may also be applicable when a meta-analysis examines only one out of several mutually exclusive outcomes. For large sample sizes and/or low event counts, the covariances between outcome-specific effect sizes are small, and either ignoring them or accounting for them would result in similar estimates for any practical purpose. However, meta-analysts should explore the robustness of the findings from individual meta-analyses when mutually exclusive outcomes are assessed. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/3PD6LCAE/Trikalinos and Olkin - 2008 - A method for the meta-analysis of mutually exclusi.pdf;/Users/rritaz/Zotero/storage/XNDA59L5/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,discrete effect sizes,multivariate},
  language = {en},
  number = {21}
}

@article{trikalinos_methods_2014,
  title = {Methods for the Joint Meta-Analysis of Multiple Tests},
  author = {Trikalinos, Thomas A. and Hoaglin, David C. and Small, Kevin M. and Terrin, Norma and Schmid, Christopher H.},
  year = {2014},
  volume = {5},
  pages = {294--312},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1115},
  abstract = {Existing methods for meta-analysis of diagnostic test accuracy focus primarily on a single index test. We propose models for the joint meta-analysis of studies comparing multiple index tests on the same participants in paired designs. These models respect the grouping of data by studies, account for the within-study correlation between the tests' true-positive rates (TPRs) and between their false-positive rates (FPRs) (induced because tests are applied to the same participants), and allow for between-study correlations between TPRs and FPRs (such as those induced by threshold effects). We estimate models in the Bayesian setting. We demonstrate using a meta-analysis of screening for Down syndrome with two tests: shortened humerus (arm bone), and shortened femur (thigh bone). Separate and joint meta-analyses yielded similar TPR and FPR estimates. For example, the summary TPR for a shortened humerus was 35.3\% (95\% credible interval (CrI): 26.9, 41.8\%) versus 37.9\% (27.7, 50.3\%) with joint versus separate meta-analysis. Joint meta-analysis is more efficient when calculating comparative accuracy: the difference in the summary TPRs was 0.0\% (-8.9, 9.5\%; TPR higher for shortened humerus) with joint versus 2.6\% (-14.7, 19.8\%) with separate meta-analyses. Simulation and empirical analyses are needed to refine the role of the proposed methodology. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NXKXK6S7/Trikalinos et al. - 2014 - Methods for the joint meta-analysis of multiple te.pdf;/Users/rritaz/Zotero/storage/Y2BWSG5Z/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {categorical MA models},
  language = {en},
  number = {4}
}

@article{trinquart_adjustment_2012,
  title = {Adjustment for Reporting Bias in Network Meta-Analysis of Antidepressant Trials},
  author = {Trinquart, Ludovic and Chatellier, Gilles and Ravaud, Philippe},
  year = {2012},
  month = dec,
  volume = {12},
  pages = {150},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-150},
  abstract = {Background: Network meta-analysis (NMA), a generalization of conventional MA, allows for assessing the relative effectiveness of multiple interventions. Reporting bias is a major threat to the validity of MA and NMA. Numerous methods are available to assess the robustness of MA results to reporting bias. We aimed to extend such methods to NMA. Methods: We introduced 2 adjustment models for Bayesian NMA. First, we extended a meta-regression model that allows the effect size to depend on its standard error. Second, we used a selection model that estimates the propensity of trial results being published and in which trials with lower propensity are weighted up in the NMA model. Both models rely on the assumption that biases are exchangeable across the network. We applied the models to 2 networks of placebo-controlled trials of 12 antidepressants, with 74 trials in the US Food and Drug Administration (FDA) database but only 51 with published results. NMA and adjustment models were used to estimate the effects of the 12 drugs relative to placebo, the 66 effect sizes for all possible pair-wise comparisons between drugs, probabilities of being the best drug and ranking of drugs. We compared the results from the 2 adjustment models applied to published data and NMAs of published data and NMAs of FDA data, considered as representing the totality of the data. Results: Both adjustment models showed reduced estimated effects for the 12 drugs relative to the placebo as compared with NMA of published data. Pair-wise effect sizes between drugs, probabilities of being the best drug and ranking of drugs were modified. Estimated drug effects relative to the placebo from both adjustment models were corrected (i.e., similar to those from NMA of FDA data) for some drugs but not others, which resulted in differences in pair-wise effect sizes between drugs and ranking. Conclusions: In this case study, adjustment models showed that NMA of published data was not robust to reporting bias and provided estimates closer to that of NMA of FDA data, although not optimal. The validity of such methods depends on the number of trials in the network and the assumption that conventional MAs in the network share a common mean bias mechanism.},
  file = {/Users/rritaz/Zotero/storage/AFBRKDMA/Trinquart et al. - 2012 - Adjustment for reporting bias in network meta-anal.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,network meta-analysis,physical/biological fields,publication bias},
  language = {en},
  number = {1}
}

@article{trinquart_test_2014,
  title = {A Test for Reporting Bias in Trial Networks: Simulation and Case Studies},
  shorttitle = {A Test for Reporting Bias in Trial Networks},
  author = {Trinquart, Ludovic and Ioannidis, John PA and Chatellier, Gilles and Ravaud, Philippe},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {112},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-112},
  abstract = {Background: Networks of trials assessing several treatment options available for the same condition are increasingly considered. Randomized trial evidence may be missing because of reporting bias. We propose a test for reporting bias in trial networks. Methods: We test whether there is an excess of trials with statistically significant results across a network of trials. The observed number of trials with nominally statistically significant p-values across the network is compared with the expected number. The performance of the test (type I error rate and power) was assessed using simulation studies under different scenarios of selective reporting bias. Examples are provided for networks of antidepressant and antipsychotic trials, where reporting biases have been previously demonstrated by comparing published to Food and Drug Administration (FDA) data. Results: In simulations, the test maintained the type I error rate and was moderately powerful after adjustment for type I error rate, except when the between-trial variance was substantial. In all, a positive test result increased moderately or markedly the probability of reporting bias being present, while a negative test result was not very informative. In the two examples, the test gave a signal for an excess of statistically significant results in the network of published data but not in the network of FDA data. Conclusion: The test could be useful to document an excess of significant findings in trial networks, providing a signal for potential publication bias or other selective analysis and outcome reporting biases.},
  file = {/Users/rritaz/Zotero/storage/CATNNQQF/Trinquart et al. - 2014 - A test for reporting bias in trial networks simul.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,network meta-analysis,publication bias},
  language = {en},
  number = {1}
}

@article{tu_using_2017,
  title = {Using Structural Equation Modeling for Network Meta-Analysis},
  author = {Tu, Yu-Kang and Wu, Yun-Chun},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {104},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0390-9},
  abstract = {Background: Network meta-analysis overcomes the limitations of traditional pair-wise meta-analysis by incorporating all available evidence into a general statistical framework for simultaneous comparisons of several treatments. Currently, network meta-analyses are undertaken either within the Bayesian hierarchical linear models or frequentist generalized linear mixed models. Structural equation modeling (SEM) is a statistical method originally developed for modeling causal relations among observed and latent variables. As random effect is explicitly modeled as a latent variable in SEM, it is very flexible for analysts to specify complex random effect structure and to make linear and nonlinear constraints on parameters. The aim of this article is to show how to undertake a network meta-analysis within the statistical framework of SEM. Methods: We used an example dataset to demonstrate the standard fixed and random effect network meta-analysis models can be easily implemented in SEM. It contains results of 26 studies that directly compared three treatment groups A, B and C for prevention of first bleeding in patients with liver cirrhosis. We also showed that a new approach to network meta-analysis based on the technique of unrestricted weighted least squares (UWLS) method can also be undertaken using SEM. Results: For both the fixed and random effect network meta-analysis, SEM yielded similar coefficients and confidence intervals to those reported in the previous literature. The point estimates of two UWLS models were identical to those in the fixed effect model but the confidence intervals were greater. This is consistent with results from the traditional pairwise meta-analyses. Comparing to UWLS model with common variance adjusted factor, UWLS model with unique variance adjusted factor has greater confidence intervals when the heterogeneity was larger in the pairwise comparison. The UWLS model with unique variance adjusted factor reflects the difference in heterogeneity within each comparison. Conclusion: SEM provides a very flexible framework for univariate and multivariate meta-analysis, and its potential as a powerful tool for advanced meta-analysis is still to be explored.},
  file = {/Users/rritaz/Zotero/storage/IS4HQ8CU/Tu and Wu - 2017 - Using structural equation modeling for network met.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {GLM MA models,multivariate,network meta-analysis},
  language = {en},
  number = {1}
}

@article{turner_bayesian_2015,
  title = {A {{Bayesian}} Framework to Account for Uncertainty Due to Missing Binary Outcome Data in Pairwise Meta-Analysis},
  author = {Turner, N. L. and Dias, S. and Ades, A. E. and Welton, N. J.},
  year = {2015},
  volume = {34},
  pages = {2062--2080},
  issn = {1097-0258},
  doi = {10.1002/sim.6475},
  abstract = {Missing outcome data are a common threat to the validity of the results from randomised controlled trials (RCTs), which, if not analysed appropriately, can lead to misleading treatment effect estimates. Studies with missing outcome data also threaten the validity of any meta-analysis that includes them. A conceptually simple Bayesian framework is proposed, to account for uncertainty due to missing binary outcome data in meta-analysis. A pattern-mixture model is fitted, which allows the incorporation of prior information on a parameter describing the missingness mechanism. We describe several alternative parameterisations, with the simplest being a prior on the probability of an event in the missing individuals. We describe a series of structural assumptions that can be made concerning the missingness parameters. We use some artificial data scenarios to demonstrate the ability of the model to produce a bias-adjusted estimate of treatment effect that accounts for uncertainty. A meta-analysis of haloperidol versus placebo for schizophrenia is used to illustrate the model. We end with a discussion of elicitation of priors, issues with poor reporting and potential extensions of the framework. Our framework allows one to make the best use of evidence produced from RCTs with missing outcome data in a meta-analysis, accounts for any uncertainty induced by missing data and fits easily into a wider evidence synthesis framework for medical decision making. \textcopyright{} 2015 The Authors. Statistics in MedicinePublished by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/E6H5ZQAW/Turner et al. - 2015 - A Bayesian framework to account for uncertainty du.pdf;/Users/rritaz/Zotero/storage/Y7ADG9VK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,missing data},
  language = {en},
  number = {12}
}

@article{turner_incorporating_2019,
  title = {Incorporating External Evidence on Between-Trial Heterogeneity in Network Meta-Analysis},
  author = {Turner, Rebecca M. and Dom{\'i}nguez-Islas, Clara P. and Jackson, Dan and Rhodes, Kirsty M. and White, Ian R.},
  year = {2019},
  volume = {38},
  pages = {1321--1335},
  issn = {1097-0258},
  doi = {10.1002/sim.8044},
  abstract = {In a network meta-analysis, between-study heterogeneity variances are often very imprecisely estimated because data are sparse, so standard errors of treatment differences can be highly unstable. External evidence can provide informative prior distributions for heterogeneity and, hence, improve inferences. We explore approaches for specifying informative priors for multiple heterogeneity variances in a network meta-analysis. First, we assume equal heterogeneity variances across all pairwise intervention comparisons (approach 1); incorporating an informative prior for the common variance is then straightforward. Models allowing unequal heterogeneity variances are more realistic; however, care must be taken to ensure implied variance-covariance matrices remain valid. We consider three strategies for specifying informative priors for multiple unequal heterogeneity variances. Initially, we choose different informative priors according to intervention comparison type and assume heterogeneity to be proportional across comparison types and equal within comparison type (approach 2). Next, we allow all heterogeneity variances in the network to differ, while specifying a common informative prior for each. We explore two different approaches to this: placing priors on variances and correlations separately (approach 3) or using an informative inverse Wishart distribution (approach 4). Our methods are exemplified through application to two network metaanalyses. Appropriate informative priors are obtained from previously published evidence-based distributions for heterogeneity. Relevant prior information on between-study heterogeneity can be incorporated into network meta-analyses, without needing to assume equal heterogeneity across treatment comparisons. The approaches proposed will be beneficial in sparse data sets and provide more appropriate intervals for treatment differences than those based on imprecise heterogeneity estimates.},
  file = {/Users/rritaz/Zotero/storage/DFH87FLJ/Turner et al. - 2019 - Incorporating external evidence on between-trial h.pdf;/Users/rritaz/Zotero/storage/2JV36KS6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {8}
}

@article{turner_multilevel_2000,
  title = {A Multilevel Model Framework for Meta-Analysis of Clinical Trials with Binary Outcomes},
  author = {Turner, Rebecca M. and Omar, Rumana Z. and Yang, Min and Goldstein, Harvey and Thompson, Simon G.},
  year = {2000},
  volume = {19},
  pages = {3417--3432},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20001230)19:24<3417::AID-SIM614>3.0.CO;2-L},
  abstract = {In this paper we explore the potential of multilevel models for meta-analysis of trials with binary outcomes for both summary data, such as log-odds ratios, and individual patient data. Conventional fixed effect and random effects models are put into a multilevel model framework, which provides maximum likelihood or restricted maximum likelihood estimation. To exemplify the methods, we use the results from 22 trials to prevent respiratory tract infections; we also make comparisons with a second example data set comprising fewer trials. Within summary data methods, confidence intervals for the overall treatment effect and for the between-trial variance may be derived from likelihood based methods or a parametric bootstrap as well as from Wald methods; the bootstrap intervals are preferred because they relax the assumptions required by the other two methods. When modelling individual patient data, a bias corrected bootstrap may be used to provide unbiased estimation and correctly located confidence intervals; this method is particularly valuable for the between-trial variance. The trial effects may be modelled as either fixed or random within individual data models, and we discuss the corresponding assumptions and implications. If random trial effects are used, the covariance between these and the random treatment effects should be included; the resulting model is equivalent to a bivariate approach to meta-analysis. Having implemented these techniques, the flexibility of multilevel modelling may be exploited in facilitating extensions to standard meta-analysis methods. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/4U3G8EQU/Turner et al. - 2000 - A multilevel model framework for meta-analysis of .pdf;/Users/rritaz/Zotero/storage/NJ5T2W6C/1097-0258(20001230)19243417AID-SIM6143.0.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates)},
  language = {en},
  number = {24}
}

@article{turner_predicting_2012,
  title = {Predicting the Extent of Heterogeneity in Meta-Analysis, Using Empirical Data from the {{Cochrane Database}} of {{Systematic Reviews}}},
  author = {Turner, Rebecca M and Davey, Jonathan and Clarke, Mike J and Thompson, Simon G and Higgins, Julian PT},
  year = {2012},
  month = jun,
  volume = {41},
  pages = {818--827},
  issn = {1464-3685, 0300-5771},
  doi = {10.1093/ije/dys041},
  file = {/Users/rritaz/Zotero/storage/FR24JTA7/Turner et al. - 2012 - Predicting the extent of heterogeneity in meta-ana.pdf},
  journal = {International Journal of Epidemiology},
  language = {en},
  number = {3}
}

@article{turner_predictive_2015,
  title = {Predictive Distributions for Between-Study Heterogeneity and Simple Methods for Their Application in {{Bayesian}} Meta-Analysis},
  author = {Turner, Rebecca M. and Jackson, Dan and Wei, Yinghui and Thompson, Simon G. and Higgins, Julian P. T.},
  year = {2015},
  volume = {34},
  pages = {984--998},
  issn = {1097-0258},
  doi = {10.1002/sim.6381},
  abstract = {Numerous meta-analyses in healthcare research combine results from only a small number of studies, for which the variance representing between-study heterogeneity is estimated imprecisely. A Bayesian approach to estimation allows external evidence on the expected magnitude of heterogeneity to be incorporated.The aim of this paper is to provide tools that improve the accessibility of Bayesian meta-analysis. We present two methods for implementing Bayesian meta-analysis, using numerical integration and importance sampling techniques. Based on 14 886 binary outcome meta-analyses in the Cochrane Database of Systematic Reviews, we derive a novel set of predictive distributions for the degree of heterogeneity expected in 80 settings depending on the outcomes assessed and comparisons made. These can be used as prior distributions for heterogeneity in future meta-analyses.The two methods are implemented in R, for which code is provided. Both methods produce equivalent results to standard but more complex Markov chain Monte Carlo approaches. The priors are derived as log-normal distributions for the between-study variance, applicable to meta-analyses of binary outcomes on the log odds-ratio scale. The methods are applied to two example meta-analyses, incorporating the relevant predictive distributions as prior distributions for between-study heterogeneity.We have provided resources to facilitate Bayesian meta-analysis, in a form accessible to applied researchers, which allow relevant prior information on the degree of heterogeneity to be incorporated. \textcopyright{} 2014 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/A3D9KXG3/Turner et al. - 2015 - Predictive distributions for between-study heterog.pdf;/Users/rritaz/Zotero/storage/YZNXYCZM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,random-effects,small meta-analysis},
  language = {en},
  number = {6}
}

@article{uhlmann_hypothesis_2018,
  title = {Hypothesis Testing in {{Bayesian}} Network Meta-Analysis},
  author = {Uhlmann, Lorenz and Jensen, Katrin and Kieser, Meinhard},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {128},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0574-y},
  abstract = {Background: Network meta-analysis is an extension of the classical pairwise meta-analysis and allows to compare multiple interventions based on both head-to-head comparisons within trials and indirect comparisons across trials. Bayesian or frequentist models are applied to obtain effect estimates with credible or confidence intervals. Furthermore, p-values or similar measures may be helpful for the comparison of the included arms but related methods are not yet addressed in the literature. In this article, we discuss how hypothesis testing can be done in a Bayesian network meta-analysis. Methods: An index is presented and discussed in a Bayesian modeling framework. Simulation studies were performed to evaluate the characteristics of this index. The approach is illustrated by a real data example. Results: The simulation studies revealed that the type I error rate is controlled. The approach can be applied in a superiority as well as in a non-inferiority setting. Conclusions: Test decisions can be based on the proposed index. The index may be a valuable complement to the commonly reported results of network meta-analyses. The method is easy to apply and of no (noticeable) additional computational cost.},
  file = {/Users/rritaz/Zotero/storage/LQ5XFMMW/Uhlmann et al. - 2018 - Hypothesis testing in Bayesian network meta-analys.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {bayesian,network meta-analysis},
  language = {en},
  number = {1}
}

@article{ulrich_properties_2018,
  title = {Some Properties of P-Curves, with an Application to Gradual Publication Bias},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2018},
  month = sep,
  volume = {23},
  pages = {546--560},
  issn = {1082-989X},
  doi = {10.1037/met0000125},
  abstract = {p-curves provide a useful window for peeking into the file drawer in a way that might reveal p-hacking (Simonsohn, Nelson, \& Simmons, 2014a). The properties of p-curves are commonly investigated by computer simulations. On the basis of these simulations, it has been proposed that the skewness of this curve can be used as a diagnostic tool to decide whether the significant p values within a certain domain of research suggest the presence of p-hacking or actually demonstrate that there is a true effect. Here we introduce a rigorous mathematical approach that allows the properties of p-curves to be examined without simulations. This approach allows the computation of a p-curve for any statistic whose sampling distribution is known and thereby allows a thorough evaluation of its properties. For example, it shows under which conditions p-curves would exhibit the shape of a monotone decreasing function. In addition, we used weighted distribution functions to analyze how 2 different types of publication bias (i.e., cliff effects and gradual publication bias) influence the shapes of p-curves. The results of 2 survey experiments with more than 1,000 participants support the existence of a cliff effect at p = .05 and also suggest that researchers tend to be more likely to recommend submission of an article as the level of statistical significance increases beyond this p level. This gradual bias produces right-skewed p-curves mimicking the existence of real effects even when no such effects are actually present. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,publication bias},
  number = {3}
}

@article{valentine_how_2010,
  title = {How {{Many Studies Do You Need}}?: {{A Primer}} on {{Statistical Power}} for {{Meta}}-{{Analysis}}},
  shorttitle = {How {{Many Studies Do You Need}}?},
  author = {Valentine, Jeffrey C. and Pigott, Therese D. and Rothstein, Hannah R.},
  year = {2010},
  month = apr,
  volume = {35},
  pages = {215--247},
  issn = {1076-9986},
  doi = {10.3102/1076998609346961},
  abstract = {In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question ?How many studies do you need to do a meta-analysis?? and show that, given the need for a conclusion, the answer is ?two studies,? because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.},
  file = {/Users/rritaz/Zotero/storage/JPVRTHGG/Valentine et al. - 2010 - How Many Studies Do You Need A Primer on Statist.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {power,random-effects,small meta-analysis},
  number = {2}
}

@article{valkenhoef_automated_2016,
  title = {Automated Generation of Node-Splitting Models for Assessment of Inconsistency in Network Meta-Analysis},
  author = {van Valkenhoef, Gert and Dias, Sofia and Ades, A. E. and Welton, Nicky J.},
  year = {2016},
  volume = {7},
  pages = {80--93},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1167},
  abstract = {Network meta-analysis enables the simultaneous synthesis of a network of clinical trials comparing any number of treatments. Potential inconsistencies between estimates of relative treatment effects are an important concern, and several methods to detect inconsistency have been proposed. This paper is concerned with the node-splitting approach, which is particularly attractive because of its straightforward interpretation, contrasting estimates from both direct and indirect evidence. However, node-splitting analyses are labour-intensive because each comparison of interest requires a separate model. It would be advantageous if node-splitting models could be estimated automatically for all comparisons of interest. We present an unambiguous decision rule to choose which comparisons to split, and prove that it selects only comparisons in potentially inconsistent loops in the network, and that all potentially inconsistent loops in the network are investigated. Moreover, the decision rule circumvents problems with the parameterisation of multi-arm trials, ensuring that model generation is trivial in all cases. Thus, our methods eliminate most of the manual work involved in using the node-splitting approach, enabling the analyst to focus on interpreting the results. \textcopyright{} 2015 The Authors Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/EW7EEHIQ/Valkenhoef et al. - 2016 - Automated generation of node-splitting models for .pdf;/Users/rritaz/Zotero/storage/B4WV9IVB/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {1}
}

@article{valkenhoef_automating_2012,
  title = {Automating Network Meta-Analysis},
  author = {van Valkenhoef, Gert and Lu, Guobing and de Brock, Bert and Hillege, Hans and Ades, A. E. and Welton, Nicky J.},
  year = {2012},
  volume = {3},
  pages = {285--299},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1054},
  abstract = {Mixed treatment comparison (MTC) (also called network meta-analysis) is an extension of traditional meta-analysis to allow the simultaneous pooling of data from clinical trials comparing more than two treatment options. Typically, MTCs are performed using general-purpose Markov chain Monte Carlo software such as WinBUGS, requiring a model and data to be specified using a specific syntax. It would be preferable if, for the most common cases, both could be derived from a well-structured data file that can be easily checked for errors. Automation is particularly valuable for simulation studies in which the large number of MTCs that have to be estimated may preclude manual model specification and analysis. Moreover, automated model generation raises issues that provide additional insight into the nature of MTC. We present a method for the automated generation of Bayesian homogeneous variance random effects consistency models, including the choice of basic parameters and trial baselines, priors, and starting values for the Markov chain(s). We validate our method against the results of five published MTCs. The method is implemented in freely available open source software. This means that performing an MTC no longer requires manually writing a statistical model. This reduces time and effort, and facilitates error checking of the dataset. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/VE8HA9QT/Valkenhoef et al. - 2012 - Automating network meta-analysis.pdf;/Users/rritaz/Zotero/storage/J93LFU6I/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {network meta-analysis},
  language = {en},
  number = {4}
}

@article{van_assen_meta-analysis_2015,
  title = {Meta-Analysis Using Effect Size Distributions of Only Statistically Significant Studies.},
  author = {{van Assen}, Marcel A. L. M. and {van Aert}, Robbie C. M. and Wicherts, Jelte M.},
  year = {2015},
  volume = {20},
  pages = {293--309},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000025},
  file = {/Users/rritaz/Zotero/storage/L79NGIQN/van Assen et al. - 2015 - Meta-analysis using effect size distributions of o.pdf},
  journal = {Psychological Methods},
  keywords = {diagnostic techniques,publication bias},
  language = {en},
  number = {3}
}

@article{van_klaveren_assessing_2014,
  title = {Assessing Discriminative Ability of Risk Models in Clustered Data},
  author = {{van Klaveren}, David and Steyerberg, Ewout W and Perel, Pablo and Vergouwe, Yvonne},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {5},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-5},
  abstract = {Background: The discriminative ability of a risk model is often measured by Harrell's concordance-index (c-index). The c-index estimates for two randomly chosen subjects the probability that the model predicts a higher risk for the subject with poorer outcome (concordance probability). When data are clustered, as in multicenter data, two types of concordance are distinguished: concordance in subjects from the same cluster (within-cluster concordance probability) and concordance in subjects from different clusters (between-cluster concordance probability). We argue that the within-cluster concordance probability is most relevant when a risk model supports decisions within clusters (e.g. who should be treated in a particular center). We aimed to explore different approaches to estimate the within-cluster concordance probability in clustered data. Methods: We used data of the CRASH trial (2,081 patients clustered in 35 centers) to develop a risk model for mortality after traumatic brain injury. To assess the discriminative ability of the risk model within centers we first calculated cluster-specific c-indexes. We then pooled the cluster-specific c-indexes into a summary estimate with different meta-analytical techniques. We considered fixed effect meta-analysis with different weights (equal; inverse variance; number of subjects, events or pairs) and random effects meta-analysis. We reflected on pooling the estimates on the log-odds scale rather than the probability scale. Results: The cluster-specific c-index varied substantially across centers (IQR = 0.70-0.81; I2 = 0.76 with 95\% confidence interval 0.66 to 0.82). Summary estimates resulting from fixed effect meta-analysis ranged from 0.75 (equal weights) to 0.84 (inverse variance weights). With random effects meta-analysis \textendash{} accounting for the observed heterogeneity in c-indexes across clusters \textendash{} we estimated a mean of 0.77, a between-cluster variance of 0.0072 and a 95\% prediction interval of 0.60 to 0.95. The normality assumptions for derivation of a prediction interval were better met on the probability than on the log-odds scale. Conclusion: When assessing the discriminative ability of risk models used to support decisions at cluster level we recommend meta-analysis of cluster-specific c-indexes. Particularly, random effects meta-analysis should be considered.},
  file = {/Users/rritaz/Zotero/storage/RSYJUKCK/van Klaveren et al. - 2014 - Assessing discriminative ability of risk models in.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes,random-effects},
  language = {en},
  number = {1}
}

@article{van_lent_recommendations_2013,
  title = {Recommendations for a Uniform Assessment of Publication Bias Related to Funding Source},
  author = {{van Lent}, Marlies and Overbeke, John and Out, Henk J},
  year = {2013},
  month = dec,
  volume = {13},
  pages = {120},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-120},
  abstract = {Background: Numerous studies on publication bias in clinical drug research have been undertaken, particularly on the association between sponsorship and favourable outcomes. However, no standardized methodology for the classification of outcomes and sponsorship has been described. Dissimilarities and ambiguities in this assessment impede the ability to compare and summarize results of studies on publication bias. To guide authors undertaking such studies, this paper provides recommendations for a uniform assessment of publication bias related to funding source. Methods and results: As part of ongoing research into publication bias, 472 manuscripts on randomised controlled trials (RCTs) with drugs, submitted to eight medical journals from January 2010 through April 2012, were reviewed. Information on trial results and sponsorship was extracted from manuscripts. During the start of this evaluation, several problems related to the classification of outcomes, inclusion of post-hoc analyses and follow-up studies of RCTs in the study sample, and assessment of the role of the funding source were encountered. A comprehensive list of recommendations addressing these problems was composed. To assess internal validity, reliability and usability of these recommendations were tested through evaluation of manuscripts submitted to journals included in our study. Conclusions: The proposed recommendations represent a first step towards a uniform method of classifying trial outcomes and sponsorship. This is essential to draw valid conclusions on the role of the funding source in publication bias and will ensure consistency across future studies.},
  file = {/Users/rritaz/Zotero/storage/7ZDEIQ69/van Lent et al. - 2013 - Recommendations for a uniform assessment of public.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {publication bias},
  language = {en},
  number = {1}
}

@article{van_ravenzwaaij_true_2019,
  title = {True and False Positive Rates for Different Criteria of Evaluating Statistical Evidence from Clinical Trials},
  author = {{van Ravenzwaaij}, Don and Ioannidis, John P. A.},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {218},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0865-y},
  abstract = {Background: Until recently a typical rule that has often been used for the endorsement of new medications by the Food and Drug Administration has been the existence of at least two statistically significant clinical trials favoring the new medication. This rule has consequences for the true positive (endorsement of an effective treatment) and false positive rates (endorsement of an ineffective treatment). Methods: In this paper, we compare true positive and false positive rates for different evaluation criteria through simulations that rely on (1) conventional p-values; (2) confidence intervals based on meta-analyses assuming fixed or random effects; and (3) Bayes factors. We varied threshold levels for statistical evidence, thresholds for what constitutes a clinically meaningful treatment effect, and number of trials conducted. Results: Our results show that Bayes factors, meta-analytic confidence intervals, and p-values often have similar performance. Bayes factors may perform better when the number of trials conducted is high and when trials have small sample sizes and clinically meaningful effects are not small, particularly in fields where the number of nonzero effects is relatively large. Conclusions: Thinking about realistic effect sizes in conjunction with desirable levels of statistical evidence, as well as quantifying statistical evidence with Bayes factors may help improve decision-making in some circumstances.},
  file = {/Users/rritaz/Zotero/storage/7RYBRLLT/van Ravenzwaaij and Ioannidis - 2019 - True and false positive rates for different criter.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {combined significance},
  language = {en},
  number = {1}
}

@article{vanhoudnos_hedges_2016,
  title = {On the {{Hedges Correction}} for a T-{{Test}}},
  author = {VanHoudnos, Nathan M. and Greenhouse, Joel B.},
  year = {2016},
  month = aug,
  volume = {41},
  pages = {392--419},
  issn = {1076-9986},
  doi = {10.3102/1076998616644990},
  abstract = {When cluster randomized experiments are analyzed as if units were independent, test statistics for treatment effects can be anticonservative. Hedges proposed a correction for such tests by scaling them to control their Type I error rate. This article generalizes the Hedges correction from a posttest-only experimental design to more common designs used in practice. We show that for many experimental designs, the generalized correction controls its Type I error while the Hedges correction does not. The generalized correction, however, necessarily has low power due to its control of the Type I error. Our results imply that using the Hedges correction as prescribed, for example, by the What Works Clearinghouse can lead to incorrect inferences and has important implications for evidence-based education.},
  file = {/Users/rritaz/Zotero/storage/QLXS684K/VanHoudnos and Greenhouse - 2016 - On the Hedges Correction for a t-Test.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {power},
  number = {4}
}

@article{verde_bayesian_2016,
  title = {Bayesian Evidence Synthesis for Exploring Generalizability of Treatment Effects: A Case Study of Combining Randomized and Non-Randomized Results in Diabetes},
  shorttitle = {Bayesian Evidence Synthesis for Exploring Generalizability of Treatment Effects},
  author = {Verde, Pablo E. and Ohmann, Christian and Morbach, Stephan and Icks, Andrea},
  year = {2016},
  volume = {35},
  pages = {1654--1675},
  issn = {1097-0258},
  doi = {10.1002/sim.6809},
  abstract = {In this paper, we present a unified modeling framework to combine aggregated data from randomized controlled trials (RCTs) with individual participant data (IPD) from observational studies. Rather than simply pooling the available evidence into an overall treatment effect, adjusted for potential confounding, the intention of this work is to explore treatment effects in specific patient populations reflected by the IPD. In this way, by collecting IPD, we can potentially gain new insights from RCTs' results, which cannot be seen using only a meta-analysis of RCTs. We present a new Bayesian hierarchical meta-regression model, which combines submodels, representing different types of data into a coherent analysis. Predictors of baseline risk are estimated from the individual data. Simultaneously, a bivariate random effects distribution of baseline risk and treatment effects is estimated from the combined individual and aggregate data. Therefore, given a subgroup of interest, the estimated treatment effect can be calculated through its correlation with baseline risk. We highlight different types of model parameters: those that are the focus of inference (e.g., treatment effect in a subgroup of patients) and those that are used to adjust for biases introduced by data collection processes (e.g., internal or external validity). The model is applied to a case study where RCTs' results, investigating efficacy in the treatment of diabetic foot problems, are extrapolated to groups of patients treated in medical routine and who were enrolled in a prospective cohort study. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/V85B6IBF/Verde et al. - 2016 - Bayesian evidence synthesis for exploring generali.pdf;/Users/rritaz/Zotero/storage/G6GKIKG6/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,Individual Patient Data IPD,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {10}
}

@article{verde_meta-analysis_2010,
  title = {Meta-Analysis of Diagnostic Test Data: {{A}} Bivariate {{Bayesian}} Modeling Approach},
  shorttitle = {Meta-Analysis of Diagnostic Test Data},
  author = {Verde, Pablo E.},
  year = {2010},
  volume = {29},
  pages = {3088--3102},
  issn = {1097-0258},
  doi = {10.1002/sim.4055},
  abstract = {In the last decades, the amount of published results on clinical diagnostic tests has expanded very rapidly. The counterpart to this development has been the formal evaluation and synthesis of diagnostic results. However, published results present substantial heterogeneity and they can be regarded as so far removed from the classical domain of meta-analysis, that they can provide a rather severe test of classical statistical methods. Recently, bivariate random effects meta-analytic methods, which model the pairs of sensitivities and specificities, have been presented from the classical point of view. In this work a bivariate Bayesian modeling approach is presented. This approach substantially extends the scope of classical bivariate methods by allowing the structural distribution of the random effects to depend on multiple sources of variability. Meta-analysis is summarized by the predictive posterior distributions for sensitivity and specificity. This new approach allows, also, to perform substantial model checking, model diagnostic and model selection. Statistical computations are implemented in the public domain statistical software (WinBUGS and R) and illustrated with real data examples. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/9XZKK52B/Verde - 2010 - Meta-analysis of diagnostic test data A bivariate.pdf;/Users/rritaz/Zotero/storage/CEXUFS7E/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate,random-effects},
  language = {en},
  number = {30}
}

@article{veroniki_characteristics_2014,
  title = {Characteristics of a Loop of Evidence That Affect Detection and Estimation of Inconsistency: A Simulation Study},
  shorttitle = {Characteristics of a Loop of Evidence That Affect Detection and Estimation of Inconsistency},
  author = {Veroniki, Areti Angeliki and Mavridis, Dimitris and Higgins, Julian PT and Salanti, Georgia},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {106},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-106},
  abstract = {Background: The assumption of consistency, defined as agreement between direct and indirect sources of evidence, underlies the increasingly popular method of network meta-analysis. This assumption is often evaluated by statistically testing for a difference between direct and indirect estimates within each loop of evidence. However, the test is believed to be underpowered. We aim to evaluate its properties when applied to a loop typically found in published networks. Methods: In a simulation study we estimate type I error, power and coverage probability of the inconsistency test for dichotomous outcomes using realistic scenarios informed by previous empirical studies. We evaluate test properties in the presence or absence of heterogeneity, using different estimators of heterogeneity and by employing different methods for inference about pairwise summary effects (Knapp-Hartung and inverse variance methods). Results: As expected, power is positively associated with sample size and frequency of the outcome and negatively associated with the presence of heterogeneity. Type I error converges to the nominal level as the total number of individuals in the loop increases. Coverage is close to the nominal level in most cases. Different estimation methods for heterogeneity do not greatly impact on test performance, but different methods to derive the variances of the direct estimates impact on inconsistency inference. The Knapp-Hartung method is more powerful, especially in the absence of heterogeneity, but exhibits larger type I error. The power for a `typical' loop (comprising of 8 trials and about 2000 participants) to detect a 35\% relative change between direct and indirect estimation of the odds ratio was 14\% for inverse variance and 21\% for Knapp-Hartung methods (with type I error 5\% in the former and 11\% in the latter). Conclusions: The study gives insight into the conditions under which the statistical test can detect important inconsistency in a loop of evidence. Although different methods to estimate the uncertainty of the mean effect may improve the test performance, this study suggests that the test has low power for the `typical' loop. Investigators should interpret results very carefully and always consider the comparability of the studies in terms of potential effect modifiers.},
  file = {/Users/rritaz/Zotero/storage/PKIKUQFH/Veroniki et al. - 2014 - Characteristics of a loop of evidence that affect .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes,network meta-analysis,power,random-effects},
  language = {en},
  number = {1}
}

@article{veroniki_methods_2016,
  title = {Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and Higgins, Julian PT and Langan, Dean and Salanti, Georgia},
  year = {2016},
  volume = {7},
  pages = {55--79},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1164},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a `generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/7HWJHU78/Veroniki et al. - 2016 - Methods to estimate the between-study variance and.pdf;/Users/rritaz/Zotero/storage/ULRTY8F5/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Confidence Intervals,heterogeneity estimators,random-effects},
  language = {en},
  number = {1}
}

@article{veroniki_methods_2016-1,
  title = {Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and Higgins, Julian PT and Langan, Dean and Salanti, Georgia},
  year = {2016},
  volume = {7},
  pages = {55--79},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1164},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a `generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1164},
  copyright = {\textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/WD8VUZ6X/Veroniki et al. - 2016 - Methods to estimate the between-study variance and.pdf;/Users/rritaz/Zotero/storage/NZ755G8X/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {bias,confidence interval,coverage probability,heterogeneity,mean squared error},
  language = {en},
  number = {1}
}

@article{veroniki_reconstructing_2013,
  title = {Reconstructing 2 x 2 Contingency Tables from Odds Ratios Using the {{Di Pietrantonj}} Method: Difficulties, Constraints and Impact in Meta-Analysis Results},
  shorttitle = {Reconstructing 2 x 2 Contingency Tables from Odds Ratios Using the {{Di Pietrantonj}} Method},
  author = {Veroniki, Areti Angeliki and Pavlides, Marios and Patsopoulos, Nikolaos A. and Salanti, Georgia},
  year = {2013},
  volume = {4},
  pages = {78--94},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1061},
  abstract = {A problem that is frequently encountered during the systematic review process is when studies that meet the inclusion criteria do not provide the appropriate numerical estimates to include in a meta-analysis. For dichotomous outcomes, a method has been suggested by Di Pietrantonj for reconstructing the 2 \texttimes{} 2 table when the Odds Ratio (OR), the Standard Error (SE(lnOR)) and the sample sizes are provided. The method produces two possible 2 \texttimes{} 2 tables; and to select the correct one, the Control Group Risk (CGR) is used. As CGR is typically unknown and only rounded figures of the OR and SE(lnOR) are provided, the accuracy of the reconstruction method varies. In this paper, we evaluate the performance of the method using simulated and empirical data. Small studies with large OR and CGR away from 50\% are reconstructed satisfactorily, and the use of SE(lnOR) rounded to the third decimal rather than the second one improves the performance of the method. However, when CGR is unknown, its estimation from other studies is problematic as it exhibits high heterogeneity. Inclusion of an incorrectly reconstructed table in the meta-analysis may result in different summary effects. Reviewers that consider applying the method should be cautious about its impact in the meta-analysis. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GIIUDTU4/Veroniki et al. - 2013 - Reconstructing 2 x 2 contingency tables from odds .pdf;/Users/rritaz/Zotero/storage/P8CAXYX3/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {discrete effect sizes,missing data},
  language = {en},
  number = {1}
}

@article{vevea_publication_2005,
  title = {Publication Bias in Research Synthesis: Sensitivity Analysis Using a Priori Weight Functions.},
  shorttitle = {Publication Bias in Research Synthesis},
  author = {Vevea, Jack L. and Woods, Carol M.},
  year = {2005},
  volume = {10},
  pages = {428--443},
  doi = {10.1037/1082-989X.10.4.428},
  abstract = {Publication bias, sometimes known as the "file-drawer problem" or "funnel-plot asymmetry," is common in empirical research. The authors review the implications of publication bias for quantitative research synthesis (meta-analysis) and describe existing techniques for detecting and correcting it. A new approach is proposed that is suitable for application to meta-analytic data sets that are too small for the application of existing methods. The model estimates parameters relevant to fixed-effects, mixed-effects or random-effects meta-analysis contingent on a hypothetical pattern of bias that is fixed independently of the data. The authors illustrate this approach for sensitivity analysis using 3 data sets adapted from a commonly cited reference work on research synthesis (H. M. Cooper \& L. V. Hedges, 1994).},
  file = {/Users/rritaz/Zotero/storage/2FQE99SK/Vevea and Woods - 2005 - Publication bias in research synthesis sensitivit.pdf},
  journal = {Psychological methods},
  keywords = {GLM MA models,modeling effect size variation (covariates),publication bias,random-effects},
  number = {4}
}

@article{viechtbauer_bias_2005,
  title = {Bias and {{Efficiency}} of {{Meta}}-{{Analytic Variance Estimators}} in the {{Random}}-{{Effects Model}}},
  author = {Viechtbauer, Wolfgang},
  year = {2005},
  month = sep,
  volume = {30},
  pages = {261--293},
  issn = {1076-9986},
  doi = {10.3102/10769986030003261},
  abstract = {The meta-analytic random effects model assumes that the variability in effect size estimates drawn from a set of studies can be decomposed into two parts: heterogeneity due to random population effects and sampling variance. In this context, the usual goal is to estimate the central tendency and the amount of heterogeneity in the population effect sizes. The amount of heterogeneity in a set of effect sizes has implications regarding the interpretation of the meta-analytic findings and often serves as an indicator for the presence of potential moderator variables. Five population heterogeneity estimators were compared in this article analytically and via Monte Carlo simulations with respect to their bias and efficiency.},
  file = {/Users/rritaz/Zotero/storage/8Y8JFDP7/Viechtbauer - 2005 - Bias and Efficiency of Meta-Analytic Variance Esti.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {heterogeneity estimators,random-effects},
  number = {3}
}

@article{viechtbauer_comparison_2015,
  title = {A Comparison of Procedures to Test for Moderators in Mixed-Effects Meta-Regression Models.},
  author = {Viechtbauer, Wolfgang and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2015},
  volume = {20},
  pages = {360--374},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000023},
  file = {/Users/rritaz/Zotero/storage/HKPBKTVY/Viechtbauer et al. - 2015 - A comparison of procedures to test for moderators .pdf},
  journal = {Psychological Methods},
  keywords = {continuous effect sizes,GLM MA models,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {3}
}

@article{viechtbauer_confidence_2007,
  title = {Confidence Intervals for the Amount of Heterogeneity in Meta-Analysis},
  author = {Viechtbauer, Wolfgang},
  year = {2007},
  volume = {26},
  pages = {37--52},
  issn = {1097-0258},
  doi = {10.1002/sim.2514},
  abstract = {Effect size estimates to be combined in a systematic review are often found to be more variable than one would expect based on sampling differences alone. This is usually interpreted as evidence that the effect sizes are heterogeneous. A random-effects model is then often used to account for the heterogeneity in the effect sizes. A novel method for constructing confidence intervals for the amount of heterogeneity in the effect sizes is proposed that guarantees nominal coverage probabilities even in small samples when model assumptions are satisfied. A variety of existing approaches for constructing such confidence intervals are summarized and the various methods are applied to an example to illustrate their use. A simulation study reveals that the newly proposed method yields the most accurate coverage probabilities under conditions more analogous to practice, where assumptions about normally distributed effect size estimates and known sampling variances only hold asymptotically. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/8AJRLFYM/Viechtbauer - 2007 - Confidence intervals for the amount of heterogenei.pdf;/Users/rritaz/Zotero/storage/FXE52LEZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Confidence Intervals,random-effects},
  language = {en},
  number = {1}
}

@article{viechtbauer_outlier_2010,
  title = {Outlier and Influence Diagnostics for Meta-Analysis},
  author = {Viechtbauer, Wolfgang and Cheung, Mike W.-L.},
  year = {2010},
  volume = {1},
  pages = {112--125},
  issn = {1759-2887},
  doi = {10.1002/jrsm.11},
  abstract = {The presence of outliers and influential cases may affect the validity and robustness of the conclusions from a meta-analysis. While researchers generally agree that it is necessary to examine outlier and influential case diagnostics when conducting a meta-analysis, limited studies have addressed how to obtain such diagnostic measures in the context of a meta-analysis. The present paper extends standard diagnostic procedures developed for linear regression analyses to the meta-analytic fixed- and random/mixed-effects models. Three examples are used to illustrate the usefulness of these procedures in various research settings. Issues related to these diagnostic procedures in meta-analysis are also discussed. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JCUEXDCP/Viechtbauer and Cheung - 2010 - Outlier and influence diagnostics for meta-analysi.pdf;/Users/rritaz/Zotero/storage/WYU6KS22/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques},
  language = {en},
  number = {2}
}

@article{villanueva_evaluating_2004,
  title = {Evaluating Heterogeneity in Cumulative Meta-Analyses},
  author = {Villanueva, Elmer V and Zavarsek, Silva},
  year = {2004},
  month = dec,
  volume = {4},
  pages = {18},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-4-18},
  abstract = {Background: Recently developed measures such as I2 and H allow the evaluation of the impact of heterogeneity in conventional meta-analyses. There has been no examination of the development of heterogeneity in the context of a cumulative meta-analysis. Methods: Cumulative meta-analyses of five smoking cessation interventions (clonidine, nicotine replacement therapy using gum and patch, physician advice and acupuncture) were used to calculate I2 and H. These values were plotted by year of publication, control event rate and sample size to trace the development of heterogeneity over these covariates. Results: The cumulative evaluation of heterogeneity varied according to the measure of heterogeneity used and the basis of cumulation. Plots produced from the calculations revealed areas of heterogeneity useful in the consideration of potential sources for further study. Conclusion: The examination of heterogeneity in conjunction with summary effect estimates in a cumulative meta-analysis offered valuable insight into the evolution of variation. Such information is not available in the context of conventional meta-analysis and has the potential to lead to the development of a richer picture of the effectiveness of interventions.},
  file = {/Users/rritaz/Zotero/storage/PDS26DLJ/Villanueva and Zavarsek - 2004 - Evaluating heterogeneity in cumulative meta-analys.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {random-effects},
  language = {en},
  number = {1}
}

@article{vo_novel_2019,
  title = {A Novel Approach for Identifying and Addressing Case-Mix Heterogeneity in Individual Participant Data Meta-Analysis},
  author = {Vo, Tat-Thang and Porcher, Raphael and Chaimani, Anna and Vansteelandt, Stijn},
  year = {2019},
  volume = {10},
  pages = {582--596},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1382},
  abstract = {Case-mix heterogeneity across studies complicates meta-analyses. As a result of this, treatments that are equally effective on patient subgroups may appear to have different effectiveness on patient populations with different case mix. It is therefore important that meta-analyses be explicit for what patient population they describe the treatment effect. To achieve this, we develop a new approach for meta-analysis of randomized clinical trials, which use individual patient data (IPD) from all trials to infer the treatment effect for the patient population in a given trial, based on direct standardization using either outcome regression (OCR) or inverse probability weighting (IPW). Accompanying random-effect meta-analysis models are developed. The new approach enables disentangling heterogeneity due to case mix from that due to beyond case-mix reasons.},
  file = {/Users/rritaz/Zotero/storage/NYC44NP3/Vo et al. - 2019 - A novel approach for identifying and addressing ca.pdf;/Users/rritaz/Zotero/storage/T4VGRWDQ/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {Individual Patient Data IPD,random-effects},
  language = {en},
  number = {4}
}

@article{von_hippel_heterogeneity_2015,
  title = {The Heterogeneity Statistic {{I2}} Can Be Biased in Small Meta-Analyses},
  author = {{von Hippel}, Paul T},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {35},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0024-z},
  abstract = {Background: Estimated effects vary across studies, partly because of random sampling error and partly because of heterogeneity. In meta-analysis, the fraction of variance that is due to heterogeneity is estimated by the statistic I2. We calculate the bias of I2, focusing on the situation where the number of studies in the meta-analysis is small. Small meta-analyses are common; in the Cochrane Library, the median number of studies per meta-analysis is 7 or fewer. Methods: We use Mathematica software to calculate the expectation and bias of I2. Results: I2 has a substantial bias when the number of studies is small. The bias is positive when the true fraction of heterogeneity is small, but the bias is typically negative when the true fraction of heterogeneity is large. For example, with 7 studies and no true heterogeneity, I2 will overestimate heterogeneity by an average of 12 percentage points, but with 7 studies and 80 percent true heterogeneity, I2 can underestimate heterogeneity by an average of 28 percentage points. Biases of 12\textendash 28 percentage points are not trivial when one considers that, in the Cochrane Library, the median I2 estimate is 21 percent. Conclusions: The point estimate I2 should be interpreted cautiously when a meta-analysis has few studies. In small meta-analyses, confidence intervals should supplement or replace the biased point estimate I2.},
  file = {/Users/rritaz/Zotero/storage/HT8PWHXS/von Hippel - 2015 - The heterogeneity statistic I2 can be biased in sm.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,random-effects,small meta-analysis},
  language = {en},
  number = {1}
}

@article{walwyn_meta-analysis_2017,
  title = {Meta-Analysis of Standardised Mean Differences from Randomised Trials with Treatment-Related Clustering Associated with Care Providers},
  author = {Walwyn, Rebecca and Roberts, Chris},
  year = {2017},
  volume = {36},
  pages = {1043--1067},
  issn = {1097-0258},
  doi = {10.1002/sim.7186},
  abstract = {In meta-analyses, where a continuous outcome is measured with different scales or standards, the summary statistic is the mean difference standardised to a common metric with a common variance. Where trial treatment is delivered by a person, nesting of patients within care providers leads to clustering that may interact with, or be limited to, one or more of the arms. Assuming a common standardising variance is less tenable and options for scaling the mean difference become numerous. Metrics suggested for cluster-randomised trials are within, between and total variances and for unequal variances, the control arm or pooled variances. We consider summary measures and individual-patient-data methods for meta-analysing standardised mean differences from trials with two-level nested clustering, relaxing independence and common variance assumptions, allowing sample sizes to differ across arms. A general metric is proposed with comparable interpretation across designs. The relationship between the method of standardisation and choice of model is explored, allowing for bias in the estimator and imprecision in the standardising metric. A meta-analysis of trials of counselling in primary care motivated this work. Assuming equal clustering effects across trials, the proposed random-effects meta-analysis model gave a pooled standardised mean difference of -0.27 (95\% CI -0.45 to -0.08) using summary measures and -0.26 (95\% CI -0.45 to -0.09) with the individual-patient-data. While treatment-related clustering has rarely been taken into account in trials, it is now recommended that it is considered in trials and meta-analyses. This paper contributes to the uptake of this guidance. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/RRZ24CSU/Walwyn and Roberts - 2017 - Meta-analysis of standardised mean differences fro.pdf;/Users/rritaz/Zotero/storage/ZTZSJSAA/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,Individual Patient Data IPD,random-effects,standardized mean difference},
  language = {en},
  number = {7}
}

@article{wan_estimating_2014,
  title = {Estimating the Sample Mean and Standard Deviation from the Sample Size, Median, Range and/or Interquartile Range},
  author = {Wan, Xiang and Wang, Wenqian and Liu, Jiming and Tong, Tiejun},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {135},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-135},
  abstract = {Background: In systematic reviews and meta-analysis, researchers often pool the results of the sample mean and standard deviation from a set of similar clinical trials. A number of the trials, however, reported the study using the median, the minimum and maximum values, and/or the first and third quartiles. Hence, in order to combine results, one may have to estimate the sample mean and standard deviation for such trials. Methods: In this paper, we propose to improve the existing literature in several directions. First, we show that the sample standard deviation estimation in Hozo et al.'s method (BMC Med Res Methodol 5:13, 2005) has some serious limitations and is always less satisfactory in practice. Inspired by this, we propose a new estimation method by incorporating the sample size. Second, we systematically study the sample mean and standard deviation estimation problem under several other interesting settings where the interquartile range is also available for the trials. Results: We demonstrate the performance of the proposed methods through simulation studies for the three frequently encountered scenarios, respectively. For the first two scenarios, our method greatly improves existing methods and provides a nearly unbiased estimate of the true sample standard deviation for normal data and a slightly biased estimate for skewed data. For the third scenario, our method still performs very well for both normal data and skewed data. Furthermore, we compare the estimators of the sample mean and standard deviation under all three scenarios and present some suggestions on which scenario is preferred in real-world applications. Conclusions: In this paper, we discuss different approximation methods in the estimation of the sample mean and standard deviation and propose some new estimation methods to improve the existing literature. We conclude our work with a summary table (an Excel spread sheet including all formulas) that serves as a comprehensive guidance for performing meta-analysis in different situations.},
  file = {/Users/rritaz/Zotero/storage/X4VER922/Wan et al. - 2014 - Estimating the sample mean and standard deviation .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {continuous effect sizes},
  language = {en},
  number = {1}
}

@article{wang_hierarchical_2008,
  title = {Hierarchical Models for {{ROC}} Curve Summary Measures: {{Design}} and Analysis of Multi-Reader, Multi-Modality Studies of Medical Tests},
  shorttitle = {Hierarchical Models for {{ROC}} Curve Summary Measures},
  author = {Wang, Fei and Gatsonis, Constantine A.},
  year = {2008},
  volume = {27},
  pages = {243--256},
  issn = {1097-0258},
  doi = {10.1002/sim.2828},
  abstract = {Comparative studies of the accuracy of diagnostic tests often involve designs according to which each study participant is examined by two or more of the tests and the diagnostic examinations are interpreted by several readers. Tests are then compared on the basis of a summary index, such as the (full or partial) area under the receiver operating characteristic (ROC) curve, averaged over the population of readers. The design and analysis of such studies naturally need to take into account the correlated nature of the diagnostic test results and interpretations. In this paper, we describe the use of hierarchical modelling for ROC summary measures derived from multi-reader, multi-modality studies. The models allow the variance of the estimates to depend on the actual value of the index and account for the correlation in the data both explicitly via parameters and implicitly via the hierarchical structure. After showing how the hierarchical models can be employed in the analysis of data from multi-reader, multi-modality studies, we discuss the design of such studies using the simulation-based, Bayesian design approach of Wang and Gelfand (Stat. Sci. 2002; 17(2):193\textendash 208). The methodology is illustrated via the analysis of data from a study conducted to evaluate a computer-aided diagnosis tool for screen film mammography and via the development of design considerations for a multi-reader study comparing display modes for digital mammography. The hierarchical model methodology described in this paper is also applicable to the meta-analysis of ROC studies. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LSDIFB6Q/Wang and Gatsonis - 2008 - Hierarchical models for ROC curve summary measures.pdf;/Users/rritaz/Zotero/storage/JBE6GHPY/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,correlated effects},
  language = {en},
  number = {2}
}

@article{wang_meta-stepp_2016,
  title = {Meta-{{STEPP}}: Subpopulation Treatment Effect Pattern Plot for Individual Patient Data Meta-Analysis},
  shorttitle = {Meta-{{STEPP}}},
  author = {Wang, Xin Victoria and Cole, Bernard and Bonetti, Marco and Gelber, Richard D.},
  year = {2016},
  volume = {35},
  pages = {3704--3716},
  issn = {1097-0258},
  doi = {10.1002/sim.6958},
  abstract = {We have developed a method, called Meta-STEPP (subpopulation treatment effect pattern plot for meta-analysis), to explore treatment effect heterogeneity across covariate values in the meta-analysis setting for time-to-event data when the covariate of interest is continuous. Meta-STEPP forms overlapping subpopulations from individual patient data containing similar numbers of events with increasing covariate values, estimates subpopulation treatment effects using standard fixed-effects meta-analysis methodology, displays the estimated subpopulation treatment effect as a function of the covariate values, and provides a statistical test to detect possibly complex treatment-covariate interactions. Simulation studies show that this test has adequate type-I error rate recovery as well as power when reasonable window sizes are chosen. When applied to eight breast cancer trials, Meta-STEPP suggests that chemotherapy is less effective for tumors with high estrogen receptor expression compared with those with low expression. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZYMSQEQN/Wang et al. - 2016 - Meta-STEPP subpopulation treatment effect pattern.pdf;/Users/rritaz/Zotero/storage/PPBZB99E/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,modeling effect size variation (covariates),random-effects},
  language = {en},
  number = {21}
}

@article{wang_semiparametric_2012,
  title = {Semiparametric Hazard Function Estimation in Meta-Analysis for Time to Event Data},
  author = {Wang, Jixian},
  year = {2012},
  volume = {3},
  pages = {240--249},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1047},
  abstract = {Meta-analyses have been widely used to combine information from survival data using estimated parameters in, for example, a Cox model. A number of approaches dealing with study level random effects have been developed. However, there are far fewer meta-analysis approaches for estimating survival or hazard functions. Typical approaches are based on the cumulative survival function using the generalized estimating equation. We propose an alternative approach following Efron's discrete logistic regression (Efron, 1988), but using generalized linear mixed models. We show that spline functions can be used in fitting the models to obtain smoothed estimates for hazard functions. The models also allow a semi-parametric structure to include factors such as random study effects and treatment groups. This approach models the hazard function based on which the survival function can be estimated too. We also propose a Bayesian bootstrap approach for statistical inference for both hazard and survival functions. This approach was applied to two meta-analysis data sets as examples to illustrate its use. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/6TTHCFXS/Wang - 2012 - Semiparametric hazard function estimation in meta-.pdf;/Users/rritaz/Zotero/storage/VWV4TDSA/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {categorical MA models},
  language = {en},
  number = {3}
}

@article{wang_simple_2019,
  title = {A Simple Method to Estimate Prediction Intervals and Predictive Distributions: {{Summarizing}} Meta-Analyses beyond Means and Confidence Intervals},
  shorttitle = {A Simple Method to Estimate Prediction Intervals and Predictive Distributions},
  author = {Wang, Chia-Chun and Lee, Wen-Chung},
  year = {2019},
  volume = {10},
  pages = {255--266},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1345},
  abstract = {A systematic review and meta-analysis is an important step in evidence synthesis. The current paradigm for meta-analyses requires a presentation of the means under a random-effects model; however, a mean with a confidence interval provides an incomplete summary of the underlying heterogeneity in meta-analysis. Prediction intervals show the range of true effects in future studies and have been advocated to be regularly presented. Most commonly, prediction intervals are estimated assuming that the underlying heterogeneity follows a normal distribution, which is not necessarily appropriate. In this article, we provide a simple method with a ready-to-use spreadsheet file to estimate prediction intervals and predictive distributions nonparametrically. Simulation studies show that this new method can provide approximately unbiased estimates compared with the conventional method. We also illustrate the advantage and real-world significance of this approach with a meta-analysis evaluating the protective effect of vaccination against tuberculosis. The nonparametric predictive distribution provides more information about the shape of the underlying distribution than does the conventional method.},
  file = {/Users/rritaz/Zotero/storage/IRYAUYQ8/Wang and Lee - 2019 - A simple method to estimate prediction intervals a.pdf;/Users/rritaz/Zotero/storage/XD5JLTWH/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {prediction intervals},
  language = {en},
  number = {2}
}

@article{warn_bayesian_2002,
  title = {Bayesian Random Effects Meta-Analysis of Trials with Binary Outcomes: Methods for the Absolute Risk Difference and Relative Risk Scales},
  shorttitle = {Bayesian Random Effects Meta-Analysis of Trials with Binary Outcomes},
  author = {Warn, D. E. and Thompson, S. G. and Spiegelhalter, D. J.},
  year = {2002},
  volume = {21},
  pages = {1601--1623},
  issn = {1097-0258},
  doi = {10.1002/sim.1189},
  abstract = {When conducting a meta-analysis of clinical trials with binary outcomes, a normal approximation for the summary treatment effect measure in each trial is inappropriate in the common situation where some of the trials in the meta-analysis are small, or the observed risks are close to 0 or 1. This problem can be avoided by making direct use of the binomial distribution within trials. A fully Bayesian method has already been developed for random effects meta-analysis on the log-odds scale using the BUGS implementation of Gibbs sampling. In this paper we demonstrate how this method can be extended to perform analyses on both the absolute and relative risk scales. Within each approach we exemplify how trial-level covariates, including underlying risk, can be considered. Data from 46 trials of the effect of single-dose ibuprofen on post-operative pain are analysed and the results contrasted with those derived from classical and Bayesian summary statistic methods. The clinical interpretation of the odds ratio scale is not straightforward. The advantages and flexibility of a fully Bayesian approach to meta-analysis of binary outcome data, considered on an absolute risk or relative risk scale, are now available. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/N7CMEDZR/Warn et al. - 2002 - Bayesian random effects meta-analysis of trials wi.pdf;/Users/rritaz/Zotero/storage/LQVH2VS8/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,modeling effect size variation (covariates)},
  language = {en},
  number = {11}
}

@article{watkins_simple_2018,
  title = {A Simple Method for Combining Binomial Counts or Proportions with Hazard Ratios for Evidence Synthesis of Time-to-Event Data},
  author = {Watkins, Claire and Bennett, Iain},
  year = {2018},
  volume = {9},
  pages = {352--360},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1301},
  abstract = {In studies with time-to-event data, outcomes may be reported as hazard ratios (HR) or binomial counts/proportions at a specific time point. If the intent is to synthesise evidence by performing a meta-analysis or network meta-analysis (NMA) using the HR as the measure of treatment effect, studies that only report binomial data cannot be included in the network. Methods for converting binomial data to HRs were investigated, so that studies reporting binomial data only could be included in a network of HR data. Estimating the log HR is relatively straightforward under the assumptions of proportional hazards and minimal censoring at the binomial data time point. Estimating the standard error of the log HR is harder, but a simple method based on using a Taylor series expansion to approximate the variance is proposed. Thus, we have 2 easy-to-calculate equations for the log HR and variance. The performance of the method was assessed using simulations and data from a NMA of multiple sclerosis treatments. In the simulation, our binomial method produced very similar HRs to those from survival analysis when censoring rates were low, and also when censoring rates were high but the event rate was low. In all situations, it outperformed using relative risk to approximate the HR. In the NMA, results were consistent between reported HRs and HRs derived from binomial data for studies that reported both types of data. This method may be useful for easily incorporating trials reporting binomial data into an evidence synthesis of HR data, under certain assumptions.},
  file = {/Users/rritaz/Zotero/storage/CNKLF6HS/Watkins and Bennett - 2018 - A simple method for combining binomial counts or p.pdf;/Users/rritaz/Zotero/storage/MUIGIID9/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,effect size combination (small sample \& discrete)},
  language = {en},
  number = {3}
}

@article{wei_bayesian_2013,
  title = {Bayesian Multivariate Meta-Analysis with Multiple Outcomes},
  author = {Wei, Yinghui and Higgins, Julian P. T.},
  year = {2013},
  volume = {32},
  pages = {2911--2934},
  issn = {1097-0258},
  doi = {10.1002/sim.5745},
  abstract = {There has been a recent growth in developments of multivariate meta-analysis. We extend the methodology of Bayesian multivariate meta-analysis to the situation when there are more than two outcomes of interest, which is underexplored in the current literature. Our objective is to meta-analyse summary data from multiple outcomes simultaneously, accounting for potential dependencies among the data. One common issue is that studies do not all report all of the outcomes of interests, and we take an approach relying on marginal modelling of only the reported data. We employ a separation prior for the between-study variance\textendash covariance matrix, which offers an improvement on the conventional inverse-Wishart prior, showing robustness in estimation and flexibility in incorporating prior information. Particular challenges arise when the number of outcomes is large relative to the number of studies because the number of parameters in the variance\textendash covariance matrix can become substantial and there can be very little information with which to estimate between-study correlation coefficients. We explore assumptions that reduce the number of parameters in this matrix, including assumptions of homogenous variances, homogenous correlations for certain outcomes and positive correlation coefficients. We illustrate the methods with an example data set from the Cochrane Database of Systematic Reviews. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/U3IC9PNF/Wei and Higgins - 2013 - Bayesian multivariate meta-analysis with multiple .pdf;/Users/rritaz/Zotero/storage/IY6S6WPF/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,random-effects,robust variance estimation},
  language = {en},
  number = {17}
}

@article{wei_estimating_2013,
  title = {Estimating Within-Study Covariances in Multivariate Meta-Analysis with Multiple Outcomes},
  author = {Wei, Yinghui and Higgins, Julian PT},
  year = {2013},
  volume = {32},
  pages = {1191--1205},
  issn = {1097-0258},
  doi = {10.1002/sim.5679},
  abstract = {Multivariate meta-analysis allows the joint synthesis of effect estimates based on multiple outcomes from multiple studies, accounting for the potential correlations among them. However, standard methods for multivariate meta-analysis for multiple outcomes are restricted to problems where the within-study correlation is known or where individual participant data are available. This paper proposes an approach to approximating the within-study covariances based on information about likely correlations between underlying outcomes. We developed methods for both continuous and dichotomous data and for combinations of the two types. An application to a meta-analysis of treatments for stroke illustrates the use of the approximated covariance in multivariate meta-analysis with correlated outcomes. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/CUKJRMJ5/Wei and Higgins - 2013 - Estimating within-study covariances in multivariat.pdf;/Users/rritaz/Zotero/storage/44C8L8WD/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {7}
}

@article{wei_meta-analysis_2015,
  title = {Meta-Analysis of Time-to-Event Outcomes from Randomized Trials Using Restricted Mean Survival Time: Application to Individual Participant Data},
  shorttitle = {Meta-Analysis of Time-to-Event Outcomes from Randomized Trials Using Restricted Mean Survival Time},
  author = {Wei, Yinghui and Royston, Patrick and Tierney, Jayne F. and Parmar, Mahesh K. B.},
  year = {2015},
  volume = {34},
  pages = {2881--2898},
  issn = {1097-0258},
  doi = {10.1002/sim.6556},
  abstract = {Meta-analysis of time-to-event outcomes using the hazard ratio as a treatment effect measure has an underlying assumption that hazards are proportional. The between-arm difference in the restricted mean survival time is a measure that avoids this assumption and allows the treatment effect to vary with time. We describe and evaluate meta-analysis based on the restricted mean survival time for dealing with non-proportional hazards and present a diagnostic method for the overall proportional hazards assumption. The methods are illustrated with the application to two individual participant meta-analyses in cancer. The examples were chosen because they differ in disease severity and the patterns of follow-up, in order to understand the potential impacts on the hazards and the overall effect estimates. We further investigate the estimation methods for restricted mean survival time by a simulation study. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ECSW6JTE/Wei et al. - 2015 - Meta-analysis of time-to-event outcomes from rando.pdf;/Users/rritaz/Zotero/storage/PT47YQ8J/sim.html},
  journal = {Statistics in Medicine},
  keywords = {diagnostic techniques,Individual Patient Data IPD},
  language = {en},
  number = {21}
}

@article{weinhandl_generalization_2012,
  title = {Generalization of Trim and Fill for Application in Meta-Regression},
  author = {Weinhandl, Eric D. and Duval, Sue},
  year = {2012},
  volume = {3},
  pages = {51--67},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1042},
  abstract = {Trim and fill is a popular method of accounting for publication bias in meta-analysis. However, the use of trim and fill is limited to the setting in which all meta-analyzed studies represent a true common effect. In many practical settings, within-study effect estimates are a function of some covariate. Because methods of accounting for publication bias in meta-regression have received little attention, we propose here a generalization of trim and fill for application in meta-regression. The proposed algorithm preserves the computational features of trim and fill and adds only an assumption of symmetry in the hypothesized distribution of the measured covariate. By simulation, we evaluate properties (mean bias, root mean squared error, and coverage probability) of meta-regression parameter estimates and corresponding confidence intervals with application of the proposed algorithm in a range of scenarios, including violation of the aforementioned assumption of symmetry. We also evaluate the performance of common estimators of the number of suppressed studies. In general, we show that the proposed algorithm is successful in identifying suppression of studies and reducing the bias in regression parameters derived from the analysis of the augmented set of studies. We apply the proposed algorithm to an analysis of the effect of cognitive\textendash behavioral therapy on the risk of recidivism. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/TNM3YS49/Weinhandl and Duval - 2012 - Generalization of trim and fill for application in.pdf;/Users/rritaz/Zotero/storage/WA54C7VV/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,modeling effect size variation (covariates),publication bias},
  language = {en},
  number = {1}
}

@article{weir_dealing_2018,
  title = {Dealing with Missing Standard Deviation and Mean Values in Meta-Analysis of Continuous Outcomes: A Systematic Review},
  shorttitle = {Dealing with Missing Standard Deviation and Mean Values in Meta-Analysis of Continuous Outcomes},
  author = {Weir, Christopher J. and Butcher, Isabella and Assi, Valentina and Lewis, Stephanie C. and Murray, Gordon D. and Langhorne, Peter and Brady, Marian C.},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {25},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0483-0},
  abstract = {Background: Rigorous, informative meta-analyses rely on availability of appropriate summary statistics or individual participant data. For continuous outcomes, especially those with naturally skewed distributions, summary information on the mean or variability often goes unreported. While full reporting of original trial data is the ideal, we sought to identify methods for handling unreported mean or variability summary statistics in meta-analysis. Methods: We undertook two systematic literature reviews to identify methodological approaches used to deal with missing mean or variability summary statistics. Five electronic databases were searched, in addition to the Cochrane Colloquium abstract books and the Cochrane Statistics Methods Group mailing list archive. We also conducted cited reference searching and emailed topic experts to identify recent methodological developments. Details recorded included the description of the method, the information required to implement the method, any underlying assumptions and whether the method could be readily applied in standard statistical software. We provided a summary description of the methods identified, illustrating selected methods in example meta-analysis scenarios. Results: For missing standard deviations (SDs), following screening of 503 articles, fifteen methods were identified in addition to those reported in a previous review. These included Bayesian hierarchical modelling at the meta-analysis level; summary statistic level imputation based on observed SD values from other trials in the meta-analysis; a practical approximation based on the range; and algebraic estimation of the SD based on other summary statistics. Following screening of 1124 articles for methods estimating the mean, one approximate Bayesian computation approach and three papers based on alternative summary statistics were identified. Illustrative meta-analyses showed that when replacing a missing SD the approximation using the range minimised loss of precision and generally performed better than omitting trials. When estimating missing means, a formula using the median, lower quartile and upper quartile performed best in preserving the precision of the meta-analysis findings, although in some scenarios, omitting trials gave superior results. Conclusions: Methods based on summary statistics (minimum, maximum, lower quartile, upper quartile, median) reported in the literature facilitate more comprehensive inclusion of randomised controlled trials with missing mean or variability summary statistics within meta-analyses.},
  file = {/Users/rritaz/Zotero/storage/3IJ9GDRY/Weir et al. - 2018 - Dealing with missing standard deviation and mean v.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {meta-meta-analyses,missing data},
  language = {en},
  number = {1}
}

@article{wetterslev_estimating_2009,
  title = {Estimating Required Information Size by Quantifying Diversity in Random-Effects Model Meta-Analyses},
  author = {Wetterslev, J{\o}rn and Thorlund, Kristian and Brok, Jesper and Gluud, Christian},
  year = {2009},
  month = dec,
  volume = {9},
  pages = {86},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-86},
  abstract = {Background: There is increasing awareness that meta-analyses require a sufficiently large information size to detect or reject an anticipated intervention effect. The required information size in a meta-analysis may be calculated from an anticipated a priori intervention effect or from an intervention effect suggested by trials with low-risk of bias. Methods: Information size calculations need to consider the total model variance in a metaanalysis to control type I and type II errors. Here, we derive an adjusting factor for the required information size under any random-effects model meta-analysis. Results: We devise a measure of diversity (D2) in a meta-analysis, which is the relative variance reduction when the meta-analysis model is changed from a random-effects into a fixed-effect model. D2 is the percentage that the between-trial variability constitutes of the sum of the betweentrial variability and a sampling error estimate considering the required information size. D2 is different from the intuitively obvious adjusting factor based on the common quantification of heterogeneity, the inconsistency (I2), which may underestimate the required information size. Thus, D2 and I2 are compared and interpreted using several simulations and clinical examples. In addition we show mathematically that diversity is equal to or greater than inconsistency, that is D2 {$\geq$} I2, for all meta-analyses. Conclusion: We conclude that D2 seems a better alternative than I2 to consider model variation in any random-effects meta-analysis despite the choice of the between trial variance estimator that constitutes the model. Furthermore, D2 can readily adjust the required information size in any random-effects model meta-analysis.},
  file = {/Users/rritaz/Zotero/storage/ZWQLPH5E/Wetterslev et al. - 2009 - Estimating required information size by quantifyin.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,random-effects},
  language = {en},
  number = {1}
}

@article{wetterslev_trial_2017,
  title = {Trial {{Sequential Analysis}} in Systematic Reviews with Meta-Analysis},
  author = {Wetterslev, J{\o}rn and Jakobsen, Janus Christian and Gluud, Christian},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {39},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0315-7},
  abstract = {Background: Most meta-analyses in systematic reviews, including Cochrane ones, do not have sufficient statistical power to detect or refute even large intervention effects. This is why a meta-analysis ought to be regarded as an interim analysis on its way towards a required information size. The results of the meta-analyses should relate the total number of randomised participants to the estimated required meta-analytic information size accounting for statistical diversity. When the number of participants and the corresponding number of trials in a meta-analysis are insufficient, the use of the traditional 95\% confidence interval or the 5\% statistical significance threshold will lead to too many false positive conclusions (type I errors) and too many false negative conclusions (type II errors). Methods: We developed a methodology for interpreting meta-analysis results, using generally accepted, valid evidence on how to adjust thresholds for significance in randomised clinical trials when the required sample size has not been reached. Results: The Lan-DeMets trial sequential monitoring boundaries in Trial Sequential Analysis offer adjusted confidence intervals and restricted thresholds for statistical significance when the diversity-adjusted required information size and the corresponding number of required trials for the meta-analysis have not been reached. Trial Sequential Analysis provides a frequentistic approach to control both type I and type II errors. We define the required information size and the corresponding number of required trials in a meta-analysis and the diversity (D2) measure of heterogeneity. We explain the reasons for using Trial Sequential Analysis of meta-analysis when the actual information size fails to reach the required information size. We present examples drawn from traditional meta-analyses using unadjusted na\"ive 95\% confidence intervals and 5\% thresholds for statistical significance. Spurious conclusions in systematic reviews with traditional meta-analyses can be reduced using Trial Sequential Analysis. Several empirical studies have demonstrated that the Trial Sequential Analysis provides better control of type I errors and of type II errors than the traditional na\"ive meta-analysis. Conclusions: Trial Sequential Analysis represents analysis of meta-analytic data, with transparent assumptions, and better control of type I and type II errors than the traditional meta-analysis using na\"ive unadjusted confidence intervals.},
  file = {/Users/rritaz/Zotero/storage/47Y2HP9Z/Wetterslev et al. - 2017 - Trial Sequential Analysis in systematic reviews wi.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {random-effects},
  language = {en},
  number = {1}
}

@article{white_allowing_2008,
  title = {Allowing for Uncertainty Due to Missing Data in Meta-Analysis\textemdash{{Part}} 1: {{Two}}-Stage Methods},
  shorttitle = {Allowing for Uncertainty Due to Missing Data in Meta-Analysis\textemdash{{Part}} 1},
  author = {White, Ian R. and Higgins, Julian P. T. and Wood, Angela M.},
  year = {2008},
  volume = {27},
  pages = {711--727},
  issn = {1097-0258},
  doi = {10.1002/sim.3008},
  abstract = {Analysis of a randomized trial with missing outcome data involves untestable assumptions, such as the missing at random (MAR) assumption. Estimated treatment effects are potentially biased if these assumptions are wrong. We quantify the degree of departure from the MAR assumption by the informative missingness odds ratio (IMOR). We incorporate prior beliefs about the IMOR in a Bayesian pattern-mixture model and derive a point estimate and standard error that take account of the uncertainty about the IMOR. In meta-analysis, this model should be used for four separate sensitivity analyses which explore the impact of IMORs that either agree or contrast across trial arms on pooled results via their effects on point estimates or on standard errors. We also propose a variance inflation factor that can be used to assess the influence of trials with many missing outcomes on the meta-analysis. We illustrate the methods using a meta-analysis on psychiatric interventions in deliberate self-harm. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SRNF9L6D/White et al. - 2008 - Allowing for uncertainty due to missing data in me.pdf;/Users/rritaz/Zotero/storage/T95MJZBA/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,missing data,random-effects},
  language = {en},
  number = {5}
}

@article{white_allowing_2008-1,
  title = {Allowing for Uncertainty Due to Missing Data in Meta-Analysis\textemdash{{Part}} 2: {{Hierarchical}} Models},
  shorttitle = {Allowing for Uncertainty Due to Missing Data in Meta-Analysis\textemdash{{Part}} 2},
  author = {White, Ian R. and Welton, Nicky J. and Wood, Angela M. and Ades, A. E. and Higgins, Julian P. T.},
  year = {2008},
  volume = {27},
  pages = {728--745},
  issn = {1097-0258},
  doi = {10.1002/sim.3007},
  abstract = {We propose a hierarchical model for the analysis of data from several randomized trials where some outcomes are missing. The degree of departure from a missing-at-random assumption in each arm of each trial is expressed by an informative missing odds ratio (IMOR). We require a realistic prior for the IMORs, including an assessment of the prior correlation between IMORs in different arms and in different trials. The model is fitted by Monte Carlo Markov Chain techniques. By applying the method in three different data sets, we show that it is possible to appropriately capture the extra uncertainty due to missing data, and we discuss in what circumstances it is possible to learn about the IMOR. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/JKDZPYP6/White et al. - 2008 - Allowing for uncertainty due to missing data in me.pdf;/Users/rritaz/Zotero/storage/8LLW775X/sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,random-effects},
  language = {en},
  number = {5}
}

@article{white_comparison_2019,
  title = {A Comparison of Arm-Based and Contrast-Based Models for Network Meta-Analysis},
  author = {White, Ian R. and Turner, Rebecca M. and Karahalios, Amalia and Salanti, Georgia},
  year = {2019},
  volume = {38},
  pages = {5197--5213},
  issn = {1097-0258},
  doi = {10.1002/sim.8360},
  abstract = {Differences between arm-based (AB) and contrast-based (CB) models for network meta-analysis (NMA) are controversial. We compare the CB model of Lu and Ades (2006), the AB model of Hong et al(2016), and two intermediate models, using hypothetical data and a selected real data set. Differences between models arise primarily from study intercepts being fixed effects in the Lu-Ades model but random effects in the Hong model, and we identify four key difference. (1) If study intercepts are fixed effects then only within-study information is used, but if they are random effects then between-study information is also used and can cause important bias. (2) Models with random study intercepts are suitable for deriving a wider range of estimands, eg, the marginal risk difference, when underlying risk is derived from the NMA data; but underlying risk is usually best derived from external data, and then models with fixed intercepts are equally good. (3) The Hong model allows treatment effects to be related to study intercepts, but the Lu-Ades model does not. (4) The Hong model is valid under a more relaxed missing data assumption, that arms (rather than contrasts) are missing at random, but this does not appear to reduce bias. We also describe an AB model with fixed study intercepts and a CB model with random study intercepts. We conclude that both AB and CB models are suitable for the analysis of NMA data, but using random study intercepts requires a strong rationale such as relating treatment effects to study intercepts.},
  file = {/Users/rritaz/Zotero/storage/3VCL3QW9/White et al. - 2019 - A comparison of arm-based and contrast-based model.pdf;/Users/rritaz/Zotero/storage/VFCFZYRM/sim.html},
  journal = {Statistics in Medicine},
  keywords = {network meta-analysis,random-effects},
  language = {en},
  number = {27}
}

@article{white_consistency_2012,
  title = {Consistency and Inconsistency in Network Meta-Analysis: Model Estimation Using Multivariate Meta-Regression},
  shorttitle = {Consistency and Inconsistency in Network Meta-Analysis},
  author = {White, Ian R. and Barrett, Jessica K. and Jackson, Dan and Higgins, Julian P. T.},
  year = {2012},
  volume = {3},
  pages = {111--125},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1045},
  abstract = {Network meta-analysis (multiple treatments meta-analysis, mixed treatment comparisons) attempts to make the best use of a set of studies comparing more than two treatments. However, it is important to assess whether a body of evidence is consistent or inconsistent. Previous work on models for network meta-analysis that allow for heterogeneity between studies has either been restricted to two-arm trials or followed a Bayesian framework. We propose two new frequentist ways to estimate consistency and inconsistency models by expressing them as multivariate random-effects meta-regressions, which can be implemented in some standard software packages. We illustrate the approach using the mvmeta package in Stata. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/LD6C9S2S/White et al. - 2012 - Consistency and inconsistency in network meta-anal.pdf;/Users/rritaz/Zotero/storage/DMAKJ7K5/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {GLM MA models,multivariate,network meta-analysis,random-effects},
  language = {en},
  number = {2}
}

@article{white_maximum_1982,
  title = {Maximum {{Likelihood Estimation}} of {{Misspecified Models}}},
  author = {White, Halbert},
  year = {1982},
  volume = {50},
  pages = {1--25},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1912526},
  abstract = {This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (OMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification.},
  file = {/Users/rritaz/Zotero/storage/D82HU6KV/White - 1982 - Maximum Likelihood Estimation of Misspecified Mode.pdf},
  journal = {Econometrica},
  number = {1}
}

@article{white_meta-analysis_2019,
  title = {Meta-Analysis of Non-Linear Exposure-Outcome Relationships Using Individual Participant Data: {{A}} Comparison of Two Methods},
  shorttitle = {Meta-Analysis of Non-Linear Exposure-Outcome Relationships Using Individual Participant Data},
  author = {White, Ian R. and Kaptoge, Stephen and Royston, Patrick and Sauerbrei, Willi},
  year = {2019},
  volume = {38},
  pages = {326--338},
  issn = {1097-0258},
  doi = {10.1002/sim.7974},
  abstract = {Non-linear exposure-outcome relationships such as between body mass index (BMI) and mortality are common. They are best explored as continuous functions using individual participant data from multiple studies. We explore two two-stage methods for meta-analysis of such relationships, where the confounder-adjusted relationship is first estimated in a non-linear regression model in each study, then combined across studies. The ``metacurve'' approach combines the estimated curves using multiple meta-analyses of the relative effect between a given exposure level and a reference level. The ``mvmeta'' approach combines the estimated model parameters in a single multivariate meta-analysis. Both methods allow the exposure-outcome relationship to differ across studies. Using theoretical arguments, we show that the methods differ most when covariate distributions differ across studies; using simulated data, we show that mvmeta gains precision but metacurve is more robust to model mis-specification. We then compare the two methods using data from the Emerging Risk Factors Collaboration on BMI, coronary heart disease events, and all-cause mortality ({$>$}80 cohorts, {$>$}18 000 events). For each outcome, we model BMI using fractional polynomials of degree 2 in each study, with adjustment for confounders. For metacurve, the powers defining the fractional polynomials may be study-specific or common across studies. For coronary heart disease, metacurve with common powers and mvmeta correctly identify a small increase in risk in the lowest levels of BMI, but metacurve with study-specific powers does not. For all-cause mortality, all methods identify a steep U-shape. The metacurve and mvmeta methods perform well in combining complex exposure-disease relationships across studies.},
  file = {/Users/rritaz/Zotero/storage/W95MNZFY/White et al. - 2019 - Meta-analysis of non-linear exposure-outcome relat.pdf;/Users/rritaz/Zotero/storage/C8DU2X87/sim.html},
  journal = {Statistics in Medicine},
  keywords = {Individual Patient Data IPD,multivariate,random-effects},
  language = {en},
  number = {3}
}

@article{whitehead_meta-analysis_2001,
  title = {Meta-Analysis of Ordinal Outcomes Using Individual Patient Data},
  author = {Whitehead, Anne and Omar, Rumana Z. and Higgins, Julian P. T. and Savaluny, Elly and Turner, Rebecca M. and Thompson, Simon G.},
  year = {2001},
  volume = {20},
  pages = {2243--2260},
  issn = {1097-0258},
  doi = {10.1002/sim.919},
  abstract = {Meta-analyses are being undertaken in an increasing diversity of diseases and conditions, some of which involve outcomes measured on an ordered categorical scale. We consider methodology for undertaking a meta-analysis on individual patient data for an ordinal response. The approach is based on the proportional odds model, in which the treatment effect is represented by the log-odds ratio. A general framework is proposed for fixed and random effect models. Tests of the validity of the various assumptions made in the meta-analysis models, such as a global test of the assumption of proportional odds between treatments, are presented. The combination of studies with different definitions or numbers of response categories is discussed. The methods are illustrated on two data sets, in a classical framework using SAS and MLn and in a Bayesian framework using BUGS. The relative merits of the three software packages for such meta-analyses are discussed. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/M88QAHU8/Whitehead et al. - 2001 - Meta-analysis of ordinal outcomes using individual.pdf;/Users/rritaz/Zotero/storage/FCTXZMYQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,effect size combination (small sample \& discrete),GLM MA models,Individual Patient Data IPD},
  language = {en},
  number = {15}
}

@article{whiting_graphical_2008,
  title = {Graphical Presentation of Diagnostic Information},
  author = {Whiting, Penny F and Sterne, Jonathan AC and Westwood, Marie E and Bachmann, Lucas M and Harbord, Roger and Egger, Matthias and Deeks, Jonathan J},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {20},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-20},
  abstract = {Background: Graphical displays of results allow researchers to summarise and communicate the key findings of their study. Diagnostic information should be presented in an easily interpretable way, which conveys both test characteristics (diagnostic accuracy) and the potential for use in clinical practice (predictive value). Methods: We discuss the types of graphical display commonly encountered in primary diagnostic accuracy studies and systematic reviews of such studies, and systematically review the use of graphical displays in recent diagnostic primary studies and systematic reviews. Results: We identified 57 primary studies and 49 systematic reviews. Fifty-six percent of primary studies and 53\% of systematic reviews used graphical displays to present results. Dot-plot or boxand- whisker plots were the most commonly used graph in primary studies and were included in 22 (39\%) studies. ROC plots were the most common type of plot included in systematic reviews and were included in 22 (45\%) reviews. One primary study and five systematic reviews included a probability-modifying plot. Conclusion: Graphical displays are currently underused in primary diagnostic accuracy studies and systematic reviews of such studies. Diagnostic accuracy studies need to include multiple types of graphic in order to provide both a detailed overview of the results (diagnostic accuracy) and to communicate information that can be used to inform clinical practice (predictive value). Work is required to improve graphical displays, to better communicate the utility of a test in clinical practice and the implications of test results for individual patients.},
  file = {/Users/rritaz/Zotero/storage/3UJHR28Z/Whiting et al. - 2008 - Graphical presentation of diagnostic information.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {diagnostic techniques,meta-meta-analyses},
  language = {en},
  number = {1}
}

@article{wiksten_hartungknapp_2016,
  title = {Hartung\textendash{{Knapp}} Method Is Not Always Conservative Compared with Fixed-Effect Meta-Analysis},
  author = {Wiksten, Anna and R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2016},
  volume = {35},
  pages = {2503--2515},
  issn = {1097-0258},
  doi = {10.1002/sim.6879},
  abstract = {A widely used method in classic random-effects meta-analysis is the DerSimonian\textendash Laird method. An alternative meta-analytical approach is the Hartung\textendash Knapp method. This article reports results of an empirical comparison and a simulation study of these two methods and presents corresponding analytical results. For the empirical evaluation, we took 157 meta-analyses with binary outcomes, analysed each one using both methods and performed a comparison of the results based on treatment estimates, standard errors and associated P-values. In several simulation scenarios, we systematically evaluated coverage probabilities and confidence interval lengths. Generally, results are more conservative with the Hartung\textendash Knapp method, giving wider confidence intervals and larger P-values for the overall treatment effect. However, in some meta-analyses with very homogeneous individual treatment results, the Hartung\textendash Knapp method yields narrower confidence intervals and smaller P-values than the classic random-effects method, which in this situation, actually reduces to a fixed-effect meta-analysis. Therefore, it is recommended to conduct a sensitivity analysis based on the fixed-effect model instead of solely relying on the result of the Hartung\textendash Knapp random-effects meta-analysis. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/2PCN99CY/Wiksten et al. - 2016 - Hartung–Knapp method is not always conservative co.pdf;/Users/rritaz/Zotero/storage/AG9MXE9E/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,random effects models,random-effects},
  language = {en},
  number = {15}
}

@article{williamson_aggregate_2002,
  title = {Aggregate Data Meta-Analysis with Time-to-Event Outcomes},
  author = {Williamson, Paula R. and Smith, Catrin Tudur and Hutton, Jane L. and Marson, Anthony G.},
  year = {2002},
  volume = {21},
  pages = {3337--3351},
  issn = {1097-0258},
  doi = {10.1002/sim.1303},
  abstract = {In a meta-analysis of randomized controlled trials with time-to-event outcomes, an aggregate data approach may be required for some or all included studies. Variation in the reporting of survival analyses in journals suggests that no single method for extracting the log(hazard ratio) estimate will suffice. Methods are described which improve upon a previously proposed method for estimating the log(HR) from survival curves. These methods extend to life-tables. In the situation where the treatment effect varies over time and the trials in the meta-analysis have different lengths of follow-up, heterogeneity may be evident. In order to assess whether the hazard ratio changes with time, several tests are proposed and compared. A cohort study comparing life expectancy of males and females with cerebral palsy and a systematic review of five trials comparing two anti-epileptic drugs, carbamazepine and sodium valproate, are used for illustration. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/398AQLNA/Williamson et al. - 2002 - Aggregate data meta-analysis with time-to-event ou.pdf;/Users/rritaz/Zotero/storage/73V863GP/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,effect size estimation (series),random-effects},
  language = {en},
  number = {22}
}

@article{williamson_identification_2005,
  title = {Identification and Impact of Outcome Selection Bias in Meta-Analysis},
  author = {Williamson, P. R. and Gamble, C.},
  year = {2005},
  volume = {24},
  pages = {1547--1561},
  issn = {1097-0258},
  doi = {10.1002/sim.2025},
  abstract = {The systematic review community has become increasingly aware of the importance of addressing the issues of heterogeneity and publication bias in meta-analyses. A potentially bigger threat to the validity of a meta-analysis appears relatively unnoticed. The within-study selective reporting of outcomes, defined as the selection of a subset of the original variables recorded for inclusion in publication of trials, can theoretically have a substantial impact on the results. A cohort of meta-analyses on the Cochrane Library was reviewed to examine how often this form of within-study publication bias was suspected and explained some of the evident funnel plot asymmetry. In cases where the level of suspicion was high, sensitivity analysis was undertaken to assess the robustness of the conclusion to this bias. Although within-study selection was evident or suspected in several trials, the impact on the conclusions of the meta-analyses was minimal. This paper deals with the identification of, sensitivity analysis for, and impact of within-study selective reporting in meta-analysis. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GK8G6Y8M/Williamson and Gamble - 2005 - Identification and impact of outcome selection bia.pdf;/Users/rritaz/Zotero/storage/KGLZWZGH/sim.html},
  journal = {Statistics in Medicine},
  keywords = {multivariate,publication bias},
  language = {en},
  number = {10}
}

@article{williamson_meta-analysis_2002,
  title = {Meta-Analysis of Method Comparison Studies},
  author = {Williamson, P. R. and Lancaster, G. A. and Craig, J. V. and Smyth, R. L.},
  year = {2002},
  volume = {21},
  pages = {2013--2025},
  issn = {1097-0258},
  doi = {10.1002/sim.1158},
  abstract = {Methods for the meta-analysis of results from randomized controlled trials are well established. However, there are currently no methods for the meta-analysis of method comparison studies. Here the combination of results from studies comparing two methods of measurement on the same unit of observation is required. We compare standard methods for the pooling of k samples from the same Normal population to those for pooling parameter estimates, in order to estimate the pooled mean difference and 95 per cent limits of agreement. Methods for investigating heterogeneity across studies and for calculating random effects estimates are proposed. We postulate that for published studies either the estimated mean or variance of the difference between measurements will tend to be smaller than for unpublished studies and investigate the evidence for the existence of such publication bias. The methods are illustrated with an example evaluating the accuracy of temperature measured at the axilla compared to the rectum in children. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/4AE4VPUI/Williamson et al. - 2002 - Meta-analysis of method comparison studies.pdf;/Users/rritaz/Zotero/storage/PNRKMSSZ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {random-effects},
  language = {en},
  number = {14}
}

@article{willis_measuring_2017,
  title = {Measuring the Statistical Validity of Summary Meta-Analysis and Meta-Regression Results for Use in Clinical Practice},
  author = {Willis, Brian H. and Riley, Richard D.},
  year = {2017},
  volume = {36},
  pages = {3283--3301},
  issn = {1097-0258},
  doi = {10.1002/sim.7372},
  abstract = {An important question for clinicians appraising a meta-analysis is: are the findings likely to be valid in their own practice\textemdash does the reported effect accurately represent the effect that would occur in their own clinical population? To this end we advance the concept of statistical validity\textemdash where the parameter being estimated equals the corresponding parameter for a new independent study. Using a simple (`leave-one-out') cross-validation technique, we demonstrate how we may test meta-analysis estimates for statistical validity using a new validation statistic, Vn, and derive its distribution. We compare this with the usual approach of investigating heterogeneity in meta-analyses and demonstrate the link between statistical validity and homogeneity. Using a simulation study, the properties of Vn and the Q statistic are compared for univariate random effects meta-analysis and a tailored meta-regression model, where information from the setting (included as model covariates) is used to calibrate the summary estimate to the setting of application. Their properties are found to be similar when there are 50 studies or more, but for fewer studies Vn has greater power but a higher type 1 error rate than Q. The power and type 1 error rate of Vn are also shown to depend on the within-study variance, between-study variance, study sample size, and the number of studies in the meta-analysis. Finally, we apply Vn to two published meta-analyses and conclude that it usefully augments standard methods when deciding upon the likely validity of summary meta-analysis estimates in clinical practice. \textcopyright{} 2017 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  file = {/Users/rritaz/Zotero/storage/YVT66QNU/Willis and Riley - 2017 - Measuring the statistical validity of summary meta.pdf;/Users/rritaz/Zotero/storage/BU62CV7X/sim.html},
  journal = {Statistics in Medicine},
  keywords = {clinical practice,power,random-effects},
  language = {en},
  number = {21}
}

@article{wilson_role_2001,
  title = {The Role of Method in Treatment Effectiveness Research: {{Evidence}} from Meta-Analysis.},
  shorttitle = {The Role of Method in Treatment Effectiveness Research},
  author = {Wilson, David B. and Lipsey, Mark W.},
  year = {2001},
  volume = {6},
  pages = {413--429},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.6.4.413},
  file = {/Users/rritaz/Zotero/storage/LEC4IZF4/Wilson and Lipsey - 2001 - The role of method in treatment effectiveness rese.pdf},
  journal = {Psychological Methods},
  keywords = {categorical MA models,meta-meta-analyses,modeling effect size variation (covariates)},
  language = {en},
  number = {4}
}

@article{won_choosing_2009,
  title = {Choosing an Optimal Method to Combine {{P}}-Values},
  author = {Won, Sungho and Morris, Nathan and Lu, Qing and Elston, Robert C.},
  year = {2009},
  volume = {28},
  pages = {1537--1553},
  issn = {1097-0258},
  doi = {10.1002/sim.3569},
  abstract = {Fisher (1925) was the first to suggest a method of combining the p-values obtained from several statistics and many other methods have been proposed since then. However, there is no agreement about what is the best method. Motivated by a situation that now often arises in genetic epidemiology, we consider the problem when it is possible to define a simple alternative hypothesis of interest for which the expected effect size of each test statistic is known and we determine the most powerful test for this simple alternative hypothesis. Based on the proposed method, we show that information about the effect sizes can be used to obtain the best weights for Liptak's method of combining p-values. We present extensive simulation results comparing methods of combining p-values and illustrate for a real example in genetic epidemiology how information about effect sizes can be deduced. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/SRG95297/Won et al. - 2009 - Choosing an optimal method to combine P-values.pdf;/Users/rritaz/Zotero/storage/5NJYKKWR/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,power},
  language = {en},
  number = {11}
}

@article{wu_synthesizing_2013,
  title = {Synthesizing Regression Results: A Factored Likelihood Method},
  shorttitle = {Synthesizing Regression Results},
  author = {Wu, Meng-Jia and Becker, Betsy Jane},
  year = {2013},
  volume = {4},
  pages = {127--143},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1063},
  abstract = {Regression methods are widely used by researchers in many fields, yet methods for synthesizing regression results are scarce. This study proposes using a factored likelihood method, originally developed to handle missing data, to appropriately synthesize regression models involving different predictors. This method uses the correlations reported in the regression studies to calculate synthesized standardized slopes. It uses available correlations to estimate missing ones through a series of regressions, allowing us to synthesize correlations among variables as if each included study contained all the same variables. Great accuracy and stability of this method under fixed-effects models were found through Monte Carlo simulation. An example was provided to demonstrate the steps for calculating the synthesized slopes through sweep operators. By rearranging the predictors in the included regression models or omitting a relatively small number of correlations from those models, we can easily apply the factored likelihood method to many situations involving synthesis of linear models. Limitations and other possible methods for synthesizing more complicated models are discussed. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/GJ94KQ9S/Wu and Becker - 2013 - Synthesizing regression results a factored likeli.pdf;/Users/rritaz/Zotero/storage/NSYRJPBU/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,correlation coefficients},
  language = {en},
  number = {2}
}

@article{wynants_random-effects_2018,
  title = {Random-Effects Meta-Analysis of the Clinical Utility of Tests and Prediction Models},
  author = {Wynants, L. and Riley, R. D. and Timmerman, D. and Calster, B. Van},
  year = {2018},
  volume = {37},
  pages = {2034--2052},
  issn = {1097-0258},
  doi = {10.1002/sim.7653},
  abstract = {The use of data from multiple studies or centers for the validation of a clinical test or a multivariable prediction model allows researchers to investigate the test's/model's performance in multiple settings and populations. Recently, meta-analytic techniques have been proposed to summarize discrimination and calibration across study populations. Here, we rather consider performance in terms of net benefit, which is a measure of clinical utility that weighs the benefits of true positive classifications against the harms of false positives. We posit that it is important to examine clinical utility across multiple settings of interest. This requires a suitable meta-analysis method, and we propose a Bayesian trivariate random-effects meta-analysis of sensitivity, specificity, and prevalence. Across a range of chosen harm-to-benefit ratios, this provides a summary measure of net benefit, a prediction interval, and an estimate of the probability that the test/model is clinically useful in a new setting. In addition, the prediction interval and probability of usefulness can be calculated conditional on the known prevalence in a new setting. The proposed methods are illustrated by 2 case studies: one on the meta-analysis of published studies on ear thermometry to diagnose fever in children and one on the validation of a multivariable clinical risk prediction model for the diagnosis of ovarian cancer in a multicenter dataset. Crucially, in both case studies the clinical utility of the test/model was heterogeneous across settings, limiting its usefulness in practice. This emphasizes that heterogeneity in clinical utility should be assessed before a test/model is routinely implemented.},
  file = {/Users/rritaz/Zotero/storage/QV3I9HWB/Wynants et al. - 2018 - Random-effects meta-analysis of the clinical utili.pdf;/Users/rritaz/Zotero/storage/9ASK3J9V/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,multivariate,random-effects},
  language = {en},
  number = {12}
}

@article{yamaguchi_meta-analysis_2014,
  title = {Meta-Analysis of a Continuous Outcome Combining Individual Patient Data and Aggregate Data: A Method Based on Simulated Individual Patient Data},
  shorttitle = {Meta-Analysis of a Continuous Outcome Combining Individual Patient Data and Aggregate Data},
  author = {Yamaguchi, Yusuke and Sakamoto, Wataru and Goto, Masashi and Staessen, Jan A. and Wang, Jiguang and Gueyffier, Francois and Riley, Richard D.},
  year = {2014},
  volume = {5},
  pages = {322--351},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1119},
  abstract = {When some trials provide individual patient data (IPD) and the others provide only aggregate data (AD), meta-analysis methods for combining IPD and AD are required. We propose a method that reconstructs the missing IPD for AD trials by a Bayesian sampling procedure and then applies an IPD meta-analysis model to the mixture of simulated IPD and collected IPD. The method is applicable when a treatment effect can be assumed fixed across trials. We focus on situations of a single continuous outcome and covariate and aim to estimate treatment\textendash covariate interactions separated into within-trial and across-trial effect. An illustration with hypertension data which has similar mean covariates across trials indicates that the method substantially reduces mean square error of the pooled within-trial interaction estimate in comparison with existing approaches. A simulation study supposing there exists one IPD trial and nine AD trials suggests that the method has suitable type I error rate and approximately zero bias as long as the available IPD contains at least 10\% of total patients, where the average gain in mean square error is up to about 40\%. However, the method is currently restricted by the fixed effect assumption, and extension to random effects to allow heterogeneity is required. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/ZTLCWY8S/Yamaguchi et al. - 2014 - Meta-analysis of a continuous outcome combining in.pdf;/Users/rritaz/Zotero/storage/HQSNZN9S/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {combined significance,Individual Patient Data IPD,missing data},
  language = {en},
  number = {4}
}

@article{yamaguchi_random_2017,
  title = {A Random Effects Meta-Analysis Model with {{Box}}-{{Cox}} Transformation},
  author = {Yamaguchi, Yusuke and Maruo, Kazushi and Partlett, Christopher and Riley, Richard D.},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {109},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0376-7},
  abstract = {Background: In a random effects meta-analysis model, true treatment effects for each study are routinely assumed to follow a normal distribution. However, normality is a restrictive assumption and the misspecification of the random effects distribution may result in a misleading estimate of overall mean for the treatment effect, an inappropriate quantification of heterogeneity across studies and a wrongly symmetric prediction interval. Methods: We focus on problems caused by an inappropriate normality assumption of the random effects distribution, and propose a novel random effects meta-analysis model where a Box-Cox transformation is applied to the observed treatment effect estimates. The proposed model aims to normalise an overall distribution of observed treatment effect estimates, which is sum of the within-study sampling distributions and the random effects distribution. When sampling distributions are approximately normal, non-normality in the overall distribution will be mainly due to the random effects distribution, especially when the between-study variation is large relative to the within-study variation. The Box-Cox transformation addresses this flexibly according to the observed departure from normality. We use a Bayesian approach for estimating parameters in the proposed model, and suggest summarising the meta-analysis results by an overall median, an interquartile range and a prediction interval. The model can be applied for any kind of variables once the treatment effect estimate is defined from the variable. Results: A simulation study suggested that when the overall distribution of treatment effect estimates are skewed, the overall mean and conventional I2 from the normal random effects model could be inappropriate summaries, and the proposed model helped reduce this issue. We illustrated the proposed model using two examples, which revealed some important differences on summary results, heterogeneity measures and prediction intervals from the normal random effects model. Conclusions: The random effects meta-analysis with the Box-Cox transformation may be an important tool for examining robustness of traditional meta-analysis results against skewness on the observed treatment effect estimates. Further critical evaluation of the method is needed.},
  file = {/Users/rritaz/Zotero/storage/DYYCPZJ2/Yamaguchi et al. - 2017 - A random effects meta-analysis model with Box-Cox .pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {random-effects},
  language = {en},
  number = {1}
}

@article{yoneoka_clinical_2019,
  title = {Clinical Heterogeneity in Random-Effect Meta-Analysis: {{Between}}-Study Boundary Estimate Problem},
  shorttitle = {Clinical Heterogeneity in Random-Effect Meta-Analysis},
  author = {Yoneoka, Daisuke and Henmi, Masayuki},
  year = {2019},
  volume = {38},
  pages = {4131--4145},
  issn = {1097-0258},
  doi = {10.1002/sim.8289},
  abstract = {Random-effect meta-analysis is commonly applied to estimate overall effects with unexplained heterogeneity across studies. However, standard methods, including (restricted) maximum likelihood (ML or REML), frequently produce (near) zero estimates for between-study variance parameters. Consequently, these methods are reduced to simple and unrealistic fixed-effect models, resulting in an ignorance of the substantial clinical heterogeneity and sometimes leading to incorrect conclusions. To solve the boundary estimate problem, we propose (1) an adjusted maximum likelihood method for the between-study variance that maximizes a likelihood defined as a product of a standard likelihood and a Gaussian class of adjustment factor and (2) a framework using sensitivity analysis by developing a new criterion to check for the occurrence of the boundary estimate. Although the adjustment introduces bias to the overall effects to ensure strictly positive estimates of the between-study variance when the number of studies K is small, the bias asymptotically approaches zero, resulting in the same estimates derived from the REML method. Moreover, the adjusted maximum likelihood estimator of the between-study variance is consistent for large K, and interestingly, the REML method and our method are equivalent in terms of mean squared error criterion, up to O(K-1). We illustrate our approach with a motivating example to examine the controversial result of a meta-analysis for 24 randomized controlled trials of human albumin. Numerical evaluations show that our approach produces no boundary estimates but similar synthesized results with the standard maximum likelihood methods as those produced by conventional methods, especially with a small number of studies.},
  file = {/Users/rritaz/Zotero/storage/LDINKNAR/Yoneoka and Henmi - 2019 - Clinical heterogeneity in random-effect meta-analy.pdf;/Users/rritaz/Zotero/storage/PYC3HB8A/sim.html},
  journal = {Statistics in Medicine},
  keywords = {heterogeneity estimators},
  language = {en},
  number = {21}
}

@article{yoneoka_meta-analytical_2017,
  title = {Meta-Analytical Synthesis of Regression Coefficients under Different Categorization Scheme of Continuous Covariates},
  author = {Yoneoka, Daisuke and Henmi, Masayuki},
  year = {2017},
  volume = {36},
  pages = {4336--4352},
  issn = {1097-0258},
  doi = {10.1002/sim.7434},
  abstract = {Recently, the number of clinical prediction models sharing the same regression task has increased in the medical literature. However, evidence synthesis methodologies that use the results of these regression models have not been sufficiently studied, particularly in meta-analysis settings where only regression coefficients are available. One of the difficulties lies in the differences between the categorization schemes of continuous covariates across different studies. In general, categorization methods using cutoff values are study specific across available models, even if they focus on the same covariates of interest. Differences in the categorization of covariates could lead to serious bias in the estimated regression coefficients and thus in subsequent syntheses. To tackle this issue, we developed synthesis methods for linear regression models with different categorization schemes of covariates. A 2-step approach to aggregate the regression coefficient estimates is proposed. The first step is to estimate the joint distribution of covariates by introducing a latent sampling distribution, which uses one set of individual participant data to estimate the marginal distribution of covariates with categorization. The second step is to use a nonlinear mixed-effects model with correction terms for the bias due to categorization to estimate the overall regression coefficients. Especially in terms of precision, numerical simulations show that our approach outperforms conventional methods, which only use studies with common covariates or ignore the differences between categorization schemes. The method developed in this study is also applied to a series of WHO epidemiologic studies on white blood cell counts.},
  file = {/Users/rritaz/Zotero/storage/ALGZASI4/Yoneoka and Henmi - 2017 - Meta-analytical synthesis of regression coefficien.pdf;/Users/rritaz/Zotero/storage/MHS34TRL/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,modeling effect size variation (covariates)},
  language = {en},
  number = {27}
}

@article{yoneoka_synthesis_2015,
  title = {Synthesis of Clinical Prediction Models under Different Sets of Covariates with One Individual Patient Data},
  author = {Yoneoka, Daisuke and Henmi, Masayuki and Sawada, Norie and Inoue, Manami},
  year = {2015},
  month = dec,
  volume = {15},
  pages = {101},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0087-x},
  abstract = {Background: Recently, increased development of clinical prediction models has been reported in the medical literature. However, evidence synthesis methodologies for these prediction models have not been sufficiently studied, especially for practical situations such as a meta-analyses where only aggregated summaries of important predictors are available. Also, in general, the covariate sets involved in the prediction models are not common across studies. As in ordinary model misspecification problems, dropping relevant covariates would raise potentially serious biases to the prediction models, and consequently to the synthesized results. Methods: We developed synthesizing methods for logistic clinical prediction models with possibly different sets of covariates. In order to aggregate the regression coefficient estimates from different prediction models, we adopted a generalized least squares approach with non-linear terms (a sort of generalization of multivariate meta-analysis). Firstly, we evaluated omitted variable biases in this approach. Then, under an assumption of homogeneity of studies, we developed bias-corrected estimating procedures for regression coefficients of the synthesized prediction models. Results: Numerical evaluations with simulations showed that our approach resulted in smaller biases and more precise estimates compared with conventional methods, which use only studies with common covariates or which utilize a mean imputation method for omitted coefficients. These methods were also applied to a series of Japanese epidemiologic studies on the incidence of a stroke. Conclusions: Our proposed methods adequately correct the biases due to different sets of covariates between studies, and would provide precise estimates compared with the conventional approach. If the assumption of homogeneity within studies is plausible, this methodology would be useful for incorporating prior published information into the construction of new prediction models.},
  file = {/Users/rritaz/Zotero/storage/6VQNASVM/Yoneoka et al. - 2015 - Synthesis of clinical prediction models under diff.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {correlation coefficients,GLM MA models,Individual Patient Data IPD,multivariate},
  language = {en},
  number = {1}
}

@article{yoneoka_synthesis_2017,
  title = {Synthesis of Linear Regression Coefficients by Recovering the Within-Study Covariance Matrix from Summary Statistics},
  author = {Yoneoka, Daisuke and Henmi, Masayuki},
  year = {2017},
  volume = {8},
  pages = {212--219},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1228},
  abstract = {Recently, the number of regression models has dramatically increased in several academic fields. However, within the context of meta-analysis, synthesis methods for such models have not been developed in a commensurate trend. One of the difficulties hindering the development is the disparity in sets of covariates among literature models. If the sets of covariates differ across models, interpretation of coefficients will differ, thereby making it difficult to synthesize them. Moreover, previous synthesis methods for regression models, such as multivariate meta-analysis, often have problems because covariance matrix of coefficients (i.e. within-study correlations) or individual patient data are not necessarily available. This study, therefore, proposes a brief explanation regarding a method to synthesize linear regression models under different covariate sets by using a generalized least squares method involving bias correction terms. Especially, we also propose an approach to recover (at most) threecorrelations of covariates, which is required for the calculation of the bias term without individual patient data. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/NEUEL7UH/Yoneoka and Henmi - 2017 - Synthesis of linear regression coefficients by rec.pdf;/Users/rritaz/Zotero/storage/YFUNFE8R/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {GLM MA models},
  language = {en},
  number = {2}
}

@article{young-xu_pooling_2008,
  title = {Pooling Overdispersed Binomial Data to Estimate Event Rate},
  author = {{Young-Xu}, Yinong and Chan, K Arnold},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {58},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-58},
  abstract = {Background: The beta-binomial model is one of the methods that can be used to validly combine event rates from overdispersed binomial data. Our objective is to provide a full description of this method and to update and broaden its applications in clinical and public health research. Methods: We describe the statistical theories behind the beta-binomial model and the associated estimation methods. We supply information about statistical software that can provide betabinomial estimations. Using a published example, we illustrate the application of the beta-binomial model when pooling overdispersed binomial data. Results: In an example regarding the safety of oral antifungal treatments, we had 41 treatment arms with event rates varying from 0\% to 13.89\%. Using the beta-binomial model, we obtained a summary event rate of 3.44\% with a standard error of 0.59\%. The parameters of the beta-binomial model took the values of 1.24 for alpha and 34.73 for beta. Conclusion: The beta-binomial model can provide a robust estimate for the summary event rate by pooling overdispersed binomial data from different studies. The explanation of the method and the demonstration of its applications should help researchers incorporate the beta-binomial method as they aggregate probabilities of events from heterogeneous studies.},
  file = {/Users/rritaz/Zotero/storage/RXYNXWR5/Young-Xu and Chan - 2008 - Pooling overdispersed binomial data to estimate ev.pdf},
  journal = {BMC Medical Research Methodology},
  keywords = {discrete effect sizes},
  language = {en},
  number = {1}
}

@article{yu_parametric_2019,
  title = {A Parametric Meta-Analysis},
  author = {Yu, Chang and Zelterman, Daniel},
  year = {2019},
  volume = {38},
  pages = {4013--4025},
  issn = {1097-0258},
  doi = {10.1002/sim.8278},
  abstract = {In a meta-analysis, we assemble a sample of independent, nonidentically distributed p-values. The Fisher's combination procedure provides a chi-squared test of whether the p-values were sampled from the null uniform distribution. After rejecting the null uniform hypothesis, we are faced with the problem of how to combine the assembled p-values. We first derive a distribution for the p-values. The distribution is parameterized by the standardized mean difference (SMD) and the sample size. It includes the uniform as a special case. The maximum likelihood estimate (MLE) of the SMD can then be obtained from the independent, nonidentically distributed p-values. The MLE can be interpreted as a weighted average of the study-specific estimate of the effect size with a shrinkage. The method is broadly applicable to p-values obtained in the maximum likelihood framework. Simulation studies show that our method can effectively estimate the effect size with as few as 6 p-values in the meta-analyses. We also present a Bayes estimator for SMD and a method to account for publication bias. We demonstrate our methods on several meta-analyses that assess the potential benefits of citicoline for patients with memory disorders or patients recovering from ischemic stroke.},
  file = {/Users/rritaz/Zotero/storage/4EVBPKAS/Yu and Zelterman - 2019 - A parametric meta-analysis.pdf;/Users/rritaz/Zotero/storage/4ELVSKJ9/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance},
  language = {en},
  number = {21}
}

@article{yuan_meta-analytical_2018,
  title = {Meta-{{Analytical SEM}}: {{Equivalence Between Maximum Likelihood}} and {{Generalized Least Squares}}},
  shorttitle = {Meta-{{Analytical SEM}}},
  author = {Yuan, Ke-Hai and Kano, Yutaka},
  year = {2018},
  month = dec,
  volume = {43},
  pages = {693--720},
  issn = {1076-9986},
  doi = {10.3102/1076998618787799},
  abstract = {Meta-analysis plays a key role in combining studies to obtain more reliable results. In social, behavioral, and health sciences, measurement units are typically not well defined. More meaningful results can be obtained by standardizing the variables and via the analysis of the correlation matrix. Structural equation modeling (SEM) with the combined correlations, called meta-analytical SEM (MASEM), is a powerful tool for examining the relationship among latent constructs as well as those between the latent constructs and the manifest variables. Three classes of methods have been proposed for MASEM: (1) generalized least squares (GLS) in combining correlations and in estimating the structural model, (2) normal-distribution-based maximum likelihood (ML) in combining the correlations and then GLS in estimating the structural model (ML-GLS), and (3) ML in combining correlations and in estimating the structural model (ML). The current article shows that these three methods are equivalent. In particular, (a) the GLS method for combining correlation matrices in meta-analysis is asymptotically equivalent to ML, (b) the three methods (GLS, ML-GLS, ML) for MASEM with correlation matrices are asymptotically equivalent, (c) they also perform equally well empirically, and (d) the GLS method for SEM with the sample correlation matrix in a single study is asymptotically equivalent to ML, which has being discussed extensively in the SEM literature regarding whether the analysis of a correlation matrix yields consistent standard errors and asymptotically valid test statistics. The results and analysis suggest that a sample-size weighted GLS method is preferred for combining correlations and for MASEM.},
  file = {/Users/rritaz/Zotero/storage/J8PT93WJ/Yuan and Kano - 2018 - Meta-Analytical SEM Equivalence Between Maximum L.pdf},
  journal = {Journal of Educational and Behavioral Statistics},
  keywords = {correlation coefficients},
  number = {6}
}

@article{zabor_comparison_2017,
  title = {A Comparison of Statistical Methods for the Study of Etiologic Heterogeneity},
  author = {Zabor, Emily C. and Begg, Colin B.},
  year = {2017},
  volume = {36},
  pages = {4050--4060},
  issn = {1097-0258},
  doi = {10.1002/sim.7405},
  abstract = {Cancer epidemiologic research has traditionally been guided by the premise that certain diseases share an underlying etiology, or cause. However, with the rise of molecular and genomic profiling, attention has increasingly focused on identifying subtypes of disease. As subtypes are identified, it is natural to ask the question of whether they share a common etiology or in fact arise from distinct sets of risk factors. In this context, epidemiologic questions of interest include (1) whether a risk factor of interest has the same effect across all subtypes of disease and (2) whether risk factor effects differ across levels of each individual tumor marker of which the subtypes are comprised. A number of statistical models have been proposed to address these questions. In an effort to determine the similarities and differences among the proposed methods, and to identify any advantages or disadvantages, we use a simplified data example to elucidate the interpretation of model parameters and available hypothesis tests, and we perform a simulation study to assess bias in effect size, type I error, and power. The results show that when the number of tumor markers is small enough that the cross-classification of markers can be evaluated in the traditional polytomous logistic regression framework, then the statistical properties are at least as good as the more complex modeling approaches that have been proposed. The potential advantage of more complex methods is in the ability to accommodate multiple tumor markers in a model of reduced parametric dimension.},
  file = {/Users/rritaz/Zotero/storage/HM5MX9CJ/Zabor and Begg - 2017 - A comparison of statistical methods for the study .pdf;/Users/rritaz/Zotero/storage/3XDY5QPQ/sim.html},
  journal = {Statistics in Medicine},
  keywords = {modeling effect size variation (covariates)},
  language = {en},
  number = {25}
}

@article{zapf_nonparametric_2015,
  title = {Nonparametric Meta-Analysis for Diagnostic Accuracy Studies},
  author = {Zapf, Antonia and Hoyer, Annika and Kramer, Katharina and Kuss, Oliver},
  year = {2015},
  volume = {34},
  pages = {3831--3841},
  issn = {1097-0258},
  doi = {10.1002/sim.6583},
  abstract = {Summarizing the information of many studies using a meta-analysis becomes more and more important, also in the field of diagnostic studies. The special challenge in meta-analysis of diagnostic accuracy studies is that in general sensitivity and specificity are co-primary endpoints. Across the studies both endpoints are correlated, and this correlation has to be considered in the analysis. The standard approach for such a meta-analysis is the bivariate logistic random effects model. An alternative approach is to use marginal beta-binomial distributions for the true positives and the true negatives, linked by copula distributions. In this article, we propose a new, nonparametric approach of analysis, which has greater flexibility with respect to the correlation structure, and always converges. In a simulation study, it becomes apparent that the empirical coverage of all three approaches is in general below the nominal level. Regarding bias, empirical coverage, and mean squared error the nonparametric model is often superior to the standard model, and comparable with the copula model. The three approaches are also applied to two example meta-analyses. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/WP458BMF/Zapf et al. - 2015 - Nonparametric meta-analysis for diagnostic accurac.pdf;/Users/rritaz/Zotero/storage/VTG4JA2V/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects},
  language = {en},
  number = {29}
}

@article{zhang_detecting_2015,
  title = {Detecting Outlying Trials in Network Meta-Analysis},
  author = {Zhang, Jing and Fu, Haoda and Carlin, Bradley P.},
  year = {2015},
  volume = {34},
  pages = {2695--2707},
  issn = {1097-0258},
  doi = {10.1002/sim.6509},
  abstract = {Network meta-analysis (NMA) expands the scope of a conventional pairwise meta-analysis to simultaneously handle multiple treatment comparisons. However, some trials may appear to deviate markedly from the others and thus be inappropriate to be synthesized in the NMA. In addition, the inclusion of these trials in evidence synthesis may lead to bias in estimation. We call such trials trial-level outliers. To the best of our knowledge, while heterogeneity and inconsistency in NMA have been extensively discussed and well addressed, few previous papers have considered the proper detection and handling of trial-level outliers. In this paper, we propose several Bayesian outlier detection measures, which are then applied to a diabetes data set. Simulation studies comparing our approaches in both arm-based and contrast-based model settings are provided in two supporting appendices. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/IDAEG8F6/Zhang et al. - 2015 - Detecting outlying trials in network meta-analysis.pdf;/Users/rritaz/Zotero/storage/U2YB6XRK/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,diagnostic techniques,network meta-analysis},
  language = {en},
  number = {19}
}

@article{zhao_diagnostics_2017,
  title = {Diagnostics for Generalized Linear Hierarchical Models in Network Meta-Analysis},
  author = {Zhao, Hong and Hodges, James S. and Carlin, Bradley P.},
  year = {2017},
  volume = {8},
  pages = {333--342},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1246},
  abstract = {Network meta-analysis (NMA) combines direct and indirect evidence comparing more than 2 treatments. Inconsistency arises when these 2 information sources differ. Previous work focuses on inconsistency detection, but little has been done on how to proceed after identifying inconsistency. The key issue is whether inconsistency changes an NMA's substantive conclusions. In this paper, we examine such discrepancies from a diagnostic point of view. Our methods seek to detect influential and outlying observations in NMA at a trial-by-arm level. These observations may have a large effect on the parameter estimates in NMA, or they may deviate markedly from other observations. We develop formal diagnostics for a Bayesian hierarchical model to check the effect of deleting any observation. Diagnostics are specified for generalized linear hierarchical NMA models and investigated for both published and simulated datasets. Results from our example dataset using either contrast- or arm-based models and from the simulated datasets indicate that the sources of inconsistency in NMA tend not to be influential, though results from the example dataset suggest that they are likely to be outliers. This mimics a familiar result from linear model theory, in which outliers with low leverage are not influential. Future extensions include incorporating baseline covariates and individual-level patient data.},
  file = {/Users/rritaz/Zotero/storage/F6S8SHHG/Zhao et al. - 2017 - Diagnostics for generalized linear hierarchical mo.pdf;/Users/rritaz/Zotero/storage/GMQ6GGCH/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {diagnostic techniques,GLM MA models,network meta-analysis},
  language = {en},
  number = {3}
}

@article{zhao_hierarchical_2016,
  title = {Hierarchical {{Bayesian}} Approaches for Detecting Inconsistency in Network Meta-Analysis},
  author = {Zhao, Hong and Hodges, James S. and Ma, Haijun and Jiang, Qi and Carlin, Bradley P.},
  year = {2016},
  volume = {35},
  pages = {3524--3536},
  issn = {1097-0258},
  doi = {10.1002/sim.6938},
  abstract = {Network meta-analysis (NMA), also known as multiple treatment comparisons, is commonly used to incorporate direct and indirect evidence comparing treatments. With recent advances in methods and software, Bayesian approaches to NMA have become quite popular and allow models of previously unanticipated complexity. However, when direct and indirect evidence differ in an NMA, the model is said to suffer from inconsistency. Current inconsistency detection in NMA is usually based on contrast-based (CB) models; however, this approach has certain limitations. In this work, we propose an arm-based random effects model, where we detect discrepancy of direct and indirect evidence for comparing two treatments using the fixed effects in the model while flagging extreme trials using the random effects. We define discrepancy factors to characterize evidence of inconsistency for particular treatment comparisons, which is novel in NMA research. Our approaches permit users to address issues previously tackled via CB models. We compare sources of inconsistency identified by our approach and existing loop-based CB methods using real and simulated datasets and demonstrate that our methods can offer powerful inconsistency detection. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/EDTE8WCH/Zhao et al. - 2016 - Hierarchical Bayesian approaches for detecting inc.pdf;/Users/rritaz/Zotero/storage/67583BLV/sim.html},
  journal = {Statistics in Medicine},
  keywords = {bayesian,network meta-analysis,random-effects},
  language = {en},
  number = {20}
}

@article{zhou_statistics_2014,
  title = {Statistics for Quantifying Heterogeneity in Univariate and Bivariate Meta-Analyses of Binary Data: {{The}} Case of Meta-Analyses of Diagnostic Accuracy},
  shorttitle = {Statistics for Quantifying Heterogeneity in Univariate and Bivariate Meta-Analyses of Binary Data},
  author = {Zhou, Yan and Dendukuri, Nandini},
  year = {2014},
  volume = {33},
  pages = {2701--2717},
  issn = {1097-0258},
  doi = {10.1002/sim.6115},
  abstract = {AbstractHeterogeneity in diagnostic meta-analyses is common because of the observational nature of diagnostic studies and the lack of standardization in the positivity criterion (cut-off value) for some tests. So far the unexplained heterogeneity across studies has been quantified by either using the I2 statistic for a single parameter (i.e. either the sensitivity or the specificity) or visually examining the data in a receiver-operating characteristic space. In this paper, we derive improved I2 statistics measuring heterogeneity for dichotomous outcomes, with a focus on diagnostic tests. We show that the currently used estimate of the `typical' within-study variance proposed by Higgins and Thompson is not able to properly account for the variability of the within-study variance across studies for dichotomous variables. Therefore, when the between-study variance is large, the `typical' within-study variance underestimates the expected within-study variance, and the corresponding I2 is overestimated. We propose to use the expected value of the within-study variation in the construction of I2 in cases of univariate and bivariate diagnostic meta-analyses. For bivariate diagnostic meta-analyses, we derive a bivariate version of I2 that is able to account for the correlation between sensitivity and specificity. We illustrate the performance of these new estimators using simulated data as well as two real data sets. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/5H48QRE9/Zhou and Dendukuri - 2014 - Statistics for quantifying heterogeneity in univar.pdf;/Users/rritaz/Zotero/storage/33AV8U6B/sim.html},
  journal = {Statistics in Medicine},
  keywords = {correlated effects,multivariate,random-effects},
  language = {en},
  number = {16}
}

@article{zhou_synthesis_2009,
  title = {{Synthesis analysis of regression models with a continuous outcome}},
  author = {Zhou, Xiao-Hua and Hu, Nan and Hu, Guizhou and Root, Martin},
  year = {2009},
  volume = {28},
  pages = {1620--1635},
  issn = {1097-0258},
  doi = {10.1002/sim.3563},
  abstract = {To estimate the multivariate regression model from multiple individual studies, it would be challenging to obtain results if the input from individual studies only provide univariate or incomplete multivariate regression information. Samsa et al. (J. Biomed. Biotechnol. 2005; 2:113\textendash 123) proposed a simple method to combine coefficients from univariate linear regression models into a multivariate linear regression model, a method known as synthesis analysis. However, the validity of this method relies on the normality assumption of the data, and it does not provide variance estimates. In this paper we propose a new synthesis method that improves on the existing synthesis method by eliminating the normality assumption, reducing bias, and allowing for the variance estimation of the estimated parameters. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/C52HLIZ9/Zhou et al. - 2009 - Synthesis analysis of regression models with a con.pdf;/Users/rritaz/Zotero/storage/KHVH82J5/sim.html},
  journal = {Statistics in Medicine},
  language = {fr},
  number = {11}
}

@article{zohar_approach_2011,
  title = {An Approach to Meta-Analysis of Dose-Finding Studies},
  author = {Zohar, Sarah and Katsahian, Sandrine and O'Quigley, John},
  year = {2011},
  volume = {30},
  pages = {2109--2116},
  issn = {1097-0258},
  doi = {10.1002/sim.4121},
  abstract = {The main goal of a Phase I dose-finding study is the estimation of the maximal tolerated dose (MTD) from a set of available dose levels. For cytotoxic clinical trials in oncology, it is not unusual to find several phases I studies carried out on a new molecule or procedure. For instance, the molecule Sorafenib (which inhibits particular tyrosine kinase enzymes in several cancers) was used alone in five published clinical trials. Each clinical trial was conducted separately in different indications and the resulting data were never pooled in any way. No attempt was made to synthesize or combine the information from the different studies. For dose-finding studies, the toxicity itself may not be related to disease. Integrating information across several Phase I trials may lead to improved inference on the dose level, or levels, corresponding to the MTD. Under strong assumptions, we will provide more accurate estimates of the MTD. Under no assumptions a pooled analysis will perform no less well than several separate analyzes and, under intermediary assumptions, there still may be scope for gains. A difficulty is that many methods are sequential in nature so that, in order to group findings under a single heading, it is necessary to retrospectively analyze data obtained according to a dynamic sequential design. We propose a solution to this difficulty. The approach is illustrated via two real examples. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  file = {/Users/rritaz/Zotero/storage/82L36SZF/Zohar et al. - 2011 - An approach to meta-analysis of dose-finding studi.pdf;/Users/rritaz/Zotero/storage/S8RY4XRX/sim.html},
  journal = {Statistics in Medicine},
  keywords = {combined significance,discrete effect sizes},
  language = {en},
  number = {17}
}


