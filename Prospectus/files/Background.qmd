\chapter{Background}

\section{Introduction}

Statistical inference as it exists today originally grew out of the desire to acquire knowledge regarding a population of interest. Since perfect knowledge of a population is impossible to obtain in all but the simplest of circumstances, it is necessary to infer properties of the population based on an incomplete sample of data drawn from it.^[Though the sampling process itself is far from trivial, the methods under discussion in this paper are only concerned with what takes place after the sample has been taken. Consequently, we will generally take it for granted that a suitably random sample of the population already exists.] 

Suppose we have already obtained such a sample of observations $x_1, ..., x_n$ from a certain population. What can we then say about it based on these observations? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? As the questions increase in complexity, so do the techniques required to answer them. Unfortunately the natural chaos of the real world all but guarantees there will never be an instrument capable of perfectly capturing the intricacies of a population whose properties we wish to infer. Hence, some amount of idealization will always be required in order to proceed. 

This idealization typically comes in the form of additional assumptions that we impose on the population of interest with the goal of sacrificing what we hope is only a small amount of accuracy in exchange for a large reduction in complexity. These assumptions are essentially never "true" in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations that are capable of producing answers with degrees of accuracy high enough to be useful in their own right. Taken as a whole, they form the basis for what is known a *statistical model*. 

The traditional framework for a statistical model begins by assuming that there exists an unknown probability distribution $P$ over the population of interest that generates the data we observe from it. We choose to model this observed data as the realized outcomes of some random variable $X$ that is distributed according to $P$. For the purposes of this paper, we may assume that $X$, along with any other random variable under discussion, is absolutely continuous with respect to some $\sigma$-finite measure (typically either Lebesgue measure or the counting measure) and therefore admits a probability density function $p(x)$. The model itself is made up of the set of density functions $\mathcal{P}$ that we are willing to consider as candidates for this true density function. Out of necessity, we will proceed as though our choice of $\mathcal{P}$ always contains this density though in reality there is nothing specifically requiring it.

We will also assume the set $\mathcal{P}$ is *parameterized*. That is, we assume there exists a *parameter* $\theta$ which indexes $\mathcal{P}$, acting as a label that allows us to differentiate between the densities it contains. For a particular value of the parameter $\theta$, say $\theta_1$, we can refer to its corresponding density in $\mathcal{P}$ with the notation $p(\cdot; \theta)$, and therefore $\mathcal{P}$ itself may be written as $\mathcal{P} = \{p(\cdot; \theta) | \> \theta \in \Theta\}$. $\Theta$ is called the *parameter space* and represents the set of all possible values $\theta$ can take on. 

We can think of $\theta$ as acting like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Making inferences regarding $\theta$ is like trying to figure out the particular value to which a population's dial has been set. 

In general, a model's parameterization is not unique, and for a given parameter $\theta$, we are free to choose any one-to-one function of $\theta$ as a new parameter. Once we have made our choice of parameterization, we will assume that the parameter space $\Theta$ does contain a singular true parameter value, which we will denote by $\theta_0$. Note the difference in interpretation between $\theta_0$ and $\theta$. We think of $\theta_0$ as a fixed but unknown constant that represents the value of the parameter corresponding to the true density function in $\mathcal{P}$. Conversely, $\theta$ represents an arbitrary parameter value that is allowed to range over all possible elements of $\Theta$, including $\theta_0$.^[Note that $\theta_0$ may change over time depending on the population. In such cases, any estimate of $\theta_0$ based on a cross-sectional sample drawn from the population is best thought of as an estimate of the true parameter value during the particular time in which the sample was collected.]

Crucially, it must always be possible to identify the parameter in our model on the basis of the data we observe. A model is considered *identifiable* if having perfect knowledge of the population would enable us to determine $\theta_0$ with absolute certainty. This is equivalent to requiring that for some observed data $x$ and any two parameters $\theta_1, \theta_2 \in \Theta$, if $p(x; \theta_1) = p(x; \theta_2)$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. For example, suppose $Y$ is distributed uniformly on the interval $(0, \alpha + \beta)$, where $\alpha, \beta > 0$. If we use $\theta = (\alpha, \beta)$ as a parameter for the distribution of $Y$, then $\theta$ is unidentifiable since, for instance, the case where $\theta_1 = (0, 1)$ and $\theta_1 = (1, 0)$ implies that $p(y; \theta_1) = p(y; \theta_2)$ despite the fact that $\theta_1 \neq \theta_2$. This is obviously an undesirable property for a model to possess, and so we will restrict our attention solely to identifiable models in this paper as a means of avoiding it.

Finally, we must make a choice regarding the dimension of the parameter space $\Theta$ when formulating our models. *Parametric* models are defined as having finite-dimensional parameter spaces. Any model that is not parametric is called *nonparametric*. For the purposes of this paper, we will restrict our attention solely to parametric models whose parameter spaces are subsets of $\mathbb{R}^d$, where $d \in \mathbb{Z}^+$.

\section{The Likelihood Function}

Once we have chosen a model, our goal then becomes to identify the true density function in $\mathcal{P}$ or, at the very least, the one that best approximates the truth. Since we have assumed our model is parametric and identifiable, this is equivalent to making inferences about the value of $\theta_0$ itself. Classically, these inferences come in the form of point estimates, interval estimates, or hypothesis tests though other techniques exist as well. A sensible choice to use as an estimate for the value of $\theta_0$ is one which causes the data actually observed to have the highest possible *post-hoc* probability of occurrence out of all possible values in $\Theta$. To formalize this notion, we need some way of analyzing the joint probability of our sample as a function of our parameter $\theta$. 

Given some observed data $X = x$, the *likelihood function* for $\theta$ is defined as $$L(\theta) \equiv L(\theta; x) = p(x; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_1} That is, the likelihood function for $\theta$ is simply equal to $p(x; \theta)$ itself. However, while $p(x; \theta)$ is viewed as a function of $x$ for fixed $\theta$, the reverse is actually true for $L(\theta; x)$; we view it as a function of $\theta$ for fixed $x$. The positioning of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\theta; x)$ as the probability that $X = x$ given that $\theta$ is the true parameter value. Crucially, this is *not* equivalent to the inverse probability that $\theta$ is the true parameter value given $X = x$. The likelihood does not directly tell us anything about the probability that $\theta$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\theta$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous direct interpretation of $L(\theta)$ as a probability since $p(x; \theta)$ no longer represents $\mathbb{P}(X = x | \theta)$. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny neighborhood of $x$. Specifically, for two different samples $x_1$ and $x_2$, if $L(\theta; x_1) = c \cdot L(\theta; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\theta$ is the true value of the parameter. 

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\theta; x)$ to make probability statements about $\theta$. Despite our use of one in its definition, the likelihood is *not* itself a probability density function for the parameter $\theta$ and need not obey the same laws as one. 

\subsection{Transformations}

There are a few useful transformations of the likelihood function that we will define here for use in future chapters. The first is the *log-likelihood function*, which is defined as the natural logarithm of the likelihood function: $$\ell(\theta) \equiv \ell(\theta; \mathbf{x}) = \log L(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-loglike} In practice, we will typically eschew direct analysis of the likelihood in favor of the log-likelihood due to the nice mathematical properties logarithms possess. Chief among these properties is the ability to turn products into sums (i.e. $\log(ab) = \log(a) + \log(b)$ for $a,b > 0$). Sums tend to be easier to differentiate than products, making this is a particularly useful feature for likelihood functions, which are often expressed as the product of marginal density functions when the observations are independent. 

The other key property of logarithms that makes the log-likelihood so useful is that they are strictly increasing functions of their arguments (i.e. $\log x > \log y$ for $x > y > 0$). This monotonicity ensures that the locations of a function's extrema are preserved when the function is passed to the argument of a logarithm. For example, for a positive function $f$ with a global maximum, $\underset{x}{\mathrm{argmax}} f(x) = \underset{x}{\mathrm{argmax}} \log f(x)$. 

We will refer to the derivatives of $\ell(\theta)$ with respect to $\theta$ with the notation $\ell_{\theta}(\theta) = \nabla \ell(\theta)$, $\ell_{\theta \theta}(\theta) = \nabla^2 \ell(\theta)$, et cetera. Since we will consider the general case in which $\theta$ is a multi-dimensional vector, it follows that $\ell_{\theta}(\theta)$ is also a vector, $\ell_{\theta \theta}(\theta)$ is a matrix, $\ell_{\theta \theta \theta}(\theta)$ is a three-dimensional array, and so forth.

The first derivative of the log-likelihood with respect to $\theta$ appears frequently enough in the analysis of likelihood functions that it has earned its own name - the *score function*. Formally, it is defined as 
$$\mathcal{S}(\theta) \equiv \mathcal{S}(\theta; \mathbf{x}) = \ell_{\theta}(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-score} Similarly, the negative second derivative of the log-likelihood function with respect to $\theta$ is called the *observed information*. Formally, it is defined as $$\mathcal{I}(\theta) \equiv \mathcal{I}(\theta; \mathbf{x}) = -\ell_{\theta \theta}(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-obs_information} The name derives from the fact that it measures the curvature of the log-likelihood around its maximum; the sharper the curve, the less uncertainty we have about $\theta$. 

Note that $\ell(\theta; \mathbf{x})$, $\mathcal{S}(\theta; \mathbf{x})$, and $\mathcal{I}(\theta; \mathbf{x})$ are all functions of the data $\mathbf{x}$ and therefore can be interpreted as random variables themselves with respect to $p(\mathbf{x}; \theta)$. Consequently, quantities such as their expectations and variances are well-defined. In particular, the variance of the score function, known as the *expected information* or the *Fisher information*, is another useful transformation that we will consider in more detail in Chapter 2. Formally, it is defined as $$\mathscr{I}(\theta) = \mathbb{V}\big[\mathcal{S}(\theta; \mathbf{x}); \theta\big], \> \> \theta \in \Theta.$${#eq-exp_information}

\subsection{Regularity Conditions}

It will be useful to establish some *regularity conditions* for the models under consideration in this paper. For our purposes, a parametric model is called *regular* if it satisfies the following conditions:

1) All of the densities in $\mathcal{P}$ have common support which does not depend on $\theta$.
2) There exists an open set $\Theta^* \subseteq \Theta$ of which the true parameter value $\theta_0$ is an interior point.
3) All of the densities in $\mathcal{P}$ are continuously differentiable with respect to $\theta$ up to third order for all $\theta^* \in \Theta^*$. 
4) $\mathscr{I}(\theta)$ is positive definite for all $\theta \in \Theta$, and $\Theta$ is a convex set.
5) For each $\theta^* \in \Theta^*$, there exists a neighborhood $N_{\theta^*} \subset \Theta^*$ such that for all $\theta \in N_{\theta^*}$, the components of the third derivative of the log-likelihood, $\ell_{\theta \theta \theta}(\theta; \mathbf{x})$, are all bounded by a random function $M(\mathbf{x})$ with finite expectation. That is, $\underset{\theta \in N_{\theta^*}}{\sup} |\ell_{\theta \theta \theta}(\theta; \mathbf{x})_{ijk}| \leq M(\mathbf{x})$ for all $i, j, k \in \{1, 2, ..., d \}$ with $\mathbb{E}[M(X); \theta_0] < \infty$.

Any model satisfying these conditions will obey the following properties:

1) Derivatives up to the second order with respect to $\theta$ can be passed under the integral sign in $\int dP(\mathbf{x}; \theta)$.
2) Guaranteed existence, uniqueness, and consistency of the maximum likelihood estimate of $\theta_0$, $\hat{\theta}$.
3) Convergence of the score function evaluated at $\theta_0$ and $\hat{\theta}$ to multivariate normal distributions as the sample size increases. More precisely, 
$$\begin{aligned}
&\sqrt n (\hat{\theta} - \theta_0) \overset{d}{\to} \text{MVN}(\mathbf{0}, \mathscr{I}(\theta_0)^{-1}), \\
&\frac{1}{\sqrt n}\mathcal{S}(\theta_0) \overset{d}{\to} \text{MVN}(\mathbf{0}, \mathscr{I}(\theta_0)).
\end{aligned}
$$

\subsection{Maximum Likelihood Estimation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters based on some observed data $\mathbf{x}$. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, say $\theta_1$ and $\theta_2$, the sensible choice is the one that makes the data we did observe more probable to have been observed. We have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\theta_1; \mathbf{x}) > L(\theta_2; \mathbf{x})$, then $\theta_1$ is the better estimate of the true parameter value and vice versa.

This can be extended to include as many candidate parameter values as we would like. For $n$ potential estimates of $\theta_0$, the best is the one that corresponds to the highest value of the likelihood function based on the observed data $x$. Taking this logic to its natural conclusion, the *maximum likelihood estimate* (MLE) of the parameter $\theta$, which we will denote by $\hat{\theta}$ (pronounced "theta hat"), is the one that maximizes the value of the likelihood function among all possible choices of $\theta$ in $\Theta$. Formally, $$\hat{\theta} = \underset{\theta \in \Theta}{\mathrm{argmax}}\, \> L(\theta; \mathbf{x}).$${#eq-mle1}

The existence and uniqueness of $\hat{\theta}$ is guaranteed for any model satisfying the regularity conditions we have assumed and can be found as the solution to the following system of equations $$\mathcal{S}(\theta) = \mathbf{0}$${#eq-mle2} Furthermore, the MLE will be consistent for $\theta_0$ and converge to a normal distribution as the sample size increases.
