\chapter{Background}

\section{Assumptions}
Consider a random sample $\mathbf{x} = (x_1, ..., x_n)$ drawn from a population. What can we say about the population based on $\mathbf{x}$? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? Questions like these were the original motivation behind the field of statistical inference, and many of the techniques devised to answer them are still used by statisticians today.

It is important to remember, however, that the real world is messy and no mathematical function will ever perfectly capture the complexities of a population or random process whose properties we wish to infer. To overcome this difficulty, statisticians sacrifice a small amount of accuracy for (hopefully) a large reduction in complexity by imposing additional assumptions on the population of interest. These assumptions are essentially never true in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations capable of producing sufficiently accurate answers in their own right. As George Box famously put it, "All models are wrong, but some are useful." 

And indeed, in the aggregate these assumptions create what is known as a statistical model. In its most general framework, a statistical model can be formulated as a tuple $(\mathcal{S}, \mathcal{P})$ where $\mathcal{S}$ is the set of all possible observations (i.e. the population), and $\mathcal{P}$ is a set of probability distributions on $\mathcal{S}$. The first and (in this author's opinion) the most fundamental assumption we make when defining our models is that there exists some unknown mechanism in the population that generates the data we observe from $\mathcal{S}$. This mechanism is what induces the "true" probability distribution on $\mathcal{S}$ though $\mathcal{P}$ need not contain this distribution, and in practice it seldom does. 

Another assumption found in almost every model is that the set $\mathcal{P}$ is considered to be *parameterized*. That is, we assume the probability distributions contained in $\mathcal{P}$ are indexed by a *parameter* that controls their features.^[We will consider the phrases "parameter", "population parameter", and "model parameter" all to have the same meaning in this paper and use them interchangeably.] This parameter acts like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Much of statistical inference can be boiled down to figuring out the particular value to which a population's dial has been set. We will denote this assumption as $\mathcal{P} = \{\mathcal{P}_{\theta} | \> \theta \in \Theta\}$, where $\theta$ denotes the parameter, and $\Theta$, the parameter space, represents the set of all possible values $\theta$ can take on.^[Note that $\theta$ can be and in fact usually is a multi-dimensional vector whose components represent various sub-parameters of the population.] 

Statisticians also like to assume the parameters in their models can be uniquely identified based on the data they observe. A model is considered *identifiable* if having perfect knowledge of the population enables us to determine the true value of its parameter with absolute certainty.^[This is of course almost always impossible in practice, but in theory it could be accomplished by obtaining an infinite number of observations from $\mathcal{S}$ or simply all of its observations if $|\mathcal{S}|$ is finite.] More formally, for any two parameters $\theta_1$ and $\theta_2$ in $\Theta$, if $\mathcal{P}_{\theta_1} = \mathcal{P}_{\theta_2}$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. Since we have already assumed $\mathcal{P}$ is the mechanism generating the data we have observed in the first place, this would make it impossible to determine which value is the "correct" one on the basis of the data alone. Statisticians impose the identifiability criterion on their models as a means of avoiding this undesirable situation.

The dimension of the parameter space $\Theta$ is another critical decision statisticians must make when choosing the best model for their research. The most frequent choice is for $\Theta$ to be of finite dimension. That is, $\Theta \subseteq \mathbb{R}^k$, where $k \in \mathbb{Z}^+$. Models that satisfy this assumption are said to be *parametric*. Common examples of parametric models include the normal family of distributions as well as the Poisson family.

In contrast, a *nonparametric model* is one in which $\Theta \subseteq V$, where $V$ is an infinite-dimensional space. The name is a bit of a misnomer in the sense that nonparametric models do not actually lack parameters, but rather they are flexible regarding the exact number and properties of the parameters they do have. \textcolor{red}{[INSERT EXAMPLE(S) OF NONPARAMETRIC MODELS]}

Finally, *semiparametric* models are those whose parameter spaces have components of both finite and infinite dimensionality. That is, $\Theta \subseteq \mathbb{R}^k \times V$, where again $V$ is an infinite-dimensional space. Usually it is only the finite-dimensional component of the parameter in which we are interested while the infinite-dimensional component is considered a nuisance parameter. \textcolor{red}{[INSERT EXAMPLE(S) OF SEMIPARAMETRIC MODELS]}


In summary, the reader may safely assume that all models to which my research applies consist of a pair $(\mathcal{S}, \mathcal{P})$ such that the probability distributions in $\mathcal{P}$ are parameterized and identifiable and there exists a "true" probability distribution inducing the data-generating process from $\mathcal{S}$, though this distribution is not necessarily contained in $\mathcal{P}$. In addition, while estimation of the parameters in nonparametric and semiparametric models is also a major topic of interest in statistical inference, I will restrict my attention in my dissertation solely to parametric models. 

\section{The Likelihood Function}

Once we have chosen a model $(\mathcal{S}, \mathcal{P})$, our goal becomes to identify the "true" distribution in $\mathcal{P}$ or, failing that, the one that best approximates the truth. Since we have assumed our model is parametric and identifiable, this is equivalent to making inferences about the value of the $k$-dimensional parameter $\theta$ indexing the distributions in $\mathcal{P}$ on the basis of some data we observe. Classically, these inferences come in the form of point estimates, interval estimates, or hypothesis tests though other techniques exist as well. A sensible choice to use as an estimate for the value of $\theta$ is one which causes the data actually observed to have the highest possible *post-hoc* probability of occurrence out of all possible values in $\Theta$. To formalize this notion, we need some way of analyzing the joint probability of our sample as a function of our parameter $\theta$. In short, we need the likelihood function.

Let $p(x; \theta)$ denote either the probability density function or the probability mass function for a random variable $X$ with parameter $\theta$, depending on whether $X$ is continuous or discrete, respectively. Suppose we observe that $X$ takes on the value $x$. Then we define the *likelihood function* for $\theta$ as follows: $$L(\theta) \equiv L(\theta; x) = p(x; \theta), \> \> \theta \in \Theta.$$ That is, when our sample consists only of a single observation, the likelihood function for $\theta$ is simply equal to the p.d.f. (or p.m.f.) of X. 

When $X$ is discrete, this means the likelihood function evaluated at $\theta$ may be interpreted as the probability that $X = x$ given that $\theta$ is the true parameter value. We forfeit this interpretation when $X$ is continuous as $p(x; \theta)$ no longer represents the probability of an individual point. However, 


Note the subtle but crucial distinction in our interpretations of the two quantities, however. While $p(x; \theta)$ is a function of $x$ for fixed $\theta$, the reverse is actually true for $L(\theta; x)$; we view it as a function of $\theta$ for fixed $x$.^[The positioning of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.] It is crucial to remember that despite our use of probability in its definition a likelihood does not itself represent a probability. For instance, there is no requirement that $\int_{\Theta} L(\theta; x) d\theta = 1$ as would be expected of a probability density function.

\subsection{Model Parameter Decomposition}

It is often the case that we are not interested in estimating the full parameter $\theta \in \Theta \subseteq \mathbb{R}^k$, but rather a sub-parameter $\psi$ taking values in a set $\Psi \subseteq \mathbb{R}^m$, where $m < k$. In such an event, we refer to $\psi$ as the *parameter of interest*. 

Consider first the case in which $\psi$ is a sub-vector of $\theta$, so that all the components of $\psi$ are also components of $\theta$. Then there exists a set $I = \{I_1, ..., I_m\} \subsetneq \{1, ..., k\}$ such that $\psi = (\psi_1, ..., \psi_m) = (\theta_{I_1}, ..., \theta_{I_m})$. We may further group the components of $\theta$ that are not a part of $\psi$ into their own sub-vector, which we will refer to as the *nuisance parameter* and denote by $\lambda \in \Lambda \subseteq \mathbb{R}^{k-m}$. Specifically, let $N = \{N_1, ..., N_{k-m}\} \subsetneq \{1, ..., k\}$ such that $I \cup N = \{1, ..., k\}$ and $I \cap N = \emptyset$. Then $\lambda = (\lambda_1, ..., \lambda_{m-k}) = (\theta_{N_1}, ..., \theta_{N_{k-m}})$. $\theta$ can therefore be decomposed as $\theta = (\psi, \lambda)$, provided we shuffle the indices appropriately. In this case, $\psi$ and $\lambda$ are referred to as an *explicit* parameter of interest and nuisance parameter, respectively.

Nuisance parameters are so named for their ability to complicate inference regarding the parameter of interest. Despite not being the object of study themselves, they nevertheless are capable of modifying the distributions of our observations and therefore must be accounted for. The process by which this is accomplished is often nontrivial and indeed can constitute a significant barrier that must be overcome. For example, suppose we are interested in estimating the mean of a random variable $Y$, where $Y \sim N(\mu, \sigma^2)$. The full model parameter is $\theta = (\mu, \sigma^2)$ but since we are only interested in estimating the mean, the parameter of interest is $\psi =\mu$ and the nuisance parameter is $\lambda = \sigma^2$. 

Now consider the case in which the parameter of interest is the output of a function $\varphi: \Theta \to \Psi$. That is, $\psi = \varphi(\theta)$.^[Note that the first case is really just a special case of this second one in which $\varphi(\theta) = \varphi(\theta_1, ..., \theta_k) = (\theta_{I_1}, ..., \theta_{I_m})$, where $\{I_1, ..., I_m\}$ is the subset of the indices of the components of $\theta$ that also belong to $\psi$.] Note that $\Psi$ is still assumed to be a subset of $\mathbb{R}^m$ where $m$ is less than $k$, the dimension of the full parameter space $\Theta$. This reduction in dimension implies the existence of a nuisance parameter $\lambda \in \Lambda$, where $\text{dim}(\Lambda) = k - m$. However, there is no guarantee that a closed form expression exists for $\lambda$ in terms of the original components of theta. We will refer to $\psi$ and $\lambda$ as being *implicit* parameters in this case.

\subsection{Pseudolikelihood Functions}

The construction of what is known as a *pseudolikelihood function* has long been the statistician's method of choice for eliminating nuisance parameters from a model. Suppose our parameter of interest is $\psi = \varphi(\theta)$ for some function $\phi$ and full model parameter $\theta \in \Theta$. If we let $\Theta(\psi) = \{\theta \in \Theta \> | \> \varphi(\theta) = \psi \},$ then corresponding to $\psi \in \Psi$ is the set of likelihoods $\mathcal{L}_{\psi} = \{L(\theta) \> | \> \theta \in \Theta(\psi)\}.$ Simply put, a pseudolikelihood function is a summary of the values in $\mathcal{L}_{\psi}$ that does not depend on $\lambda$. There exist a variety of methods to obtain this summary but among the most popular are maximization, conditioning, and integration, each with respect to the nuisance parameter. We will explore each of these methods in more detail in the sections to come.
    
\subsubsection{The Profile Likelihood}

The profile likelihood is the most straightforward method for eliminating a nuisance parameter.  


\subsubsection{The Conditional Likelihood}

\subsubsection{The Marginal Likelihood}

\subsubsection{The Integrated Likelihood}

\subsection{The Appeal of the Integrated Likelihood}

The appeal of the integrated likelihood function as a means of eliminating nuisance parameters from the model is that it incorporates








