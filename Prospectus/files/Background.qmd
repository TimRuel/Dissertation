\chapter{Introduction}

\section{Introduction}

The acquisition of knowledge regarding a population of interest has long been the impetus for the field of statistical inference. In all but the most basic of circumstances, limiting constraints such as time, accessibility, and cost make perfect knowledge of a population essentially impossible to obtain. It therefore becomes necessary to infer properties of the population based on a representative sample of observations drawn from it. The procedures by which these samples might be procured are themselves far from trivial, and indeed an entire branch of statistics has been dedicated to their study. However, for the purposes of this paper we are primarily concerned with what occurs after the sample has been taken, and so we will generally take it for granted that a suitably representative sample of the population already exists. 

Suppose $\mathbf{x} = (x_1, ..., x_n)$ is one such sample. What information can we then glean from $\mathbf{x}$ about the population from which it has been drawn? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? As the questions increase in complexity, so do the techniques required to answer them. Unfortunately the natural chaos of the real world all but guarantees there will never be an instrument capable of completely capturing the intricacies of a population whose properties we wish to infer. Hence, some amount of idealization will always be required in order to proceed. 

This idealization typically comes in the form of additional assumptions that we impose on the population of interest with the goal of sacrificing what we hope is only a small amount of accuracy in exchange for a large reduction in complexity. These assumptions are essentially never "true" in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations that are capable of producing answers with degrees of accuracy high enough to be useful in their own right. Taken as a whole, they form the basis for what is known a *statistical model*. 

The traditional framework for a statistical model begins by assuming that there exists an unknown probability distribution $P$ over the population of interest that generates the data we observe from it. We choose to model this observed data as the realized outcomes of some random variable $X$ that is distributed according to $P$. We will restrict our attention in this paper to distributions that are either discrete or absolutely continuous, meaning they admit a probability density/frequency function over their support. Let $p(x)$ denote the unknown density/frequency function associated with $P$ and $\mathcal{P}$ the set of functions that we are willing to consider as candidates for $p(x)$. Out of necessity, we will proceed as though our choice of $\mathcal{P}$ always contains $p(x)$ though in reality there is nothing specifically requiring it.

We will also assume the set $\mathcal{P}$ is *parameterized*. That is, we assume there exists a *parameter* $\theta$ which indexes $\mathcal{P}$, acting as a label that allows us to differentiate between the densities it contains. For a particular value of the parameter $\theta$, say $\theta_1$, we can refer to its corresponding density in $\mathcal{P}$ with the notation $p(\cdot; \theta)$, and therefore $\mathcal{P}$ itself may be written as $\mathcal{P} = \{p(\cdot; \theta) | \> \theta \in \Theta\}$. $\Theta$ is called the *parameter space* and represents the set of all possible values $\theta$ can take on. 

We can think of $\theta$ as acting like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Making inferences regarding $\theta$ is like trying to figure out the particular value to which a population's dial has been set. 

In general, a model's parameterization is not unique, and for a given parameter $\theta$, we are free to choose any one-to-one function of $\theta$ as a new parameter. Once we have made our choice of parameterization, we will assume that the parameter space $\Theta$ does contain a singular true parameter value, which we will denote by $\theta_0$. Note the difference in interpretation between $\theta_0$ and $\theta$. We think of $\theta_0$ as a fixed but unknown constant that represents the value of the parameter corresponding to the true density function in $\mathcal{P}$. Conversely, $\theta$ represents an arbitrary parameter value that is allowed to range over all possible elements of $\Theta$, including $\theta_0$.^[Note that $\theta_0$ may change over time depending on the population. In such cases, any estimate of $\theta_0$ based on a cross-sectional sample drawn from the population is best thought of as an estimate of the true parameter value during the particular time in which the sample was collected.]

Crucially, it must always be possible to identify the parameter in our model on the basis of the data we observe. A model is considered *identifiable* if having perfect knowledge of the population would enable us to determine $\theta_0$ with absolute certainty. This is equivalent to requiring that for some observed data $x$ and any two parameters $\theta_1, \theta_2 \in \Theta$, if $p(x; \theta_1) = p(x; \theta_2)$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. For example, suppose $Y$ is distributed uniformly on the interval $(0, \alpha + \beta)$, where $\alpha, \beta > 0$. If we use $\theta = (\alpha, \beta)$ as a parameter for the distribution of $Y$, then $\theta$ is unidentifiable since, for instance, the case where $\theta_1 = (0, 1)$ and $\theta_1 = (1, 0)$ implies that $p(y; \theta_1) = p(y; \theta_2)$ despite the fact that $\theta_1 \neq \theta_2$. This is obviously an undesirable property for a model to possess, and so we will restrict our attention solely to identifiable models in this paper as a means of avoiding it.

Finally, we must make a choice regarding the dimension of the parameter space $\Theta$ when formulating our models. *Parametric* models are defined as having finite-dimensional parameter spaces. Any model that is not parametric is called *nonparametric*. We will restrict our attention in this paper solely to parametric models whose parameter spaces are subsets of $\mathbb{R}^d$, where $d \in \mathbb{Z}^+$.

\section{The Likelihood Function}

Once we have chosen a model, our goal then becomes to identify the true density function in $\mathcal{P}$ or, at the very least, the one that best approximates the truth. Since we have assumed our model is parametric and identifiable, this is equivalent to making inferences about the value of $\theta_0$ itself. Classically, these inferences come in the form of point estimates, interval estimates, or hypothesis tests though other techniques exist as well. A sensible choice to use as an estimate for the value of $\theta_0$ is one which causes the data actually observed to have the highest possible *post-hoc* probability of occurrence out of all possible values in $\Theta$. To formalize this notion, we need some way of analyzing the joint probability of our sample as a function of our parameter $\theta$. 

Given some observed data $X = x$, the *likelihood function* for $\theta$ is defined as $$L(\theta) \equiv L(\theta; x) = p(x; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_1} That is, the likelihood function for $\theta$ is simply equal to $p(x; \theta)$ itself. However, while $p(x; \theta)$ is viewed as a function of $x$ for fixed $\theta$, the reverse is actually true for $L(\theta; x)$; we view it as a function of $\theta$ for fixed $x$. The positioning of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\theta; x)$ as the probability that $X = x$ given that $\theta$ is the true parameter value. Crucially, this is *not* equivalent to the inverse probability that $\theta$ is the true parameter value given $X = x$. The likelihood does not directly tell us anything about the probability that $\theta$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\theta$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous direct interpretation of $L(\theta)$ as a probability since $p(x; \theta)$ no longer represents $\mathbb{P}(X = x | \theta)$. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny neighborhood of $x$. Specifically, for two different samples $x_1$ and $x_2$, if $L(\theta; x_1) = c \cdot L(\theta; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\theta$ is the true value of the parameter. 

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\theta; x)$ to make probability statements about $\theta$. Despite our use of one in its definition, the likelihood is *not* itself a probability density function for the parameter $\theta$ and need not obey the same laws as one. 

\subsection{Transformations}

There are a few useful transformations of the likelihood function that we will define here for use in future chapters. The first is the *log-likelihood function*, which is defined as the natural logarithm of the likelihood function: $$\ell(\theta) \equiv \ell(\theta; \mathbf{x}) = \log L(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-loglike} In practice, we will typically eschew direct analysis of the likelihood in favor of the log-likelihood due to the nice mathematical properties logarithms possess. Chief among these properties is the ability to turn products into sums (i.e. $\log(ab) = \log(a) + \log(b)$ for $a,b > 0$). Sums tend to be easier to differentiate than products, making this is a particularly useful feature for likelihood functions, which are often expressed as the product of marginal density functions when the observations are independent. 

The other key property of logarithms that makes the log-likelihood so useful is that they are strictly increasing functions of their arguments (i.e. $\log x > \log y$ for $x > y > 0$). This monotonicity ensures that the locations of a function's extrema are preserved when the function is passed to the argument of a logarithm. For example, for a positive function $f$ with a global maximum, $\underset{x}{\mathrm{argmax}} f(x) = \underset{x}{\mathrm{argmax}} \log f(x)$. 

We will refer to the derivatives of $\ell(\theta)$ with respect to $\theta$ with the notation $\ell_{\theta}(\theta) = \nabla \ell(\theta)$, $\ell_{\theta \theta}(\theta) = \nabla^2 \ell(\theta)$, et cetera. Since we will consider the general case in which $\theta$ is a multi-dimensional vector, it follows that $\ell_{\theta}(\theta)$ is also a vector, $\ell_{\theta \theta}(\theta)$ is a matrix, $\ell_{\theta \theta \theta}(\theta)$ is a three-dimensional array, and so forth. 

The first derivative of the log-likelihood with respect to $\theta$ appears frequently enough in the analysis of likelihood functions that it has earned its own name - the *score function*. Formally, it is defined as 
$$\mathcal{S}(\theta) \equiv \mathcal{S}(\theta; \mathbf{x}) = \ell_{\theta}(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-score} 

Similarly, the negative second derivative of the log-likelihood function with respect to $\theta$ is called the *observed information*. Formally, it is defined as $$\mathcal{I}(\theta) \equiv \mathcal{I}(\theta; \mathbf{x}) = -\ell_{\theta \theta}(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-obs_information} The name derives from the fact that it measures the curvature of the log-likelihood around its maximum; the sharper the curve, the less uncertainty we have about $\theta$. 

Note that $\ell(\theta; \mathbf{x})$, $\mathcal{S}(\theta; \mathbf{x})$, and $\mathcal{I}(\theta; \mathbf{x})$ are all functions of the data $\mathbf{x}$ and therefore can be interpreted as random variables themselves with respect to $p(\mathbf{x}; \theta)$. Consequently, quantities such as their expectations and variances are well-defined. In particular, the variance of the score function, known as the *expected information* or the *Fisher information*, is another useful transformation that we will consider in more detail in Chapter 2. Formally, it is defined as $$\mathscr{I}(\theta) = \mathbb{V}\big[\mathcal{S}(\theta; \mathbf{x}); \theta\big], \> \> \theta \in \Theta.$${#eq-exp_information}

