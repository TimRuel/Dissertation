\chapter{Background}

\section{Assumptions}
Consider a random sample $\mathbf{x} = (x_1, ..., x_n)$ drawn from a population. What can we say about the population based on $\mathbf{x}$? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? Questions like these were the original motivation behind the field of statistical inference, and many of the techniques devised to answer them are still used by statisticians today.

It is important to remember, however, that the real world is messy and no mathematical function will ever perfectly capture the complexities of a population or random process whose properties we wish to infer. To overcome this difficulty, statisticians sacrifice a small amount of accuracy for (hopefully) a large reduction in complexity by imposing additional assumptions on the population of interest. These assumptions are essentially never true in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations capable of producing sufficiently accurate answers in their own right. As George Box famously put it, "All models are wrong, but some are useful." 

\vspace{-0.5cm}

\subsection{Data-Generating Mechanism}

And indeed, in the aggregate these assumptions create what is known as a statistical model. In its most general framework, a statistical model can be formulated as a tuple $(\mathcal{S}, \mathcal{P})$ where $\mathcal{S}$ is the set of all possible observations (i.e. the population), and $\mathcal{P}$ is a set of probability distributions on $\mathcal{S}$. The first and most fundamental assumption we make when defining our models is that there exists some unknown mechanism in the population that generates the data we observe from $\mathcal{S}$. This mechanism is what induces the "true" probability distribution on $\mathcal{S}$ though $\mathcal{P}$ need not contain this distribution, and in practice it seldom does. 

\subsection{Parameter Existence}

Another assumption found in almost every model is that the set $\mathcal{P}$ is considered to be *parameterized*. That is, we assume the probability distributions contained in $\mathcal{P}$ are indexed by a *parameter* that controls their features.^[We will consider the phrases "parameter", "population parameter", and "model parameter" all to have the same meaning in this paper and use them interchangeably.] This parameter acts like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Much of statistical inference can be boiled down to figuring out the particular value to which a population's dial has been set. We will denote this assumption as $\mathcal{P} = \{\mathcal{P}_{\theta} | \> \theta \in \Theta\}$, where $\theta$ denotes the parameter, and $\Theta$, the parameter space, represents the set of all possible values $\theta$ can take on.^[$\theta$ can be and in fact usually is a multi-dimensional vector whose components represent various sub-parameters of the population.] 

Statisticians also like to assume the parameters in their models can be uniquely identified based on the data they observe. A model is considered *identifiable* if having perfect knowledge of the population enables us to determine the true value of its parameter with absolute certainty.^[This is of course almost always impossible in practice, but in theory it could be accomplished by obtaining an infinite number of observations from $\mathcal{S}$ or simply all of its observations if $|\mathcal{S}|$ is finite.] More formally, for any two parameters $\theta_1$ and $\theta_2$ in $\Theta$, if $\mathcal{P}_{\theta_1} = \mathcal{P}_{\theta_2}$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. Since we have already assumed $\mathcal{P}$ is the mechanism generating the data we have observed in the first place, this would make it impossible to determine which value is the "correct" one on the basis of the data alone. Statisticians impose the identifiability criterion on their models as a means of avoiding this undesirable situation.

\subsection{Parameter Space Dimension}

The dimension of the parameter space $\Theta$ is another critical decision statisticians must make when choosing the best model for their research. When $\Theta \subseteq V$, where $V$ is an infinite-dimensional space, the model is said to be a *nonparametric*. The name is a bit of a misnomer in the sense that nonparametric models do not actually lack parameters, but rather they are flexible regarding the exact number and properties of the parameters they do have. 

*Semiparametric* models are those whose parameter spaces have components of both finite and infinite dimensionality. That is, $\Theta \subseteq \mathbb{R}^k \times V$, where again $V$ is an infinite-dimensional space. Usually it is only the finite-dimensional component of the parameter in which we are interested while the infinite-dimensional component is considered a nuisance parameter.

Models for which $\Theta$ is of finite dimension are called *parametric*. That is, $\Theta \subseteq \mathbb{R}^k$, where $k \in \mathbb{Z}^+$. This is the most common type of model used by statisticians, with examples including the normal family of distributions as well as the Poisson family. For the purposes of this paper, we will assume all statistical models under discussion are parametric in form.

\subsection{Regularity}

Finally, we will assume that all probability distributions in $\mathcal{P}$ obey a certain regularity condition. A parametric model $(\mathcal{S}, \mathcal{P})$ is called *regular* if it satisfies one of the following:

1) All of the distributions in $\mathcal{P}$ are continuous with densities $p(x; \theta)$;
2) All of the distributions in $\mathcal{P}$ are discrete with frequency functions $p(x; \theta)$ and the set $\{x: p(x; \theta) > 0\}$ is the same set for all $\theta$.

In other words, a regular model is one in which its distributions all have a common support that does not depend on the unknown parameter $\theta$.

\section{The Likelihood Function}

Once we have chosen a model $(\mathcal{S}, \mathcal{P})$, our goal becomes to identify the "true" distribution in $\mathcal{P}$ or, failing that, the one that best approximates the truth. Since we have assumed our model is parametric and identifiable, this is equivalent to making inferences about the value of the $k$-dimensional parameter $\theta$ indexing the distributions in $\mathcal{P}$ on the basis of some data we observe. Classically, these inferences come in the form of point estimates, interval estimates, or hypothesis tests though other techniques exist as well. A sensible choice to use as an estimate for the value of $\theta$ is one which causes the data actually observed to have the highest possible *post-hoc* probability of occurrence out of all possible values in $\Theta$. To formalize this notion, we need some way of analyzing the joint probability of our sample as a function of our parameter $\theta$. 

\subsection{The Discrete Case}

Suppose X is a discrete random variable with probability mass function $p(x; \theta)$. For a single observation $X = x$, the *likelihood function* for $\theta$ is defined as
$$L(\theta) \equiv L(\theta; x) = p(x; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_single} 
That is, when our sample consists only of a single observation, the likelihood function for $\theta$ is simply equal to $p(x;\theta)$ itself. However, while $p(x; \theta)$ is viewed as a function of $x$ for fixed $\theta$, the reverse is actually true for $L(\theta; x)$; we view it as a function of $\theta$ for fixed $x$. The positioning of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.

In this case, we may interpret $L(\theta)$ as the probability that $X = x$ given that $\theta$ is the true parameter value. Crucially, this is *not* equivalent to the inverse probability that $\theta$ is the true parameter value given $X = x$. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

This definition be extended to include the more common scenario in which the sample consists of multiple observations. For a sample of size $n$ taken from our sample space $\mathcal{S}$, the likelihood is defined as $$L(\theta;x_1, ..., x_n) = p(x_1, ..., x_n; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_multi} That is, it is equal to the joint probability of the observations $x_1, ..., x_n$, considered as a function of $\theta$. When the observations are independent and identically distributed, we can further express the likelihood as $$L(\theta;x_1, ..., x_n) = \prod_{i=1}^n p(x_i; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_multi_prod}

\subsection{The Continuous Case}

When $X$ is instead a continuous random variable, the likelihood for $\theta$ may still be defined as it is in @eq-likelihood_single and @eq-likelihood_multi. However, $p(x; \theta)$ has switched from being a probability *mass* function to a probability *density* function over the support of $X$. We must therefore forfeit our previous direct interpretation of $L(\theta)$ as a probability since $p(x; \theta)$ no longer represents $\mathbb{P}(X = x | \theta)$. We may however still think of the likelihood as being proportional to the probability that $X$ is "close" to the value $x$.^[Here, "close" means that that $X$ is within a tiny neighborhood of $x$.] Specifically, for two different samples $x_1$ and $x_2$, if $L(\theta; x_1) = c \cdot L(\theta; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\theta$ is the true value of the parameter. 

As in the discrete case, we must also be careful here to avoid using $L(\theta)$ to make direct statements of probability about $\theta$. Indeed, despite our use of one in its definition, the likelihood function is *not* itself a probability density over the parameter $\theta$ and need not obey the same laws as one. 

\subsection{Maximum Likelihood Estimation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters based on some observed data $x$. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, say $\theta_1$ and $\theta_2$, the sensible choice is the one that makes the data we did observe more probable to have been observed. Fortunately, we have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\theta_1; x) > L(\theta_2; x)$, then $\theta_1$ is the better estimate of the true parameter value and vice versa.

This can be extended to include as many parameter values as we would like. For $n$ potential estimates of the true parameter, the best is the one that corresponds to the highest value of the likelihood function based on the observed data $x$. Taking this logic to its natural conclusion, the *maximum likelihood estimate* (MLE) of the parameter $\theta$, which we will denote by $\hat{\theta}$ (pronounced "theta hat"), is the one that maximizes the value of the likelihood function among all possible choices of $\theta$ in the parameter space $\Theta$. Formally, $$\hat{\theta} = \underset{\theta \in \Theta}{\mathrm{argmax}}\, \> L(\theta; x).$${#eq-mle}

There is no singular method for finding the maximum likelihood estimate of a parameter. However, when the likelihood function is differentiable, it is often possible to calculate the MLE analytically using the derivative test for locating the local maxima of a function. In such cases, the MLE can be found by finding the value of $\theta$ that makes the derivative of the likelihood function with respect to $\theta$ vanish. A popular technique when finding this value is to take the natural logarithm of the likelihood first. This transformation is common enough that it has its own name - the *log-likelihood function*. Formally, it is defined as $$\ell(\theta) \equiv \ell(\theta; x) = \log L(\theta; x), \> \> \theta \in \Theta.$${#eq-loglike} When working with $\ell(\theta)$ instead of $L(\theta)$, any products in the latter have been transformed into sums in the former, making derivative calculations more tractable while still preserving the argument that corresponds to the global maximum, if it exists, of $L(\theta)$.

\subsection{Model Parameter Decomposition}

It is often the case that we are not interested in estimating the full parameter $\theta \in \Theta \subseteq \mathbb{R}^k$, but rather a different parameter $\psi$ taking values in a set $\Psi \subseteq \mathbb{R}^m$, where $m < k$. In such an event, we refer to $\psi$ as the *parameter of interest*. Since $\psi$ is of lower dimension than $\theta$, it necessarily follows that there is another parameter $\lambda$, taking values in a set $\Lambda\subseteq{\mathbb{R}^{k-m}}$, that is made up of whatever is "left over" from the full parameter $\theta$. 

We refer to $\lambda$ as the *nuisance parameter* due to its ability to complicate inference regarding the parameter of interest. Despite not being the object of study themselves, nuisance parameters are nevertheless capable of modifying the distributions of our observations and therefore must be accounted for when conducting inference regarding the parameter of interest. The process by which this is accomplished is often nontrivial and can constitute a significant barrier that must be overcome. 

Parameters of interest and nuisance parameters can be broadly classified into two categories, explicit or implicit. For a given statistical model, both types of parameter must occupy the same category - it is not possible for $\psi$ to be explicit and $\lambda$ to be implicit, or vice versa. In the following two sections, we will explore each of these cases in more detail.

\subsection{Explicit Parameters}

Let us first consider the case in which $\psi$ and $\lambda$ are *explicit* parameters. This means that $\psi$ is a sub-vector of $\theta$, so that all the components of $\psi$ are also components of $\theta$. Then there exists a set $I = \{I_1, ..., I_m\} \subsetneq \{1, ..., k\}$ such that $$\psi = (\theta_{I_1}, ..., \theta_{I_m}).$${#eq-expl_param1} It immediately follows that $\lambda$ is the sub-vector of all components of $\theta$ that are not part of $\psi$. More precisely, if we let $J = \{J_1, ..., J_{k-m}\} \subsetneq \{1, ..., k\}$ such that $I \cup J = \{1, ..., k\}$ and $I \cap J = \emptyset$, then $$\lambda = (\theta_{J_1}, ..., \theta_{J_{k-m}}).$${#eq-expl_param2} $\theta$ can therefore be decomposed as $\theta = (\psi, \lambda)$ when $\psi$ and $\lambda$ are explicit, provided we shuffle the indices appropriately.

\subsection{Implicit Parameters}

Now let us consider the case in which $\psi$ and $\lambda$ are *implicit* parameters. This means there exists some function $\varphi: \Theta \to \Psi$ for which the parameter of interest can be written as $$\psi = \varphi(\theta).$${#eq-impl_param} As before, $\Psi$ is still assumed to be a subset of $\mathbb{R}^m$ where $m$ is less than $k$, the dimension of the full parameter space $\Theta$. This reduction in dimension again implies the existence of a nuisance parameter $\lambda \in \Lambda \subseteq{\mathbb{R}}^{k-m}$. However, unlike in the explicit case, a closed form expression for $\lambda$ in terms of the original components of $\theta$ need not exist. For this reason, implicit nuisance parameters are in general more difficult to eliminate compared to their explicit counterparts.

Note that when the parameter of interest and nuisance parameter are explicit, it is always possible to define a function $\varphi$ such that $$\varphi(\theta) = (\theta_{I_1}, ..., \theta_{I_m}) \equiv \psi ,$${#eq-expl_param3} where $\{I_1, ..., I_m\}$ is defined as above. Hence, the first case is really just a special example of this broader one in which $\psi = \varphi(\theta)$. To avoid blurring the lines between explicit and implicit parameters, we will refer to $\psi$ and $\lambda$ as being implicit if and only if there does not exist a function $\varphi$ satisfying @eq-expl_param3. When such a function does exist, we will simply write $\theta = (\psi, \lambda)$ and refer to $\psi$ and $\lambda$ as explicit parameters.

\section{Pseudolikelihood Functions}

As mentioned previously, nuisance parameters hinder inference regarding the parameter of interest in a statistical model. The natural solution to this problem is to find a method for eliminating the nuisance parameter from the model altogether. Since one way of uniquely specifying a model is through its likelihood function, this is equivalent to eliminating the nuisance parameters from the likelihood function itself. The result of this elimination is what is known as a pseudolikelihood function.

In general, a *pseudolikelihood function* for $\psi$ is defined as being a function of $\psi$ and the data alone, having properties resembling that of a genuine likelihood function. Suppose $\psi$ and $\lambda$ are implicit, so that $\psi = \varphi(\theta)$ for some function $\phi$ and parameter $\theta \in \Theta$. If we let $\Theta(\psi) = \{\theta \in \Theta \> : \> \varphi(\theta) = \psi \},$ then associated with each $\psi \in \Psi$ is the set of likelihoods $\mathcal{L}_{\psi} = \{L(\theta) \> : \> \theta \in \Theta(\psi)\}.$ When $\psi$ and $\lambda$ are instead explicit, a one-to-one mapping  exists between $\psi$ and $\theta$, so $\Theta(\psi) = \Theta$ and therefore $\mathcal{L}_{\psi}$ simply reduces to $L(\theta)$. 

Any summary of the values in $\mathcal{L}_{\psi}$ that does not depend on $\lambda$ theoretically constitutes a pseudolikehood function for $\psi$. There exist a variety of methods to obtain this summary but among the most popular are maximization, conditioning, and integration, each with respect to the nuisance parameter. We will explore each of these methods in more detail in the sections to come.

\subsection{The Bartlett Identities}

The Bartlett identities are a set of equations relating to the expectations of functions of derivatives of a log-likelihood function. A well-specified genuine likelihood function will automatically satisfy each of the Bartlett identites; however, an arbitrary function of $\theta$ and $X$ will not. For this reason, the identities act as a litmus test of sorts for determining the validity of a pseudolikelihood as an approximation to the genuine likelihood from which it originated.^[The Bartlett identities offer an alternative way of characterizing the difference between likelihood and pseudolikelihood functions. A genuine likelihood function of $\theta$ is any nonnegative random function of $\theta$ for which all of the Bartlett identities hold. A pseudolikelihood of $\theta$ is any nonnegative random function of $\theta$ for which at least one of the Bartlett identities does not hold.]

Consider the case in which a random variable $X$ has a probability density $f$ that depends on a scalar parameter $\theta$. Denote the log-likelihood function for $\theta$ by $\ell(\theta; x) = \log f(x; \theta)$ and its first derivative with respect to $\theta$ by $\ell_{\theta}(\theta;x ) = \frac{\partial}{\partial \theta} \ell(\theta; x)$. We previously assumed in Section 2.1.4. that all probability distributions for which the results in this paper apply are regular. One consequence of this assumption is that derivatives and integrals of the density functions for these distributions may be interchanged. Now, taking the expectation of $\ell_{\theta}(\theta; x)$ gives

$$
\begin{aligned}
\mathbb{E}\big[\ell_{\theta}(\theta; x) \big]  &= \mathbb{E}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] &&(\text{by definition of } \ell_{\theta})\\
                                                 &= \int_{\mathbb{R}} \bigg[\frac{\partial}{\partial \theta} \ell(\theta; x)\bigg] f(x; \theta) dx &&(\text{by definition of expectation})\\
                                                 &= \int_{\mathbb{R}} \bigg[\frac{\partial}{\partial \theta} \log f(x; \theta)\bigg] f(x; \theta) dx &&(\text{by definition of } \ell)\\
                                                 &=  \int_{\mathbb{R}} \frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x; \theta)} f(x; \theta) dx &&(\text{by the chain rule})\\
                                                 &=  \int_{\mathbb{R}} \frac{\partial}{\partial \theta} f(x; \theta) dx &&(\text{by cancellation})\\
                                                 &= \frac{d}{d \theta} \int_{\mathbb{R}} f(x; \theta) dx &&(\text{by regularity of } f)\\
                                                 &= \frac{d}{d \theta} 1 &&(\text{by definition of } f)\\
                                                 &= 0. &&(\text{by simple differentiation})
\end{aligned}
$$
Therefore, $$\mathbb{E}\big[\ell_{\theta}(\theta; x) \big] = 0 \text{ for all } \theta.$${#eq-BI1} @eq-BI1 is called the first Bartlett identity. In words, it states that the expectation of the first derivative of the log-likelihood function of a statistical model with respect to the model parameter will always be 0. Another name for $\ell_{\theta}$ is the *score function*, and any pseudolikelihood that also satisfies the first Bartlett identity is said to be *score-unbiased*.

If we now consider the second derivative of $\ell(\theta; x)$, we have

$$\begin{aligned}
\ell_{\theta \theta}(\theta; x) &= \frac{\partial^2}{\partial \theta^2} \ell(\theta; x) &&(\text{by definition of } \ell_{\theta \theta})\\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] &&(\text{by properties of the derivative})\\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \log f(x;\theta) \bigg] &&(\text{by definition of } \ell_{\theta})\\
                                &= \frac{\partial}{\partial \theta}\Bigg[\frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x; \theta)} \Bigg] &&(\text{by the chain rule})\\
                                &= \frac{\Big[\frac{\partial^2}{\partial \theta^2} f(x; \theta)\Big] f(x;\theta) - \Big[\frac{\partial}{\partial \theta} f(x; \theta)\Big]\Big[\frac{\partial}{\partial \theta} f(x; \theta)\Big]}{\big[f(x; \theta) \big]^2} &&(\text{by the quotient rule})\\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \Bigg[\frac{\frac{\partial}{\partial \theta} f(x; \theta)}{ f(x; \theta)}\Bigg]^2 &&(\text{by simple algebra})\\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \big[\ell_{\theta}(\theta; x) \big]^2. &&(\text{by definition of } \ell_{\theta})
\end{aligned} 
$$
Rearranging terms and taking expectations yields 
$$
\begin{aligned}
\mathbb{E}[\ell_{\theta \theta}(\theta; x)] + \mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2\Big] &= \mathbb{E}\Bigg[\frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)}\Bigg] &&(\text{by above result})\\
             &= \int_{\mathbb{R}} \Bigg[\frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)}\Bigg] f(x; \theta) dx &&(\text{by definition of expectation})\\
             &= \int_{\mathbb{R}} \Bigg[\frac{\partial^2}{\partial \theta^2} f(x; \theta)\Bigg] dx &&(\text{by cancellation})\\
             &= \frac{d^2}{d \theta^2} \int_{\mathbb{R}} f(x; \theta) dx &&(\text{by regularity of } f)\\
             &= \frac{d^2}{d \theta^2} 1 &&(\text{by definition of } f)\\
             &= 0. &&(\text{by simple differentiation})
\end{aligned}
$$
Therefore, $$\mathbb{E}[\ell_{\theta \theta}(\theta; x)] + \mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2\Big] = 0 \text{ for all } \theta.$${#eq-BI2}

@eq-BI2 is called the second Bartlett identity. The second term on the left-hand side can be further rewritten as 

$$
\begin{aligned}
\mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2\Big] &= \mathbb{V}[\ell_{\theta}(\theta; x)] +\Big(\mathbb{E}\big[\ell_{\theta}(\theta; x) \big]\Big)^2 &&(\text{by definition of variance})\\
                                                          &= \mathbb{V}[\ell_{\theta}(\theta; x)]. &&(\text{by the first Bartlett identity})
\end{aligned}
$$
Another name for this quantity is the *expected information*. It follows from the second Bartlett identity that $$\mathbb{E}[-\ell_{\theta \theta}(\theta; x)] = \mathbb{V}[\ell_{\theta}(\theta; x)].$${#eq-BI3} The quantity $-\ell_{\theta \theta}(\theta; x)$ is called the *observed information*. Any pseudolikelihood that satisfies the second Bartlett identity is said to be *information-unbiased*.

It is possible to derive further Bartlett identities by continuing in this manner for an arbitrary number of derivatives of the log-likelihood function, provided that they exist. However, the first two are sufficient for our purposes of evaluating the validity of pseudolikelihoods as approximations to a genuine likelihood so we will not go further. Note that while the above derivations were performed under the assumption that $\theta$ is a scalar, the Bartlett identities also hold in the case where $\theta$ is a multi-dimensional vector.

\subsection{Asymptotic Analysis of Pseudolikelihoods}




\subsection{The Profile Likelihood}

The profile likelihood is the most straightforward method for eliminating a nuisance parameter from a likelihood function.  

For example, suppose we are interested in estimating the mean of a random variable $Y$, where $Y \sim N(\mu, \sigma^2)$. The full model parameter is $\theta = (\mu, \sigma^2)$ but since we are only interested in estimating the mean, the parameter of interest is $\psi =\mu$ and the nuisance parameter is $\lambda = \sigma^2$.  

\subsection{The Conditional Likelihood}

\subsection{The Marginal Likelihood}

\subsection{The Integrated Likelihood}

\section{The Appeal of the Integrated Likelihood Function}

The appeal of the integrated likelihood function as a means of eliminating nuisance parameters from the model is that it incorporates








