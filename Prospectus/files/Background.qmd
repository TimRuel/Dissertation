\chapter{Background}

\section{Introduction}

Statistical inference as it exists today originally grew out of the desire to acquire knowledge regarding a population of interest. Since perfect knowledge of a population is impossible to obtain in all but the simplest of circumstances, it is necessary to infer properties of the population based on an incomplete sample of data drawn from it.^[Though the sampling process itself is far from trivial, the methods under discussion in this paper are only concerned with what takes place after the sample has been taken. Consequently, we will generally take it for granted that a suitably random sample of the population already exists.] 

Suppose we have already obtained such a sample of observations $x_1, ..., x_n$ from a certain population. What can we then say about it based on these observations? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? As the questions increase in complexity, so do the techniques required to answer them. Unfortunately the natural chaos of the real world all but guarantees there will never be an instrument capable of perfectly capturing the intricacies of a population whose properties we wish to infer. Hence, some amount of idealization will always be required in order to proceed. 

This idealization typically comes in the form of additional assumptions that we impose on the population of interest with the goal of sacrificing what we hope is only a small amount of accuracy in exchange for a large reduction in complexity. These assumptions are essentially never "true" in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations that are capable of producing answers with degrees of accuracy high enough to be useful in their own right. Taken as a whole, they form the basis for what is known a *statistical model*. 

The traditional framework for a statistical model begins by assuming that there exists an unknown probability distribution $P$ over the population of interest that generates the data we observe from it. We choose to model this observed data as the realized outcomes of some random variable $X$ that is distributed according to $P$. For the purposes of this paper, we may assume that $X$, along with any other random variable under discussion, is absolutely continuous with respect to some $\sigma$-finite measure (typically either Lebesgue measure or the counting measure) and therefore admits a probability density function $p(x)$. The model itself is made up of the set of density functions $\mathcal{P}$ that we are willing to consider as candidates for this true density function. Out of necessity, we will proceed as though our choice of $\mathcal{P}$ always contains this density though in reality there is nothing specifically requiring it.

We will also assume the set $\mathcal{P}$ is *parameterized*. That is, we assume there exists a *parameter* $\theta$ which indexes $\mathcal{P}$, acting as a label that allows us to differentiate between the densities it contains. For a particular value of the parameter $\theta$, say $\theta_1$, we can refer to its corresponding density in $\mathcal{P}$ with the notation $p(\cdot; \theta)$, and therefore $\mathcal{P}$ itself may be written as $\mathcal{P} = \{p(\cdot; \theta) | \> \theta \in \Theta\}$. $\Theta$ is called the *parameter space* and represents the set of all possible values $\theta$ can take on. 

We can think of $\theta$ as acting like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Making inferences regarding $\theta$ is like trying to figure out the particular value to which a population's dial has been set. 

In general, a model's parameterization is not unique, and for a given parameter $\theta$, we are free to choose any one-to-one function of $\theta$ as a new parameter. Once we have made our choice of parameterization, we will assume that the parameter space $\Theta$ does contain a singular true parameter value, which we will denote by $\theta_0$. Note the difference in interpretation between $\theta_0$ and $\theta$. We think of $\theta_0$ as a fixed but unknown constant that represents the value of the parameter corresponding to the true density function in $\mathcal{P}$. Conversely, $\theta$ represents an arbitrary parameter value that is allowed to range over all possible elements of $\Theta$, including $\theta_0$.^[Note that $\theta_0$ may change over time depending on the population. In such cases, any estimate of $\theta_0$ based on a cross-sectional sample drawn from the population is best thought of as an estimate of the true parameter value during the particular time in which the sample was collected.]

Crucially, it must always be possible to identify the parameter in our model on the basis of the data we observe. A model is considered *identifiable* if having perfect knowledge of the population would enable us to determine $\theta_0$ with absolute certainty. This is equivalent to requiring that for some observed data $x$ and any two parameters $\theta_1, \theta_2 \in \Theta$, if $p(x; \theta_1) = p(x; \theta_2)$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. For example, suppose $Y$ is distributed uniformly on the interval $(0, \alpha + \beta)$, where $\alpha, \beta > 0$. If we use $\theta = (\alpha, \beta)$ as a parameter for the distribution of $Y$, then $\theta$ is unidentifiable since, for instance, the case where $\theta_1 = (0, 1)$ and $\theta_1 = (1, 0)$ implies that $p(y; \theta_1) = p(y; \theta_2)$ despite the fact that $\theta_1 \neq \theta_2$. This is obviously an undesirable property for a model to possess, and so we will restrict our attention solely to identifiable models in this paper as a means of avoiding it.

Finally, we must make a choice regarding the dimension of the parameter space $\Theta$ when formulating our models. *Parametric* models are defined as having finite-dimensional parameter spaces. Any model that is not parametric is called *nonparametric*. For the purposes of this paper, we will restrict our attention solely to parametric models whose parameter spaces are subsets of $\mathbb{R}^d$, where $d \in \mathbb{Z}^+$.

\section{The Likelihood Function}

Once we have chosen a model, our goal then becomes to identify the true density function in $\mathcal{P}$ or, at the very least, the one that best approximates the truth. Since we have assumed our model is parametric and identifiable, this is equivalent to making inferences about the value of $\theta_0$ itself. Classically, these inferences come in the form of point estimates, interval estimates, or hypothesis tests though other techniques exist as well. A sensible choice to use as an estimate for the value of $\theta_0$ is one which causes the data actually observed to have the highest possible *post-hoc* probability of occurrence out of all possible values in $\Theta$. To formalize this notion, we need some way of analyzing the joint probability of our sample as a function of our parameter $\theta$. 

Given some observed data $X = x$, the *likelihood function* for $\theta$ is defined as $$L(\theta) \equiv L(\theta; x) = p(x; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_1} That is, the likelihood function for $\theta$ is simply equal to $p(x; \theta)$ itself. However, while $p(x; \theta)$ is viewed as a function of $x$ for fixed $\theta$, the reverse is actually true for $L(\theta; x)$; we view it as a function of $\theta$ for fixed $x$. The positioning of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\theta; x)$ as the probability that $X = x$ given that $\theta$ is the true parameter value. Crucially, this is *not* equivalent to the inverse probability that $\theta$ is the true parameter value given $X = x$. The likelihood does not directly tell us anything about the probability that $\theta$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\theta$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous direct interpretation of $L(\theta)$ as a probability since $p(x; \theta)$ no longer represents $\mathbb{P}(X = x | \theta)$. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny neighborhood of $x$. Specifically, for two different samples $x_1$ and $x_2$, if $L(\theta; x_1) = c \cdot L(\theta; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\theta$ is the true value of the parameter. 

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\theta; x)$ to make probability statements about $\theta$. Despite our use of one in its definition, the likelihood is *not* itself a probability density function for the parameter $\theta$ and need not obey the same laws as one. 

In practice, we will typically eschew direct analysis of the likelihood in favor of the *log-likelihood function*, which is defined as the natural logarithm of the likelihood: $$\ell(\theta) \equiv \ell(\theta; x) = \log L(\theta; x), \> \> \theta \in \Theta.$$ We will also have need of the derivative of the log-likelihood with respect to $\theta$:
$$u(\theta; x) \equiv \nabla \ell(\theta; x) = \ell_{\theta}(\theta ;x), \> \> \theta \in \Theta.$${#eq-score} $u(\theta; x)$ appears frequently enough in likelihood analysis that it has earned its own name as well - the *score function*. Note that both $\ell(\theta; x)$ and $\u(\theta; x)$ are functions of the data $x$ and therefore can be interpreted as random variables themselves. This will be important in future sections where we analyze

\subsection{Regularity Conditions}

It will be useful to establish some *regularity conditions* for the models under consideration in this paper. The exact specifications of these conditions can vary depending on the requirements of the researcher, but they are generally made with the goal of ensuring that the log-likelihood function obeys various "nice" properties. For our purposes, a parametric model is called *regular* if it satisfies the following conditions:

1) All of the densities in $\mathcal{P}$ have common support which does not depend on $\theta$;
2) There exists an open set $\Theta_0 \subseteq \Theta$ of which $\theta_0$ is an interior point.
3)
4) $\mathbb{E}[-\nabla^2\ell(\theta; x); \theta_0]$ is positive definite for all $\theta \in \Theta$, and $\Theta$ is a convex set.

\subsection{Maximum Likelihood Estimation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters based on some observed data $x$. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, say $\theta_1$ and $\theta_2$, the sensible choice is the one that makes the data we did observe more probable to have been observed. We have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\theta_1; x) > L(\theta_2; x)$, then $\theta_1$ is the better estimate of the true parameter value and vice versa.

This can be extended to include as many candidate parameter values as we would like. For $n$ potential estimates of $\theta_0$, the best is the one that corresponds to the highest value of the likelihood function based on the observed data $x$. Taking this logic to its natural conclusion, the *maximum likelihood estimate* (MLE) of the parameter $\theta$, which we will denote by $\hat{\theta}$ (pronounced "theta hat"), is the one that maximizes the value of the likelihood function among all possible choices of $\theta$ in the parameter space $\Theta$. Formally, $$\hat{\theta} = \underset{\theta \in \Theta}{\mathrm{argmax}}\, \> L(\theta; x).$${#eq-mle}

There is no singular method for finding the maximum likelihood estimate of a parameter. However, when the likelihood function is differentiable, it is often possible to calculate the MLE analytically using the derivative test for locating the local maxima of a function. In such cases, the MLE can be found by finding the value of $\theta$ that makes the derivative of the likelihood function with respect to $\theta$ vanish. A popular technique when finding this value is to take the natural logarithm of the likelihood first. This transformation is common enough that it has its own name - the *log-likelihood function*. Formally, it is defined as $$\ell(\theta) \equiv \ell(\theta; x) = \log L(\theta; x), \> \> \theta \in \Theta.$${#eq-loglike} When working with $\ell(\theta)$ instead of $L(\theta)$, any products in the latter have been transformed into sums in the former, making derivative calculations more tractable while still preserving the argument that corresponds to the global maximum, if it exists, of $L(\theta)$.









