\chapter{Pseudolikelihood Analysis}

\section{Model Parameter Decomposition}

\subsection{Introduction}

It is often the case that we are not interested in estimating the full parameter $\theta \in \Theta \subseteq \mathbb{R}^k$, but rather a different parameter $\psi$ taking values in a set $\Psi \subseteq \mathbb{R}^m$, where $m < k$. In such an event, we refer to $\psi$ as the *parameter of interest*. 

Since $\psi$ is of lower dimension than $\theta$, it necessarily follows that there is another parameter $\lambda$, taking values in a set $\Lambda\subseteq{\mathbb{R}^{k-m}}$, that is made up of whatever is "left over" from the full parameter $\theta$. We refer to $\lambda$ as the *nuisance parameter* due to its ability to complicate inference regarding the parameter of interest. Despite not being the object of study themselves, nuisance parameters are nevertheless capable of modifying the distributions of our observations and therefore must be accounted for when conducting inference or estimation regarding the parameter of interest.^[Note that nuisance parameters are not always uniquely defined. Depending on the choice of parameter of interest, there may be multiple or even infinite ways to define a nuisance parameter.] The process by which this is accomplished is often nontrivial and can constitute a significant barrier that must be overcome. 

While not strictly required, we will assume the parameter of interest $\psi$ is always one-dimensional for the purposes of this paper. That is, $\Psi \subseteq \mathbb{R}$ and consequently $\Lambda \subseteq \mathbb{R}^{k-1}$. This restriction reflects the common habit of researchers to focus on scalar-valued summaries of vector quantities. For example, suppose we observe data $Y = (y_1, ..., y_n)$, where each $y_i$ is the outcome of some random variable $Y_i \sim N(\mu_i, \sigma^2_i)$, and we are interested in estimating the average of the population means, $\frac{1}{n}\sum_{i = 1}^n \mu_i$. Rather than defining $\psi = (\mu_1, ..., \mu_n)$, we can instead define $\psi = \frac{1}{n}\sum_{i = 1}^n \mu_i$ directly, bypassing the need to estimate each $\mu_i$ individually before taking their average. This does carry the trade-off of increasing the dimension of the nuisance parameter, which must be dealt with before conducting inference or estimation on $\psi$. However, as we will discuss in Chapter 3, having a high-dimensional nuisance parameter is not necessarily an issue, especially for the integrated likelihood methods under discussion in this paper which have been shown to work particularly well in situations where the dimension of $\lambda$ is large relative to the sample size; see, for example, De Bin et al. (2015) and Schumann et al. (2021).

\subsection{Explicit Parameters}

Parameters of interest and nuisance parameters can be broadly classified into two categories, explicit or implicit. For a given statistical model, both types of parameter must occupy the same category - it is not possible for $\psi$ to be explicit and $\lambda$ to be implicit, or vice versa. 

Let us first consider the case in which $\psi$ and $\lambda$ are *explicit* parameters. This means that $\psi$ is a sub-vector of $\theta$, so that all the components of $\psi$ are also components of $\theta$. Then there exists a set $I = \{I_1, ..., I_m\} \subsetneq \{1, ..., k\}$ such that $$\psi = (\theta_{I_1}, ..., \theta_{I_m}).$${#eq-expl_param1} It immediately follows that $\lambda$ is the sub-vector of all components of $\theta$ that are not part of $\psi$. More precisely, if we let $J = \{J_1, ..., J_{k-m}\} \subsetneq \{1, ..., k\}$ such that $I \cup J = \{1, ..., k\}$ and $I \cap J = \emptyset$, then $$\lambda = (\theta_{J_1}, ..., \theta_{J_{k-m}}).$${#eq-expl_param2} $\theta$ can therefore be decomposed as $\theta = (\psi, \lambda)$ when $\psi$ and $\lambda$ are explicit, provided we shuffle the indices appropriately.

\subsection{Implicit Parameters}

Now let us consider the case in which $\psi$ and $\lambda$ are *implicit* parameters. This means there exists some function $\varphi: \Theta \to \Psi$ for which the parameter of interest can be written as $$\psi = \varphi(\theta).$${#eq-impl_param} As before, $\Psi$ is still assumed to be a subset of $\mathbb{R}^m$ where $m$ is less than $k$, the dimension of the full parameter space $\Theta$. This reduction in dimension again implies the existence of a nuisance parameter $\lambda \in \Lambda \subseteq{\mathbb{R}}^{k-m}$. However, unlike in the explicit case, a closed form expression for $\lambda$ in terms of the original components of $\theta$ need not exist. For this reason, implicit nuisance parameters are in general more difficult to eliminate compared to their explicit counterparts.

Note that when the parameter of interest and nuisance parameter are explicit, it is always possible to define a function $\varphi$ such that $$\varphi(\theta) = (\theta_{I_1}, ..., \theta_{I_m}) \equiv \psi ,$${#eq-expl_param3} where $\{I_1, ..., I_m\}$ is defined as above. Hence, the first case is really just a special example of this more general one in which $\psi = \varphi(\theta)$. With this understanding in mind, we will use the notation $\psi = \varphi(\theta)$ to refer to the parameter of interest in general, only making the distinction between implicitness and explicitness when the difference is relevant to the situation.

\section{Pseudolikelihood Functions}

\subsection{Introduction}

The natural solution to the hindrance nuisance parameters pose to making inferences on the parameter of interest is to find a method for eliminating them from the model altogether. Since one way of uniquely specifying a model is through its likelihood function, this is equivalent to eliminating the nuisance parameters from the likelihood function itself. The result of this elimination is what is known as a pseudolikelihood function.

In general, a *pseudolikelihood function* for $\psi$ is defined as being a function of $\psi$ and the data alone, having properties resembling that of a genuine likelihood function. Suppose $\psi = \varphi(\theta)$ for some function $\phi$ and parameter $\theta \in \Theta$. If we let $\Theta(\psi) = \{\theta \in \Theta \> : \> \varphi(\theta) = \psi \},$ then associated with each $\psi \in \Psi$ is the set of likelihoods $\mathcal{L}_{\psi} = \{L(\theta) \> : \> \theta \in \Theta(\psi)\}.$ 

Any summary of the values in $\mathcal{L}_{\psi}$ that does not depend on $\lambda$ theoretically constitutes a pseudolikehood function for $\psi$. There exist a variety of methods to obtain this summary but among the most popular are maximization, conditioning, and integration, each with respect to the nuisance parameter. We will explore each of these methods in more detail in the sections to come.

\subsection{The Profile Likelihood}

The profile likelihood is the most straightforward method for eliminating a nuisance parameter from a likelihood function.  

For example, suppose we are interested in estimating the mean of a random variable $Y$, where $Y \sim N(\mu, \sigma^2)$. The full model parameter is $\theta = (\mu, \sigma^2)$ but since we are only interested in estimating the mean, the parameter of interest is $\psi =\mu$ and the nuisance parameter is $\lambda = \sigma^2$.  

\subsection{The Conditional Likelihood}

\subsection{The Marginal Likelihood}

\subsection{The Integrated Likelihood}

\section{Asymptotic Analysis of Likelihoods and Pseudolikelihoods}

\subsection{Regularity Conditions}

Knowledge of how likelihood functions behave as the sample size increases is a useful tool for understanding the properties of the estimates they produce. We will base our analysis of this asymptotic behavior on second-order Taylor expansions of the log-likelihood function and its derivatives. In order to guarantee that these expansions exist, we will consider only a particular class of statistical models that obey certain *regularity conditions*. The exact specifications of these conditions can vary depending on the requirements of the researcher, but they are usually made with the goal of ensuring that the log-likelihood function obeys various "nice" properties. Typical examples of these properties include asymptotic normality of the log-likelihood and its first derivative as well as guaranteed existence and consistency of the MLE. 

Suppose we have a model with a family of distributions $\mathcal{P}$ indexed by a parameter $\theta \in \Theta$. Let $\theta_0$ denote the true value of $\theta$. For our purposes, a parametric model is called *regular* if it satisfies the following conditions:

1) All of the distributions in $\mathcal{P}$ are absolutely continuous with respect to a $\sigma$-finite measure $\mu$ and therefore admit a density function $p(x; \theta)$;
2) Any observations $X_1, ..., X_n$ that we draw from $\mathcal{S}$ are independent and identically distributed according to $p(x; \theta)$;
3) The densities
3) The parameter space $\Theta$ contains an open subset $\Theta_0$ of which the true parameter value $\theta_0$ is an interior point.

All of the distributions in $\mathcal{P}$ have common support;
2) All of the distributions in $\mathcal{P}$ are absolutely continuous with respect to a sigma-finite measure $\mu$;
3) Any observations $X_1, ..., X_n$ taken from $\mathcal{S}$ are independent and identically distributed with probability density function $f(x; \theta)$ with respect to $\mu$.

\subsection{The Bartlett Identities}

The Bartlett identities are a set of equations relating to the expectations of functions of derivatives of a log-likelihood function. A well-specified genuine likelihood function will automatically satisfy each of the Bartlett identites; however, an arbitrary function of $\theta$ and $X$ will not. For this reason, the identities act as a litmus test of sorts for determining the validity of a pseudolikelihood as an approximation to the genuine likelihood from which it originated.^[The Bartlett identities offer an alternative way of characterizing the difference between likelihood and pseudolikelihood functions. A genuine likelihood function of $\theta$ is any nonnegative random function of $\theta$ for which all of the Bartlett identities hold. A pseudolikelihood of $\theta$ is any nonnegative random function of $\theta$ for which at least one of the Bartlett identities does not hold.]

Consider the case in which a random variable $X$ has a probability density $f$ that depends on a scalar parameter $\theta$. Denote the log-likelihood function for $\theta$ by $\ell(\theta; x) = \log f(x; \theta)$ and its first derivative with respect to $\theta$ by $\ell_{\theta}(\theta;x ) = \frac{\partial}{\partial \theta} \ell(\theta; x)$. We previously assumed in Section 2.1.4. that all probability distributions for which the results in this paper apply are regular. One consequence of this assumption is that derivatives and integrals of the density functions for these distributions may be interchanged. Now, taking the expectation of $\ell_{\theta}(\theta; x)$ gives

$$
\begin{aligned}
\mathbb{E}\big[\ell_{\theta}(\theta; x); \theta \big]  &= \mathbb{E}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x); \theta\bigg] \\
                                                 &= \int_{\mathbb{R}} \bigg[\frac{\partial}{\partial \theta} \ell(\theta; x)\bigg] f(x; \theta) dx \\
                                                 &= \int_{\mathbb{R}} \bigg[\frac{\partial}{\partial \theta} \log f(x; \theta)\bigg] f(x; \theta) dx \\
                                                 &=  \int_{\mathbb{R}} \frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x; \theta)} f(x; \theta) dx \\
                                                 &=  \int_{\mathbb{R}} \frac{\partial}{\partial \theta} f(x; \theta) dx \\
                                                 &= \frac{d}{d \theta} \int_{\mathbb{R}} f(x; \theta) dx &&(\text{by regularity of } f)\\
                                                 &= \frac{d}{d \theta} 1 \\
                                                 &= 0. 
\end{aligned}
$$
Therefore, $$\mathbb{E}\big[\ell_{\theta}(\theta; x); \theta \big] = 0 \text{ for all } \theta.$${#eq-BI1} @eq-BI1 is called the first Bartlett identity. In words, it states that the expectation of the first derivative of the log-likelihood function of a statistical model with respect to the model parameter will always be 0. Another name for $\ell_{\theta}$ is the *score function*, and any pseudolikelihood that also satisfies the first Bartlett identity is said to be *score-unbiased*.

If we now consider the second derivative of $\ell(\theta; x)$, we have

$$\begin{aligned}
\ell_{\theta \theta}(\theta; x) &= \frac{\partial^2}{\partial \theta^2} \ell(\theta; x)\\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \log f(x;\theta) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\Bigg[\frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x; \theta)} \Bigg] \\
                                &= \frac{\Big[\frac{\partial^2}{\partial \theta^2} f(x; \theta)\Big] f(x;\theta) - \Big[\frac{\partial}{\partial \theta} f(x; \theta)\Big]\Big[\frac{\partial}{\partial \theta} f(x; \theta)\Big]}{\big[f(x; \theta) \big]^2} \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \Bigg[\frac{\frac{\partial}{\partial \theta} f(x; \theta)}{ f(x; \theta)}\Bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \bigg[\frac{\partial}{\partial \theta} \log f(x; \theta)\bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \big[\ell_{\theta}(\theta; x) \big]^2. 
\end{aligned} 
$$
Rearranging terms and taking expectations yields 
$$
\begin{aligned}
\mathbb{E}[\ell_{\theta \theta}(\theta; x); \theta] + \mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2; \theta\Big] &= \mathbb{E}\Bigg[\frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)}; \theta\Bigg] \\
            &= \int_{\mathbb{R}} \Bigg[\frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)}\Bigg] f(x; \theta) dx \\
            &= \int_{\mathbb{R}} \frac{\partial^2}{\partial \theta^2} f(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} \int_{\mathbb{R}} f(x; \theta) dx &&(\text{by regularity of } f)\\
            &= \frac{d^2}{d \theta^2} 1 \\
            &= 0. 
\end{aligned}
$$
Therefore, $$\mathbb{E}[\ell_{\theta \theta}(\theta; x); \theta] + \mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2; \theta\Big] = 0 \text{ for all } \theta.$${#eq-BI2}

@eq-BI2 is called the second Bartlett identity. The second term on the left-hand side can be further rewritten as 

$$
\begin{aligned}
\mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2; \theta\Big] &= \mathbb{V}[\ell_{\theta}(\theta; x); \theta] +\Big(\mathbb{E}\big[\ell_{\theta}(\theta; x); \theta \big]\Big)^2 \\
                                                          &= \mathbb{V}[\ell_{\theta}(\theta; x); \theta]. &&(\text{by the first Bartlett identity})
\end{aligned}
$$
Another name for this quantity is the *expected information*. It follows from the second Bartlett identity that $$\mathbb{E}[-\ell_{\theta \theta}(\theta; x); \theta] = \mathbb{V}[\ell_{\theta}(\theta; x); \theta].$${#eq-BI3} The quantity $-\ell_{\theta \theta}(\theta; x)$ is called the *observed information*. Any pseudolikelihood that satisfies the second Bartlett identity is said to be *information-unbiased*.

It is possible to derive further Bartlett identities by continuing in this manner for an arbitrary number of derivatives of the log-likelihood function, provided that they exist. However, the first two are sufficient for our purposes of evaluating the validity of pseudolikelihoods as approximations to a genuine likelihood so we will not go further here. Note that while the above derivations were performed under the assumption that $\theta$ is a scalar, the Bartlett identities also hold in the case where $\theta$ is a multi-dimensional vector.

\subsection{Single-Index Asymptotic Theory}

Single-index asymptotic theory describes the behavior of pseudolikelihood functions as the sample size ($n$) grows to infinity while the dimension of the nuisance parameter ($m$) remains fixed. 



Let $X_1, ..., X_n$ be independent and identically distributed observations taken from some random variable $X$ with distribution $P_{\theta}$ and parameter $\theta \in \Theta \subseteq \mathbb{R}^d$. Since we have assumed $\mathcal{P}_{\theta}$ is absolutely 

Let $\hat{\theta}_n$ denote the maximum likelihood estimate for $\theta$.



A second-order Taylor expansion of the log-likelihood $\ell(\theta;x) = \log f(x; \theta)$ around the point $\theta = \hat{\theta}_n$ is given as follows:

$$\ell(\theta) = \ell(\hat{\theta}_n) + \ell_{\theta}(\hat{\theta}_n)(\theta - \hat{\theta}_n) + \frac{1}{2}\ell_{\theta\theta}(\hat{\theta}_n)(\theta - \hat{\theta}_n)^2 + R_n(\theta),$$ where $R_n(\theta) = \frac{1}{6}\ell_{\theta\theta\theta}(\theta^*)(\theta - \hat{\theta}_n)^3$, for some $\theta^* \in \Theta$.





A second-order Taylor expansion of the score function $\ell_{\theta}(\theta)$ around the point $\theta = \hat{\theta}_n$ is given as follows:

$$\ell_{\theta}(\theta) = \ell_{\theta}(\hat{\theta}_n) + \ell_{\theta\theta}(\hat{\theta}_n)(\theta - \hat{\theta}_n) + \frac{1}{2}\ell_{\theta\theta\theta}(\hat{\theta}_n)(\theta - \hat{\theta}_n)^2 + R_n(\theta),$$ where $R_n(\theta) = \frac{1}{6}\ell_{\theta\theta\theta\theta}(\theta^*)(\theta - \hat{\theta}_n)^3$, for some $\theta^* \in \Theta$.

\subsection{Two-Index Asymptotic Theory}

Two-index asymptotic theory describes the behavior of pseudolikelihood functions as $n$ and $m$ both tend to infinity, with $m$ growing at least as fast as $n$. 






