\chapter{Applications}

\section{Multinomial Distribution}

For $n$ independent trials each of which leads to a success for exactly one of $d$ categories, with the $j$th category having a fixed probability of success $\theta_j$, the multinomial distribution allows us to calculate the probability of any particular combination of numbers of successes for the various categories. If $N_j$ represents the number of successes in the $j$th category, then the random vector $(N_1, ..., N_d)$ will follow a multinomial distribution with parameter $\symbf{\theta} = (\theta_1, ..., \theta_j)$ such that $\text{E}_{\symbf{\theta}}(N_j) = n\theta_j$ for $j = 1, ..., d$, where $n = \sum_{j=1}^d N_j$. Since each $\theta_j$ is meant to be interpreted as a probability, the parameter space $\Theta$ must be the probability simplex in $\mathbbm{R}^d$ so that $\theta_j \geq 0$ for all $j$ and $\sum_{j=1}^d \theta_j = 1$. 

Suppose we are interested in the entropy of this distribution, so that the parameter of interest is $\psi = \varphi(\symbf{\theta}),$ where $$\varphi(\theta) = -\sum_{j=1}^d \theta_j \log(\theta_j).$$ Let $(n_1, ..., n_d)$ denote the observed counts of $(N_1, ..., N_d)$ so that the likelihood function is given by $$L(\symbf{\theta}) = \prod_{j=1}^d \theta_j^{n_j}.$$ The unresricted MLE of $\theta_j$ is given by $\hat{\theta}_j = \frac{n_j}{n}$. Similarly, due to the invariance property of the MLE, the unrestricted MLE of $\psi$ will simply be $\hat{\psi} = \varphi(\hat{\symbf{\theta}})$.

To obtain an integrated likelihood for $\psi$ alone, we can use the procedure described in the previous chapter to find an approximation to the integral in @eq-ZSE_IL11 where $\overset{\sim}{L}(u; \psi)$ is the likelihood function reparameterized in terms of the ZSE parameter, and $\check{L}(u)$ and $\check{\pi}(u)$ are chosen such that $\check{\pi}$ is a conjugate prior for $\check{L}$.^[This procedure is an adapted version of another algorithm developed by @severini2022 for approximating the integrated likelihood for the entropy of a multinomial distribution.] In this case, a natural choice exists due to the fact that the Dirichlet distribution is a conjugate prior for the multinomial distribution. Since our original likelihood function $L$ is based on a multinomial distribution, we can simply set $\check{L}(u) := L(u)$ and $\check{\pi}(u) \sim \text{Dir}(\symbf{\alpha})$, where $\symbf{\alpha} = (\alpha_1, ..., \alpha_d)$. Then the posterior distribution for $u$ based on data $\symbf{n} = (n_1, ..., n_d)$ is given by $L(u)\check{\pi}(u) \sim \text{Dir}(\symbf{n} + \symbf{\alpha})$. Consequently, we will take $\check{\pi}(u)$ to be the symmetric Dirichlet distribution on the probability simplex in $\mathbbm{R}^d$ with $\alpha_j = 1$ for all $j$ so that random variates of $u$ can be sampled from a $\text{Dir}(\symbf{n} + 1)$ distribution.

From @eq-ZSE_IL9, finding $\overset{\sim}{L}(u; \psi)$ is a matter of defining two functions, $Q$ and $T_\psi$ such that $\overset{\sim}{L}(u; \psi) = L(T_{\psi}(Q(u))$. $Q$ maps a random variate $u$ sampled from the above posterior to an element $\omega$ in $\Omega_{\hat{\psi}}$, and $T_{\psi}$ then maps $\omega$ to an element $\symbf{\theta}$ in $\Theta(\psi)$. Since in this situation, these quantities $u$, $\omega$, and $\symbf{\theta}$ are all members of the probability simplex, the maps $Q$ and $T_{\psi}$ can be defined as returning the elements in their respective output spaces that are closest to the input element they have each received. That is, $Q$ returns the element $\omega$ that minimizes the distance to an input $u$, subject to the constraints that the sum of the components of $\omega$ equal 1 and $\varphi(\omega) = \hat{\psi}$. Similarly, $T_{\psi}$ returns the element $\symbf{\theta}$ that minimizes the distance to an input $\omega$, subject to the constraints that the sum of the components of $\symbf{\theta}$ equal 1 and $\varphi(\symbf{\theta}) = \psi$. 

From here, we sample $R$ random variates from the appropriate Dirichlet distribution, calculate $\overset{\sim}{L}(u; \psi)$ and $L(u)$ for each one, and use @eq-ZSE_IL12 to approximate the integrated likelihood at a particular value of $\psi$. We then repeat this process over a finely spaced sequence of the possible values of $\psi$ in order to get a shape of the overall integrated likelihood. See Appendix \ref{appendix:C} for a graph comparing the plot of one such integrated likelihood to the profile likelihood, for observed data $(n_1, ..., n_6) = (1, 1, 4, 7, 10)$ and 250 samples of the appropriate Dirichlet distribution drawn for each value of $\psi$.^[The samples were obtained using the 'LaplacesDemon' R package. The distance minimizations needed for $Q$ and $T_{\psi}$ were computed numerically using the 'nloptr' R package.]

\section{Standardized Mean Difference}

Broadly speaking, meta-analysis seeks to synthesize the results of multiple studies, typically with the goal of estimating with heightened precision the effect of a shared treatment among the studies in question. An effect size frequently used in a meta-analysis when the outcome variable is continuous is the *standardized mean difference* (SMD), defined as the difference in means between the treatment and control group divided by their common standard deviation. The aim of this section is to provide a blueprint for obtaining the integrated likelihood function of an SMD in a meta-analysis.

Consider a sample of $q$ independent studies, each comparing a treatment group to a control group. Suppose the $j$th observations of the treatment and control groups in the $i$th study are given by $$Y_{ij}^T \sim\text{N}(\mu_i^T, \sigma_i^2)$$ and $$Y_{ij}^C \sim\text{N}(\mu_i^C, \sigma_i^2),$$ respectively, where $j = 1, ..., m_i$ observations and $i = 1, ..., q$ studies. The number of observations in study $i$ may be further decomposed as $m_i = m_i^T + m_i^C$. The true SMD effect size for study $i$ is defined as $$\delta_i = \frac{\mu_i^T - \mu_i^C}{\sigma_i}.$${#eq-SMD1} Standard practice is to estimate $\delta_i$ by $$d_i = \frac{\bar{y}_i^T - \bar{y}_i^C}{s_i},$${#eq-SMD2} where $\bar{y}$ is the traditional sample mean and $s$ is the pooled sample standard deviation. @hedges1985 showed that as the number of observations in the study tends to infinity, $d_i$ converges to a $\text{N}(\delta_i, \sigma_{d_i}^2)$ distribution, where $$\sigma_{d_i}^2 = \frac{m_i^T + m_i^C}{m_i^T m_i^C} + \frac{\delta_i^2}{2(m_i^T + m_i^C -2)}.$${#eq-SMD3} $\sigma_{d_i}^2$ represents the variance of all observations within the $i$th study.

Under a linear fixed-effect model, $$d_i = \delta + \epsilon_i,$${#eq-SMD4} with $\epsilon_i \sim \text{N}(0, \sigma_{d_i}^2)$ for $i = 1,..., q$, where $\delta = \frac{1}{q}\sum_{i=1}^q \delta_i$ is the common mean. The overarching objective of a meta-analysis using this framework is to find an estimate of $\delta$ and its associated standard error. Hence, $\delta$ can be considered an implicit parameter of interest in this model, and so it makes sense to consider the efficacy of an integrated likelihood function as a tool for its estimation.

Let $\symbf{\theta}_i = (\mu_i^T, \mu_i^C, \sigma_i)$ denote the vector of parameters associated with study $i$ so that the overall model parameter is given by $\symbf{\theta} = (\symbf{\theta}_1, ..., \symbf{\theta}_q) = \big((\mu_1^T, \mu_1^C, \sigma_1), ..., (\mu_q^T, \mu_q^C, \sigma_q)\big)$. The likelihood function for $\symbf{\theta}$ is 
$$
\begin{aligned}
L(\symbf{\theta}) &= \Bigg[\prod_{i=1}^q \prod_{j=1}^{m_i^T} p(y_{ij}^T; \mu_i^T, \sigma_i)\Bigg] \Bigg[\prod_{i=1}^q \prod_{j=1}^{m_i^C} p(y_{ij}^C; \mu_i^C, \sigma_i)\Bigg] \\
                  &\propto \Bigg[\prod_{i=1}^q \prod_{j=1}^{m_i^T} \frac{1}{\sigma_i} \exp\bigg\{-\frac{1}{2} \bigg(\frac{y_{ij}^T - \mu_i^T}{\sigma_i}\bigg)^2\bigg\}\Bigg] \Bigg[\prod_{i=1}^q \prod_{j=1}^{m_i^C} \frac{1}{\sigma_i} \exp\bigg\{-\frac{1}{2} \bigg(\frac{y_{ij}^C - \mu_i^C}{\sigma_i}\bigg)^2\bigg\}\Bigg] \\
                  &=\Bigg[\prod_{i=1}^q \sigma_i^{-m_i}\Bigg]\exp\Bigg\{-\frac{1}{2} \sum_{i=1}^q \frac{1}{\sigma_i^2} \Bigg[\sum_{j=1}^{m_i^T}(y_{ij}^T - \mu_i^T)^2 + \sum_{j=1}^{m_i^C}(y_{ij}^C - \mu_i^C)^2 \Bigg]\Bigg\}.
\end{aligned}
$$
Let $$\varphi(\symbf{\theta}) =  \frac{1}{q}\sum_{i=1}^q \frac{\mu_i^T - \mu_i^C}{\sigma_i}$${#eq-SMD5} so that $\delta = \varphi(\symbf{\theta})$. It follows that the MLE of $\delta$ is simply $\hat{\delta} = \varphi(\hat{\symbf{\theta}}).$

The MLEs of $\mu_i^T$ and $\mu_i^C$ are $$\hat{\mu}_i^T = \bar{y}_i^T = \frac{1}{m_i^T}\sum_{j=1}^{m_i^T} y_{ij}^T$${#eq-SMD6} and $$\hat{\mu}_i^C = \bar{y}_i^C = \frac{1}{m_i^C}\sum_{j=1}^{m_i^C} y_{ij}^C.$${#eq-SMD7} Since $\sigma_i$ represents the common standard deviation of both the treatment and control groups in study $i$, the standard way to estimate it is by pooling the sample variances of the two groups and then taking the square root. Let $s_i^{T^2}$ and $s_i^{C^2}$ denote the sample variances of the treatment and control groups, so that $$s_i^{T^2} = \frac{1}{m_i^T - 1} \sum_{j=1}^{m_i^T} (y_{ij}^T - \bar{y}_i^T)^2$${#eq-SMD8} and $$s_i^{C^2} = \frac{1}{m_i^C - 1} \sum_{j=1}^{m_i^C} (y_{ij}^C - \bar{y}_i^C)^2.$${#eq-SMD9} Then the pooled estimator of the common standard deviation $\sigma_i$ is $$s_i = \Bigg[\frac{(m_i^T - 1)s_i^{T^2} + (m_i^C - 1)s_i^{C^2}}{m_i^T + m_i^C - 2} \Bigg]^{\frac{1}{2}}.$${#eq-SMD10}

Note, however, that these sample standard deviation estimators include Bessel's correction to account for the bias in the estimates given by maximum likelihood estimation, and hence are technically not MLEs themselves (though they are equivalent asymptotically). Since the ZSE parameter depends on the MLE of the parameter of interest, we will undo this correction in our estimate of $\sigma_i$ in order to obtain its MLE. Therefore, we have the following:

$$
\begin{aligned}
\hat{\sigma}_i^{T^2} &= \frac{m_i^T - 1}{m_i^T} s_i^{T^2}; \\
\hat{\sigma}_i^{C^2} &= \frac{m_i^C - 1}{m_i^T} s_i^{C^2}; \\
\hat{\sigma}_i^2 &= \frac{m_i^T + m_i^C - 2}{m_i^T + m_i^C} s_i^2, \\
\end{aligned}
$${#eq-SMD11}
and so $$\hat{\sigma}_i = \sqrt\frac{m_i - 2}{m_i} s_i.$${#eq-SMD12} This allows us to rewrite @eq-SMD2 as $$d_i = \sqrt\frac{m_i - 2}{m_i} \frac{\bar{y}_i^T - \bar{y}_i^C}{\hat{\sigma}_i}.$${#eq-SMD13}

Going forward, one of our objectives will be to find appropriate choices for $\check{L}(u)\check{\pi}(u)$, $Q(u)$, and $T_{\delta}(\omega)$ that allow us to form an approximation to the integrated likelihood function $$\bar{L}(\delta) = \int_{\mathcal{U}} \frac{L\big(T_{\delta}(Q(u))\big)}{\check{L}(u)}\check{L}(u)\check{\pi}(u)du$$ using the procedure described in the previous chapter. We will also explore a similar procedure under the framework of a random-effects model in which we form approximations to the integrated likelihood for an SMD, as well as one for the heterogeneity variance parameter $\tau^2$.


