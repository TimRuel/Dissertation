\chapter{Approximating the Integrated Likelihood}

\section{The Zero-Score Expectation Parameter}

Let $\psi = \varphi(\theta)$ and $\lambda$ denote the parameter of interest and nuisance parameter, respectively, for some statistical model $(\mathcal{S}, \mathcal{P_{\theta}})$. Then the general expression to obtain an integrated likelihood for $\psi$ may be written as $$\bar{L}(\psi) = \int_{\Lambda}L(\psi, \lambda)\pi(\lambda|\psi)d\lambda,$${#eq-IL}
where $\pi(\lambda|\psi)$ is a conditional prior density for $\lambda$ given $\psi$. 

@severini2007 considered the problem of selecting $\pi(\lambda|\psi)$ such that when the likelihood function is integrated with respect to this density, the result is useful for non-Bayesian inference. To do this, he outlined four properties (see Appendix \ref{appendix:A}) that an integrated likelihood function must satisfy if it is to be of any use. He went on to prove that an integrated likelihood satisfying these properties could be obtained by first constructing a new nuisance parameter $\phi \in \Phi$ that is unrelated to the parameter of interest (in the sense that its maximum likelihood estimator remains roughly unchanged for all values of $\psi$)  and then choosing a prior density $\pi(\phi)$ that is independent of $\psi$. Once chosen, the desired integrated likelihood function for $\psi$ is given by $$\bar{L}(\psi) = \int_{\Phi} \tilde{L}(\psi, \phi) \pi(\phi) d\phi,$${#eq-ZSEP_IL1} where $\tilde{L}(\psi, \phi)$ is the likelihood function for the model after it has been reparameterized in terms of $\phi$.  It is important to note that the exact choice of prior density for $\phi$ is not particularly important; the only restriction we place upon it is that it must not depend on $\psi$.  

Suppose that we have an explicit parameter of interest and nuisance parameter, so that $\theta = (\psi, \lambda)$. Then @severini2007 defines this new nuisance parameter $\phi$ as the solution to the equation $$\mathbb{E}(\ell_{\lambda}(\psi, \lambda); \psi_0, \lambda_0)\bigg|_{(\psi_0, \lambda_0) = (\hat{\psi}, \phi)} = 0,$${#eq-zse} where $\ell_{\lambda}(\psi, \lambda) = \frac{\partial \ell(\psi, \lambda)}{\partial \lambda}$, $\psi_0$ and $\lambda_0$ denote the true values of $\psi$ and $\lambda$, and $\hat{\psi}$ is the MLE for $\psi_0$. In other words, for a particular value of $(\psi, \lambda, \hat{\psi})$, we can find the corresponding value of $\phi$ by solving for it in @eq-zse. $\phi$ is called the *zero-score expectation parameter* (ZSEP) because it is defined as the value that makes the expectation of the score function (see Appendix \ref{appendix:A}) evaluated at the point $(\hat{\psi}, \phi)$ equal to zero. 

For a given value of $(\psi, \phi, \hat{\psi})$, it is also possible to solve @eq-zse for $\lambda$. This allows us to write @eq-ZSEP_IL1 in terms of $L(\psi, \lambda)$: $$\bar{L}(\psi) = \int_{\Phi} L(\psi, \lambda(\psi, \phi)) \pi(\phi) d\phi.$${#eq-ZSEP_IL2} Note that $\phi$ depends on the data through $\hat{\psi}$. In most circumstances, such a parameterization would be considered pointless as the reason we collect data in the first place is to estimate parameters. However, from the perspective of the likelihood function, once the data have been collected, they are considered fixed in place, and so there is no issue with using a quantity such as $\phi$ that depends on the data to parameterize it.

@severini2018 proved that reparameterizing the nuisance parameter in terms of the ZSEP yields the same desirable properties in the subsequent integrated likelihood when $\psi$ and $\lambda$ are implicit. Suppose $\psi$ = $\varphi(\theta)$, for some function $\varphi: \Theta \to \Psi$, and consider the set of all values of $\theta$ satisfying $\varphi(\theta) = \hat{\psi}$. Call this set $\Omega_{\hat{\psi}}$ so that $$\Omega_{\hat{\psi}} = \Big\{\omega \in \Theta: \varphi(\omega) = \hat{\psi}\Big\}.$${#eq-omega_psi_hat}

\section{Markov Chain Monte Carlo}

\section{The IL Algorithm}




