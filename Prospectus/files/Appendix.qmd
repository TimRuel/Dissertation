\appendix

\chapter{Chapter 3}\label{appendix:A}
\index{Appendix@\emph{Appendix A}}

\section{Definitions and Notation}

\subsection{Neighborhoods}

A *neighborhood* of a point $\mathbf{p} \in \mathbbm{R}^d$ is the set of all points $\mathbf{x} \in \mathbbm{R}^d$ such that the Euclidean distance between $\mathbf{p}$ and $\mathbf{x}$ is less than some radius $r > 0$. We use the following notation to refer to such neighborhoods: $$N_r(\mathbf{p}) = \{\mathbf{x} \in \mathbbm{R}^d: ||\mathbf{x} - \mathbf{p}|| < r \}.$$

\subsection{Line Segments}

For two points $\mathbf{x}_1, \mathbf{x}_2 \in \mathbbm{R}^d$, a third point $\bar{\mathbf{x}}$ is said to be on the *line segment* connecting $\mathbf{x}_1$ and $\mathbf{x}_2$ if there exists $\omega \in [0, 1]$ such that $\bar{\mathbf{x}} = \omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2$.  We use the following notation to refer to such line segments: $$LS(\mathbf{x}_1, \mathbf{x}_2) = \{\omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2: \> \omega \in [0, 1]\}.$$

\section{Taylor's Theorem}

Let $f: \mathbbm{R}^d \to \mathbbm{R}$ be a function that is $(k+1)$-times continuously differentiable in a neighborhood $N_r(\mathbf{x}_0)$ of some point $\mathbf{x}_0 \in \mathbbm{R}^d$ and suppose there exists $M$ satisfying $|D^{\symbf\alpha}f| \leq M$ for all $x \in N_r(x_0)$ and all $\symbf{\alpha}$ such that $|\symbf{\alpha}| = k + 1$. Then $$f(\mathbf{x}) = \sum_{0 \leq |\symbf{\alpha}| \leq k} \frac{D^{\symbf{\alpha}} f(\mathbf{x}_0)}{\symbf{\alpha}!}(\mathbf{x} - \mathbf{x}_0)^{\symbf{\alpha}} + R_k(\mathbf{x}),$$ where the remainder term $R_k(\mathbf{x})$ satisfies $$|R_k(\mathbf{x})| \leq \frac{M}{\symbf{\alpha}!}|\mathbf{x} - \mathbf{x}_0|^{\symbf{\alpha}}$$ for all $\symbf{\alpha}$ such that $|\symbf{\alpha}| = k + 1$

\section{Jensen's Inequality}

Jensen's inequality states that for a real-valued random variable $X$ with finite expectation and a strictly concave function $\varphi$, $$\text{E}[\varphi(X)] < \varphi(\text{E}[X]).$$

\chapter{Chapter 5}\label{appendix:B}
\index{Appendix@\emph{Appendix B}}

\section{Desirable Properties of the Integrated Likelihood}

\subsection{Property 1}

Suppose the likelihood function for a parameter $\theta$ can be decomposed as the product $L(\theta) = L_1(\psi)L_2(\lambda)$. Then the integrated likelihood for $\psi$ should satisfy $$\bar{L}(\psi) = L_1(\psi).$$

\subsection{Property 2}

\subsection{Property 3}

\subsection{Property 4}

\section{Laplace's Method}

Let $\theta$ be a scalar parameter taking values in $\mathbbm{R}$ and consider an integral of the form $$I = \int_{-\infty}^{\infty} f(\theta) \exp[-nh(\theta)]d\theta.$$ Suppose the function $-h(\theta)$ is smooth, bounded, and unimodal so that it attains a maximum at a point $\hat{\theta}$. Then Laplace's method states that an approximation for $I$ is given by $$\hat{I} = f(\hat{\theta})\sqrt{\frac{2 \pi}{n}}\sigma \exp[-nh(\hat{\theta})],$$
where $$\sigma = \Bigg[\frac{\partial^2 h}{\partial \theta^2} \Bigg|_{\theta = \hat{\theta}} \Bigg]^{-1/2}.$$ It can further be shown that $$I = \hat{I}\Bigg\{1 + O\bigg(\frac{1}{n}\bigg)\Bigg\},$$ where the term $n$ may be interpreted as the sample size.



