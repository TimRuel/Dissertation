\appendix

\chapter{Chapter 3}\label{appendix:A}
\index{Appendix@\emph{Appendix A}}

\section{Definitions and Notation}

\subsection{Open and Closed Balls}

\subsubsection{Open Ball} The *open ball of radius* $r > 0$ centered at a point $p \in \mathbbm{R}^d$, is the set of all points $x \in \mathbbm{R}^d$ such that the distance between $p$ and $x$ is less than $r$ We denote this set using the notation $$B_r(\mathbf{p}) = \{\mathbf{x} \in \mathbbm{R}^d: ||\mathbf{x} - \mathbf{p}|| < r \},$$ where $||\cdot||$ indicates the Euclidean norm, i.e., $||\mathbf{x}|| = \Big(\sum_{i=1}^d x_i^2\Big)^{1/2}$ for $\mathbf{x} \in \mathbbm{R}^d$.

\subsubsection{Closed Ball} The *closed ball of radius* $r > 0$ centered at a point $p \in \mathbbm{R}^d$, is the set of all points $x \in \mathbbm{R}^d$ such that the distance between $p$ and $x$ is less than or equal to $r$. We denote this set using the notation $$B_r[\mathbf{p}] = \{\mathbf{x} \in \mathbbm{R}^d: \> ||\mathbf{x} - \mathbf{p}||\} \leq r.$$

\subsection{Open and Closed Sets}

\subsubsection{Open Set} A subset $A$ of $\mathbbm{R}^d$ is called an *open set* of $\mathbbm{R}^d$ if every point in $A$ is the center of an open ball entirely contained in $A$. That is, $A$ is open if and only if for any $a \in A$, there exists a radius $r > 0$ such that $B_r(a) \subseteq A$.

\subsubsection{Closed Set} A subset $B$ of $\mathbbm{R}^d$ is called a *closed set* of $\mathbbm{R}^d$ if its complement $B^c = \mathbbm{R}^d \mathbin{\backslash} B$ is open.

\subsection{Neighborhoods}

A set $N_{\mathbf{p}} \subseteq \mathbbm{R}^d$ is called a *neighborhood* of a point $\mathbf{p} \in \mathbbm{R}^d$ if it contains an open ball centered at $\mathbf{p}$, i.e., for some radius $r > 0$ there exists an open ball $B_r(\mathbf{p})$ such that $B_r(\mathbf{p}) \subseteq N_{\mathbf{p}}$.

\subsection{Boundedness}

\subsubsection{Bounded Set} A set $S \subset \mathbb{R}^d$ is called *bounded* if there exists some radius $r > 0$ such that $B_r(\mathbb{\mathbf{0}}) \subset S$.

\subsubsection{Bounded Function} A function $f: X \to \mathbb{R}$ is called *bounded* if there exists a real number $M$ such that $|f(x)| \leq M$ for all $x \in X$.

\subsection{Compact Set}

A subset of $\mathbb{R}^d$ is called *compact* if it is closed and bounded.

\subsection{Interiority}

\subsubsection{Interior Point}

A point $p \in S \subset \mathbbm{R}^d$ is called an *interior point* of $S$ if there exists some radius $r > 0$ such that $\subseteq B_r(\mathbb{p})$.

\subsubsection{Interior of a Set} The *interior of a set* $S \subset \mathbbm{R}^d$, denoted by $\text{int} \> S$, is the set of all interior points of $S$.

\subsection{Line Segments}

For two points $\mathbf{x}_1, \mathbf{x}_2 \in \mathbbm{R}^d$, a third point $\bar{\mathbf{x}}$ is said to be on the *line segment* connecting $\mathbf{x}_1$ and $\mathbf{x}_2$ if there exists $\omega \in [0, 1]$ such that $\bar{\mathbf{x}} = \omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2$.  We use the following notation to refer to such line segments: $$\text{LS}(\mathbf{x}_1, \mathbf{x}_2) = \{\omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2: \> \omega \in [0, 1]\}.$$

\subsection{Convexity and Concavity}

\subsubsection{Convex Set} A set $S \subseteq \mathbbm{R}^d$ is called *convex* if for any two points $\mathbf{x}_1, \mathbf{x}_2 \in S$, the line segment connecting $\mathbf{x}_1$ and $\mathbf{x}_2$ is entirely contained within $S$, i.e., $$\text{LS}(\mathbf{x}_1, \mathbf{x}_2) \subseteq S \text{ for all } \mathbf{x}_1, \mathbf{x}_2 \in S.$$

\subsubsection{Convex Function} Let $f: X \to \mathbb{R}$, where $X$ is a convex set. $f$ is called a *convex function* if for all $t \in [0, 1]$ and all $x_1, x_2 \in X$, $$f(t x_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2).$$ If it is possible to graph the function on the coordinate plane, this is equivalent to saying that the line segment between any two distinct points on the graph of the function lies above the graph.

$f$ is called *strictly convex* if the equality is tightened, i.e., $$f(t x_1 + (1-t)x_2) < tf(x_1) + (1-t)f(x_2).$$ 

\subsubsection{Concave Function} Let $f: X \to \mathbb{R}$, where $X$ is a convex set. $f$ is called a *concave function* if for all $t \in [0, 1]$ and all $x_1, x_2 \in X$, $$f(t x_1 + (1-t)x_2) \geq tf(x_1) + (1-t)f(x_2).$$ If it is possible to graph the function on the coordinate plane, this is equivalent to saying that the line segment between any two distinct points on the graph of the function lies below the graph.

$f$ is called *strictly concave* if the equality is tightened, i.e., $$f(t x_1 + (1-t)x_2) > tf(x_1) + (1-t)f(x_2).$$ 

\subsection{Positive Definiteness}

A $d \times d$ symmetric real matrix $M$ is called *positive definite* if $\mathbf{x}^\top M \mathbf{x} > 0$ for all non-zero $\mathbf{x} \in \mathbbm{R}^d$.

\subsection{Derivatives of Multivariable Functions}

\subsubsection{Gradient}

The *gradient* of a multivariable scalar-valued function $f: \mathbbm{R}^d \to \mathbbm{R}$ is given by its $d\times 1$ vector of partial derivatives:
$$\nabla f(x_1, x_2, ..., x_d) = 
\begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_d}
\end{pmatrix}.$$

\subsubsection{Jacobian Matrix}

The *Jacobian matrix* of a multivariable vector-valued function $\mathbf{f}: \mathbbm{R}^d \to \mathbbm{R}^k$ given by
$$\mathbf{f}(x_1, x_2, ..., x_d) = 
\begin{pmatrix}
f_1(x_1, x_2, ..., x_d) \\
f_2(x_1, x_2, ..., x_d) \\
\vdots \\
f_k(x_1, x_2, ..., x_d)
\end{pmatrix}.
$$ is defined as its $k \times d$ matrix of partial derivatives:
$$\mathbf{J}(\mathbf{f}) = 
\begin{pmatrix} 
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_d} \\ 
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial f_k}{\partial x_1} & \frac{\partial f_k}{\partial x_2} & \cdots & \frac{\partial f_k}{\partial x_d}
\end{pmatrix}.
$$ In the case where $k = 1$, the Jacobian matrix simply reduces to the transpose of the gradient of $\mathbf{f}$.

\subsubsection{Hessian Matrix} The Hessian matrix of a multivariable scalar-valued function $f: \mathbbm{R}^d \to \mathbbm{R}$ is given by its $d\times d$ matrix of second partial derivatives:
$$\mathbf{H}(f) = 
\begin{pmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2  f}{\partial x_1\partial x_2} & \cdots & \frac{\partial^2  f}{\partial x_1\partial x_d} \\ 
\frac{\partial^2  f}{\partial  x_2 \partial x_1} & \frac{\partial^2  f}{\partial x_2^2} & \cdots & \frac{\partial^2  f}{\partial x_2\partial x_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2  f}{\partial  x_d \partial x_1} & \frac{\partial^2  f}{\partial x_d \partial x_2} & \cdots & \frac{\partial^2  f}{\partial x_d^2}
\end{pmatrix}
$$ The *Hessian matrix* of $f$ is also equal to the transpose of the Jacobian matrix of the gradient of $f$, i.e., $\mathbf{H}(f) = \mathbf{J}(\nabla f)^\top$. When the order of differentiation does not matter, which occurs if and only if all of $f$'s second partial derivatives are continuous, both matrices become symmetric and we simply have $\mathbf{H}(f) = \mathbf{J}(\nabla f)$. 

A twice differentiable function of several variables is strictly convex (concave) on a convex set if and only if its Hessian matrix is positive (negative) definite on the interior of the convex set.

\section{Theorems}

\subsection{The Extreme Value Theorem}

If $K$ is a compact set and $f: K \to \mathbb{R}$ is a continuous function, then $f$ is bounded and there exists $p, q \in K$ such that $$f(p) = \sup_{x \in K}f(x)$$ and $$f(q) = \inf_{x \in K}f(x).$$

\subsection{The Mean-Value Theorem for Multivariable Vector-Valued Functions}

Suppose the function $\mathbf{f}: \mathbbm{R}^d \to \mathbbm{R}^k$ is continuously differentiable for all points $\mathbf{x} \in B_r(\mathbf{x}_0)$. Then for $||\mathbf{t}|| < r$, $$f(\mathbf{x_0} + \mathbf{t}) = f(\mathbf{x_0}) + \Bigg[\int_0^1\mathbf{J}\Big(f(\mathbf{x} + u\mathbf{t})\Big)du\Bigg]\mathbf{t}.$$ The term in the square brackets above denotes a $k \times d$ matrix whose $(i, j)$th entry is given by $$\int_{0}^1 \frac{\partial f_i(\mathbf{x} + u \mathbf{t})}{\partial x_j}du.$$

\subsection{Taylor's Theorem}

Let $f: \mathbbm{R}^d \to \mathbbm{R}$ be a function that is $(k+1)$-times continuously differentiable in a neighborhood $N_r(\mathbf{x}_0)$ of some point $\mathbf{x}_0 \in \mathbbm{R}^d$ and suppose there exists $M$ satisfying $|D^{\symbf\alpha}f| \leq M$ for all $x \in N_r(x_0)$ and all $\symbf{\alpha}$ such that $|\symbf{\alpha}| = k + 1$. Then $$f(\mathbf{x}) = \sum_{0 \leq |\symbf{\alpha}| \leq k} \frac{D^{\symbf{\alpha}} f(\mathbf{x}_0)}{\symbf{\alpha}!}(\mathbf{x} - \mathbf{x}_0)^{\symbf{\alpha}} + R_k(\mathbf{x}),$$ where the remainder term $R_k(\mathbf{x})$ satisfies $$|R_k(\mathbf{x})| \leq \frac{M}{\symbf{\alpha}!}|\mathbf{x} - \mathbf{x}_0|^{\symbf{\alpha}}$$ for all $\symbf{\alpha}$ such that $|\symbf{\alpha}| = k + 1$

\subsection{Jensen's Inequality}

For a real-valued random variable $X$ with finite expectation and a strictly concave function $\varphi$, $$\text{E}[\varphi(X)] < \varphi(\text{E}[X]).$$

\chapter{Chapter 5}\label{appendix:B}
\index{Appendix@\emph{Appendix B}}

\section{Desirable Properties of the Integrated Likelihood}

\subsection{Property 1}

Suppose the likelihood function for a parameter $\symbf{\theta}$ can be decomposed as the product $L( \symbf{\theta}) = L_1(\psi)L_2(\lambda)$. Then the integrated likelihood for $\psi$ should satisfy $$\bar{L}(\psi) = L_1(\psi).$$

\subsection{Property 2}

\subsection{Property 3}

\subsection{Property 4}

\section{Laplace's Method}

Let $\symbf{\theta}$ be a scalar parameter taking values in $\mathbbm{R}$ and consider an integral of the form $$I = \int_{-\infty}^{\infty} f(\symbf{\theta}) \exp[-nh(\symbf{\theta})]d\symbf{\theta}.$$ Suppose the function $-h(\symbf{\theta})$ is smooth, bounded, and unimodal so that it attains a maximum at a point $\hat{ \symbf{\theta}}$. Then Laplace's method states that an approximation for $I$ is given by $$\hat{I} = f(\hat{ \symbf{\theta}})\sqrt{\frac{2 \pi}{n}}\sigma \exp[-nh(\hat{\symbf{\theta}})],$$
where $$\sigma = \Bigg[\frac{\partial^2 h}{\partial \symbf{\theta}^2} \Bigg|_{\symbf{\theta} = \hat{\symbf{\theta}}} \Bigg]^{-1/2}.$$ It can further be shown that $$I = \hat{I}\Bigg\{1 + O\bigg(\frac{1}{n}\bigg)\Bigg\},$$ where the term $n$ may be interpreted as the sample size.



