\appendix

\chapter{Chapter 3}\label{appendix:A}
\index{Appendix@\emph{Appendix A}}

\section{Definitions and Notation}

\subsection{Open Balls and Closed Balls}

The *open ball of radius* $r > 0$ centered at a point $p \in M$, where $M$ is a set equipped with some distance function $d$, is the set of all points $x \in M$ such that the distance between $p$ and $x$ is less than $r$. Similarly, the *closed ball of radius* $r > 0$ centered at $p$ is the set of all points in $M$ of a distance less than or equal to $r$ away from $p$. We use the following notation to refer to open and closed balls, respectively: $$B_r(p) = \{x \in M: d(x, p) < r \}$$ and $$B_r[p] = \{x \in M: d(x, p) \leq r \}.$$

The methods in this paper generally only make use of open and closed balls in the context where $M$ is the Cartesian coordinate space $\mathbbm{R}^d$ equipped with the Euclidean norm $||\mathbf{x}|| = \Big(\sum_{i=1}^d x_i^2\Big)^{1/2}$ as its distance function. Hence, the open and closed balls of radius $r$ around a point $\mathbf{p} \in \mathbbm{R}^d$ may be written as $$B_r(\mathbf{p}) = \{\mathbf{x} \in \mathbbm{R}^d: ||\mathbf{x} - \mathbf{p}|| < r \}$$ and $$B_r[\mathbf{p}] = \{\mathbf{x} \in \mathbbm{R}^d: \> ||\mathbf{x} - \mathbf{p}|| \leq r \}.$$ The reader may safely assume this latter definition is the one that is meant when open and closed balls are referenced in this paper.

\subsection{Neighborhoods}

A set $N_{\mathbf{p}} \subseteq \mathbbm{R}^d$ is called a *neighborhood* of a point $\mathbf{p} \in \mathbbm{R}^d$ if it contains an open ball centered at $\mathbf{p}$, i.e., for some radius $r > 0$ there exists an open ball $B_r(\mathbf{p})$ such that $B_r(\mathbf{p}) \subseteq N_{\mathbf{p}}$.

\subsection{Line Segments}

For two points $\mathbf{x}_1, \mathbf{x}_2 \in \mathbbm{R}^d$, a third point $\bar{\mathbf{x}}$ is said to be on the *line segment* connecting $\mathbf{x}_1$ and $\mathbf{x}_2$ if there exists $\omega \in [0, 1]$ such that $\bar{\mathbf{x}} = \omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2$.  We use the following notation to refer to such line segments: $$LS(\mathbf{x}_1, \mathbf{x}_2) = \{\omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2: \> \omega \in [0, 1]\}.$$

\subsection{Derivatives of Multivariable Functions}

\subsubsection{Gradient}

The *gradient* of a multivariable scalar-valued function $f: \mathbbm{R}^d \to \mathbbm{R}$ is given by its $d\times 1$ vector of partial derivatives:
$$\nabla f(x_1, x_2, ..., x_d) = 
\begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_d}
\end{pmatrix}.$$

\subsubsection{Jacobian Matrix}

The *Jacobian matrix* of a multivariable vector-valued function $\mathbf{f}: \mathbbm{R}^d \to \mathbbm{R}^k$ given by
$$\mathbf{f}(x_1, x_2, ..., x_d) = 
\begin{pmatrix}
f_1(x_1, x_2, ..., x_d) \\
f_2(x_1, x_2, ..., x_d) \\
\vdots \\
f_k(x_1, x_2, ..., x_d)
\end{pmatrix}.
$$ is defined as its $k \times d$ matrix of partial derivatives:
$$\mathbf{J}(\mathbf{f}) = 
\begin{pmatrix} 
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_d} \\ 
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial f_k}{\partial x_1} & \frac{\partial f_k}{\partial x_2} & \cdots & \frac{\partial f_k}{\partial x_d}
\end{pmatrix}.
$$ In the case where $k = 1$, the Jacobian matrix simply reduces to the transpose of the gradient of $\mathbf{f}$.

\subsubsection{Hessian Matrix} The Hessian matrix of a multivariable scalar-valued function $f: \mathbbm{R}^d \to \mathbbm{R}$ is given by its $d\times d$ matrix of second partial derivatives:
$$\mathbf{H}(f) = 
\begin{pmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2  f}{\partial x_1\partial x_2} & \cdots & \frac{\partial^2  f}{\partial x_1\partial x_d} \\ 
\frac{\partial^2  f}{\partial  x_2 \partial x_1} & \frac{\partial^2  f}{\partial x_2^2} & \cdots & \frac{\partial^2  f}{\partial x_2\partial x_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2  f}{\partial  x_d \partial x_1} & \frac{\partial^2  f}{\partial x_d \partial x_2} & \cdots & \frac{\partial^2  f}{\partial x_d^2}
\end{pmatrix}
$$ The *Hessian matrix* of $f$ is also equal to the transpose of the Jacobian matrix of the gradient of $f$, i.e., $\mathbf{H}(f) = \mathbf{J}(\nabla f)^\top$. When the order of differentiation does not matter, which occurs if and only if all of $f$'s second partial derivatives are continuous, both matrices become symmetric and we simply have $\mathbf{H}(f) = \mathbf{J}(\nabla f)$. 

\section{The Mean-Value Theorem for Multivariable Vector-Valued Functions}

Suppose the function $\mathbf{f}: \mathbbm{R}^d \to \mathbbm{R}^k$ is continuously differentiable for all points $\mathbf{x} \in B_r(\mathbf{x}_0)$. Then for $||\mathbf{t}|| < r$, $$f(\mathbf{x_0} + \mathbf{t}) = f(\mathbf{x_0}) + \Bigg[\int_0^1\mathbf{J}\Big(f(\mathbf{x} + u\mathbf{t})\Big)du\Bigg]\mathbf{t}.$$ The term in the square brackets above denotes a $k \times d$ matrix whose $(i, j)$th entry is given by $$\int_{0}^1 \frac{\partial f_i(\mathbf{x} + u \mathbf{t})}{\partial x_j}du.$$

\section{Taylor's Theorem}

Let $f: \mathbbm{R}^d \to \mathbbm{R}$ be a function that is $(k+1)$-times continuously differentiable in a neighborhood $N_r(\mathbf{x}_0)$ of some point $\mathbf{x}_0 \in \mathbbm{R}^d$ and suppose there exists $M$ satisfying $|D^{\symbf\alpha}f| \leq M$ for all $x \in N_r(x_0)$ and all $\symbf{\alpha}$ such that $|\symbf{\alpha}| = k + 1$. Then $$f(\mathbf{x}) = \sum_{0 \leq |\symbf{\alpha}| \leq k} \frac{D^{\symbf{\alpha}} f(\mathbf{x}_0)}{\symbf{\alpha}!}(\mathbf{x} - \mathbf{x}_0)^{\symbf{\alpha}} + R_k(\mathbf{x}),$$ where the remainder term $R_k(\mathbf{x})$ satisfies $$|R_k(\mathbf{x})| \leq \frac{M}{\symbf{\alpha}!}|\mathbf{x} - \mathbf{x}_0|^{\symbf{\alpha}}$$ for all $\symbf{\alpha}$ such that $|\symbf{\alpha}| = k + 1$

\section{Jensen's Inequality}

For a real-valued random variable $X$ with finite expectation and a strictly concave function $\varphi$, $$\text{E}[\varphi(X)] < \varphi(\text{E}[X]).$$

\chapter{Chapter 5}\label{appendix:B}
\index{Appendix@\emph{Appendix B}}

\section{Desirable Properties of the Integrated Likelihood}

\subsection{Property 1}

Suppose the likelihood function for a parameter $\symbf{\theta}$ can be decomposed as the product $L( \symbf{\theta}) = L_1(\psi)L_2(\lambda)$. Then the integrated likelihood for $\psi$ should satisfy $$\bar{L}(\psi) = L_1(\psi).$$

\subsection{Property 2}

\subsection{Property 3}

\subsection{Property 4}

\section{Laplace's Method}

Let $\symbf{\theta}$ be a scalar parameter taking values in $\mathbbm{R}$ and consider an integral of the form $$I = \int_{-\infty}^{\infty} f(\symbf{\theta}) \exp[-nh(\symbf{\theta})]d\symbf{\theta}.$$ Suppose the function $-h(\symbf{\theta})$ is smooth, bounded, and unimodal so that it attains a maximum at a point $\hat{ \symbf{\theta}}$. Then Laplace's method states that an approximation for $I$ is given by $$\hat{I} = f(\hat{ \symbf{\theta}})\sqrt{\frac{2 \pi}{n}}\sigma \exp[-nh(\hat{\symbf{\theta}})],$$
where $$\sigma = \Bigg[\frac{\partial^2 h}{\partial \symbf{\theta}^2} \Bigg|_{\symbf{\theta} = \hat{\symbf{\theta}}} \Bigg]^{-1/2}.$$ It can further be shown that $$I = \hat{I}\Bigg\{1 + O\bigg(\frac{1}{n}\bigg)\Bigg\},$$ where the term $n$ may be interpreted as the sample size.



