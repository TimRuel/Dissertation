\appendix

\chapter{Chapter 3}\label{appendix:A}
\index{Appendix@\emph{Appendix A}}

\section{Definitions and Notation}

\subsection{Open and Closed Balls}

\subsubsection{Open Ball} The *open ball of radius* $r > 0$ centered at a point $p \in \mathbbm{R}^d$, is the set of all points $x \in \mathbbm{R}^d$ such that the distance between $p$ and $x$ is less than $r$ We denote this set using the notation $$B_r(\mathbf{p}) = \{\mathbf{x} \in \mathbbm{R}^d: ||\mathbf{x} - \mathbf{p}|| < r \},$$ where $||\cdot||$ indicates the Euclidean norm, i.e., $||\mathbf{x}|| = \Big(\sum_{i=1}^d x_i^2\Big)^{1/2}$ for $\mathbf{x} \in \mathbbm{R}^d$.

\subsubsection{Closed Ball} The *closed ball of radius* $r > 0$ centered at a point $\mathbf{p} \in \mathbbm{R}^d$, is the set of all points $\mathbf{x} \in \mathbbm{R}^d$ such that the distance between $\mathbf{p}$ and $\mathbf{x}$ is less than or equal to $r$. We denote this set using the notation $$B_r[\mathbf{p}] = \{\mathbf{x} \in \mathbbm{R}^d: \> ||\mathbf{x} - \mathbf{p}|| \leq r\}.$$

\subsection{Open and Closed Sets}

\subsubsection{Open Set} A subset $S$ of $\mathbbm{R}^d$ is called an *open set* of $\mathbbm{R}^d$ if every point in $S$ is the center of an open ball entirely contained in $S$. That is, $S$ is open if and only if for any $\mathbf{p} \in A$, there exists a radius $r > 0$ such that $B_r(\mathbf{p}) \subseteq S$.

\subsubsection{Closed Set} A subset $S$ of $\mathbbm{R}^d$ is called a *closed set* of $\mathbbm{R}^d$ if its complement $S^c = \mathbbm{R}^d \mathbin{\backslash} S$ is open.

\subsection{Neighborhoods}

A set $N_{\mathbf{p}} \subseteq \mathbbm{R}^d$ is called a *neighborhood* of a point $\mathbf{p} \in \mathbbm{R}^d$ if it contains an open ball centered at $\mathbf{p}$, i.e., for some radius $r > 0$ there exists an open ball $B_r(\mathbf{p})$ such that $B_r(\mathbf{p}) \subseteq N_{\mathbf{p}}$.

\subsection{Boundedness}

\subsubsection{Bounded Set} A set $S \subset \mathbb{R}^d$ is called *bounded* if there exists some radius $r > 0$ such that $B_r(\mathbf{0}) \subset S$.

\subsubsection{Bounded Function} A function $f: X \to \mathbbm{R}$ is called *bounded* if there exists a real number $M$ such that $|f(x)| \leq M$ for all $x \in X$.

\subsection{Compact Set}

A subset of $\mathbb{R}^d$ is called *compact* if it is closed and bounded.

\subsection{Interiority}

\subsubsection{Interior Point}

A point $\mathbf{p} \in S \subseteq \mathbbm{R}^d$ is called an *interior point* of $S$ if there exists some radius $r > 0$ such that $B_r(\mathbf{p}) \subseteq S$.

\subsubsection{Interior of a Set} The *interior of a set* $S \subset \mathbbm{R}^d$, denoted by int $S$, is the set of all interior points of $S$.

\subsection{Line Segments}

For two points $\mathbf{x}_1, \mathbf{x}_2 \in \mathbbm{R}^d$, a third point $\tilde{\mathbf{x}}$ is said to be on the *line segment* connecting $\mathbf{x}_1$ and $\mathbf{x}_2$ if there exists $\omega \in [0, 1]$ such that $\tilde{\mathbf{x}} = \omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2$.  We use the following notation to refer to such line segments: $$\text{LS}(\mathbf{x}_1, \mathbf{x}_2) = \{\omega\mathbf{x}_1 + (1- \omega)\mathbf{x}_2: \> \omega \in [0, 1]\}.$$

\subsection{Convexity and Concavity}

\subsubsection{Convex Set} A set $S \subseteq \mathbbm{R}^d$ is called *convex* if for any two points $\mathbf{x}_1, \mathbf{x}_2 \in S$, the line segment connecting $\mathbf{x}_1$ and $\mathbf{x}_2$ is entirely contained within $S$, i.e., $$\text{LS}(\mathbf{x}_1, \mathbf{x}_2) \subseteq S \text{ for all } \mathbf{x}_1, \mathbf{x}_2 \in S.$$

\subsubsection{Convex Function} Let $f: X \to \mathbb{R}$, where $X$ is a convex set. $f$ is called a *convex function* if for all $t \in [0, 1]$ and all $x_1, x_2 \in X$, $$f(t x_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2).$$ If it is possible to graph the function on the coordinate plane, this is equivalent to saying that the line segment between any two distinct points on the graph of the function lies above the graph.

$f$ is called *strictly convex* if the equality is tightened, i.e., $$f(t x_1 + (1-t)x_2) < tf(x_1) + (1-t)f(x_2).$$ 
\subsubsection{Concave Function} Let $f: X \to \mathbb{R}$, where $X$ is a convex set. $f$ is called a *concave function* if for all $t \in [0, 1]$ and all $x_1, x_2 \in X$, $$f(t x_1 + (1-t)x_2) \geq tf(x_1) + (1-t)f(x_2).$$ If it is possible to graph the function on the coordinate plane, this is equivalent to saying that the line segment between any two distinct points on the graph of the function lies below the graph.

$f$ is called *strictly concave* if the equality is tightened, i.e., $$f(t x_1 + (1-t)x_2) > tf(x_1) + (1-t)f(x_2).$$ 

\subsection{Positive Definiteness}

A $d \times d$ symmetric real matrix $M$ is called *positive definite* if $\mathbf{x}^\top M \mathbf{x} > 0$ for all non-zero $\mathbf{x} \in \mathbbm{R}^d$.

\subsection{Derivatives of Multivariable Functions}

\subsubsection{Gradient}

The *gradient* of a multivariable scalar-valued function $f: \mathbbm{R}^d \to \mathbbm{R}$ is given by its $d\times 1$ vector of partial derivatives:
$$\nabla f(x_1, x_2, ..., x_d) = 
\begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_d}
\end{pmatrix}.$$

\subsubsection{Jacobian Matrix}

The *Jacobian matrix* of a multivariable vector-valued function $\mathbf{f}: \mathbbm{R}^d \to \mathbbm{R}^k$ given by
$$\mathbf{f}(x_1, x_2, ..., x_d) = 
\begin{pmatrix}
f_1(x_1, x_2, ..., x_d) \\
f_2(x_1, x_2, ..., x_d) \\
\vdots \\
f_k(x_1, x_2, ..., x_d)
\end{pmatrix}.
$$ is defined as its $k \times d$ matrix of partial derivatives:
$$\mathbf{J}(\mathbf{f}) = 
\begin{pmatrix} 
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_d} \\ 
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial f_k}{\partial x_1} & \frac{\partial f_k}{\partial x_2} & \cdots & \frac{\partial f_k}{\partial x_d}
\end{pmatrix} =
\begin{pmatrix}
\nabla^\top f_1 \\
\vdots \\
\nabla^\top f_k
\end{pmatrix}
$$ In the case where $k = 1$, the Jacobian matrix simply reduces to $\nabla^\top f$, the transpose of the gradient of $\mathbf{f}$.

\subsubsection{Hessian Matrix} The Hessian matrix of a multivariable scalar-valued function $f: \mathbbm{R}^d \to \mathbbm{R}$ is given by its $d\times d$ matrix of second partial derivatives:
$$\mathbf{H}(f) = 
\begin{pmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2  f}{\partial x_1\partial x_2} & \cdots & \frac{\partial^2  f}{\partial x_1\partial x_d} \\ 
\frac{\partial^2  f}{\partial  x_2 \partial x_1} & \frac{\partial^2  f}{\partial x_2^2} & \cdots & \frac{\partial^2  f}{\partial x_2\partial x_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2  f}{\partial  x_d \partial x_1} & \frac{\partial^2  f}{\partial x_d \partial x_2} & \cdots & \frac{\partial^2  f}{\partial x_d^2}
\end{pmatrix}
$$ The *Hessian matrix* of $f$ is also equal to the transpose of the Jacobian matrix of the gradient of $f$, i.e., $\mathbf{H}(f) = \mathbf{J}(\nabla f)^\top$. When the order of differentiation does not matter, which occurs if and only if all of $f$'s second partial derivatives are continuous, both matrices become symmetric and we simply have $\mathbf{H}(f) = \mathbf{J}(\nabla f)$. 

A twice differentiable function of several variables is strictly convex (concave) on a convex set if and only if its Hessian matrix is positive (negative) definite on the interior of the convex set.

\section{Theorems}

\subsection{Jensen's Inequality}

For a real-valued random variable $X$ with finite expectation and a strictly concave function $\varphi$, $$\text{E}[\varphi(X)] < \varphi(\text{E}[X]).$$

\subsection{The Extreme Value Theorem}

If $K$ is a compact set and $f: K \to \mathbb{R}$ is a continuous function, then $f$ is bounded and there exists $p, q \in K$ such that $$f(p) = \sup_{x \in K}f(x)$$ and $$f(q) = \inf_{x \in K}f(x).$$

\subsection{Taylor's Theorem}

Suppose $f: \mathbbm{R}^d \to \mathbbm{R}$ is twice continuously differentiable in a neighborhood $N_{\mathbf{x}_0}$ of some point $\mathbf{x}_0 \in \mathbbm{R}^d$. Then for any $\mathbf{x} \in N_{\mathbf{x}_0}$, there exists $\tilde{\mathbf{x}} \in \text{LS}(\mathbf{x}, \mathbf{x}_0)$ such that $$f(\mathbf{x}) = f(\mathbf{x}_0) + (\mathbf{x} - \mathbf{x}_0) \nabla f(\mathbf{x}) + \frac{1}{2}(\mathbf{x} - \mathbf{x}_0)\textbf{H}\big(f(\tilde{\mathbf{x}})\big)(\mathbf{x} - \mathbf{x}_0)^\top,$$ where $\textbf{H}\big(f(\tilde{\mathbf{x}})\big)$ denotes the Hessian matrix of $f(\mathbf{x})$ evaluated at $\mathbf{x} = \tilde{\mathbf{x}}$.

This is not Taylor's theorem in its full generality, but rather a particular case of it that is well-suited for use in this paper. More accurately, we could refer to it as a second-order Taylor series expansion of a multivariable scalar-valued function with the Lagrange form of the remainder.

\subsection{Slutsky's Theorem}

Let $X_n,Y_n$ be sequences of scalar, vector, or matrix random variables. If $X_n \overset{d}{\to} X$, where $X$ is another random variable, and $Y_n \overset{p}{\to} c$, where $c$ is a constant, then
\begin{enumerate}[label = \roman*)]
  \item $X_n + Y_n \overset{d}{\to} X + c$;
  \item $X_nY_n \overset{d}{\to} Xc$;
  \item $X_n / Y_n \overset{d}{\to} X / c$, assuming $c$ is invertible.
\end{enumerate}

As an extension, suppose $\mathbf{X}_n \overset{d}{\to} \mathbf{X}$, where $\mathbf{X}$ is a $d \times 1$ vector, $\mathbf{A}_n \overset{p}{\to} \mathbf{A}$, where $\mathbf{A}$ is a positive definite matrix, and $\mathbf{X}_n = \mathbf{A}_n \mathbf{Y}_n$. Then $$\mathbf{Y}_n \overset{d}{\to} \mathbf{A}^{-1}\mathbf{X}.$$

\chapter{Chapter 5}\label{appendix:B}
\index{Appendix@\emph{Appendix B}}

\section{Desirable Properties of the Integrated Likelihood}

@severini2007 considered the following four statements to be essential properties that any integrated likelihood function must satisfy if it is to be useful for the purpose of non-Bayesian inference. 

\subsection{Property 1}

Suppose the likelihood function for a parameter $\symbf{\theta} = (\psi, \lambda)$ can be decomposed as the product $L(\symbf{\theta}) = L_1(\psi)L_2(\lambda)$. Then the integrated likelihood for $\psi$ should satisfy $$\bar{L}(\psi) = L_1(\psi).$$ 

\subsection{Property 2}

The frequency properties of an integrated likelihood function should mirror that of a genuine likelihood function as the sample size tends to infinity. In particular, $\bar{L}(\psi)$ should be be approximately score- and information-unbiased, i.e., asymptotically satisfy the first and second Bartlett identities, respectively. 

\subsection{Property 3}

The integrated likelihood function should be insensitive to the choice of prior density.

\subsection{Property 4}

The integrated likelihood function should be invariant with respect to reparameterizations of the model that leave the parameter of interest unchanged.

\section{Laplace's Method}

Let $\symbf{\theta}$ be a scalar parameter taking values in $\mathbbm{R}$ and consider an integral of the form $$I = \int_{-\infty}^{\infty} f(\symbf{\theta}) \exp[-nh(\symbf{\theta})]d\symbf{\theta}.$$ Suppose the function $-h(\symbf{\theta})$ is smooth, bounded, and unimodal so that it attains a maximum at a point $\hat{ \symbf{\theta}}$. Then Laplace's method states that an approximation for $I$ is given by $$\hat{I} = f(\hat{ \symbf{\theta}})\sqrt{\frac{2 \pi}{n}}\sigma \exp[-nh(\hat{\symbf{\theta}})],$$
where $$\sigma = \Bigg[\frac{\partial^2 h}{\partial \symbf{\theta}^2} \Bigg|_{\symbf{\theta} = \hat{\symbf{\theta}}} \Bigg]^{-1/2}.$$ It can further be shown that $$I = \hat{I}\Bigg\{1 + O\bigg(\frac{1}{n}\bigg)\Bigg\},$$ where the term $n$ may be interpreted as the sample size.



