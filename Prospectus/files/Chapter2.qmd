\chapter{Experiments and Statistical Models}

The acquisition of knowledge regarding a population of interest has long been the impetus for the field of statistical inference. In all but the most basic of circumstances, limiting constraints such as time, accessibility, and cost make perfect knowledge of a population essentially impossible to obtain. It therefore becomes necessary to infer characteristics of the population based on a random and representative sample of observations drawn from it. The procedures by which these samples might be procured are themselves far from trivial, and indeed an entire branch of statistics has been dedicated to their study. However, we are primarily concerned in this paper with what occurs after the sample has been taken, and so we will generally take it for granted that a suitably representative sample of the population already exists.

Suppose $(x_1, ..., x_n)$ is one such sample. What information can we then glean from this sample about the population from which it has been drawn? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? As the questions increase in complexity, so do the techniques required to answer them. Unfortunately the natural chaos of the real world all but guarantees there will never be an instrument capable of completely capturing the intricacies of a population whose properties we wish to infer. Hence, some amount of idealization will always be required in order to proceed. 

This idealization typically comes in the form of additional assumptions that we impose on the population with the goal of sacrificing what we hope is only a small amount of accuracy in exchange for a large reduction in complexity. These assumptions are essentially never "true" in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations that are capable of producing answers with degrees of accuracy high enough to be useful in their own right. Taken as a whole, they form the basis for a statistical model. 

A statistical model is best understood through the lens of what is known as an *experiment*. From a probabilistic perspective, an experiment refers to any procedure that could theoretically be replicated an infinite number of times and in which each replication results in exactly one outcome taken. The set of possible outcomes of an experiment, known as the *sample space*, must be well-defined, meaning that 

assuming that there exists an unknown probability distribution $P$ over the population of interest that generates the data $(x_1, ..., x_n)$ we observe from it. 

We can think of this observed data as being the realized outcomes ("realizations") of some random variable $X$ that is distributed according to $P$. We will restrict our attention in this paper to distributions that are either absolutely continuous or discrete, meaning they admit a probability density or probability mass function, respectively, over their support. For the sake of brevity, we will refer to all such functions as density functions, with the understanding that it is actually a probability mass function in the event that $X$ is discrete. Let $p(x)$ denote the unknown density function associated with $P$ and $\mathcal{P}$ the set of functions that we are willing to consider as candidates for $p(x)$. Out of necessity, we will proceed as though our choice of $\mathcal{P}$ always contains $p(x)$ though in reality there is nothing specifically requiring it.

We will also assume the set $\mathcal{P}$ is *parameterized*. That is, we assume there exists a *parameter* $\theta$ which indexes $\mathcal{P}$, acting as a label that allows us to differentiate between the densities it contains. For a particular value of the parameter $\theta$, say $\theta_1$, we can refer to its corresponding density in $\mathcal{P}$ with the notation $p(\cdot; \theta_1)$, and therefore $\mathcal{P}$ itself may be written as $\mathcal{P} = \{p(\cdot; \theta) | \> \theta \in \Theta\}$. $\Theta$ is called the *parameter space* and represents the set of all possible values $\theta$ can take on. 

In general, a model's parameterization is not unique, and for a given parameter $\theta$, we are free to choose any one-to-one function of $\theta$ as a new parameter. Once we have made our choice of parameterization, we will assume that $\Theta$ does contain a singular true parameter value, which we will denote by $\theta_0$. The conventional interpretation of $\theta_0$ is as a fixed but unknown constant that represents the value of the parameter corresponding to the true density function in $\mathcal{P}$. Conversely, $\theta$ represents an arbitrary parameter value (corresponding to an arbitrary density in $\mathcal{P})$ that is allowed to range over all possible elements of $\Theta$, including $\theta_0$. In other words, $\theta$ acts like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Making inferences regarding $\theta_0$ is like trying to figure out the particular value ($\theta_0$) to which a population's dial ($\theta$) has been set. Note that it is possible for the value of $\theta_0$ itself to change over time as well, depending on the population. In such cases, any estimate of $\theta_0$ based on a cross-sectional sample drawn from the population is best thought of as an estimate of the true parameter value during the particular time in which the sample was collected.

Crucially, it must always be possible to identify the parameter in our model on the basis of the data we observe. A model is considered *identifiable* if having perfect knowledge of the population would enable us to determine $\theta_0$ with absolute certainty. This is equivalent to requiring that for some observed data $x$ and any two parameters $\theta_1, \theta_2 \in \Theta$, if $p(x; \theta_1) = p(x; \theta_2)$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. For example, suppose $Y$ is distributed uniformly on the interval $(0, \alpha + \beta)$, where $\alpha, \beta > 0$. If we use $\theta = (\alpha, \beta)$ as a parameter for the distribution of $Y$, then $\theta$ is unidentifiable since, for instance, the case where $\theta_1 = (0, 1)$ and $\theta_1 = (1, 0)$ implies that $p(y; \theta_1) = p(y; \theta_2)$ despite the fact that $\theta_1 \neq \theta_2$. This is obviously an undesirable property for a model to possess, and so we will consider only identifiable models in this paper as a means of avoiding it.

Finally, we must make a choice regarding the dimension of the parameter space $\Theta$ when formulating our models. *Parametric* models are defined as having finite-dimensional parameter spaces. Any model that is not parametric is called *nonparametric*. We will restrict our attention in this paper solely to parametric models whose parameter spaces are subsets of $\mathbb{R}^d$, where $d \in \mathbb{Z}^+$.
