\chapter{Experiments and Statistical Models}

The acquisition of knowledge regarding a population of interest has long been the impetus for the field of statistical inference. In all but the most basic of circumstances, limiting constraints such as time, accessibility, and cost make perfect knowledge of a population essentially impossible to obtain. It therefore becomes necessary to infer characteristics of the population based on a random and representative sample of observations drawn from it. The procedures by which these samples might be procured are themselves far from trivial, and indeed an entire branch of statistics has been dedicated to their study. However, we are primarily concerned in this paper with what occurs after the sample has been taken, and so we will generally take it for granted that a suitably representative sample of the population already exists.

Suppose $(x_1, ..., x_n)$ is one such sample. What information can we then glean from this sample about the population from which it has been drawn? Where is its point of central tendency located? Are its values clustered tightly around this point, or are they more diffuse? Are they distributed symmetrically or skewed to one side or the other? As the questions increase in complexity, so do the techniques required to answer them. Unfortunately the natural chaos of the real world all but guarantees there will never be an instrument capable of completely capturing the intricacies of a population whose properties we wish to infer. Hence, some amount of idealization will always be required in order to proceed. 

This idealization typically comes in the form of additional assumptions that we impose on the population with the goal of sacrificing what we hope is only a small amount of accuracy in exchange for a large reduction in complexity. These assumptions are essentially never "true" in the sense that they are not a flawless representation of reality, but they may nevertheless serve as convenient approximations that are capable of producing answers with degrees of accuracy high enough to be useful in their own right. Taken as a whole, they form the basis for a statistical model. 

The traditional framework for a statistical model begins by assuming that there exists an unknown probability distribution $P$ over the population of interest that generates the data we observe from it. We choose to model this observed data as being the realized outcomes ("realizations") of some random variable $X$ that is distributed according to $P$. Let $\mathcal{P}$ denote the set of all such distributions that we are willing to consider as candidates for the true distribution $P$. Out of necessity, we will proceed as though our choice of $\mathcal{P}$ always contains $P$ though in reality there is nothing specifically requiring it.

The next assumption we make is that $\mathcal{P}$ is *parameterized*. That is, there exists a *parameter* $\theta$ which indexes $\mathcal{P}$, acting as a label that allows us to differentiate between the distributions it contains. For a particular value of the parameter $\theta$, say $\theta_1$, we can refer to its corresponding distribution in $\mathcal{P}$ with the notation $P_{\theta_1}$, and therefore $\mathcal{P}$ itself may be written as $\mathcal{P} = \{P_{\theta} | \> \theta \in \Theta\}$. $\Theta$ is called the *parameter space* and represents the set of all possible values $\theta$ can take on. 

We will restrict our attention in this paper to distributions that are absolutely continuous with respect to some $\sigma$-finite measure $\mu$, so that they admit a probability density function by the Radon-Nikodym Theorem. Let $p_{\theta}$ denote the density function associated with the distribution $P_{\theta}$. This one-to-one correspondence between distribution and density allows us to define $\mathcal{P}$ as $\mathcal{P} = \{p_{\theta} | \> \theta \in \Theta\}$. Going forward, we will use the notation $p_{\theta}(x)$ and $p(x; \theta)$ interchangeably to refer to a density function with parameter $\theta$. We will also simply write $\int p_{\theta}(x)dx$ instead of $\int p_{\theta} d\mu$ or $\int p_{\theta}(x)d\mu(x)$ with the understanding that $p_{\theta}$ is always defined with respect to some dominating measure $\mu$.

In general, a model's parameterization is not unique, and for a given parameter $\theta$, we are free to choose any invertible function of $\theta$ as a new parameter. Once we have made our choice of parameterization, we will assume that $\Theta$ does contain a singular true parameter value, which we will denote by $\theta_0$. The conventional interpretation of $\theta_0$ is as a fixed but unknown constant that represents the value of the parameter corresponding to the true density function $p_{\theta_0}$ in $\mathcal{P}$. Conversely, $\theta$ represents an arbitrary parameter value that is allowed to range over all possible elements of $\Theta$, including $\theta_0$. In other words, $\theta$ acts like a tuning dial for the population - rotate the dial and certain behaviors of the population (e.g. its location, scale, or shape) will change. Making inferences regarding $\theta_0$ is like trying to figure out the particular value ($\theta_0$) to which a population's dial ($\theta$) has been set. Note that it is possible for the value of $\theta_0$ itself to change over time as well, depending on the population. In such cases, any estimate of $\theta_0$ based on a cross-sectional sample drawn from the population is best thought of as an estimate of the true parameter value during the particular time in which the sample was collected.

Crucially, it must always be possible to identify the parameter in our model on the basis of the data we observe. A model is considered *identifiable* if having perfect knowledge of the population would enable us to determine $\theta_0$ with absolute certainty. This is equivalent to requiring that for some observed data $x$ and any two parameters $\theta_1, \theta_2 \in \Theta$, if $p_{\theta_1}(x) = p_{\theta_2}(x)$, then it must follow that $\theta_1 = \theta_2$. A model that is not identifiable could potentially have two or more distinct parameter values that give rise to the same probability distribution. For example, suppose $Y$ is distributed uniformly on the interval $(0, \alpha + \beta)$, where $\alpha, \beta > 0$. If we use $\theta = (\alpha, \beta)$ as a parameter for the distribution of $Y$, then $\theta$ is unidentifiable since, for instance, the case where $\theta_1 = (0, 1)$ and $\theta_1 = (1, 0)$ implies that $p_{\theta_1}(y) = p_{\theta_2}(y)$ despite the fact that $\theta_1 \neq \theta_2$. This is clearly an undesirable property for a model to possess, and so we will consider only identifiable models in this paper as a means of avoiding it.

Finally, we must make a choice regarding the dimension of the parameter space $\Theta$ when formulating our models. *Parametric* models are defined as having finite-dimensional parameter spaces. Any model that is not parametric is either *semi-parametric* or *nonparametric*. In this paper, we will consider only parametric models whose parameter spaces are subsets of the $d$-dimensional real coordinate space, i.e., $\Theta \subseteq \mathbbm{R}^d$, where $d \in \mathbbm{Z}^+$.
