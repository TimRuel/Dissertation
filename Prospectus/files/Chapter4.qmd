\chapter{Analyzing the Integrated Likelihood Function}

\section{Two-Index Asymptotics}

Earlier we discussed the one-index asymptotics setting, in which the sample size ($n$) of the model diverged to infinity while the dimension of the nuisance parameter ($q$) remained fixed. Now we turn our attention to the two-index asymptotics setting which describes the behavior of likelihood and pseudolikelihood functions as $n$ and $q$ both tend to infinity, with $q$ growing at least as fast as $n$. Under such a framework, @DeBin2015 showed that estimates for $\psi$ based on a suitably constructed integrated likelihood function will outperform those coming from more traditional pseudolikelihoods, such as the profile likelihood. Such findings provide the motivation for our ensuing examination of two-index asymptotics theory, insofar as it relates to the performance of the integrated likelihood function as a method of inference regarding a parameter of interest.

To mirror the strategy used by @Sartori2003 and @DeBin2015, we will frame our discussion in terms of a stratified sample of data in which each stratum contributes one component to the overall nuisance parameter. Consider a model with parameter $\symbf{\theta} = (\psi, \lambda)$ where $\psi$ is the parameter of interest and $\lambda = (\lambda_1, ..., \lambda_q)$ is a $q$-dimensional nuisance parameter. For the sake of reducing complexity in our notation, we will only consider the case in which $\psi$ and the individual components of $\lambda$ are scalar parameters, though the results of this section should hold in the case where they are all vectors as well. Suppose that we have divided the model's population into $q$ strata and collected a sample of size $m_i$ from each stratum such that observation $j$ from stratum $i$ may be modeled as $$X_{ij} \sim p_{ij}(x_{ij}; \psi, \lambda_i),$${#eq-2idx_model} where $i = 1, ..., q$ and $j = 1, ..., n_i$, making the total sample size $n = \sum_{i = 1}^q m_i$.^[It will be convenient to work under the restriction that the stratum sample sizes are all identical, meaning there exists some positive integer $m$ such that $m_i = m$ for all $i$. However, as both @Sartori2003 and @DeBin2015 note, we could also assume a looser condition in which $m_i = K_i m$ where $0 < K_i < \infty$ without compromising our results.] Hence, there is a one-to-one correspondence between the strata and the components of $\lambda$; assume that each $\lambda_i \in \Lambda$, where the space $\Lambda$ is the same for all $i$, and they all have the same interpretation within their respective strata.

Assume that all the regularity conditions set forth in Section 3.4 apply, except possibly for **RC1** - it is not necessary to assume that the observations are i.i.d. here, and in fact it is perfectly acceptable for the $p_{ij}$'s in @eq-2idx_model to differ from one another. We will also allow for the possibility of dependence among observations within a stratum, though not between them. 

Let $\mathbf{x}_i = (x_{i1}, ..., x_{im})$ denote the sample of observations from stratum $i$, so that their joint density may be written as $p_i(\mathbf{x}_i; \psi, \lambda_i)$. Therefore, the likelihood and log-likelihood for the $i$th stratum are $$L^{(i)}(\psi, \lambda_i) = p_i(\mathbf{x}_i; \psi, \lambda_i),$${#eq-2idx_L_i} and $$\ell^{(i)}(\psi, \lambda_i) = \log L^{(i)}(\psi, \lambda_i),$${#eq-2idx_l_i} respectively. For a particular choice of weight function $g(\lambda_i; \psi)$, the integrated likelihood for $\psi$ in stratum $i$ is given by $$\bar{L}^{(i)}(\psi) = \int_\Lambda L^{(i)}(\psi, \lambda_i) g(\lambda_i; \psi)d\lambda_i.$${#eq-2idx_Lbar_i} 

From here, we proceed by using Laplace's method as described by @Tierney1986 (see Appendix B for a brief review) to obtain an analytic approximation to $\bar{L}^{(i)}(\psi)$. Setting $h(\lambda_i) = -\frac{1}{m} \ell^{(i)}(\psi, \lambda_i)$ and $f(\lambda_i) = g(\lambda_i; \psi)$, we may rewrite the integral in @eq-2idx_Lbar_i as $$\bar{L}^{(i)}(\psi) = \int_{\Lambda} f(\lambda_i) \exp[-mh(\lambda_i)]d\lambda_i.$$ One consequence of the regularity conditions we have assumed is that $L^{(i)}(\psi, \lambda_i)$ is that an MLE for $\symbf{\theta}_0$, $\hat{\symbf{\theta}}$, exists and is unique. This further implies the existence and uniqueness of a conditional MLE for the true value of each stratum-specific nuisance parameter given $\psi$ - denote this value for the $i$th stratum by $\hat{\lambda}_{i\psi}$. By definition this value maximizes $\ell^{(i)}(\psi, \lambda_i)$ as a function of $\lambda_i$, and so it also maximizes $-h(\lambda_i)$ since the two functions differ only by a multiplicative constant $\frac{1}{m}$. 

The Laplace approximation to $\bar{L}^{(i)}(\psi)$ is then given by

$$\hat{\bar{L}}^{(i)}(\psi) = f(\hat{\lambda}_{i\psi})\sqrt{\frac{2 \pi}{m}}\sigma \exp[-mh(\hat{\lambda}_{i\psi})],$${#eq-Laplace1} where 
$$
\begin{aligned}
\sigma &= \Bigg[\frac{\partial^2 h}{\partial \lambda_i^2} \Bigg|_{\lambda_i = \hat{\lambda}_{i\psi}} \Bigg]^{-1/2} \\
       &= \Bigg[-\frac{1}{m}\frac{\partial^2 \ell^{(i)}(\psi, \lambda_i)}{\partial \lambda_i^2} \Bigg|_{\lambda_i = \hat{\lambda}_{i\psi}} \Bigg]^{-1/2} \\
       &= \Bigg[\frac{1}{m} \mathcal{I}(\hat{\lambda}_{i\psi})\Bigg]^{-1/2}.
\end{aligned}$$
Here, $\mathcal{I}(\hat{\lambda}_{i\psi})$ denotes the observed information function for $\lambda_i$ only (i.e. the negative second partial derivative of the log-likelihood with respect to $\lambda_i$) evaluated at $\hat{\lambda}_{i\psi}$. Plugging in the appropriate quantities for $f$, $h$, and $\sigma$ into @eq-Laplace1, we arrive at 
$$
\begin{aligned}
\hat{\bar{L}}^{(i)}(\psi) &= f(\hat{\lambda}_{i\psi})\sqrt{\frac{2 \pi}{m}}\sigma \exp[-mh(\hat{\lambda}_{i\psi})] \\
                          &= g(\hat{\lambda}_{i\psi}; \psi)\sqrt{\frac{2 \pi}{m}} \Bigg[\frac{1}{m} \mathcal{I}(\hat{\lambda}_{i\psi})\Bigg]^{-1/2} \exp\Big\{-m \cdot -\frac{1}{m} \ell^{(i)}(\psi, \hat{\lambda}_{i\psi})\Big\} \\
                          &= \frac{\sqrt{2 \pi}}{m} L^{(i)}(\psi, \hat{\lambda}_{i\psi})g(\hat{\lambda}_{i\psi}; \psi)\big[\mathcal{I}(\hat{\lambda}_{i\psi})\big]^{-1/2} \\
                          &= \frac{\sqrt{2 \pi}}{m} L_P^{(i)}(\psi)g(\hat{\lambda}_{i\psi}; \psi)\big[\mathcal{I}(\hat{\lambda}_{i\psi})\big]^{-1/2},
\end{aligned}
$${#eq-Laplace2} where $L_P^{(i)}(\psi) = L^{(i)}(\psi, \hat{\lambda}_{i\psi})$ is the profile likelihood for $\psi$. The error in this approximation is $$\bar{L}^{(i)}(\psi) = \hat{\bar{L}}^{(i)}(\psi)\Bigg\{1 + O\bigg(\frac{1}{m}\bigg)\Bigg\} \> \>\text{ as } \>  \> m \to \infty.$${#eq-Laplace3}

Let $$\bar{\ell}^{(i)}(\psi) = \log \bar{L}^{(i)}(\psi)$${#eq-2idx_lbar_i} denote the integrated log-likelihood for $\psi$. Putting the results in @eq-Laplace2, @eq-Laplace3, and @eq-2idx_lbar_i together, we have
$$
\begin{aligned}
\bar{\ell}^{(i)}(\psi) &= \log \bar{L}^{(i)}(\psi) &&(\text{by Equation 4.2.8})\\
                       &= \log\Bigg(\hat{\bar{L}}^{(i)}(\psi)\Bigg\{1 + O\bigg(\frac{1}{m}\bigg)\Bigg\}\Bigg) &&(\text{by Equation 4.2.7})\\
                       &= \log \hat{\bar{L}}^{(i)}(\psi) + \log\Bigg\{1 + O\bigg(\frac{1}{m}\bigg)\Bigg\} &&(\text{by Equation 4.2.6})\\
                       &= \log\Bigg\{\frac{\sqrt{2 \pi}}{m} L_P^{(i)}(\psi)g(\hat{\lambda}_{i\psi}; \psi)\big[\mathcal{I}(\hat{\lambda}_{i\psi})\big]^{-1/2} \Bigg\} + O\bigg(\frac{1}{m}\bigg) \\
                       &= \frac{1}{2}\log(2\pi) - \log(m) + \log L_P^{(i)}(\psi) + \log g(\hat{\lambda}_{i\psi}; \psi) - \frac{1}{2}\log \mathcal{I}(\hat{\lambda}_{i\psi}) + O\bigg(\frac{1}{m}\bigg) \\
                       &= \ell_P^{(i)}(\psi) + \log g(\hat{\lambda}_{i\psi}; \psi) - \frac{1}{2}\log \mathcal{I}(\hat{\lambda}_{i\psi}) + \frac{1}{2}\log(2\pi) - \log(m) + O\bigg(\frac{1}{m}\bigg) \> \>\text{ as } \>  \> m \to \infty,
\end{aligned}
$$
where $\ell_P^{(i)}(\psi) = \ell^{(i)}(\psi, \hat{\lambda}_{i\psi})$ is the profile log-likelihood for $\psi$. Since log-likelihoods are equivalent up to additive constants, we can discard the $\frac{1}{2} \log(2\pi)$ and $\log(m)$ terms in the final line above to arrive at our final approximation for the integrated log-likelihood in stratum $i$: $$\hat{\bar{\ell}}^{(i)}(\psi) = \ell_P^{(i)}(\psi) + \log g(\hat{\lambda}_{i\psi}; \psi) - \frac{1}{2}\log \mathcal{I}(\hat{\lambda}_{i\psi}).$${#eq-Laplace4} The error in this approximation is given by $$\bar{\ell}^{(i)}(\psi) = \hat{\bar{\ell}}^{(i)}(\psi) + O\bigg(\frac{1}{m}\bigg).$${#eq-Laplace5}

Since the observations between the strata are independent, we may write the likelihood and log-likelihood functions for the entire model as $$L(\psi, \lambda) = \prod_{i=1}^q L^{(i)}(\psi, \lambda_i)$${#eq-2idx_L} and $$\ell(\psi, \lambda) = \sum_{i=1}^q \ell^{(i)}(\psi, \lambda_i),$${#eq-2idx_l} respectively. If we define the weight function $$G(\lambda; \psi) \equiv \prod_{i=1}^qg(\lambda_i; \psi)$${#eq-2idx_weight} then the integrated likelihood function for $\psi$ becomes separable. That is,
$$
\begin{aligned}
\bar{L}(\psi) &= \int_{\Lambda^q}L(\psi, \lambda) G(\lambda; \psi)d\lambda \\
              &= \int_\Lambda\cdots\int_\Lambda \Bigg[\prod_{i=1}^q L^{(i)}(\psi, \lambda_i)\Bigg] \Bigg[\prod_{i=1}^q g(\lambda_i; \psi)\Bigg]d\lambda_1 \cdots d\lambda_q \\
              &= \prod_{i=1}^q \int_\Lambda L^{(i)}(\psi, \lambda_i)g(\lambda_i; \psi)d\lambda_i \\
              &= \prod_{i=1}^q \bar{L}^{(i)}(\psi).
\end{aligned}
$${#eq-2idx_Lbar}

Let $\bar{\ell}(\psi) = \log \bar{L}(\psi)$ denote the integrated log-likelihood function for $\psi$. Taking the logarithm of both sides in @eq-2idx_Lbar, we have $$\bar{\ell}(\psi) = \sum_{i=1}^q \bar{\ell}^{(i)}(\psi).$${#eq-2idx_lbar} Plugging in our approximation to $\bar{\ell}^{(i)}(\psi)$ and its error term in @eq-Laplace4 and @eq-Laplace5, respectively, yields 
$$
\begin{aligned}
\bar{\ell}(\psi) &= \sum_{i=1}^q\Bigg[\ell_P^{(i)}(\psi) + \log g(\hat{\lambda}_{i\psi}; \psi) - \log \mathcal{I}(\hat{\lambda}_{i\psi}) + O\bigg(\frac{1}{m}\bigg) \Bigg] \\
                 &= \ell_P(\psi) + \sum_{i=1}^q \log g(\hat{\lambda}_{i\psi}; \psi) - \frac{1}{2}\sum_{i=1}^q\log \mathcal{I}(\hat{\lambda}_{i\psi}) + O\bigg(\frac{q}{m}\bigg) \> \>\text{ as } \>  \> m \to \infty.
\end{aligned}
$${#eq-Laplace6}

\section{The Zero-Score Expectation Parameter}

@severini2007 considered the problem of selecting a weight function $\pi(\lambda|\psi)$ such that when the likelihood function is integrated with respect to this density over the nuisance parameter space, the result is useful for non-Bayesian inference. To do this, he outlined four properties (see Appendix \ref{appendix:B}) that an integrated likelihood must satisfy if it is to be of any use and went on to show that such a function could be obtained by doing the following:

\begin{enumerate}[label = \arabic*)]
  \item Find a reparameterization $(\psi, \lambda) \mapsto (\psi, \phi)$ of the model such that the new nuisance parameter $\phi$ is unrelated to $\psi$ in the sense that $\hat{\phi}_{\psi} = \hat{\phi}$; that is, the conditional maximum likelihood estimate of $\phi$ given $\psi$ is simply equal to the unrestricted maximum likelihood estimate of $\phi$.
  \item Select a prior density $\pi(\lambda|\psi)$ such that when the model undergoes the above reparameterization, the resulting prior density $\pi(\phi)$ does not depend on $\psi$.
\end{enumerate}

An integrated likelihood function for $\psi$ that possesses the desired properties will then be given by $$\bar{L}(\psi) = \int_{\Phi} \tilde{L}(\psi, \phi) \pi(\phi) d\phi,$${#eq-ZSE_IL1} where $\tilde{L}(\psi, \phi)$ is the likelihood function for the model after it has been reparameterized in terms of $\phi$. The exact choice of prior density for $\phi$ is not particularly important; the only restriction placed upon it is that it must not depend on $\psi$. Hence, the crux of the matter really lies in completing the first step. The approach taken by @severini2007 is to define a new nuisance parameter $\phi$ as the solution to the equation $$\text{E}_{(\psi_0, \lambda_0)}\Big[\nabla_{\lambda}\ell(\psi, \lambda; \mathbf{X}_n)\Big]\Bigg|_{(\psi_0, \lambda_0) = (\hat{\psi}, \phi)} = \mathbf{0},$${#eq-ZSE_IL2} where $\psi_0$ and $\lambda_0$ denote the true values of $\psi$ and $\lambda$, and $\hat{\psi}$ is the unrestricted MLE for $\psi_0$. The expectation here is being taken with respect to the data $\mathbf{X}_n = \mathbf{x}_n$ and not the parameters themselves. @eq-ZSE_IL2 can thus be rewritten as $$I(\psi, \lambda, \hat{\psi}, \phi) = \mathbf{0},$$ where $$I(\psi, \lambda, \psi_0, \lambda_0) = \int_{\mathbbm{R}^n} \big[\nabla_{\lambda}\ell(\psi, \lambda; \mathbf{x}_n)\big] p(\mathbf{x}_n; \psi_0, \lambda_0))d\mathbf{x}_n.$$

Assuming $I$ is invertible, for a particular value of $(\psi, \lambda, \hat{\psi})$, there will be a unique value of $\phi$ that solves @eq-ZSE_IL2. $\phi$ is called the *zero-score expectation* (ZSE) parameter because it is defined as the value that makes the expectation of the score function (in terms of $\lambda$, not the full parameter) with respect to $p(\mathbf{x}_n; \psi_0, \lambda_0)$ evaluated at the point $(\psi_0, \lambda_0) = (\hat{\psi}, \phi)$ equal to zero. This means that $\phi$ is really a function of $(\psi, \lambda, \hat{\psi})$, i.e., $\phi = \phi(\psi, \lambda, \hat{\psi})$. This in turn implies that $\phi$ is a function of the data through $\hat{\psi}$. Normally we try to avoid creating such dependencies in our parameters as it renders them useless for the purpose of parameterizing a statistical model. However, from the perspective of the likelihood function, once the data have been collected they are considered fixed in place and there is no issue with using a quantity such as $\phi$ that depends on the data to parameterize it.

For a given value of $\phi$, the corresponding value of $\lambda$ can be found by $$\lambda(\psi, \phi) = \underset{\lambda \in \Lambda}{\mathrm{argmax}} \> \text{E}_{(\hat{\psi}, \phi)}\big[\ell(\psi, \lambda)\big].$${#eq-ZSE_IL3} For a certain choice of prior density $\pi(\phi)$, this allows us to write @eq-ZSE_IL1 in terms of $L(\psi, \lambda)$: $$\bar{L}(\psi) = \int_{\Phi} L(\psi, \lambda(\psi, \phi)) \pi(\phi) d\phi.$${#eq-ZSE_IL4} 

The procedure described above is  However, @severini2018 proved that reparameterizing the model in terms of the ZSE parameter yields the same nice properties in the subsequent integrated likelihood when $\psi$ and $\lambda$ are implicit. Suppose $\psi$ = $\varphi(\symbf{\theta})$, for some function $\varphi: \symbf{\theta} \to \Psi$, and consider the set of all values of $\symbf{\theta}$ satisfying $\varphi(\symbf{\theta}) = \hat{\psi}$. Call this set $\Omega_{\hat{\psi}}$ so that $$\Omega_{\hat{\psi}} = \Big\{\omega \in \Theta: \varphi(\omega) = \hat{\psi}\Big\}.$${#eq-ZSE_IL5} For a given element $\omega \in \Omega_{\hat{\psi}}$, the associated value of $\theta$ is 

A member of $\Omega_{\hat{\psi}}$ for a model without an explicit nuisance parameter is functionally equivalent to a value $\symbf{\theta} = (\hat{\psi}, \phi)$ for a model with an explicit nuisance parameter.

\subsection{Approximation Algorithm}

The procedure described in the previous section is based on the assumption that $\lambda$ is an explicit nuisance parameter so that taking partial derivatives of $\ell$ with respect to its components is a well-defined operation. In this section, we consider an approach to approximating the integrated likelihood function that 

Consider a model with parameter $\theta \in \Theta$ and parameter of interest $\varphi(\theta)$ for some function $\varphi: \Theta \to \mathbbm{R}$. For a given value of $\psi$, define $$\Omega_{\psi} = \Big\{\omega \in \Theta: \varphi(\omega) = \psi\Big\}.$${#eq-ZSE_IL5}







