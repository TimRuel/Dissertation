\chapter{The Likelihood Function}

\section{Definition}

Upon choosing a statistical model that we think best characterizes our population of interest, the obvious next step is to identify the true distribution in $\mathcal{P}$ or at the very least, the one that best approximates the truth. This is equivalent to making inferences about $\symbf{\theta}_0$ in the case where the model is parametric and identifiable. That is, given the particular form(s) we have chosen for the distributions in $\mathcal{P}$, the only unknown remaining is the value of $\symbf{\theta}_0$ itself. Since this value is ultimately what controls the mechanism generating any sample of data $\mathbf{x}_n = (x_1, ..., x_n)$ that we might observe from the population, it stands to reason that information regarding $\symbf{\theta}_0$ can be inferred from the specific values of $x_1, ..., x_n$ that we obtain. To make this notion more rigorous, we require some method of analyzing the joint probability of our sample as a function of our parameter $\symbf{\theta}$.

Given some observed data $\mathbf{x}_n$, the *likelihood function* for $\symbf{\theta}$ is defined as $$L(\symbf{\theta}) = L(\symbf{\theta}; \mathbf{x}_n) = p(\mathbf{x}_n; \symbf{\theta}), \> \> \symbf{\theta} \in \Theta.$${#eq-likelihood_1} In other words, the value of the likelihood function evaluated at a particular $\symbf{\theta} \in \Theta$ is simply equal to the output of the model's density function evaluated at the same inputs. However, while $p(\mathbf{x}_n; \symbf{\theta})$ is viewed primarily as a function of $\mathbf{x}_n$ for fixed $\symbf{\theta}$, the reverse is actually true for $L(\symbf{\theta}; \mathbf{x}_n)$. Indeed, we regard the likelihood as being a function of the parameter $\symbf{\theta}$ for fixed $\mathbf{x}_n$. The reversal of the order of the arguments $\symbf{\theta}$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\symbf{\theta}; x)$ as the probability that $X = x$ given that $\symbf{\theta}$ is the true parameter value.^[It is important to note here that whichever value of $\symbf{\theta}$ we choose to plug into $L(\symbf{\theta}; x)$ is the value that we are currently "pretending" is the true one, regardless of whether or not it actually equals $\symbf{\theta}_0$ in reality.] Crucially, this is *not* equivalent to the inverse probability that $\symbf{\theta}$ is the true parameter value given $X = x$. The likelihood does not directly tell us anything about the probability that $\symbf{\theta}$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\symbf{\theta}$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous interpretation of $L(\symbf{\theta})$ as a probability since the probability that $X$ takes on any particular value is now 0. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny ball centered at $x$. Specifically, for two different observations $x_1$ and $x_2$, if $L(\symbf{\theta}; x_1) = c \cdot L(\symbf{\theta}; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\symbf{\theta}$ is the true value of the parameter.

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\symbf{\theta}; \mathbf{x}_n)$ to make probabilistic assertions regarding $\symbf{\theta}$. Despite our use of probability in its definition, the likelihood itself is *not* a probability density function for the parameter $\symbf{\theta}$ and is subject to neither the same rules nor interpretations as one. 

\section{Transformations}

There are a few useful transformations of the likelihood function that we will define here for use in future sections. The first is the *log-likelihood function*, which is defined as the natural logarithm of the likelihood function: $$\ell(\symbf{\theta}) = \ell(\symbf{\theta}; \mathbf{x}_n) = \log L(\symbf{\theta}; \mathbf{x}_n).$${#eq-loglike} In practice, we will typically eschew direct analysis of the likelihood in favor of the log-likelihood due to the nice mathematical properties logarithms possess. Chief among these properties is the ability to turn products into sums (i.e. $\log(ab) = \log(a) + \log(b)$ for $a,b > 0$). Sums tend to be easier to differentiate than products, making this is a particularly useful feature for likelihood functions, which are often expressed as the product of marginal density functions when the observations are independent. 

The other key property of logarithms that makes the log-likelihood so useful is that they are strictly increasing functions of their arguments (i.e. $\log x > \log y$ for $x > y > 0$). This monotonicity ensures that the locations of a function's extrema are preserved when the function is passed to the argument of a logarithm. For example, for a positive function $f$ with a global maximum, $\underset{x}{\mathrm{argmax}} f(x) = \underset{x}{\mathrm{argmax}} \log f(x)$. 

In the general case in which $\symbf{\theta}$ is a $d$-dimensional vector, where $d$ is an integer greater than 1, it follows that the first derivative of the log-likelihood with respect to $\symbf{\theta}$ will also be a $d$-dimensional vector, the second derivative will be a $d \times d$ matrix, the third derivative will be a $d \times d \times d$ array, and so forth. To emphasize the multidimensional nature of these results, we will use notation typically associated with partial derivatives involving functions of more than one variable (e.g. $\nabla, \mathbf{J}, \mathbf{H}$, etc.) along with subscripts that indicate the variable with respect to which the partial derivatives are being taken. See Appendix \ref{appendix:A} for a review of this notation.

The gradient of $\ell$ with respect to $\symbf{\theta}$ appears frequently enough in the analysis of likelihood functions that it has earned its own name - the *score function*, or just the *score*. Formally, it is defined as 
$$\mathcal{S}(\symbf{\theta}; \mathbf{x}_n) = \nabla_{\symbf{\theta}}\ell(\symbf{\theta}; \mathbf{x}_n) = \begin{pmatrix} 
\frac{\partial }{\partial\symbf{\theta}_1}\ell(\symbf{\theta}; \mathbf{x}_n) \\ 
\vdots \\ 
\frac{\partial }{\partial\symbf{\theta}_d}  \ell(\symbf{\theta}; \mathbf{x}_n)
\end{pmatrix} =
\begin{pmatrix} 
\mathcal{S}_1(\symbf{\theta}; \mathbf{x}_n) \\ 
\vdots \\ 
\mathcal{S}_d(\symbf{\theta}; \mathbf{x}_n) 
\end{pmatrix},$${#eq-score} where we think of each component as being a function $S_j: \Theta \to \mathbbm{R}$.

Similarly, the Hessian matrix of the log-likelihood function with respect to $\symbf{\theta}$ (i.e. the transpose of the Jacobian matrix of the score) multiplied by $-1$ is called the *observed information*, or just the *information*, and is denoted by
$$\mathcal{I}(\symbf{\theta}) = -\mathbf{H}_{\symbf{\theta}}\Big(\ell(\symbf{\theta}; \mathbf{x}_n)\Big) = -\mathbf{J}_{\symbf{\theta}}\Big(\mathcal{S}(\symbf{\theta}; \mathbf{x}_n)\Big)^\top = -
\begin{pmatrix} 
\frac{\partial^2 \ell}{\partial\theta_1^2} & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_d} \\ 
\frac{\partial^2 \ell}{\partial \theta_2 \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_2^2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_2\partial\theta_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2 \ell}{\partial \theta_d \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_d \partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_d^2}
\end{pmatrix}.$${#eq-obs_information} The use of the term "information" here derives from the fact that the second partial derivatives of $\ell$ with respect to the components of $\symbf{\theta}$ are all related to the curvature of $\ell$ near its maximum - the sharper the curve, the less uncertainty and therefore more information we have about $\symbf{\theta}$. 

Recall that $L(\symbf{\theta}; \mathbf{x}_n)$ is defined as a function of $\symbf{\theta}$ for a fixed sample of observations $\mathbf{x}_n = (x_1, ..., x_n)$, where we think of each $x_i$ as being a realization of a random variable $X_i$. We may therefore interpret $L(\symbf{\theta}; \mathbf{x}_n)$ as a random variable in the following sense: for a given $\symbf{\theta}$, the value of $L(\symbf{\theta}; \mathbf{x}_n)$ depends entirely on the values of $X_1, ..., X_n$ that we happened to observe, and so $L(\symbf{\theta}; \mathbf{X}_n)$ is itself a random variable with respect to the joint probability distribution of $\mathbf{X}_n = (X_1, ..., X_n)$. The same is also true for any function or estimate based on the likelihood, as they ultimately will all depend on the data through it as well. Going forward, we will use capital letters inside these functions when we want to emphasize this interpretation. For example, $\mathcal{S}(\symbf{\theta}; \mathbf{X}_n)$ is a random variable for which we have observed the value $\mathcal{S}(\symbf{\theta}; \mathbf{x}_n)$.

The random nature of these likelihood-based quantities further implies that finding their expectations and variances with respect to $p_{\symbf{\theta}}(\mathbf{x}_n)$ is a well-defined, nontrivial task. The variance of the score function will be of particular importance, as it also relates to the amount of information pertaining to $\symbf{\theta}_0$ that is contained within the log-likelihood function of our model. Properly known as the *Fisher information* or the *expected information*, it is defined as $$\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) = \text{Var}_{\symbf{\theta}}\big[\mathcal{S}(\symbf{\theta}; \mathbf{X}_n)\big].$${#eq-exp_information1}  

Since we are working in the more general framework in which $\mathcal{S}(\symbf{\theta})$ is a $d \times 1$ random vector, it would be more accurate to speak of the *Fisher information matrix*, which is equal to the variance-covariance matrix of $\mathcal{S}(\symbf{\theta})$. Hence, we have 
$$
\begin{aligned}
\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) &= \text{Var}_{\symbf{\theta}}\big[\mathcal{S}(\symbf{\theta}; \mathbf{X}_n)\big] &&(\text{by Eq. 2.2.4})\\
                    &= \text{Cov}_{\symbf{\theta}}\bigg[\begin{pmatrix} \frac{\partial \ell}{\partial\theta_1,} \cdots,  \frac{\partial \ell}{\partial\theta_d}  \end{pmatrix}^T\bigg] &&(\text{by Eq. 2.2.2})\\
                    &= \begin{pmatrix} 
\text{Var}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Var}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Var}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_d}\Big)
\end{pmatrix}.
\end{aligned}
$${#eq-exp_information2}

Note that if the observations are independent, the Fisher information of the whole sample is equal to the sum of the Fisher information values for each of the observations individually. That is, $$\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) = \sum_{i=1}^n \mathscr{I}_{X_i}(\symbf{\theta}).$${#eq-exp_information3} If the observations are also identically distributed according to the distribution of some random variable $X$, then $\mathscr{I}_{X_i}(\symbf{\theta}) = \mathscr{I}_X(\symbf{\theta})$ for all $i$, and so the Fisher information for the entire sample is simply equal to the Fisher information for a single observation of $X$ multiplied by a factor of $n$: $$\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) = n\mathscr{I}_{X}(\symbf{\theta}).$${#eq-exp_information4}

\section{Maximum Likelihood Estimation}

\subsection{Motivation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, the sensible choice is the one which makes the data we actually did observe more probable to have been observed. We have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\symbf{\theta}_1; \mathbf{x}_n) > L(\symbf{\theta}_2; \mathbf{x}_n)$, then under the preceding logic, $\symbf{\theta}_1$ is the better estimate of the true parameter value.

This can be extended to include as many candidate parameter values as we would like. For $n$ potential estimates of $\symbf{\theta}_0$, the best is the one that corresponds to the highest value of the likelihood function. Following this line of reasoning to its natural conclusion, a sensible choice for an estimate of $\symbf{\theta}_0$ is any value that maximizes the likelihood function based on an observed dataset $\mathbf{x}_n$. 

To help make this argument rigorous, we can show that with probability tending to $1$ as the sample size tends toward infinity, the likelihood for a regular model will be strictly larger at $\symbf{\theta}_0$ than for any other $\symbf{\theta} \in \Theta$. We start by observing that **RC1** implies $$L(\symbf{\theta}; \mathbf{x}_n) = p(\mathbf{x}_n; \symbf{\theta}) = \prod_{i=1}^n p(x_i; \symbf{\theta})$${#eq-MLE1} and $$\ell(\symbf{\theta}; \mathbf{x}_n) = \log p(\mathbf{x}_n; \symbf{\theta}) = \sum_{i=1}^n \log p(x_i; \symbf{\theta}).$${#eq-MLE2}  It follows that 
$$
\begin{aligned}
L(\symbf{\theta}; \mathbf{x}_n) < L(\symbf{\theta}_0; \mathbf{x}_n) &\iff \ell(\symbf{\theta}; \mathbf{x}_n) < \ell(\symbf{\theta}_0; \mathbf{x}_n) \\
                                                    &\iff \sum_{i=1}^n \log p(x_i; \symbf{\theta}) - \sum_{i=1}^n \log p(x_i; \symbf{\theta}_0) < 0 \\
                                                    &\iff \sum_{i=1}^n \big[\log p(x_i; \symbf{\theta}) - \log p(x_i; \symbf{\theta}_0)\big] < 0 \\
                                                    &\iff \sum_{i=1}^n \log\frac{p(x_i; \symbf{\theta})}{p(x_i; \symbf{\theta}_0)} < 0 \\
                                                    &\iff \frac{1}{n}\sum_{i=1}^n \log\frac{p(x_i; \symbf{\theta})}{p(x_i; \symbf{\theta}_0)} < 0.
\end{aligned}
$${#eq-MLE3}
Note that **RC3** guarantees that the ratio $p(x; \symbf{\theta}) / p(x; \symbf{\theta}_0)$ is well-defined and finite for all $x \in \mathcal{X}$, the region of common support. Then by the Weak Law of Large Numbers, $$\frac{1}{n}\sum_{i=1}^n \log\frac{p(X_i; \symbf{\theta})}{p(X_i; \symbf{\theta}_0)} \to \text{E}_{\symbf{\theta}_0}\Bigg[ \log \frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg]$${#eq-MLE4} in probability as $n \to \infty$. Furthermore, $$\text{E}_{\symbf{\theta}_0} \Bigg[\frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg] = \int_{\mathcal{X}}\Bigg[\frac{p(x; \symbf{\theta})}{p(x; \symbf{\theta}_0)} \Bigg] p(x; \symbf{\theta}_0)dx = \int_{\mathcal{X}}p(x; \symbf{\theta})dx = 1.$${#eq-MLE5}
Since $\log(x)$ is a strictly concave function, it follows from Jensen's inequality (see Appendix \ref{appendix:A}) and @eq-MLE5 $$\text{E}_{\symbf{\theta}_0}\Bigg[ \log \frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg] < \log \text{E}_{\symbf{\theta}_0} \Bigg[\frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg] = \log 1 = 0.$${#eq-MLE6} Hence, the quantity on the left-hand side of @eq-MLE4 is converging in probability to a constant that is less than 0 as $n$ tends to infinity. From this and the equivalence we established in @eq-MLE3, it follows that $$\lim_{n \to \infty} P_{\symbf{\theta}_0} \big[L(\symbf{\theta}; \mathbf{x}_n) < L(\symbf{\theta}_0; \mathbf{x}_n)\big] = \lim_{n \to \infty} P_{\symbf{\theta}_0} \Bigg[\frac{1}{n}\sum_{i=1}^n \log\frac{p(X_i; \symbf{\theta})}{p(X_i; \symbf{\theta}_0)} < 0\Bigg] = 1,$${#eq-MLE7} which proves the claim.

Let $\hat{\symbf{\theta}}\in \Theta$ be any parameter value that renders the likelihood at the observed $\mathbf{X}_n = \mathbf{x}_n$ as large as possible, i.e., $$L(\hat{\symbf{\theta}}, \mathbf{x}_n) = \sup_{\symbf{\theta} \in \Theta} L(\symbf{\theta}).$${#eq-MLE8} We call such a value a *maximum likelihood estimate* of $\theta_0$. Note that this definition of $\hat{\theta}$ as a maximizer of $L(\hat{\symbf{\theta}}, \mathbf{x}_n)$ necessarily makes it a function of the observed data. When this function is measurable, then we can further define the *maximum likelihood estimator* (MLE) of $\symbf{\theta}_0$ as the statistic $\hat{\symbf{\theta}}(\mathbf{X}_n)$ for which we observe the value $\hat{\symbf{\theta}}(\mathbf{x}_n)$. 

\section{Regularity Conditions}

As a consequence of the random variable interpretation of likelihood-based quantities, a natural line of inquiry to investigate is how the behavior of these random variables changes as the sample size $n$ increases. Of particular interest is the distribution to which the MLE converges, if any, as $n$ tends toward infinity. To that end, it will be useful to establish some *regularity conditions* for our models. We can think of these conditions as being assumptions similar to those we discussed in the introduction to this paper that, when satisfied, endow our models with certain properties that enable us, among other things, to determine the aforementioned distribution. 

For our purposes, we will call a model *regular* if it satisfies the following conditions:
\begin{enumerate}[label = \textbf{RC\arabic*)}]
  \item Any observations $x_1, ..., x_n$ belonging to a sample that has been drawn from the model's sample space are independent and identically distributed (i.i.d.) realizations of a random variable $X$ with density function $p_{\symbf{\theta}}(x)$.
  \item $P_{\symbf{\theta}_1} = P_{\symbf{\theta}_2} \implies \symbf{\theta}_1 = \symbf{\theta}_2$ for all $\symbf{\theta}_1, \symbf{\theta}_2 \in \Theta$.
  \item The distributions in $\mathcal{P}$ have a common support $\mathcal{X} = \{x: p_{\symbf{\theta}}(x) > 0 \}\subseteq \mathbbm{R}$ not depending on $\symbf{\theta}$.
  \item There exists an open set $\Theta^* \subseteq \Theta$ of which $\symbf{\theta}_0$ is an interior point.
  \item $p(x; \symbf{\theta})$ is twice continuously differentiable with respect to $\symbf{\theta}$ for all $\symbf{\theta}$ in a neighborhood of $\symbf{\theta}_0$.
  \item There exists a random function $M(x)$ (that does not depend on $\symbf{\theta}$) satisfying $\text{E}[M(X)] < \infty$ such that each third partial derivative of $\ell(\symbf{\theta}; x)$ is bounded in absolute value by $M(x)$ uniformly in some neighborhood of $\symbf{\theta}_0$.
  \item The integral $\int_{\mathcal{X}} p(x; \symbf{\theta}) dx$ can be differentiated twice under the integral sign with respect to the components of $\symbf{\theta}\in \Theta^*$.
  \item $\mathscr{I}_{X}(\symbf{\theta})$ is positive definite for all $\symbf{\theta} \in \Theta$.
  \item $\Theta$ is a compact and convex subset of $\mathbbm{R}^d$.
\end{enumerate}
While not strictly necessary, **RC1** is often assumed as a matter of convenience since it tends to simplify calculations greatly. We include it here for that purpose and to frame our discussion in the context of a standard case in which likelihood theory holds. Of course, it is possible to construct models lacking i.i.d. observations yet still possessing real world applications for which the results discussed in this paper hold. 

The implication in **RC2** is simply the identifiability property we mentioned in Chapter 2. We repeat it here as it is necessary to guarantee the consistency of the MLE, i.e., that it converges in probability to $\symbf{\theta}_0$ as $n \to \infty$.

**RC3** requires that the distributions in $\mathcal{P}$ be supported on a common subset of the real line, and the definition of this subset cannot depend on $\symbf{\theta}$. This is to prevent situations in which, for example, the event $\{X_i \leq x_i \}$ occurs with positive probability when $\symbf{\theta} = \symbf{\theta}_1$ but not $\symbf{\theta} = \symbf{\theta}_2$. 

**RC4** guarantees the existence of an open subset $\Theta^*$ of $\Theta$ containing $\symbf{\theta}_0$ as an interior point. The fact that $\symbf{\theta}_0$ is an interior point of $\Theta^*$ further implies that it is possible to find a neighborhood of $\symbf{\theta}_0$ that is contained in $\Theta^*$. **RC5** then goes on to assert the existence and continuity of the first two partial derivatives with respect to the components of $\symbf{\theta}$ of $p(x; \symbf{\theta})$ in this neighborhood. This is a necessary requirement for defining a first-order Taylor series expansion of the score function around $\symbf{\theta}_0$. 

Another way of stating **RC6** is that for all $\symbf{\theta}$ in a neighborhood $N_{\symbf{\theta}_0}$, there exists a random function $M(x)$ with finite expectation such that $$\sup_{\symbf{\theta} \in N_{\symbf{\theta}_0}}\Bigg|\frac{\partial^3 \ell(\symbf{\theta};x)}{\partial \theta_i \partial \theta_j \partial \theta_k}\Bigg| \leq M(x)$$ for all integers $1 \leq i, j, k \leq d$. Equivalently, we could say that the entries of the Hessian matrix for each component of the score function are all bounded by $M(x)$ as well. This ensures the remainder terms in a second-order Taylor series expansion of the score function around $\symbf{\theta_0}$ become negligible as the sample size increases to infinity.

**RC7** grants us the ability to freely interchange integration and second-order partial differentiation with respect to the components of $\symbf{\theta}$, i.e., $$\frac{\partial^2}{\partial \theta_i \partial \theta_j}\int_{\mathcal{X}} p(x; \symbf{\theta}) dx = \int_{\mathcal{X}} \frac{\partial^2}{\partial \theta_i \partial \theta_j}p(x; \symbf{\theta}) dx$$ for all $\symbf{\theta} \in \Theta^*$ and $i, j = 1, ..., d$. Note that this implies first-order partial derivatives can be passed under the integral sign as well. This will prove useful in our discussion of the Bartlett identities in Section 3.5. 

Finally, **RC8-9** play an important role in ensuring the existence and uniqueness of the maximum likelihood estimator of $\symbf{\theta}_0$.

\subsection{Properties}

In general, there is no guarantee that an MLE for a model's parameter will exist, and even if it does, it will not necessarily be unique. However, since maximum likelihood estimation plays such an important role in our discussion in Chapters 5 and 6, it would come as a great convenience if we possessed the ability to speak freely of *the* MLE of a model's parameter without having to clarify *which* MLE we mean or whether one even exists at all. Hence, some discussion of the conditions under which the MLE of a model's parameter exists and is unique is warranted.

The extreme value theorem (see Appendix \ref{appendix:A}) implies that sufficient conditions for the existence of a model's MLE are that $\Theta$ is compact, and $\ell(\symbf{\theta}; x)$ is continuous on $\Theta$. The former is satisfied directly by **RC9** and the latter is implied through our assumption in **RC5** of the differentiability of $p(x; \symbf{\theta})$ in $\symbf{\theta}$. Therefore, at least  one MLE will always exist for the true parameter value of a regular model. These are only sufficient conditions, however, and not necessary; MLEs may exist for parameters of non-regular models as wel.

Similarly, when the MLE does exist, a sufficient condition for its uniqueness is that $\Theta$ is convex, and $\ell(\symbf{\theta}; x)$ is strictly concave on $\Theta$, as this ensures that it has exactly one global maximum, which it attains at the $\hat{\symbf{\theta}}$. **RC9** directly satisfies the compactness criterion. Our assumption in **RC8** that the Fisher information matrix is positive definite forces the Hessian matrix of $\ell(\symbf{\theta}; x)$ to be negative definite. This in turn implies that $\ell(\symbf{\theta}; x)$ is strictly concave, so the requirement is met. Hence, a regular model will always have a unique MLE for its parameter.

As a global maximizer of the log-likelihood function, the MLE $\hat{\symbf{\theta}}$ must be a root of the log-likelihood function, i.e., it must satisfy the *likelihood equation*, $$\nabla_{\symbf{\theta}}\ell(\hat{\symbf{\theta}}) = \mathbf{0}.$${#eq-MLE9} whenever it exists. It is important to note that for an arbitrary model there may be other roots as well, even when the MLE doesn't exist. Assuming **RC2** and **RC5-8**, it can be shown that there will always be at least one sequence of roots $\hat{\symbf{\theta}}_n$ of its log-likelihood such that $\hat{\symbf{\theta}}_n$ tends to $\symbf{\theta}_0$ in probability as $n \to \infty$ (Cf. CramÃ©r 1945). The MLE will not necessarily be a part of this sequence though, even if it exists. Adding **RC9** is enough to ensure the MLE must be the unique solution to the likelihood equation, however, and therefore this sequence of roots will also be unique and for a given sample $\mathbf{x}_n$, the corresponding root $\hat{\symbf{\theta}}_n = \hat{\symbf{\theta}}(\mathbf{x}_n)$ will be the unique MLE of $\symbf{\theta}_0$. It follows that the MLE is a consistent estimator of $\symbf{\theta}_0$ for regular models. We will explore the asymptotic properties of the MLE in more detail in Section 3.6.

One last useful property of the maximum likelihood estimator is its functional invariance. If $\hat{\symbf{\theta}}$ is an MLE of $\symbf{\theta}_0$, then any function $h(\symbf{\theta}_0)$ will have $h(\hat{\symbf{\theta}})$ as its MLE. Hence, it is straightforward to find the MLE of a model that has undergone a reparameterization given that we know the MLE of the original parameter. 

\section{The Bartlett Identities}

The Bartlett identities are a set of equations relating to the expectations of the derivatives of a log-likelihood function to one another. In general, there is no guarantee that an arbitrary function of a random variable $X$ and its parameter $\symbf{\theta}$ will satisfy the Bartlett identities. It is guaranteed, however, that the log-likelihood function associated with $X$ and $\symbf{\theta}$ will satisfy them, provided that the model is regular. Thus, we can think of any function that does satisfy the Bartlett identities (or at least some of them) as resembling that of a genuine log-likelihood.

Consider the case where a random variable $X$ has density function $p_{\theta}(x)$, where $\theta$ is a scalar. For a single observation $X = x$, the expectation of $\frac{\partial}{\partial \theta}\ell(\theta; X)$ gives

$$
\begin{aligned}
E_{\theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\bigg] &= \int_{\mathbbm{R}} \bigg[\frac{\partial}{\partial \theta} \log p(x; \theta)\bigg] p(x; \theta) dx \\
                                                                       &=  \int_{\mathbbm{R}} \frac{\frac{\partial}{\partial \theta} p(x; \theta)}{p(x; \symbf{\theta})} p(x; \theta) dx \\
                                                                       &=  \int_{\mathbbm{R}} \frac{\partial}{\partial \theta} p(x; \theta) dx \\
                                                                       &= \frac{d}{d \theta} \int_{\mathbbm{R}} p(x; \theta) dx \\
                                                                       &= \frac{d}{d \theta} 1 \\
                                                                       &= 0. 
\end{aligned}
$${#eq-BI1}
@eq-BI1 is called the first Bartlett identity. In words, it states that the expectation of the first partial derivative of the log-likelihood function of a statistical model with respect to the parameter will always be 0. Since the score is defined as $\frac{\partial}{\partial \theta}\ell(\theta; x)$, any function that satisfies the first Bartlett identity is said to be *score-unbiased*.

For any model with a log-likelihood satisfying the first Bartlett identity, the expected information for its parameter $\theta$ may be rewritten as 
$$
\begin{aligned}
\mathscr{I}_{X}(\theta) &= \text{Var}_{\theta}\big[\mathcal{S}(\theta; X)\big] \\
                    &= \text{Var}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg] \\
                    &= \text{Var}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg] +\Bigg(\text{E}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg]\Bigg)^2 &&(\text{by the first Bartlett identity}) \\
                    &= \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg].
\end{aligned}
$${#eq-BI2}

If we now consider the second partial derivative of $\ell(\theta; x)$ with respect to $\theta$, we have
$$
\begin{aligned}
\frac{\partial^2}{\partial \theta^2} \ell(\theta; x) &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \log p(x;\theta) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\Bigg[\frac{\frac{\partial}{\partial \theta} p(x;\theta)}{p(x;\theta)} \Bigg] \\
                                &= \frac{\Big[\frac{\partial^2}{\partial \theta^2} p(x;\theta)\Big] p(x;\theta) - \Big[\frac{\partial}{\partial \theta} p(x; \theta)\Big]\Big[\frac{\partial}{\partial \theta} p(x; \theta)\Big]}{\big[p(x; \theta) \big]^2} \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \Bigg[\frac{\frac{\partial}{\partial \theta} p(x; \theta)}{ p(x; \theta)}\Bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \bigg[\frac{\partial}{\partial \theta} \log p(x; \theta)\bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \Bigg[\frac{\partial}{\partial \theta}\ell(\theta; x) \Bigg]^2. 
\end{aligned} 
$$
Rearranging terms and taking expectations yields 
$$
\begin{aligned}
\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2\Bigg] &= \text{E}_{\theta}\Bigg[\frac{\frac{\partial^2}{\partial \theta^2} p(X; \theta)}{p(X; \theta)}\Bigg] \\
            &= \int_{\mathbbm{R}} \Bigg[\frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)}\Bigg] p(x; \theta) dx \\
            &= \int_{\mathbbm{R}} \frac{\partial^2}{\partial \theta^2} p(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} \int_{\mathbbm{R}} p(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} 1 \\
            &= 0. 
\end{aligned}
$$
Therefore, $$\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2 \Bigg] = 0.$${#eq-BI3}

@eq-BI2 is called the second Bartlett identity. Any function that satisfies it is said to be *information-unbiased*. Regular models as we have defined them will automatically satisfy both the first and second Bartlett identities. Hence, for any regular model, the statements in @eq-exp_information1, @eq-BI2, and @eq-BI3 imply that the following definitions for its expected information regarding its parameter $\theta$ are all equivalent: $$\mathscr{I}_{X}(\theta) = \text{Var}_{\theta}\Bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg] = \text{E}_{\theta}\Bigg[-\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\big[\mathcal{I}_X(\theta)\big].$${#eq-BI4} 

It is possible to derive further Bartlett identities by continuing in this manner for an arbitrary number of partial $\theta$-derivatives of the log-likelihood function, provided that they exist. However, the first two are sufficient for our purposes of evaluating the validity of approximations to genuine likelihoods so we will not go further here. Note that while the above derivations were performed under the assumption that $\theta$ is a scalar, the Bartlett identities also hold in the case where $\symbf{\theta}$ is a $d \times 1$ vector.

\section{One-Index Asymptotics}

The one-index asymptotics framework describes the behavior of likelihood-based statistics as the sample size ($n$) grows to infinity while the dimension of the nuisance parameter ($q$) remains fixed. The aim of this section is to present a basic overview of the theory's results so that we will have a readily available baseline against which to compare the results of the following section discussing the two-index asymptotics framework, in which $q$ is allowed to increase with $n$.

Assume the regularity conditions of the previous section apply and let $\hat{\symbf{\theta}}$ denote the MLE for $\symbf{\theta}_0$. **RC1** implies the score function is equal to
$$
\begin{aligned}
\mathcal{S}(\symbf{\theta}; \mathbf{x}_n) &= \nabla_{\symbf{\theta}} \ell(\symbf{\theta}; \mathbf{x}_n) \\
                                  &= \nabla_{\symbf{\theta}} \sum_{i=1}^n \ell(\symbf{\theta}; x_i) \\
                                  &= \sum_{i=1}^n \nabla_{\symbf{\theta}} \ell(\symbf{\theta}; x_i) \\
                                  &= \sum_{i=1}^n \mathcal{S}(\symbf{\theta}; x_i).
\end{aligned}
$${#eq-score_additive} where the last equality is true by . 
In other words, the score function for the parameter $\symbf{\theta}$ based on data $x_1, ..., x_n$ can be written as the sum of independent contributions $\mathcal{S}(\symbf{\theta}; x_i)$ ($i = 1,..., n$) where each $\mathcal{S}(\symbf{\theta}; x_i)$ can be thought of as the score function for $\symbf{\theta}$ based only on observation $x_i$. This implies that a Taylor series expansion of $\mathcal{S}(\symbf{\theta}; \mathbf{x}_n)$ will be equal to the sum of the Taylor series expansions of its individual contributions, plus a remainder term that depends on $n$. Since the observations are identically distributed, it suffices to consider the expansion for an arbitrary contribution, $\mathcal{S}(\symbf{\theta}; x_i)$. 

**RC4-5** guarantee the existence of a neighborhood of $\symbf{\theta}_0$ on which the first two partial derivatives of $\mathcal{S}(\symbf{\theta}; x_i)$ with respect to $\symbf{\theta}$ exist and are continuous. Without loss of generality, we may assume this neighborhood, call it $N_{\symbf{\theta}_0}$, is convex so that it contains all of the line segments connecting any two of its points. In particular, for any $\symbf{\theta} \in N_{\symbf{\theta}_0}$, $\text{LS}(\symbf{\theta}, \symbf{\theta}_0) \subset N_{\symbf{\theta}_0}$. Then by Taylor's theorem with the Lagrange form of the remainder, there exists $\bar{\symbf{\theta}}_j \in \text{LS}(\symbf{\theta}, \symbf{\theta}_0)$ such that the $j$-th component of $\mathcal{S}(\symbf{\theta}; x_i)$ may be expanded as
$$
\begin{aligned}
\mathcal{S}_j(\symbf{\theta}; x_i) &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + (\symbf{\theta} - \symbf{\theta}_0)\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i) + \frac{1}{2}(\symbf{\theta} - \symbf{\theta}_0)\textbf{H}_{\symbf{\theta}}\Big(\mathcal{S}_j(\bar{\symbf{\theta}}_j; x_i)\Big)(\symbf{\theta} - \symbf{\theta}_0)^\top \\
                                   &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + (\symbf{\theta} - \symbf{\theta}_0)\Big[\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i) + \frac{1}{2}\textbf{H}_{\symbf{\theta}}\Big( \mathcal{S}_j(\bar{\symbf{\theta}}_j; x_i)\Big)(\symbf{\theta} - \symbf{\theta}_0)^\top\Big] \\
                                   &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + (\symbf{\theta} - \symbf{\theta}_0)\Big[\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)\Big] &&(\text{by } \textbf{RC6}) \\
                                   &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + \Big[\nabla_{\symbf{\theta}}^\top \mathcal{S}_j(\symbf{\theta}_0; x_i) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)\Big](\symbf{\theta} - \symbf{\theta}_0)^\top.
\end{aligned}
$$
When we stack each of these individual equations into a system of equations, we get 
$$\begin{pmatrix} 
\mathcal{S}_1(\symbf{\theta}; x_i) \\ 
\vdots \\ 
\mathcal{S}_d(\symbf{\theta}; x_i) 
\end{pmatrix} = 
\begin{pmatrix} 
\mathcal{S}_1(\symbf{\theta}_0; x_i) \\ 
\vdots \\ 
\mathcal{S}_d(\symbf{\theta}_0; x_i) 
\end{pmatrix} + \Bigg[
\begin{pmatrix}
\nabla^\top \mathcal{S}_1(\symbf{\theta}_0; x_i) \\
\vdots \\
\nabla^\top \mathcal{S}_d(\symbf{\theta}_0; x_i)
\end{pmatrix} + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Bigg]
(\symbf{\theta} - \symbf{\theta}_0)^\top.
$${#eq-score_Taylor_matrix} The matrix of gradient vectors in the second term on the right-hand side of the above equation is simply the Jacobian of the score function evaluated at $\symbf{\theta} = \symbf{\theta}_0$, i.e., $\textbf{J}_{\symbf{\theta}}\Big(\mathcal{S}(\symbf{\theta}_0; x_i)\Big)$. However, by @eq-obs_information, this is just the negative transpose of the observed information matrix also evaluated at $\symbf{\theta} = \symbf{\theta}_0$, $\mathcal{I}(\symbf{\theta}_0)$. Furthermore, since we have assumed $\ell$ is continuous in $\symbf{\theta}$, we can freely swap the order of differentiation in all of its second partial derivatives with respect to $\symbf{\theta}$. This implies the Jacobian of the score will be a symmetric matrix, and so we simply have $\textbf{J}_{\symbf{\theta}}\Big(\mathcal{S}(\symbf{\theta}_0; x_i)\Big) = -\mathcal{I}_i(\symbf{\theta}_0)$. Hence, using more compact notation, @eq-score_Taylor_matrix becomes 
$$
\begin{aligned}
\mathcal{S}(\symbf{\theta}; x_i) &= \mathcal{S}(\symbf{\theta}_0; x_i) + \Big[\textbf{J}_{\symbf{\theta}}\Big(\mathcal{S}(\symbf{\theta}_0; x_i)\Big) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Big]
(\symbf{\theta} - \symbf{\theta}_0)^\top \\
                                 &= \mathcal{S}(\symbf{\theta}_0; x_i) - \Big[\mathcal{I}_i(\symbf{\theta}_0) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Big]
(\symbf{\theta} - \symbf{\theta}_0)^\top
\end{aligned}
$${#eq-score_Taylor_compact}

The above represents the second-order Taylor expansion around $\symbf{\theta}_0$ for an individual observation $x_i$'s contribution to the score function. Summing over all of these contributions yields
$$
\begin{aligned}
\mathcal{S}(\symbf{\theta}; \mathbf{x}_n) &= \sum_{i=1}^n \mathcal{S}(\symbf{\theta}; x_i) \\
                                  &= \sum_{i=1}^n\Bigg[\mathcal{S}(\symbf{\theta}_0; x_i) - \bigg[\mathcal{I}_i(\symbf{\theta}_0) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\bigg]
(\symbf{\theta} - \symbf{\theta}_0)^\top\Bigg] \\
                                  &= \mathcal{S}(\symbf{\theta}_0; \mathbf{x}_n) - \Bigg[\mathcal{I}_n(\symbf{\theta}_0) + \Bigg\{\sum_{i=1}^nM(x_i) \Bigg\}O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Bigg](\symbf{\theta} - \symbf{\theta}_0)^\top \\
                                  &= \mathcal{S}(\symbf{\theta}_0; \mathbf{x}_n) - \Bigg[\frac{1}{n}\mathcal{I}_n(\symbf{\theta}_0) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Bigg]n(\symbf{\theta} - \symbf{\theta}_0)^\top.
\end{aligned} 
$${#eq-score_Taylor_summed_1}
If we divide through by $\sqrt n$, we arrive at $$\frac{1}{\sqrt n}\mathcal{S}(\symbf{\theta}; \mathbf{x}_n) = \frac{1}{\sqrt n}\mathcal{S}(\symbf{\theta}_0; \mathbf{x}_n) - \Bigg[\frac{1}{n}\mathcal{I}_n(\symbf{\theta}_0) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\symbf{\theta} - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Bigg]\sqrt n(\symbf{\theta} - \symbf{\theta}_0)^\top.$${#eq-score_Taylor_summed_2}

We established in Section 3.4.1 that regular models will always have a sequence of MLEs $\hat{\symbf{\theta}}_n$ that converge in probability to $\symbf{\theta}_0$ as $n \to \infty$, and that each $\hat{\symbf{\theta}}_n$ in this sequence will satisfy $\mathcal{S}(\hat{\symbf{\theta}}_n; \mathbf{x}_n) = \mathbf{0}$. Plugging $\hat{\symbf{\theta}}_n$ in for $\symbf{\theta}$ in @eq-score_Taylor_summed_2 gives
$$\frac{1}{\sqrt n}\mathcal{S}(\hat{\symbf{\theta}}_n; \mathbf{x}_n) = \frac{1}{\sqrt n}\mathcal{S}(\symbf{\theta}_0; \mathbf{x}_n) - \Bigg[\frac{1}{n}\mathcal{I}_n(\symbf{\theta}_0) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\hat{\symbf{\theta}}_n - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Bigg]\sqrt n(\hat{\symbf{\theta}}_n - \symbf{\theta}_0)^\top$$ and therefore, 
$$\frac{1}{\sqrt n}\mathcal{S}(\symbf{\theta}_0; \mathbf{x}_n) = \Bigg[\frac{1}{n}\mathcal{I}_n(\symbf{\theta}_0) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O_p(||\hat{\symbf{\theta}}_n - \symbf{\theta}_0||)\mathbf{1}_{d \times d}\Bigg]\sqrt n(\hat{\symbf{\theta}}_n - \symbf{\theta}_0)^\top.$${#eq-score_Taylor_MLE1}

Note the following observations about the terms in the square brackets in the line above:
\begin{enumerate}[label = \arabic*)]
  \item By Equation 3.5.4, $\text{E}_{\symbf{\theta}_0}\big[\mathcal{I}(\symbf{\theta}_0)\big] = \mathscr{I}(\symbf{\theta}_0)$, and thus $\frac{1}{n}\mathcal{I}_n(\symbf{\theta}_0) = \frac{1}{n} \sum_{i=1}^n \mathcal{I}_i(\symbf{\theta}_0)$ is converging in probability to $\mathscr{I}(\symbf{\theta}_0)$ as $n \to \infty$ by the Weak Law of Large Numbers, i.e., $\frac{1}{n}\mathcal{I}_n(\symbf{\theta}_0) = \mathscr{I}(\symbf{\theta}) + o_p(1)$.
  \item $M(x_i)$ has finite expectation and does not depend on $\symbf{\theta}$ by \textbf{RC6}, which implies through Markov's inequality that it is bounded in probability as $n \to \infty$, i.e., $M(x_i) = O_p(1)$. It follows that $\frac{1}{n}\sum_{i=1}^nM(x_i) = O_p(1)$ as well.
  \item The fact that $\hat{\symbf{\theta}}_n$ is converging in probability to $\symbf{\theta}_0$ as $n \to \infty$ implies that $||\hat{\symbf{\theta}}_n - \symbf{\theta}_0||$ is $o_p(1)$.
\end{enumerate}
From these three facts we can conclude that the entire term inside the square brackets is converging in probability to $\mathscr{I}(\symbf{\theta})$ as $n \to \infty$. This allow us to rewrite @eq-score_Taylor_MLE1 as $$\frac{1}{\sqrt n}\mathcal{S}(\symbf{\theta}_0; \mathbf{x}_n) = \Big[\mathscr{I}(\symbf{\theta}_0) + o_p(1)\Big]\sqrt n(\hat{\symbf{\theta}}_n - \symbf{\theta}_0)^\top.$${#eq-score_Taylor_MLE2}









