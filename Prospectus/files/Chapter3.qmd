\chapter{The Likelihood Function}

\section{Definition}

Upon choosing a statistical model that we think best characterizes our population of interest, the obvious next step is to identify the true distribution in $\mathcal{P}$ or at the very least, the one that best approximates the truth. This is equivalent to making inferences about $\theta_0$ in the case where the model is parametric and identifiable. That is, given the particular form(s) we have chosen for the distributions in $\mathcal{P}$, the only unknown remaining is the value of $\theta_0$ itself. Since this value is ultimately what controls the mechanism generating any sample of data $\mathbf{x} = (x_1, ..., x_n)$ that we might observe from the population, it stands to reason that information regarding $\theta_0$ can be inferred from the specific values of $x_1, ..., x_n$ that we obtain. To make this notion more rigorous, we require some method of analyzing the joint probability of our sample as a function of our parameter $\theta$.

Given some observed data $X = \mathbf{x}$, the *likelihood function* for $\theta$ is defined as $$L(\theta) \equiv L(\theta; \mathbf{x}) = p(\mathbf{x}; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_1} In other words, the value of the likelihood function evaluated at a particular $\theta \in \Theta$ is simply equal to the output of the model's density function evaluated at the same inputs. However, while $p(\mathbf{x}; \theta)$ is viewed primarily as a function of $\mathbf{x}$ for fixed $\theta$, the reverse is actually true for $L(\theta; \mathbf{x})$. Indeed, we regard the likelihood as being a function of the parameter $\theta$ for fixed $\mathbf{x}$. The positioning of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\theta; x)$ as the probability that $X = \mathbf{x}$ given that $\theta$ is the true parameter value. Crucially, this is *not* equivalent to the inverse probability that $\theta$ is the true parameter value given $X = \mathbf{x}$. The likelihood does not directly tell us anything about the probability that $\theta$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\theta$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous interpretation of $L(\theta)$ as a probability since the probability that $X$ takes on any particular value is now 0. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny neighborhood of $x$. Specifically, for two different samples $x_1$ and $x_2$, if $L(\theta; \mathbf{x}_1) = c \cdot L(\theta; \mathbf{x}_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $\mathbf{x}_1$ than $\mathbf{x}_2$ given that $\theta$ is the true value of the parameter.

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\theta; \mathbf{x})$ to make probability statements about $\theta$. Despite our use of one in its definition, the likelihood is *not* itself a probability density function for the parameter $\theta$ and need not obey the same laws as one. 

\section{Transformations}

There are a few useful transformations of the likelihood function that we will define here for use in future sections. The first is the *log-likelihood function*, which is defined as the natural logarithm of the likelihood function: $$\ell(\theta) \equiv \ell(\theta; \mathbf{x}) = \log L(\theta; \mathbf{x}), \> \> \theta \in \Theta.$${#eq-loglike} In practice, we will typically eschew direct analysis of the likelihood in favor of the log-likelihood due to the nice mathematical properties logarithms possess. Chief among these properties is the ability to turn products into sums (i.e. $\log(ab) = \log(a) + \log(b)$ for $a,b > 0$). Sums tend to be easier to differentiate than products, making this is a particularly useful feature for likelihood functions, which are often expressed as the product of marginal density functions when the observations are independent. 

The other key property of logarithms that makes the log-likelihood so useful is that they are strictly increasing functions of their arguments (i.e. $\log x > \log y$ for $x > y > 0$). This monotonicity ensures that the locations of a function's extrema are preserved when the function is passed to the argument of a logarithm. For example, for a positive function $f$ with a global maximum, $\underset{x}{\mathrm{argmax}} f(x) = \underset{x}{\mathrm{argmax}} \log f(x)$. 

In the general case in which $\theta$ is a $d$-dimensional vector, where $d$ is an integer greater than 1, it follows that the first derivative of the log-likelihood with respect to $\theta$ will also be a $d$-dimensional vector, the second derivative will be a $d \times d$ matrix, the third derivative will be a $d \times d \times d$ array, and so forth. Consequently, we will use gradient notation (i.e. $\nabla$, $\nabla^2$, etc.) to emphasize the vector nature of these results.

The gradient of $\ell$ with respect to $\theta$ appears frequently enough in the analysis of likelihood functions that it has earned its own name - the *score function*. Formally, it is defined as 
$$\mathcal{S}(\theta) \equiv \mathcal{S}(\theta; \mathbf{x}) = \nabla\ell(\theta; \mathbf{x}) = \begin{pmatrix} \frac{\partial \ell}{\partial\theta_1} \\ \vdots \\ \frac{\partial \ell}{\partial\theta_d}  \end{pmatrix}, \> \> \theta \in \Theta.$${#eq-score} 

Similarly, the negative Hessian matrix of the log-likelihood function with respect to $\theta$ is called the *observed information* or just the *information* and is denoted by
$$\mathcal{I}(\theta) \equiv \mathcal{I}(\theta; \mathbf{x}) = -\nabla^2\ell(\theta; \mathbf{x}) = -
\begin{pmatrix} 
\frac{\partial^2 \ell}{\partial\theta_1^2} & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_d} \\ 
\frac{\partial^2 \ell}{\partial \theta_2 \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_2^2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2 \ell}{\partial \theta_d \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_d \partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_d^2}
\end{pmatrix}, \> \> \theta \in \Theta.$${#eq-obs_information} Note that this is also equal to the negative gradient of the score function. The use of the term "information" here derives from the fact that the second partial derivatives of $\ell$ with respect to the components of $\theta$ are all related to the curvature of $\ell$ near its maximum - the sharper the curve, the less uncertainty and therefore more information we have about $\theta$. 

Recall that $L(\theta; x_1, ..., x_n)$ is defined as a function of $\theta$ for a fixed sample of observations $x_1, ..., x_n$, where we think of each $x_i$ as being a realization of a random variable $X_i$. We may therefore interpret $L(\theta; x_1, ..., x_n)$ as a random variable in the following sense: for a given $\theta$, the value of $L(\theta; x_1, ..., x_n)$ depends entirely on the values of $X_1, ..., X_n$ that we happened to observe, and so $L(\theta; x_1, ..., x_n)$ is itself a random variable with respect to the joint probability distribution of $X_1, ..., X_n$. The same is also true for any function or estimate based on the likelihood, as they ultimately will all depend on the data through it as well.

The random nature of these likelihood-based quantities further implies that finding their expectations and variances respect to $p(x_1, ..., x_n; \theta_0)$ is a well-defined, nontrivial task. The variance of the score function in particular will be useful to know, as it also relates to the amount of information pertaining to $\theta_0$ that is contained within the log-likelihood function of our model. Properly known as the *Fisher information* or the *expected information*, it is defined as $$\mathscr{I}(\theta) = \mathbb{V}\big[\mathcal{S}(\theta; \mathbf{x}); \theta_0\big], \> \> \theta \in \Theta.$${#eq-exp_information} However, since we are working in the more general framework in which $\mathcal{S}(\theta)$ is a $d \times 1$ random vector, it would be more accurate to speak of the *Fisher information matrix*, which is equal to the variance-covariance matrix of $\mathcal{S}(\theta)$. Hence, we have 
$$
\begin{aligned}
\mathscr{I}(\theta) &= \mathbb{V}\big[\mathcal{S}(\theta; \mathbf{x}); \theta_0\big] &&(\text{by Eq. 2.2.4})\\
                    &= \text{Cov}\bigg[\begin{pmatrix} \frac{\partial \ell}{\partial\theta_1} \cdots  \frac{\partial \ell}{\partial\theta_d}  \end{pmatrix}^T; \theta_0\bigg] &&(\text{by Eq. 2.2.2})\\
                    &= \begin{pmatrix} 
\mathbb{V}\Big(\frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\text{Cov}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_1}\Big) & \mathbb{V}\Big(\frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\text{Cov}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \mathbb{V}\Big(\frac{\partial \ell}{\partial\theta_d}\Big)
\end{pmatrix}. &&(\text{by definition of covariance of a random vector})
\end{aligned}
$$
We have omitted the $\theta_0$ arguments inside each of the variance and covariance operators in the final line above for the sake of avoiding overly cluttered notation, but it should be understood that all of these quantities are really integrals of the form 

\section{Maximum Likelihood Estimation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, the sensible choice is the one which makes the data we actually did observe more probable to have been observed. We have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\theta_1; \mathbf{x}) > L(\theta_2; \mathbf{x})$, then under the preceding logic, $\theta_1$ is the better estimate of the true parameter value.

This can be extended to include as many candidate parameter values as we would like. For $n$ potential estimates of $\theta_0$, the best is the one that corresponds to the highest value of the likelihood function. If we follow this line of reasoning to its natural conclusion, we arrive at the notion of a *maximum likelihood estimate* of the parameter $\theta$, which is defined as any value of $\theta$ that maximizes the output of the likelihood function among all possible choices of $\theta$ in $\Theta$ for fixed data $\mathbf{x}$. We will denote this using the notation $\hat{\theta}$ (pronounced "theta hat"). Formally, a maximum likelihood estimate of $\theta$ is defined as a function $\hat{\theta} = \hat{\theta}(\mathbf{x}) \in \Theta$ for which $$L(\hat{\theta}; \mathbf{x}) = \underset{\theta \in \Theta}{\mathrm{sup}}\, \> L(\theta; \mathbf{x}),$${#eq-mle1} or equivalently $$\ell(\hat{\theta}; \mathbf{x}) = \underset{\theta \in \Theta}{\mathrm{sup}}\, \> \ell(\theta; \mathbf{x}).$${#eq-mle2}

In general, there is no guarantee that a maximum likelihood estimate for the parameter of a statistical model will exist, and if it does, it will not necessarily be unique.

\section{Asymptotic Analysis}

\subsection{Regularity Conditions}

In light of this, a natural line of inquiry to follow is how the behavior of these random variables changes as the sample size $n$ increases. Of particular interest is the distribution to which the maximum likelihood estimate converges as $n$ tends toward infinity, if indeed one exists. To that end, it will be useful to establish some *regularity conditions* for our models. We can think of these conditions as being assumptions similar to those we discussed in the introduction to this paper that, when satisfied, endow our models with certain properties that enable us to determine the aforementioned distribution. 

For our purposes, we will call a model *regular* if it satisfies the following conditions:
\begin{enumerate}[label = RC\arabic*)]
  \item Any observations $x_1, ..., x_n$ belonging to a sample that has been drawn from the model's sample space are independent and identically distributed (i.i.d.) realizations of a random variable $X$ with density function $p(x; \theta)$.
  \item For any two parameters $\theta_1, \theta_2 \in \Theta$, $\theta_1 \neq \Theta_2 \implies p(x; \theta_1) \neq p(x; \theta_2)$.
  \item The parameter space $\Theta$ is a convex subset of $\mathbb{R}^d$.
  \item $\mathbb{E}\Big(\big[\mathcal{S}(\theta;x)\big]\big[\mathcal{S}(\theta;x)\big]^T; \theta\Big)$ is positive definite for all $\theta \in \Theta$.
  \item There exists an open set $\Theta^* \subseteq \Theta$ of which $\theta_0$ is an interior point.
  \item $p(x; \theta)$ is twice continuously differentiable with respect to $\theta$ for all $\theta \in \Theta^*$.
  \item The set $\{x \in \mathbb{R}: p(x; \theta) > 0\}$ does not depend on $\theta$.
  \item The integral $\int_{-\infty}^{\infty} p(x; \theta) dx$ can be differentiated twice under the integral sign with respect to $\theta$.
  \item For all $\theta \in \Theta^*$, there exists a radius $r > 0$ and a random function $M(\mathbf{x})$ which is allowed to depend on the data $\mathbf{x}$ but not $\theta$ such that $\mathbb{E}[M(X); \theta_0] < \infty$ and $\underset{\theta \in \Theta^*}{\sup} |\nabla^2\mathcal{S}(\theta;x)_{ijk}| \leq M(\mathbf{x})$ for all integers $1 \leq i, j, k \leq d$. 
\end{enumerate}

RC1) is the identifiability assumption of our model that we discussed in Section 1.1. We repeat it here  is included merely to simplify calculations and frame our discussion in the context of a standard case in which likelihood theory holds. RC2-3) guarantee that it is always possible to solve the likelihood equation for $\hat{\theta}$. RC4-6) ensure that there is no issue with defining derivatives of the log-likelihood function. Finally, by Leibniz's integral rule RC7) implies that , and that the remainder term in the first-order Taylor series expansion of the score function centered at $\theta_0$ is negligible.

\section{The Bartlett Identities}

The Bartlett identities are a set of equations relating to the expectations of functions of derivatives of a log-likelihood function. 

Consider the case in which a random variable $X$ has a probability density $f$ that depends on a scalar parameter $\theta$. Denote the log-likelihood function for $\theta$ by $\ell(\theta; x) = \log f(x; \theta)$ and its first derivative with respect to $\theta$ by $\ell_{\theta}(\theta;x ) = \frac{\partial}{\partial \theta} \ell(\theta; x)$. Taking the expectation of $\ell_{\theta}(\theta; x)$ gives

$$
\begin{aligned}
\mathbb{E}\big[\ell_{\theta}(\theta; x); \theta \big]  &= \mathbb{E}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x); \theta\bigg] \\
                                                 &= \int_{\mathbb{R}} \bigg[\frac{\partial}{\partial \theta} \ell(\theta; x)\bigg] f(x; \theta) dx \\
                                                 &= \int_{\mathbb{R}} \bigg[\frac{\partial}{\partial \theta} \log f(x; \theta)\bigg] f(x; \theta) dx \\
                                                 &=  \int_{\mathbb{R}} \frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x; \theta)} f(x; \theta) dx \\
                                                 &=  \int_{\mathbb{R}} \frac{\partial}{\partial \theta} f(x; \theta) dx \\
                                                 &= \frac{d}{d \theta} \int_{\mathbb{R}} f(x; \theta) dx &&(\text{by regularity of } f)\\
                                                 &= \frac{d}{d \theta} 1 \\
                                                 &= 0. 
\end{aligned}
$$
Therefore, $$\mathbb{E}\big[\ell_{\theta}(\theta; x); \theta \big] = 0 \text{ for all } \theta.$${#eq-BI1} @eq-BI1 is called the first Bartlett identity. In words, it states that the expectation of the first derivative of the log-likelihood function of a statistical model with respect to the model parameter will always be 0. Another name for $\ell_{\theta}$ is the *score function*, and any pseudolikelihood that also satisfies the first Bartlett identity is said to be *score-unbiased*.

If we now consider the second derivative of $\ell(\theta; x)$, we have

$$\begin{aligned}
\ell_{\theta \theta}(\theta; x) &= \frac{\partial^2}{\partial \theta^2} \ell(\theta; x)\\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \log f(x;\theta) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\Bigg[\frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x; \theta)} \Bigg] \\
                                &= \frac{\Big[\frac{\partial^2}{\partial \theta^2} f(x; \theta)\Big] f(x;\theta) - \Big[\frac{\partial}{\partial \theta} f(x; \theta)\Big]\Big[\frac{\partial}{\partial \theta} f(x; \theta)\Big]}{\big[f(x; \theta) \big]^2} \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \Bigg[\frac{\frac{\partial}{\partial \theta} f(x; \theta)}{ f(x; \theta)}\Bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \bigg[\frac{\partial}{\partial \theta} \log f(x; \theta)\bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)} - \big[\ell_{\theta}(\theta; x) \big]^2. 
\end{aligned} 
$$
Rearranging terms and taking expectations yields 
$$
\begin{aligned}
\mathbb{E}[\ell_{\theta \theta}(\theta; x); \theta] + \mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2; \theta\Big] &= \mathbb{E}\Bigg[\frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)}; \theta\Bigg] \\
            &= \int_{\mathbb{R}} \Bigg[\frac{\frac{\partial^2}{\partial \theta^2} f(x; \theta)}{f(x; \theta)}\Bigg] f(x; \theta) dx \\
            &= \int_{\mathbb{R}} \frac{\partial^2}{\partial \theta^2} f(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} \int_{\mathbb{R}} f(x; \theta) dx &&(\text{by regularity of } f)\\
            &= \frac{d^2}{d \theta^2} 1 \\
            &= 0. 
\end{aligned}
$$
Therefore, $$\mathbb{E}[\ell_{\theta \theta}(\theta; x); \theta] + \mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2; \theta\Big] = 0 \text{ for all } \theta.$${#eq-BI2}

@eq-BI2 is called the second Bartlett identity. The second term on the left-hand side can be further rewritten as 

$$
\begin{aligned}
\mathbb{E}\Big[\big(\ell_{\theta}(\theta; x) \big)^2; \theta\Big] &= \mathbb{V}[\ell_{\theta}(\theta; x); \theta] +\Big(\mathbb{E}\big[\ell_{\theta}(\theta; x); \theta \big]\Big)^2 \\
                                                          &= \mathbb{V}[\ell_{\theta}(\theta; x); \theta]. &&(\text{by the first Bartlett identity})
\end{aligned}
$$
Another name for this quantity is the *expected information*. It follows from the second Bartlett identity that $$\mathbb{E}[-\ell_{\theta \theta}(\theta; x); \theta] = \mathbb{V}[\ell_{\theta}(\theta; x); \theta].$${#eq-BI3} The quantity $-\ell_{\theta \theta}(\theta; x)$ is called the *observed information*. Any pseudolikelihood that satisfies the second Bartlett identity is said to be *information-unbiased*.

It is possible to derive further Bartlett identities by continuing in this manner for an arbitrary number of derivatives of the log-likelihood function, provided that they exist. However, the first two are sufficient for our purposes of evaluating the validity of pseudolikelihoods as approximations to a genuine likelihood so we will not go further here. While the above derivations were performed under the assumption that $\theta$ is a scalar, the Bartlett identities also hold in the case where $\theta$ is a multi-dimensional vector.

\subsection{Single-Index Asymptotic Theory}

Single-index asymptotic theory describes the behavior of likelihood-based statistics as the sample size ($n$) grows to infinity while the dimension of the nuisance parameter ($m$) remains fixed. The aim of this section is to present a basic overview of the theory's results so that we will have a readily available baseline against which to compare the results of the following section discussing two-index asymptotic theory, in which the $m$ is allowed to increase with $n$.

Assume the regularity conditions of the previous section apply to our model and let $\hat{\theta}$ denote the MLE for $\theta_0$. The traditional method for analyzing the asymptotic behavior of $\hat{\theta}$ is to use a first-order Taylor series expansion of the score function.^[See Appendix \ref{appendix:A} for a review of Taylor's theorem and the conditions under which it is satisfied.] 

To do this, first note that the likelihood function for $\theta$ based on $\mathbf{x} = (x_1, ..., x_n)$ as $$L(\theta; \mathbf{x}) \equiv p(\mathbf{x}; \theta) = \prod_{i=1}^n p(x_i; \theta).$$ We may therefore write the log-likelihood function as 
$$
\begin{aligned}
\ell(\theta; \mathbf{x}) &= \log L(\theta; \mathbf{x}) \\
                         &= \log p(\mathbf{x}; \theta) \\
                         &= \log\Bigg[ \prod_{i=1}^n p(x_i; \theta)\Bigg] \\
                         &= \sum_{i=1}^n \log p(x_i; \theta) \\
                         &= \sum_{i=1}^n \ell(\theta; x_i).
\end{aligned}
$$
It follows that the score function can be written as
$$
\begin{aligned}
\mathcal{S}(\theta; \mathbf{x}) &= \frac{\partial}{\partial \theta} \ell(\theta; \mathbf{x}) \\
             &= \frac{\partial}{\partial \theta} \sum_{i=1}^n \ell(\theta; x_i) \\
             &= \sum_{i=1}^n \frac{\partial}{\partial \theta} \ell(\theta; x_i) \\
             &= \sum_{i=1}^n \mathcal{S}(\theta; x_i).
\end{aligned}
$${#eq-score_additive}
In other words, the score function for the parameter $\theta$ based on data $x_1, ..., x_n$ can be written as the sum of individual contributions $\mathcal{S}(\theta; x_i)$, $i = 1,..., n$, where each $\mathcal{S}(\theta; x_i)$ can be thought of as the score function for $\theta$ had we drawn only one observation, $x_i$. Note that these individual contributions are all independent from one another, a consequence of the independence assumption in RC1).

@eq-score_additive implies that a Taylor series expansion of $\mathcal{S}(\theta; \mathbf{x})$ will be equal to the sum of the Taylor series expansions of its individual contributions, plus a remainder term that grows with $n$. We have assumed each $x_i$ is identically distributed, so it suffices to consider the expansion for an arbitrary contribution, $\mathcal{S}(\theta; x_i)$.

Since $\theta$ is a $d \times 1$ vector, the $k$-th derivative of the log-likelihood with respect to $\theta$ will be a $k$-dimensional array having $d$ entries along each of its $k$ indices. In particular, the score function (i.e. the first derivative of the log-likelihood) will also be a $d \times 1$ vector, $$\mathcal{S}(\theta; x_i) = \begin{pmatrix} \mathcal{S}_1(\theta; x_i) \\ \vdots \\ \mathcal{S}_d(\theta; x_i) \end{pmatrix},$$ where each component is a function $S_j: \Theta \to \mathbb{R}$. Similarly, the first and second derivatives of the score function will be a $d \times d$ matrix and a three-dimensional $d \times d \times d$ array, respectively. To simplify notation, we will perform a component-wise first-order Taylor series expansion of the score function around the point $\theta = \theta_0$ wherein each component $\mathcal{S}_j(\theta; x_i)$ of $\mathcal{S}(\theta; x_i)$ is expanded separately.

For any $\theta \in N_r(\theta_0)$, there exists $\bar{\theta}_j$ on the line segment connecting $\theta$ and $\theta_0$ such that 
$$
\begin{aligned}
\mathcal{S}_j(\theta; x_i) &= \mathcal{S}_j(\theta_0; x_i) + \nabla \mathcal{S}_j(\theta_0; x_i)^T(\theta - \theta_0) + \frac{1}{2}(\theta - \theta_0)^T\nabla^2 \mathcal{S}_j(\bar{\theta}_j; x_i)(\theta - \theta_0) \\
                 &= \mathcal{S}_j(\theta_0; x_i) + [\nabla \mathcal{S}_j(\theta_0; x_i) + \frac{1}{2}\nabla^2 \mathcal{S}_j(\bar{\theta}_j; x_i)(\theta - \theta_0)]^T(\theta - \theta_0) \\
                 &= \mathcal{S}_j(\theta_0; x_i) + [\nabla \mathcal{S}_j(\theta_0; x_i) + M(x_i)O(||\theta - \theta_0||)]^T(\theta - \theta_0),
\end{aligned}
$$
where the last equality follows as a result of RC7). Summing over each of the individual contributions to the score function yields
$$
\begin{aligned}
\mathcal{S}_j(\theta; \mathbf{x}) &= \sum_{i=1}^n \mathcal{S}_j(\theta; x_i) \\
                                  &= \sum_{i=1}^n\big[\mathcal{S}_j(\theta_0; x_i) + [\nabla \mathcal{S}_j(\theta_0; x_i) + M(x_i)O(||\theta - \theta_0||)]^T(\theta - \theta_0)\big] \\
                                  &= \mathcal{S}_j(\theta_0; \mathbf{x}) + \Bigg[\nabla \mathcal{S}_j(\theta_0; \mathbf{x}) + \Bigg\{\sum_{i=1}^nM(x_i) \Bigg\}O(||\theta - \theta_0||)\Bigg]^T(\theta - \theta_0) \\
                                  &= \mathcal{S}_j(\theta_0; \mathbf{x}) + \Bigg[\frac{1}{n}\nabla \mathcal{S}_j(\theta_0; \mathbf{x}) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\theta - \theta_0||)\Bigg]^Tn(\theta - \theta_0).
\end{aligned} 
$$
If we divide through by $\sqrt n$, we arrive at $$\frac{1}{\sqrt n}\mathcal{S}_j(\theta; \mathbf{x}) = \frac{1}{\sqrt n}\mathcal{S}_j(\theta_0; \mathbf{x}) + \Bigg[\frac{1}{n}\nabla \mathcal{S}_j(\theta_0; \mathbf{x}) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\theta - \theta_0||)\Bigg]^T\sqrt{n}(\theta - \theta_0).$$







