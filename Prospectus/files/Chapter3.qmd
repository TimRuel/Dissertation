## The Likelihood Function

\doublespacing

Once we have chosen a model $(\mathcal{S}, \mathcal{P})$, the goal of statistical inference is to identify the distribution in $\mathcal{P}$ that best approximates the truth. 

Since we have assumed our model is identifiable, this is equivalent to estimating the value of the $k$-dimensional parameter $\theta$ on the basis of some data we observe.

We can accomplish task by analyzing the model's likelihood function.

## What is a Pseudolikelihood Function?

Nuisance parameters in a statistical model are a serious hindrance to making inferences about the parameter of interest. Consequently, statisticians have dedicated much time to developing techniques that will eliminate them from their models.

If we let $$\Theta(\psi) = \{\theta \in \Theta \> | \> \varphi(\theta) = \psi \},$$ then corresponding to $\psi \in \Psi$ is the set of likelihoods $$\mathcal{L}_{\psi} = \{L(\theta) \> | \> \theta \in \Theta(\psi)\}.$$

Statisticians will often attempt to find a summary of the values in $\mathcal{L}_{\psi}$ that does not depend on $\lambda$. This summary is called a **pseudolikelihood function** and can take several different forms:
    - Maximizing
    - Conditioning
    - Marginalizing*
    - **Integrating**

### Profile likelihood

### Conditional Likelihood

### Marginal Likelihood

### Integrated Likelihood

## Why Use an Integrated Likelihood?

The appeal of the integrated likelihood function as a means of eliminating nuisance parameters from the model is that it incorporates

# Approximating the Integrated Likelihood

## The Zero-Score Expectation Parameter

## Markov Chain Monte Carlo

## The IL Algorithm

# Applications

## Multinomial Distribution

## Standardized Mean Difference

# References