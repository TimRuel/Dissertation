\chapter{The Likelihood Function}

\section{Definition}

Upon choosing a statistical model that we think best characterizes our population of interest, the obvious next step is to identify the true distribution in $\mathcal{P}$ or at the very least, the one that best approximates the truth. This is equivalent to making inferences about $\theta_0$ in the case where the model is parametric and identifiable. That is, given the particular form(s) we have chosen for the distributions in $\mathcal{P}$, the only unknown remaining is the value of $\theta_0$ itself. Since this value is ultimately what controls the mechanism generating any sample of data $\mathbf{x}_n = (x_1, ..., x_n)$ that we might observe from the population, it stands to reason that information regarding $\theta_0$ can be inferred from the specific values of $x_1, ..., x_n$ that we obtain. To make this notion more rigorous, we require some method of analyzing the joint probability of our sample as a function of our parameter $\theta$.

Given some observed data $\mathbf{x}_n$, the *likelihood function* for $\theta$ is defined as $$L(\theta) = L(\theta; \mathbf{x}_n) = p(\mathbf{x}_n; \theta), \> \> \theta \in \Theta.$${#eq-likelihood_1} In other words, the value of the likelihood function evaluated at a particular $\theta \in \Theta$ is simply equal to the output of the model's density function evaluated at the same inputs. However, while $p(\mathbf{x}_n; \theta)$ is viewed primarily as a function of $\mathbf{x}_n$ for fixed $\theta$, the reverse is actually true for $L(\theta; \mathbf{x}_n)$. Indeed, we regard the likelihood as being a function of the parameter $\theta$ for fixed $\mathbf{x}_n$. The reversal of the order of the arguments $\theta$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\theta; x)$ as the probability that $X = x$ given that $\theta$ is the true parameter value. Crucially, this is *not* equivalent to the inverse probability that $\theta$ is the true parameter value given $X = x$. The likelihood does not directly tell us anything about the probability that $\theta$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\theta$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous interpretation of $L(\theta)$ as a probability since the probability that $X$ takes on any particular value is now 0. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny ball centered at $x$. Specifically, for two different observations $x_1$ and $x_2$, if $L(\theta; x_1) = c \cdot L(\theta; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\theta$ is the true value of the parameter.

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\theta; \mathbf{x}_n)$ to make probabilistic assertions regarding $\theta$. Despite our use of probability in its definition, the likelihood itself is *not* a probability density function for the parameter $\theta$ and is subject to neither the same rules nor interpretations as one. 

\section{Transformations}

There are a few useful transformations of the likelihood function that we will define here for use in future sections. The first is the *log-likelihood function*, which is defined as the natural logarithm of the likelihood function: $$\ell(\theta) = \ell(\theta; \mathbf{x}_n) = \log L(\theta; \mathbf{x}_n), \> \> \theta \in \Theta.$${#eq-loglike} In practice, we will typically eschew direct analysis of the likelihood in favor of the log-likelihood due to the nice mathematical properties logarithms possess. Chief among these properties is the ability to turn products into sums (i.e. $\log(ab) = \log(a) + \log(b)$ for $a,b > 0$). Sums tend to be easier to differentiate than products, making this is a particularly useful feature for likelihood functions, which are often expressed as the product of marginal density functions when the observations are independent. 

The other key property of logarithms that makes the log-likelihood so useful is that they are strictly increasing functions of their arguments (i.e. $\log x > \log y$ for $x > y > 0$). This monotonicity ensures that the locations of a function's extrema are preserved when the function is passed to the argument of a logarithm. For example, for a positive function $f$ with a global maximum, $\underset{x}{\mathrm{argmax}} f(x) = \underset{x}{\mathrm{argmax}} \log f(x)$. 

In the general case in which $\theta$ is a $d$-dimensional vector, where $d$ is an integer greater than 1, it follows that the first derivative of the log-likelihood with respect to $\theta$ will also be a $d$-dimensional vector, the second derivative will be a $d \times d$ matrix, the third derivative will be a $d \times d \times d$ array, and so forth. To emphasize the multidimensional nature of these results, we will use notation typically associated with partial derivatives involving functions of more than one variable (e.g. $\nabla, \mathbf{J}, \mathbf{H}$, etc.) along with subscripts that indicate the variable with respect to which the partial derivatives are being taken. See Appendix \ref{appendix:A} for a review of this notation.

The gradient of $\ell$ with respect to $\theta$ appears frequently enough in the analysis of likelihood functions that it has earned its own name - the *score function*, or just the *score*. Formally, it is defined as 
$$\mathcal{S}(\theta; \mathbf{x}_n) = \nabla_{\theta}\ell(\theta; \mathbf{x}_n) = \begin{pmatrix} 
\frac{\partial }{\partial\theta_1}\ell(\theta; \mathbf{x}_n) \\ 
\vdots \\ 
\frac{\partial }{\partial\theta_d}  \ell(\theta; \mathbf{x}_n)
\end{pmatrix} =
\begin{pmatrix} 
\mathcal{S}_1(\theta; \mathbf{x}_n) \\ 
\vdots \\ 
\mathcal{S}_d(\theta; \mathbf{x}_n) 
\end{pmatrix},$${#eq-score} where we think of each component as being a function $S_j: \Theta \to \mathbbm{R}$.

Similarly, the Hessian matrix of the log-likelihood function with respect to $\theta$ (i.e. the transpose of the Jacobian matrix of the score) multiplied by $-1$ is called the *observed information*, or just the *information*, and is denoted by
$$\mathcal{I}(\theta) = -\mathbf{H}_{\theta}\Big(\ell(\theta; \mathbf{x}_n)\Big) = -\mathbf{J}_{\theta}\Big(\mathcal{S}(\theta; \mathbf{x}_n)\Big)^\top = -
\begin{pmatrix} 
\frac{\partial^2 \ell}{\partial\theta_1^2} & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_d} \\ 
\frac{\partial^2 \ell}{\partial \theta_2 \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_2^2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_2\partial\theta_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2 \ell}{\partial \theta_d \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_d \partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_d^2}
\end{pmatrix}.$${#eq-obs_information} The use of the term "information" here derives from the fact that the second partial derivatives of $\ell$ with respect to the components of $\theta$ are all related to the curvature of $\ell$ near its maximum - the sharper the curve, the less uncertainty and therefore more information we have about $\theta$. 

Recall that $L(\theta; \mathbf{x}_n)$ is defined as a function of $\theta$ for a fixed sample of observations $\mathbf{x}_n = (x_1, ..., x_n)$, where we think of each $x_i$ as being a realization of a random variable $X_i$. We may therefore interpret $L(\theta; \mathbf{x}_n)$ as a random variable in the following sense: for a given $\theta$, the value of $L(\theta; \mathbf{x}_n)$ depends entirely on the values of $X_1, ..., X_n$ that we happened to observe, and so $L(\theta; \mathbf{X}_n)$ is itself a random variable with respect to the joint probability distribution of $\mathbf{X}_n = (X_1, ..., X_n)$. The same is also true for any function or estimate based on the likelihood, as they ultimately will all depend on the data through it as well. Going forward, we will use capital letters inside these functions when we want to emphasize this interpretation. For example, $\mathcal{S}(\theta; \mathbf{X}_n)$ is a random variable for which we have observed the value $\mathcal{S}(\theta; \mathbf{x}_n)$.

The random nature of these likelihood-based quantities further implies that finding their expectations and variances with respect to $p_{\theta}(\mathbf{x}_n)$ is a well-defined, nontrivial task. The variance of the score function will be of particular importance, as it also relates to the amount of information pertaining to $\theta_0$ that is contained within the log-likelihood function of our model. Properly known as the *Fisher information* or the *expected information*, it is defined as $$\mathscr{I}_{\mathbf{X}_n}(\theta) = \text{Var}_{\theta}\big[\mathcal{S}(\theta; \mathbf{X}_n)\big].$${#eq-exp_information1}  

Since we are working in the more general framework in which $\mathcal{S}(\theta)$ is a $d \times 1$ random vector, it would be more accurate to speak of the *Fisher information matrix*, which is equal to the variance-covariance matrix of $\mathcal{S}(\theta)$. Hence, we have 
$$
\begin{aligned}
\mathscr{I}_{\mathbf{X}_n}(\theta) &= \text{Var}_{\theta}\big[\mathcal{S}(\theta; \mathbf{X}_n)\big] &&(\text{by Eq. 2.2.4})\\
                    &= \text{Cov}_{\theta}\bigg[\begin{pmatrix} \frac{\partial \ell}{\partial\theta_1,} \cdots,  \frac{\partial \ell}{\partial\theta_d}  \end{pmatrix}^T\bigg] &&(\text{by Eq. 2.2.2})\\
                    &= \begin{pmatrix} 
\text{Var}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\text{Cov}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Var}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\text{Cov}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Var}_{\theta}\Big(\frac{\partial \ell}{\partial\theta_d}\Big)
\end{pmatrix}.
\end{aligned}
$${#eq-exp_information2}

Note that if the observations are independent, the Fisher information of the whole sample is equal to the sum of the Fisher information values for each of the observations individually. That is, $$\mathscr{I}_{\mathbf{X}_n}(\theta) = \sum_{i=1}^n \mathscr{I}_{X_i}(\theta).$${#eq-exp_information3} If the observations are also identically distributed according to the distribution of some random variable $X$, then $\mathscr{I}_{X_i}(\theta) = \mathscr{I}_X(\theta)$ for all $i$, and so the Fisher information for the entire sample is simply equal to the Fisher information for a single observation of $X$ multiplied by a factor of $n$: $$\mathscr{I}_{\mathbf{X}_n}(\theta) = n\mathscr{I}_{X}(\theta).$${#eq-exp_information4}

\section{Regularity Conditions}

As a consequence of the random variable interpretation of likelihood-based quantities, a natural line of inquiry to investigate is how the behavior of these random variables changes as the sample size $n$ increases. Of particular interest is the distribution to which the score function converges, if any, as $n$ tends toward infinity. To that end, it will be useful to establish some *regularity conditions* for our models. We can think of these conditions as being assumptions similar to those we discussed in the introduction to this paper that, when satisfied, endow our models with certain properties that enable us to determine the aforementioned distribution. 

For our purposes, we will call a model *regular* if it satisfies the following conditions:
\begin{enumerate}[label = \textbf{RC\arabic*)}]
  \item Any observations $x_1, ..., x_n$ belonging to a sample that has been drawn from the model's sample space are independent and identically distributed (i.i.d.) realizations of a random variable $X$ with density function $p_{\theta}(x)$.
  \item $P_{\theta_1} = P_{\theta_2} \implies \theta_1 = \theta_2$ for all $\theta_1, \theta_2 \in \Theta$.
  \item The distributions in $\mathcal{P}$ have a common support $\mathcal{X} = \{x: p_{\theta}(x) > 0 \}\subseteq \mathbbm{R}$ not depending on $\theta$.
  \item There exists an open set $\Theta^* \subseteq \Theta$ of which $\theta_0$ is an interior point.
  \item For all $\theta \in \Theta^*$, $p(x; \theta)$ is twice continuously differentiable with respect to $\theta$.
  \item There exists a random function $M(x)$ (that does not depend on $\theta$) satisfying $E_{\theta_0}[M(X)] < \infty$ and a closed ball $B_r[\theta_0] \subseteq \Theta^*$ such that for all integers $1 \leq i, j \leq d$, $$\sup_{\theta \in B_r[\theta_0]}\Bigg|\frac{\partial^2 \ell(\theta;x)}{\partial \theta_i \partial \theta_j}\Bigg| \leq M(x).$$
  \item The integral $\int_{\mathcal{X}} p(x; \theta) dx$ can be differentiated twice under the integral sign with respect to $\theta\in \Theta^*$.
  \item The Fisher information matrix $\mathscr{I}_{X}(\theta)$ is positive definite with finite components.
\end{enumerate}
While not strictly necessary, **RC1)** is often assumed as a matter of convenience since it tends to simplify calculations greatly. We include it here for that purpose and to frame our discussion in the context of a standard case in which likelihood theory holds. Of course, many of the results discussed in this paper will hold for models with observations not satisfying the i.i.d. criterion as well. 

The implication in **RC2)** is simply the identifiability assumption we mentioned in Chapter 2. We have restated it here as it is necessary to guarantee the uniqueness of the maximum likelihood estimator. In particular, we must have that $p(\mathbf{x}_n; \theta) = p(\mathbf{x}_n; \theta_0)$ implies $\theta = \theta_0$. 

**RC3)** requires that the distributions in $\mathcal{P}$ be supported on a common subset of the real line, and the definition of this subset cannot depend on $\theta$. This is to prevent situations in which, for example, the event $\{X_i \leq x_i \}$ occurs with positive probability when $\theta = \theta_1$ but not $\theta = \theta_2$. 

**RC4)** guarantees the existence of a ball centered at $\theta_0$ on which partial derivatives with respect to $\theta$ can be defined. **RC5)** goes on to assert directly the existence and continuity of the first two of these partial derivatives, which is necessary for defining second-order Taylor series expansions of the score function around $\theta_0$. **RC6)** is included to ensure the remainder terms in these expansions are negligible. 

**RC7)** grants us the ability to freely interchange integration and second-order partial differentiation with respect to $\theta$, i.e., $$\nabla^2_{\theta}\int_{\mathcal{X}} p(x; \theta) dx = \int_{\mathcal{X}} \nabla^2_{\theta}p(x; \theta) dx$$ for all $\theta \in \Theta^*$. Note that this implies first-order partial derivatives can be passed under the integral sign as well.

As we will see in Section 3.6, the variance of the limiting distribution of the score function involves the inverse of the Fisher information. The requirement in **RC8)** that the Fisher information matrix be positive definite, i.e., $\mathbf{z}^\top\mathscr{I}_X(\theta)\mathbf{z} > 0$ for all $d \times 1$ column vectors $\mathbf{z}$, along with all of its components being finite precludes any transformation of this limiting distribution from having infinite variance or degenerating to a single point.

\section{Maximum Likelihood Estimation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, the sensible choice is the one which makes the data we actually did observe more probable to have been observed. We have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\theta_1; \mathbf{x}_n) > L(\theta_2; \mathbf{x}_n)$, then under the preceding logic, $\theta_1$ is the better estimate of the true parameter value.

This can be extended to include as many candidate parameter values as we would like. For $n$ potential estimates of $\theta_0$, the best is the one that corresponds to the highest value of the likelihood function. Following this line of reasoning to its natural conclusion, it would seem that a sensible choice for an estimate of $\theta_0$ is the value that maximizes the likelihood function based on an observed dataset $\mathbf{x}_n$. To make this argument rigorous, it suffices to show that with probability tending to $1$ as the sample size tends toward infinity, the likelihood will be strictly larger at $\theta_0$ than for any other $\theta \in \Theta$. We start by observing that **RC1)** implies $$L(\theta; \mathbf{x}_n) = p(\mathbf{x}_n; \theta) = \prod_{i=1}^n p(x_i; \theta)$${#eq-MLE1} and $$\ell(\theta; \mathbf{x}_n) = \log p(\mathbf{x}_n; \theta) = \sum_{i=1}^n \log p(x_i; \theta).$${#eq-MLE2}  It follows that 
$$
\begin{aligned}
L(\theta; \mathbf{x}_n) < L(\theta_0; \mathbf{x}_n) &\iff \ell(\theta; \mathbf{x}_n) < \ell(\theta_0; \mathbf{x}_n) \\
                                                    &\iff \sum_{i=1}^n \log p(x_i; \theta) - \sum_{i=1}^n \log p(x_i; \theta_0) < 0 \\
                                                    &\iff \sum_{i=1}^n \big[\log p(x_i; \theta) - \log p(x_i; \theta_0)\big] < 0 \\
                                                    &\iff \sum_{i=1}^n \log\frac{p(x_i; \theta)}{p(x_i; \theta_0)} < 0 \\
                                                    &\iff \frac{1}{n}\sum_{i=1}^n \log\frac{p(x_i; \theta)}{p(x_i; \theta_0)} < 0.
\end{aligned}
$${#eq-MLE3}
Note that **RC3)** guarantees that the ratio $p(x; \theta) / p(x; \theta_0)$ is well-defined and finite for all $x \in \mathcal{X}$, the region of common support. Then by the Weak Law of Large Numbers, $$\frac{1}{n}\sum_{i=1}^n \log\frac{p(X_i; \theta)}{p(X_i; \theta_0)} \to \text{E}_{\theta_0}\Bigg[ \log \frac{p(X; \theta)}{p(X; \theta_0)} \Bigg]$${#eq-MLE4} in probability as $n \to \infty$. Furthermore, $$\text{E}_{\theta_0} \Bigg[\frac{p(X; \theta)}{p(X; \theta_0)} \Bigg] = \int_{\mathcal{X}}\Bigg[\frac{p(x; \theta)}{p(x; \theta_0)} \Bigg] p(x; \theta_0)dx = \int_{\mathcal{X}}p(x; \theta)dx = 1.$${#eq-MLE5}
Since $\log(x)$ is a strictly concave function, it follows from Jensen's inequality (see Appendix \ref{appendix:A}) and @eq-MLE5 $$\text{E}_{\theta_0}\Bigg[ \log \frac{p(X; \theta)}{p(X; \theta_0)} \Bigg] < \log \text{E}_{\theta_0} \Bigg[\frac{p(X; \theta)}{p(X; \theta_0)} \Bigg] = \log 1 = 0.$${#eq-MLE6} Hence, the quantity on the left-hand side of @eq-MLE4 is converging in probability to a constant that is less than 0 as $n$ tends to infinity. From this and the equivalence we established in @eq-MLE3, it therefore follows that $$\lim_{n \to \infty} P_{\theta_0} \big[L(\theta; \mathbf{x}_n) < L(\theta_0; \mathbf{x}_n)\big] = \lim_{n \to \infty} P_{\theta_0} \Bigg[\frac{1}{n}\sum_{i=1}^n \log\frac{p(X_i; \theta)}{p(X_i; \theta_0)} < 0\Bigg] = 1,$${#eq-MLE7} and the result is proved.

Let $\hat{\theta} = \hat{\theta}(\mathbf{x}_n)$ be the value that renders the likelihood at the observed $\mathbf{X}_n = \mathbf{x}_n$ as large as possible, i.e., $$L(\hat{\theta}(\mathbf{x}_n)) = \sup_{\theta \in \Theta} L(\theta).$${#eq-MLE8} If $\hat{\theta}(\mathbf{X}_n)$ is unique, it is called the *maximum likelihood estimator* (MLE) of $\theta_0$. For an arbitrarily chosen model, there is no guarantee that $\hat{\theta}(\mathbf{x}_n)$ will exist, and even if it does, it will not necessarily be unique. We say that the MLE is undefined in these cases. 

If $\Theta$ is a finite parameter space, meaning there exists some positive integer $K$ such that $\Theta = \{\theta_0, \theta_1, ..., \theta_K\}$, then it can be shown that for models satisfying **RC1-3)**, the MLE exists, is unique with probability tending to 1, and is consistent for $\theta_0$, i.e., it converges in probability to $\theta_0$ as $n \to \infty$. In this case, we can define the MLE as $$\hat{\theta}(\mathbf{X}_n) = \underset{\theta \in \Theta}{\mathrm{argmax}} \> L(\theta; \mathbf{X}_n) = \underset{\theta \in \Theta}{\mathrm{argmax}} \> \ell(\theta; \mathbf{X}_n).$${#eq-MLE8} 

Of course, it is rare in practice to use a model with a parameter space made up of only finitely many or even countably infinitely many elements. And in the event that we do encounter a situation where such a model is warranted, the lack of continuity in $\theta$ would make it impossible to define partial derivatives of the log-likelihood with respect to $\theta$, invalidating many of our other regularity conditions. Much more practical is the case in which $\Theta$ contains uncountably infinite parameter values. This is satisfied by our earlier assumption that $\Theta \subseteq \mathbbm{R}^d$, provided that it is possible for each component of $\theta$ to take on values in at least one non-degenerate interval of $\mathbbm{R}$. 

Supposing this is true, if **RC4-5)** hold in addition to **RC1-3)**, then there will exist at least one sequence of roots of the score function $\hat{\theta}_n = \hat{\theta}(\mathbf{x}_n)$, i.e. solutions to the following *likelihood equation*, $$\nabla_{\theta} \ell(\theta; \mathbf{x}_n) = \mathbf{0},$${#eq-MLE9} such that $\hat{\theta}(\mathbf{x}_n)$ tends to $\theta_0$ in probability as $n \to \infty$ (Cf. Cram√©r 1945). If this sequence of roots is unique, then the MLE exists and is equal to $\hat{\theta}(\mathbf{X}_n)$, and must therefore be a consistent estimator of $\theta_0$. We will explore more of the asymptotic properties of the MLE in Section 3.6.

\section{The Bartlett Identities}

The Bartlett identities are a set of equations relating to the expectations of the derivatives of a log-likelihood function to one another. In general, there is no guarantee that an arbitrary function of a random variable $X$ and its parameter $\theta$ will satisfy the Bartlett identities. It is guaranteed, however, that the log-likelihood function associated with $X$ and $\theta$ will satisfy them, provided that the model is regular. Thus, we can think of any function that does satisfy the Bartlett identities (or at least some of them) as resembling that of a genuine log-likelihood.

Consider the case where a random variable $X$ has density function $p_{\theta}(x)$, where $\theta$ is a scalar. For a single observation $X = x$, the expectation of $\frac{\partial}{\partial \theta}\ell(\theta; X)$ gives

$$
\begin{aligned}
E_{\theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\bigg] &= \int_{\mathbbm{R}} \bigg[\frac{\partial}{\partial \theta} \log p(x; \theta)\bigg] p(x; \theta) dx \\
                                                                       &=  \int_{\mathbbm{R}} \frac{\frac{\partial}{\partial \theta} p(x; \theta)}{p(x; \theta)} p(x; \theta) dx \\
                                                                       &=  \int_{\mathbbm{R}} \frac{\partial}{\partial \theta} p(x; \theta) dx \\
                                                                       &= \frac{d}{d \theta} \int_{\mathbbm{R}} p(x; \theta) dx \\
                                                                       &= \frac{d}{d \theta} 1 \\
                                                                       &= 0. 
\end{aligned}
$${#eq-BI1}
@eq-BI1 is called the first Bartlett identity. In words, it states that the expectation of the first partial derivative of the log-likelihood function of a statistical model with respect to the parameter will always be 0. Since the score is defined as $\frac{\partial}{\partial \theta}\ell(\theta; x)$, any function that satisfies the first Bartlett identity is said to be *score-unbiased*.

For any model with a log-likelihood satisfying the first Bartlett identity, the expected information for its parameter $\theta$ may be rewritten as 
$$
\begin{aligned}
\mathscr{I}_{X}(\theta) &= \text{Var}_{\theta}\big[\mathcal{S}(\theta; X)\big] \\
                    &= \text{Var}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg] \\
                    &= \text{Var}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg] +\Bigg(\text{E}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg]\Bigg)^2 &&(\text{by the first Bartlett identity}) \\
                    &= \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg].
\end{aligned}
$${#eq-BI2}

If we now consider the second partial derivative of $\ell(\theta; x)$ with respect to $\theta$, we have
$$
\begin{aligned}
\frac{\partial^2}{\partial \theta^2} \ell(\theta; x) &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \log p(x;\theta) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\Bigg[\frac{\frac{\partial}{\partial \theta} p(x;\theta)}{p(x;\theta)} \Bigg] \\
                                &= \frac{\Big[\frac{\partial^2}{\partial \theta^2} p(x;\theta)\Big] p(x;\theta) - \Big[\frac{\partial}{\partial \theta} p(x; \theta)\Big]\Big[\frac{\partial}{\partial \theta} p(x; \theta)\Big]}{\big[p(x; \theta) \big]^2} \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \Bigg[\frac{\frac{\partial}{\partial \theta} p(x; \theta)}{ p(x; \theta)}\Bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \bigg[\frac{\partial}{\partial \theta} \log p(x; \theta)\bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \Bigg[\frac{\partial}{\partial \theta}\ell(\theta; x) \Bigg]^2. 
\end{aligned} 
$$
Rearranging terms and taking expectations yields 
$$
\begin{aligned}
\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2\Bigg] &= \text{E}_{\theta}\Bigg[\frac{\frac{\partial^2}{\partial \theta^2} p(X; \theta)}{p(X; \theta)}\Bigg] \\
            &= \int_{\mathbbm{R}} \Bigg[\frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)}\Bigg] p(x; \theta) dx \\
            &= \int_{\mathbbm{R}} \frac{\partial^2}{\partial \theta^2} p(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} \int_{\mathbbm{R}} p(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} 1 \\
            &= 0. 
\end{aligned}
$$
Therefore, $$\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2 \Bigg] = 0.$${#eq-BI3}

@eq-BI2 is called the second Bartlett identity. Any function that satisfies it is said to be *information-unbiased*. Regular models as we have defined them will automatically satisfy both the first and second Bartlett identities. Hence, for any regular model, the statements in @eq-exp_information1, @eq-BI2, and @eq-BI3 imply that the following definitions for its expected information regarding its parameter $\theta$ are all equivalent: $$\mathscr{I}_{X}(\theta) = \text{Var}_{\theta}\Bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg] = \text{E}_{\theta}\Bigg[-\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg].$${#eq-BI4} 

It is possible to derive further Bartlett identities by continuing in this manner for an arbitrary number of partial $\theta$-derivatives of the log-likelihood function, provided that they exist. However, the first two are sufficient for our purposes of evaluating the validity of approximations to genuine likelihoods so we will not go further here. Note that while the above derivations were performed under the assumption that $\theta$ is a scalar, the Bartlett identities also hold in the case where $\theta$ is a multi-dimensional vector.

\section{One-Index Asymptotics}

The one-index asymptotics framework describes the behavior of likelihood-based statistics as the sample size ($n$) grows to infinity while the dimension of the nuisance parameter ($q$) remains fixed. The aim of this section is to present a basic overview of the theory's results so that we will have a readily available baseline against which to compare the results of the following section discussing the two-index asymptotics framework, in which $q$ is allowed to increase with $n$.

Assume the regularity conditions of the previous section apply to our model and let $\hat{\theta}$ denote the MLE for $\theta_0$. The traditional method for analyzing the asymptotic behavior of $\hat{\theta}$ is to use a first-order Taylor series expansion of the score function.^[See Appendix \ref{appendix:A} for a review of Taylor's theorem and the conditions under which it is satisfied.] 

We start by noting that under our assumption of i.i.d. observations, the score function is equal to
$$
\begin{aligned}
\mathcal{S}(\theta; \mathbf{x}_n) &= \nabla_{\theta} \ell(\theta; \mathbf{x}_n) \\
                                  &= \nabla_{\theta} \sum_{i=1}^n \ell(\theta; x_i) \\
                                  &= \sum_{i=1}^n \nabla_{\theta} \ell(\theta; x_i) \\
                                  &= \sum_{i=1}^n \mathcal{S}(\theta; x_i).
\end{aligned}
$${#eq-score_additive}
In other words, the score function for the parameter $\theta$ based on data $x_1, ..., x_n$ can be written as the sum of independent contributions $\mathcal{S}(\theta; x_i)$ ($i = 1,..., n$) where each $\mathcal{S}(\theta; x_i)$ can be thought of as the score function for $\theta$ based only on observation $x_i$. 

@eq-score_additive implies that a Taylor series expansion of $\mathcal{S}(\theta; \mathbf{x}_n)$ will be equal to the sum of the Taylor series expansions of its individual contributions, plus a remainder term that grows with $n$. Since the observations are identically distributed, it suffices to consider the expansion for an arbitrary contribution, $\mathcal{S}(\theta; x_i)$. Replacing the function $\mathcal{S}(\theta; x_i)$ in @eq-score_additive with its vector form as defined in @eq-score, we get $$\mathcal{S}(\theta; \mathbf{x}_n) = \sum_{i=1}^n \begin{pmatrix} 
\mathcal{S}_1(\theta; x_i) \\ 
\vdots \\ 
\mathcal{S}_d(\theta; x_i) 
\end{pmatrix}.$${#eq-score_additive_vector}

Note that **RC5)** guarantees the first partial derivative of $\mathcal{S}(\theta; x_i)$ exists and is continuous for all $x$we can expand $\mathcal{S}(\theta; x_i)$

We can therefore perform a component-wise first-order Taylor series expansion of a given score contribution $\mathcal{S}(\theta; x_i)$ around the point $\theta = \theta_0$, wherein each component $\mathcal{S}_j(\theta; x_i)$ of $\mathcal{S}(\theta; x_i)$ ($j = 1, ..., d$) is expanded separately.

For any $\theta \in N_r(\theta_0)$, there exists $\bar{\theta}_j$ on the line segment connecting $\theta$ and $\theta_0$ such that 
$$
\begin{aligned}
\mathcal{S}_j(\theta; x_i) &= \mathcal{S}_j(\theta_0; x_i) + \nabla_{\theta} \mathcal{S}_j(\theta_0; x_i)^T(\theta - \theta_0) + \frac{1}{2}(\theta - \theta_0)^T\nabla^2_{\theta} \mathcal{S}_j(\bar{\theta}_j; x_i)(\theta - \theta_0) \\
                 &= \mathcal{S}_j(\theta_0; x_i) + [\nabla_{\theta} \mathcal{S}_j(\theta_0; x_i) + \frac{1}{2}\nabla^2_{\theta} \mathcal{S}_j(\bar{\theta}_j; x_i)(\theta - \theta_0)]^T(\theta - \theta_0) \\
                 &= \mathcal{S}_j(\theta_0; x_i) + [\nabla_{\theta} \mathcal{S}_j(\theta_0; x_i) + M(x_i)O(||\theta - \theta_0||)]^T(\theta - \theta_0),
\end{aligned}
$$
where the last equality follows as a result of **RC6)**. Summing over each of the individual contributions to the score function yields
$$
\begin{aligned}
\mathcal{S}_j(\theta; \mathbf{x}_n) &= \sum_{i=1}^n \mathcal{S}_j(\theta; x_i) \\
                                  &= \sum_{i=1}^n\big[\mathcal{S}_j(\theta_0; x_i) + [\nabla_{\theta} \mathcal{S}_j(\theta_0; x_i) + M(x_i)O(||\theta - \theta_0||)]^T(\theta - \theta_0)\big] \\
                                  &= \mathcal{S}_j(\theta_0; \mathbf{x}_n) + \Bigg[\nabla_{\theta} \mathcal{S}_j(\theta_0; \mathbf{x}_n) + \Bigg\{\sum_{i=1}^nM(x_i) \Bigg\}O(||\theta - \theta_0||)\Bigg]^T(\theta - \theta_0) \\
                                  &= \mathcal{S}_j(\theta_0; \mathbf{x}_n) + \Bigg[\frac{1}{n}\nabla_{\theta} \mathcal{S}_j(\theta_0; \mathbf{x}_n) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\theta - \theta_0||)\Bigg]^Tn(\theta - \theta_0).
\end{aligned} 
$$
If we divide through by $\sqrt n$, we arrive at $$\frac{1}{\sqrt n}\mathcal{S}_j(\theta; \mathbf{x}_n) = \frac{1}{\sqrt n}\mathcal{S}_j(\theta_0; \mathbf{x}_n) + \Bigg[\frac{1}{n}\nabla_{\theta} \mathcal{S}_j(\theta_0; \mathbf{x}_n) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\theta - \theta_0||)\Bigg]^T\sqrt{n}(\theta - \theta_0).$$







