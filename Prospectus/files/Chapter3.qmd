\chapter{The Likelihood Function}

\section{Definition}

Upon choosing a statistical model that we think best characterizes our population of interest, the obvious next step is to identify the true distribution in $\mathcal{P}$ or at the very least, the one that best approximates the truth. This is equivalent to making inferences about $\symbf{\theta}_0$ in the case where the model is parametric and identifiable. That is, given the particular form(s) we have chosen for the distributions in $\mathcal{P}$, the only unknown remaining is the value of $\symbf{\theta}_0$ itself. Since this value is ultimately what controls the mechanism generating any sample of data $\mathbf{x}_n = (x_1, ..., x_n)$ that we might observe from the population, it stands to reason that information regarding $\symbf{\theta}_0$ can be inferred from the specific values of $x_1, ..., x_n$ that we obtain. To make this notion more rigorous, we require some method of analyzing the joint probability of our sample as a function of our parameter $\symbf{\theta}$.

Given some observed data $\mathbf{x}_n$, the *likelihood function* for $\symbf{\theta}$ is defined as $$L(\symbf{\theta}) = L(\symbf{\theta}; \mathbf{x}_n) = p(\mathbf{x}_n; \symbf{\theta}), \> \> \symbf{\theta} \in \Theta.$${#eq-likelihood_1} In other words, the value of the likelihood function evaluated at a particular $\symbf{\theta} \in \Theta$ is simply equal to the output of the model's density function evaluated at the same inputs. However, while $p(\mathbf{x}_n; \symbf{\theta})$ is viewed primarily as a function of $\mathbf{x}_n$ for fixed $\symbf{\theta}$, the reverse is actually true for $L(\symbf{\theta}; \mathbf{x}_n)$. Indeed, we regard the likelihood as being a function of the parameter $\symbf{\theta}$ for fixed $\mathbf{x}_n$. The reversal of the order of the arguments $\symbf{\theta}$ and $x$ is a reflection of this difference in perspectives.

When $X$ is discrete, we may interpret $L(\symbf{\theta}; x)$ as the probability that $X = x$ given that $\symbf{\theta}$ is the true parameter value.^[It is important to note here that whichever value of $\symbf{\theta}$ we choose to plug into $L(\symbf{\theta}; x)$ is the value that we are currently "pretending" is the true one, regardless of whether or not it actually equals $\symbf{\theta}_0$ in reality.] Crucially, this is *not* equivalent to the inverse probability that $\symbf{\theta}$ is the true parameter value given $X = x$. The likelihood does not directly tell us anything about the probability that $\symbf{\theta}$ assumes any particular value at all. Though intuitively appealing, this interpretation constitutes a fundamental misunderstanding of what a likelihood function is, and great care must be taken to avoid it. 

When $X$ is continuous, the likelihood for $\symbf{\theta}$ may still be defined as it is in @eq-likelihood_1. However, we must forfeit our previous interpretation of $L(\symbf{\theta})$ as a probability since the probability that $X$ takes on any particular value is now 0. We may however still think of the likelihood as being proportional to the probability that $X$ takes on a value "close" to $x$, meaning that that $X$ is within a tiny ball centered at $x$. Specifically, for two different observations $x_1$ and $x_2$, if $L(\symbf{\theta}; x_1) = c \cdot L(\symbf{\theta}; x_2)$, where $c > 1$, then under this model we may conclude $X$ is $c$ times more likely to assume a value closer to $x_1$ than $x_2$ given that $\symbf{\theta}$ is the true value of the parameter.

As in the discrete case, we must also be careful when $X$ is continuous to avoid using $L(\symbf{\theta}; \mathbf{x}_n)$ to make probabilistic assertions regarding $\symbf{\theta}$. Despite our use of probability in its definition, the likelihood itself is *not* a probability density function for the parameter $\symbf{\theta}$ and is subject to neither the same rules nor interpretations as one. 

\section{Transformations}

There are a few useful transformations of the likelihood function that we will define here for use in future sections. The first is the *log-likelihood function*, which is defined as the natural logarithm of the likelihood function: $$\ell(\symbf{\theta}) = \ell(\symbf{\theta}; \mathbf{x}_n) = \log L(\symbf{\theta}; \mathbf{x}_n).$${#eq-loglike} In practice, we will typically eschew direct analysis of the likelihood in favor of the log-likelihood due to the nice mathematical properties logarithms possess. Chief among these properties is the ability to turn products into sums (i.e. $\log(ab) = \log(a) + \log(b)$ for $a,b > 0$). Sums tend to be easier to differentiate than products, making this is a particularly useful feature for likelihood functions, which are often expressed as the product of marginal density functions when the observations are independent. 

The other key property of logarithms that makes the log-likelihood so useful is that they are strictly increasing functions of their arguments (i.e. $\log x > \log y$ for $x > y > 0$). This monotonicity ensures that the locations of a function's extrema are preserved when the function is passed to the argument of a logarithm. For example, for a positive function $f$ with a global maximum, $\underset{x}{\mathrm{argmax}} f(x) = \underset{x}{\mathrm{argmax}} \log f(x)$. 

In the general case in which $\symbf{\theta}$ is a $d$-dimensional vector, where $d$ is an integer greater than 1, it follows that the first derivative of the log-likelihood with respect to $\symbf{\theta}$ will also be a $d$-dimensional vector, the second derivative will be a $d \times d$ matrix, the third derivative will be a $d \times d \times d$ array, and so forth. To emphasize the multidimensional nature of these results, we will use notation typically associated with partial derivatives involving functions of more than one variable (e.g. $\nabla, \mathbf{J}, \mathbf{H}$, etc.) along with subscripts that indicate the variable with respect to which the partial derivatives are being taken. See Appendix \ref{appendix:A} for a review of this notation.

The gradient of $\ell$ with respect to $\symbf{\theta}$ appears frequently enough in the analysis of likelihood functions that it has earned its own name - the *score function*, or just the *score*. Formally, it is defined as 
$$\mathcal{S}(\symbf{\theta}; \mathbf{x}_n) = \nabla_{\symbf{\theta}}\ell(\symbf{\theta}; \mathbf{x}_n) = \begin{pmatrix} 
\frac{\partial }{\partial\symbf{\theta}_1}\ell(\symbf{\theta}; \mathbf{x}_n) \\ 
\vdots \\ 
\frac{\partial }{\partial\symbf{\theta}_d}  \ell(\symbf{\theta}; \mathbf{x}_n)
\end{pmatrix} =
\begin{pmatrix} 
\mathcal{S}_1(\symbf{\theta}; \mathbf{x}_n) \\ 
\vdots \\ 
\mathcal{S}_d(\symbf{\theta}; \mathbf{x}_n) 
\end{pmatrix},$${#eq-score} where we think of each component as being a function $S_j: \Theta \to \mathbbm{R}$.

Similarly, the Hessian matrix of the log-likelihood function with respect to $\symbf{\theta}$ (i.e. the transpose of the Jacobian matrix of the score) multiplied by $-1$ is called the *observed information*, or just the *information*, and is denoted by
$$\mathcal{I}(\symbf{\theta}) = -\mathbf{H}_{\symbf{\theta}}\Big(\ell(\symbf{\theta}; \mathbf{x}_n)\Big) = -\mathbf{J}_{\symbf{\theta}}\Big(\mathcal{S}(\symbf{\theta}; \mathbf{x}_n)\Big)^\top = -
\begin{pmatrix} 
\frac{\partial^2 \ell}{\partial\theta_1^2} & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_1\partial\theta_d} \\ 
\frac{\partial^2 \ell}{\partial \theta_2 \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_2^2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_2\partial\theta_d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2 \ell}{\partial \theta_d \partial\theta_1} & \frac{\partial^2 \ell}{\partial\theta_d \partial\theta_2} & \cdots & \frac{\partial^2 \ell}{\partial\theta_d^2}
\end{pmatrix}.$${#eq-obs_information} The use of the term "information" here derives from the fact that the second partial derivatives of $\ell$ with respect to the components of $\symbf{\theta}$ are all related to the curvature of $\ell$ near its maximum - the sharper the curve, the less uncertainty and therefore more information we have about $\symbf{\theta}$. 

Recall that $L(\symbf{\theta}; \mathbf{x}_n)$ is defined as a function of $\symbf{\theta}$ for a fixed sample of observations $\mathbf{x}_n = (x_1, ..., x_n)$, where we think of each $x_i$ as being a realization of a random variable $X_i$. We may therefore interpret $L(\symbf{\theta}; \mathbf{x}_n)$ as a random variable in the following sense: for a given $\symbf{\theta}$, the value of $L(\symbf{\theta}; \mathbf{x}_n)$ depends entirely on the values of $X_1, ..., X_n$ that we happened to observe, and so $L(\symbf{\theta}; \mathbf{X}_n)$ is itself a random variable with respect to the joint probability distribution of $\mathbf{X}_n = (X_1, ..., X_n)$. The same is also true for any function or estimate based on the likelihood, as they ultimately will all depend on the data through it as well. Going forward, we will use capital letters inside these functions when we want to emphasize this interpretation. For example, $\mathcal{S}(\symbf{\theta}; \mathbf{X}_n)$ is a random variable for which we have observed the value $\mathcal{S}(\symbf{\theta}; \mathbf{x}_n)$.

The random nature of these likelihood-based quantities further implies that finding their expectations and variances with respect to $p_{\symbf{\theta}}(\mathbf{x}_n)$ is a well-defined, nontrivial task. The variance of the score function will be of particular importance, as it also relates to the amount of information pertaining to $\symbf{\theta}_0$ that is contained within the log-likelihood function of our model. Properly known as the *Fisher information* or the *expected information*, it is defined as $$\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) = \text{Var}_{\symbf{\theta}}\big[\mathcal{S}(\symbf{\theta}; \mathbf{X}_n)\big].$${#eq-exp_information1}  

Since we are working in the more general framework in which $\mathcal{S}(\symbf{\theta})$ is a $d \times 1$ random vector, it would be more accurate to speak of the *Fisher information matrix*, which is equal to the variance-covariance matrix of $\mathcal{S}(\symbf{\theta})$. Hence, we have 
$$
\begin{aligned}
\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) &= \text{Var}_{\symbf{\theta}}\big[\mathcal{S}(\symbf{\theta}; \mathbf{X}_n)\big] &&(\text{by Eq. 2.2.4})\\
                    &= \text{Cov}_{\symbf{\theta}}\bigg[\begin{pmatrix} \frac{\partial \ell}{\partial\theta_1,} \cdots,  \frac{\partial \ell}{\partial\theta_d}  \end{pmatrix}^T\bigg] &&(\text{by Eq. 2.2.2})\\
                    &= \begin{pmatrix} 
\text{Var}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_1}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Var}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_2}, \frac{\partial \ell}{\partial\theta_d}\Big) \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_1}\Big) & \text{Cov}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_d}, \frac{\partial \ell}{\partial\theta_2}\Big) & \cdots & \text{Var}_{\symbf{\theta}}\Big(\frac{\partial \ell}{\partial\theta_d}\Big)
\end{pmatrix}.
\end{aligned}
$${#eq-exp_information2}

Note that if the observations are independent, the Fisher information of the whole sample is equal to the sum of the Fisher information values for each of the observations individually. That is, $$\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) = \sum_{i=1}^n \mathscr{I}_{X_i}(\symbf{\theta}).$${#eq-exp_information3} If the observations are also identically distributed according to the distribution of some random variable $X$, then $\mathscr{I}_{X_i}(\symbf{\theta}) = \mathscr{I}_X(\symbf{\theta})$ for all $i$, and so the Fisher information for the entire sample is simply equal to the Fisher information for a single observation of $X$ multiplied by a factor of $n$: $$\mathscr{I}_{\mathbf{X}_n}(\symbf{\theta}) = n\mathscr{I}_{X}(\symbf{\theta}).$${#eq-exp_information4}

\section{Regularity Conditions}

As a consequence of the random variable interpretation of likelihood-based quantities, a natural line of inquiry to investigate is how the behavior of these random variables changes as the sample size $n$ increases. Of particular interest is the distribution to which the score function converges, if any, as $n$ tends toward infinity. To that end, it will be useful to establish some *regularity conditions* for our models. We can think of these conditions as being assumptions similar to those we discussed in the introduction to this paper that, when satisfied, endow our models with certain properties that enable us to determine the aforementioned distribution. 

For our purposes, we will call a model *regular* if it satisfies the following conditions:
\begin{enumerate}[label = \textbf{RC\arabic*)}]
  \item Any observations $x_1, ..., x_n$ belonging to a sample that has been drawn from the model's sample space are independent and identically distributed (i.i.d.) realizations of a random variable $X$ with density function $p_{\symbf{\theta}}(x)$.
  \item $P_{\symbf{\theta}_1} = P_{\symbf{\theta}_2} \implies \symbf{\theta}_1 = \symbf{\theta}_2$ for all $\symbf{\theta}_1, \symbf{\theta}_2 \in \Theta$.
  \item The distributions in $\mathcal{P}$ have a common support $\mathcal{X} = \{x: p_{\symbf{\theta}}(x) > 0 \}\subseteq \mathbbm{R}$ not depending on $\symbf{\theta}$.
  \item There exists an open set $\Theta^* \subseteq \Theta$ of which $\symbf{\theta}_0$ is an interior point.
  \item $p(x; \symbf{\theta})$ is twice continuously differentiable with respect to $\symbf{\theta}$ for all $\symbf{\theta}$ in a neighborhood of $\symbf{\theta}_0$.
  \item There exists a random function $M(x)$ (that does not depend on $\symbf{\theta}$) satisfying $E_{\symbf{\theta}_0}[M(X)] < \infty$ such that each component of the observed information matrix $\mathcal{I}(\symbf{\theta})$ is bounded in absolute value by $M(x)$ uniformly in some neighborhood of $\symbf{\theta}_0$.
  \item The integral $\int_{\mathcal{X}} p(x; \symbf{\theta}) dx$ can be differentiated twice under the integral sign with respect to the components of $\symbf{\theta}\in \Theta^*$.
  \item $\mathscr{I}_{X}(\symbf{\theta})$ is positive definite for all $\symbf{\theta} \in \Theta$.
  \item $\Theta$ is a compact and convex subset of $\mathbbm{R}^d$.
\end{enumerate}
While not strictly necessary, **RC1)** is often assumed as a matter of convenience since it tends to simplify calculations greatly. We include it here for that purpose and to frame our discussion in the context of a standard case in which likelihood theory holds. Of course, it is possible to construct models lacking i.i.d. observations yet still possessing real world applications for which the results discussed in this paper hold. 

The implication in **RC2)** is simply the identifiability property we mentioned in Chapter 2. The theoretical capacity to identify the true parameter that generated the data we observed is sufficiently important to the underpinnings of statistical inference that any sensible definition of a regular model should include it.

**RC3)** requires that the distributions in $\mathcal{P}$ be supported on a common subset of the real line, and the definition of this subset cannot depend on $\symbf{\theta}$. This is to prevent situations in which, for example, the event $\{X_i \leq x_i \}$ occurs with positive probability when $\symbf{\theta} = \symbf{\theta}_1$ but not $\symbf{\theta} = \symbf{\theta}_2$. 

**RC4)** guarantees the existence of an open subset $\Theta^*$ of $\Theta$ containing $\symbf{\theta}_0$ as an interior point. The fact that $\symbf{\theta}_0$ is an interior point of $\Theta^*$ further implies that it is possible to find a neighborhood of $\symbf{\theta}_0$ that is contained in $\Theta^*$. **RC5)** then goes on to assert the existence and continuity of the first two partial derivatives with respect to the components of $\symbf{\theta}$ of $p(x; \symbf{\theta})$ in this neighborhood. This is a necessary requirement for defining a first-order Taylor series expansion of the score function around $\symbf{\theta}_0$. 

Another way of stating **RC6)** is that for all $\symbf{\theta}$ in a neighborhood $N_{\symbf{\theta}_0}$, there exists a random function $M(x)$ with finite expectation such that $$\sup_{\symbf{\theta} \in N_{\symbf{\theta}_0}}\Bigg|\frac{\partial^2 \ell(\symbf{\theta};x)}{\partial \theta_i \partial \theta_j}\Bigg| \leq M(x)$$ for all integers $1 \leq i, j \leq d$. This ensures the remainder terms in the Taylor series expansion of the score function become negligible as the sample size increases to infinity.

**RC7)** grants us the ability to freely interchange integration and second-order partial differentiation with respect to the components of $\symbf{\theta}$, i.e., $$\frac{\partial^2}{\partial \theta_i \partial \theta_j}\int_{\mathcal{X}} p(x; \symbf{\theta}) dx = \int_{\mathcal{X}} \frac{\partial^2}{\partial \theta_i \partial \theta_j}p(x; \symbf{\theta}) dx$$ for all $\symbf{\theta} \in \Theta^*$ and $i, j = 1, ..., d$. Note that this implies first-order partial derivatives can be passed under the integral sign as well. This will prove useful in our discussion of the Bartlett identities in Section 3.5. 

\section{Maximum Likelihood Estimation}

Maximum likelihood estimation is one of the most powerful and widespread techniques for obtaining point estimates of model parameters. The original intuition behind the method derives from the observation that when faced with a choice between two possible values of a parameter, the sensible choice is the one which makes the data we actually did observe more probable to have been observed. We have already defined the likelihood function as a means of capturing this probability, which makes expressing this decision rule in terms of it very easy - we simply choose for our estimate the option that produces the higher value of the likelihood function. That is, if $L(\symbf{\theta}_1; \mathbf{x}_n) > L(\symbf{\theta}_2; \mathbf{x}_n)$, then under the preceding logic, $\symbf{\theta}_1$ is the better estimate of the true parameter value.

This can be extended to include as many candidate parameter values as we would like. For $n$ potential estimates of $\symbf{\theta}_0$, the best is the one that corresponds to the highest value of the likelihood function. Following this line of reasoning to its natural conclusion, a sensible choice for an estimate of $\symbf{\theta}_0$ is any value that maximizes the likelihood function based on an observed dataset $\mathbf{x}_n$. To make this argument rigorous, it suffices to show that with probability tending to $1$ as the sample size tends toward infinity, the likelihood will be strictly larger at $\symbf{\theta}_0$ than for any other $\symbf{\theta} \in \Theta$. We start by observing that **RC1)** implies $$L(\symbf{\theta}; \mathbf{x}_n) = p(\mathbf{x}_n; \symbf{\theta}) = \prod_{i=1}^n p(x_i; \symbf{\theta})$${#eq-MLE1} and $$\ell(\symbf{\theta}; \mathbf{x}_n) = \log p(\mathbf{x}_n; \symbf{\theta}) = \sum_{i=1}^n \log p(x_i; \symbf{\theta}).$${#eq-MLE2}  It follows that 
$$
\begin{aligned}
L(\symbf{\theta}; \mathbf{x}_n) < L(\symbf{\theta}_0; \mathbf{x}_n) &\iff \ell(\symbf{\theta}; \mathbf{x}_n) < \ell(\symbf{\theta}_0; \mathbf{x}_n) \\
                                                    &\iff \sum_{i=1}^n \log p(x_i; \symbf{\theta}) - \sum_{i=1}^n \log p(x_i; \symbf{\theta}_0) < 0 \\
                                                    &\iff \sum_{i=1}^n \big[\log p(x_i; \symbf{\theta}) - \log p(x_i; \symbf{\theta}_0)\big] < 0 \\
                                                    &\iff \sum_{i=1}^n \log\frac{p(x_i; \symbf{\theta})}{p(x_i; \symbf{\theta}_0)} < 0 \\
                                                    &\iff \frac{1}{n}\sum_{i=1}^n \log\frac{p(x_i; \symbf{\theta})}{p(x_i; \symbf{\theta}_0)} < 0.
\end{aligned}
$${#eq-MLE3}
Note that **RC3)** guarantees that the ratio $p(x; \symbf{\theta}) / p(x; \symbf{\theta}_0)$ is well-defined and finite for all $x \in \mathcal{X}$, the region of common support. Then by the Weak Law of Large Numbers, $$\frac{1}{n}\sum_{i=1}^n \log\frac{p(X_i; \symbf{\theta})}{p(X_i; \symbf{\theta}_0)} \to \text{E}_{\symbf{\theta}_0}\Bigg[ \log \frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg]$${#eq-MLE4} in probability as $n \to \infty$. Furthermore, $$\text{E}_{\symbf{\theta}_0} \Bigg[\frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg] = \int_{\mathcal{X}}\Bigg[\frac{p(x; \symbf{\theta})}{p(x; \symbf{\theta}_0)} \Bigg] p(x; \symbf{\theta}_0)dx = \int_{\mathcal{X}}p(x; \symbf{\theta})dx = 1.$${#eq-MLE5}
Since $\log(x)$ is a strictly concave function, it follows from Jensen's inequality (see Appendix \ref{appendix:A}) and @eq-MLE5 $$\text{E}_{\symbf{\theta}_0}\Bigg[ \log \frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg] < \log \text{E}_{\symbf{\theta}_0} \Bigg[\frac{p(X; \symbf{\theta})}{p(X; \symbf{\theta}_0)} \Bigg] = \log 1 = 0.$${#eq-MLE6} Hence, the quantity on the left-hand side of @eq-MLE4 is converging in probability to a constant that is less than 0 as $n$ tends to infinity. From this and the equivalence we established in @eq-MLE3, it follows that $$\lim_{n \to \infty} P_{\symbf{\theta}_0} \big[L(\symbf{\theta}; \mathbf{x}_n) < L(\symbf{\theta}_0; \mathbf{x}_n)\big] = \lim_{n \to \infty} P_{\symbf{\theta}_0} \Bigg[\frac{1}{n}\sum_{i=1}^n \log\frac{p(X_i; \symbf{\theta})}{p(X_i; \symbf{\theta}_0)} < 0\Bigg] = 1,$${#eq-MLE7} which proves the claim.

Let $\hat{\symbf{\theta}}\in \Theta$ be any parameter value that renders the likelihood at the observed $\mathbf{X}_n = \mathbf{x}_n$ as large as possible, i.e., $$L(\hat{\symbf{\theta}}, \mathbf{x}_n) = \sup_{\symbf{\theta} \in \Theta} L(\symbf{\theta}).$${#eq-MLE8} We call such a value a *maximum likelihood estimate* of $\theta_0$. Note that this definition of $\hat{\theta}$ as a maximizer of  $L(\hat{\symbf{\theta}}, \mathbf{x}_n)$ necessarily makes it a function of the observed data. We will often write $\hat{\theta} = \hat{\theta}(\mathbf{x}_n)$ when we wish to emphasize this dependence. We can further define the *maximum likelihood estimator* of $\symbf{\theta}_0$ as the statistic $\hat{\symbf{\theta}}(\mathbf{X}_n)$ for which we observe the value $\hat{\symbf{\theta}}(\mathbf{x}_n)$. 

An important property of the MLE is its functional invariance. If $\hat{\symbf{\theta}}$ is an MLE of $\symbf{\theta}_0$, then any function $h(\symbf{\theta})$ will have $h(\hat{\symbf{\theta}})$ as its MLE. Hence, it is straightforward to find the MLE of a model that has undergone a reparameterization given that we knew the MLE of the original parameter. 
For an arbitrary model, there is no guarantee that a maximum likelihood estimator (hereafter referred to using the acronym "MLE") for its parameter will exist, and even if it does, it will not necessarily be unique. However, the existence and uniqueness of a model's MLE are two very desirable properties for it to have, and since the MLE plays an important role in our discussion in Chapters 5 and 6, we would like to consider only models with a unique MLE. 
Hence, some discussion of the conditions under which the MLE of a model's parameter exists and is unique is warranted.

A sufficient condition for the existence of a model's MLE is that $\Theta$ is compact, and $\ell$ is continuous on $\Theta$. Similarly, when the MLE does exist, a sufficient condition for its uniqueness is that $\Theta$ is convex, and $\ell$ is strictly concave on $\Theta$.  **RC9)** directly satisfies the compactness and convexity requirements. Our assumption of the differentiability of $\ell$ in $\theta$ guarantees its continuity. Finally, the strict concavity of $\ell$ is implied through our assumption of the positive definiteness of the Fisher information matrix. Hence, the existence and uniqueness of the MLE is guaranteed for any model satisfying our regularity conditions.

If **RC4-5)** hold in addition to **RC1-3)**, then there will exist at least one sequence of roots $\hat{\symbf{\theta}}_n = \hat{\symbf{\theta}}(\mathbf{x}_n)$ of the score function, i.e. solutions to the following *likelihood equation*, $$\nabla_{\symbf{\theta}}\ell(\hat{\symbf{\theta}}_n) = \mathbf{0},$${#eq-MLE9} such that $\hat{\symbf{\theta}}_n$ tends to $\symbf{\theta}_0$ in probability as $n \to \infty$ (Cf. Cramér 1945). 

If this sequence of roots is unique, then the MLE exists and is equal to $\hat{\symbf{\theta}}(\mathbf{X}_n)$, and therefore it must be a consistent estimator of $\symbf{\theta}_0$ as well. We will explore more of the asymptotic properties of the MLE in Section 3.6.

\section{The Bartlett Identities}

The Bartlett identities are a set of equations relating to the expectations of the derivatives of a log-likelihood function to one another. In general, there is no guarantee that an arbitrary function of a random variable $X$ and its parameter $\symbf{\theta}$ will satisfy the Bartlett identities. It is guaranteed, however, that the log-likelihood function associated with $X$ and $\symbf{\theta}$ will satisfy them, provided that the model is regular. Thus, we can think of any function that does satisfy the Bartlett identities (or at least some of them) as resembling that of a genuine log-likelihood.

Consider the case where a random variable $X$ has density function $p_{\theta}(x)$, where $\theta$ is a scalar. For a single observation $X = x$, the expectation of $\frac{\partial}{\partial \theta}\ell(\theta; X)$ gives

$$
\begin{aligned}
E_{\theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\bigg] &= \int_{\mathbbm{R}} \bigg[\frac{\partial}{\partial \theta} \log p(x; \theta)\bigg] p(x; \theta) dx \\
                                                                       &=  \int_{\mathbbm{R}} \frac{\frac{\partial}{\partial \theta} p(x; \theta)}{p(x; \symbf{\theta})} p(x; \theta) dx \\
                                                                       &=  \int_{\mathbbm{R}} \frac{\partial}{\partial \theta} p(x; \theta) dx \\
                                                                       &= \frac{d}{d \theta} \int_{\mathbbm{R}} p(x; \theta) dx \\
                                                                       &= \frac{d}{d \theta} 1 \\
                                                                       &= 0. 
\end{aligned}
$${#eq-BI1}
@eq-BI1 is called the first Bartlett identity. In words, it states that the expectation of the first partial derivative of the log-likelihood function of a statistical model with respect to the parameter will always be 0. Since the score is defined as $\frac{\partial}{\partial \theta}\ell(\theta; x)$, any function that satisfies the first Bartlett identity is said to be *score-unbiased*.

For any model with a log-likelihood satisfying the first Bartlett identity, the expected information for its parameter $\theta$ may be rewritten as 
$$
\begin{aligned}
\mathscr{I}_{X}(\theta) &= \text{Var}_{\theta}\big[\mathcal{S}(\theta; X)\big] \\
                    &= \text{Var}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg] \\
                    &= \text{Var}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg] +\Bigg(\text{E}_{\theta}\bigg[\frac{\partial}{\partial \theta}\ell(\theta; X)\bigg]\Bigg)^2 &&(\text{by the first Bartlett identity}) \\
                    &= \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg].
\end{aligned}
$${#eq-BI2}

If we now consider the second partial derivative of $\ell(\theta; x)$ with respect to $\theta$, we have
$$
\begin{aligned}
\frac{\partial^2}{\partial \theta^2} \ell(\theta; x) &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; x) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\bigg[\frac{\partial}{\partial \theta} \log p(x;\theta) \bigg] \\
                                &= \frac{\partial}{\partial \theta}\Bigg[\frac{\frac{\partial}{\partial \theta} p(x;\theta)}{p(x;\theta)} \Bigg] \\
                                &= \frac{\Big[\frac{\partial^2}{\partial \theta^2} p(x;\theta)\Big] p(x;\theta) - \Big[\frac{\partial}{\partial \theta} p(x; \theta)\Big]\Big[\frac{\partial}{\partial \theta} p(x; \theta)\Big]}{\big[p(x; \theta) \big]^2} \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \Bigg[\frac{\frac{\partial}{\partial \theta} p(x; \theta)}{ p(x; \theta)}\Bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \bigg[\frac{\partial}{\partial \theta} \log p(x; \theta)\bigg]^2 \\
                                &= \frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)} - \Bigg[\frac{\partial}{\partial \theta}\ell(\theta; x) \Bigg]^2. 
\end{aligned} 
$$
Rearranging terms and taking expectations yields 
$$
\begin{aligned}
\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2\Bigg] &= \text{E}_{\theta}\Bigg[\frac{\frac{\partial^2}{\partial \theta^2} p(X; \theta)}{p(X; \theta)}\Bigg] \\
            &= \int_{\mathbbm{R}} \Bigg[\frac{\frac{\partial^2}{\partial \theta^2} p(x; \theta)}{p(x; \theta)}\Bigg] p(x; \theta) dx \\
            &= \int_{\mathbbm{R}} \frac{\partial^2}{\partial \theta^2} p(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} \int_{\mathbbm{R}} p(x; \theta) dx \\
            &= \frac{d^2}{d \theta^2} 1 \\
            &= 0. 
\end{aligned}
$$
Therefore, $$\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2 \Bigg] = 0.$${#eq-BI3}

@eq-BI2 is called the second Bartlett identity. Any function that satisfies it is said to be *information-unbiased*. Regular models as we have defined them will automatically satisfy both the first and second Bartlett identities. Hence, for any regular model, the statements in @eq-exp_information1, @eq-BI2, and @eq-BI3 imply that the following definitions for its expected information regarding its parameter $\theta$ are all equivalent: $$\mathscr{I}_{X}(\theta) = \text{Var}_{\theta}\Bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg] = \text{E}_{\theta}\Bigg[-\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg].$${#eq-BI4} 

It is possible to derive further Bartlett identities by continuing in this manner for an arbitrary number of partial $\theta$-derivatives of the log-likelihood function, provided that they exist. However, the first two are sufficient for our purposes of evaluating the validity of approximations to genuine likelihoods so we will not go further here. Note that while the above derivations were performed under the assumption that $\theta$ is a scalar, the Bartlett identities also hold in the case where $\symbf{\theta}$ is a $d \times 1$ vector.

\section{One-Index Asymptotics}

The one-index asymptotics framework describes the behavior of likelihood-based statistics as the sample size ($n$) grows to infinity while the dimension of the nuisance parameter ($q$) remains fixed. The aim of this section is to present a basic overview of the theory's results so that we will have a readily available baseline against which to compare the results of the following section discussing the two-index asymptotics framework, in which $q$ is allowed to increase with $n$.

Assume the regularity conditions of the previous section apply to our model and let $\hat{\symbf{\theta}}$ denote the MLE for $\symbf{\theta}_0$. The traditional method for analyzing the asymptotic behavior of $\hat{\symbf{\theta}}$ is to use a first-order Taylor series expansion of the score function.^[See Appendix \ref{appendix:A} for a review of Taylor's theorem and the conditions under which it is satisfied.] 

We start by noting that under our assumption of i.i.d. observations, the score function is equal to
$$
\begin{aligned}
\mathcal{S}(\symbf{\theta}; \mathbf{x}_n) &= \nabla_{\symbf{\theta}} \ell(\symbf{\theta}; \mathbf{x}_n) \\
                                  &= \nabla_{\symbf{\theta}} \sum_{i=1}^n \ell(\symbf{\theta}; x_i) \\
                                  &= \sum_{i=1}^n \nabla_{\symbf{\theta}} \ell(\symbf{\theta}; x_i) \\
                                  &= \sum_{i=1}^n \mathcal{S}(\symbf{\theta}; x_i).
\end{aligned}
$${#eq-score_additive}
In other words, the score function for the parameter $\symbf{\theta}$ based on data $x_1, ..., x_n$ can be written as the sum of independent contributions $\mathcal{S}(\symbf{\theta}; x_i)$ ($i = 1,..., n$) where each $\mathcal{S}(\symbf{\theta}; x_i)$ can be thought of as the score function for $\symbf{\theta}$ based only on observation $x_i$. 

@eq-score_additive implies that a Taylor series expansion of $\mathcal{S}(\symbf{\theta}; \mathbf{x}_n)$ will be equal to the sum of the Taylor series expansions of its individual contributions, plus a remainder term that grows with $n$. Since the observations are identically distributed, it suffices to consider the expansion for an arbitrary contribution, $\mathcal{S}(\symbf{\theta}; x_i)$. 

Note that **RC5)** guarantees the first partial derivative of $\mathcal{S}(\symbf{\theta}; x_i)$ with respect to $\symbf{\theta}$ exists and is continuous for all $\symbf{\theta} \in \Theta^*$. Since $\Theta^*$ is an open set and $\symbf{\theta}_0 \in \Theta^*$, there must exist an open ball $B_r(\symbf{\theta}) \subset \Theta^*$ on which $\mathcal{S}(\symbf{\theta}; x_i)$ is continuously differentiable with respect to $\symbf{\theta}$. This means $\mathcal{S}(\symbf{\theta}; x_i)$ satisfies the criteria necessary to use a version of the Mean-Value Theorem that has been adapted to suit multivariable vector-valued functions (see Appendix \ref{appendix:A}). Hence, we can expand $\mathcal{S}(\symbf{\theta}; X_i)$ as $$\mathcal{S}(\symbf{\theta}; X_i) = \mathcal{S}(\symbf{\theta}_0; X_i) + \Bigg[\int_0^1 \mathbf{J}\Big(\mathcal{S}(\symbf{\theta}_0 + u(\symbf{\theta} - \symbf{\theta}_0); x_i)\Big)du\Bigg](\symbf{\theta} - \symbf{\theta}_0).$$

in an open ball $B_r(\symbf{\theta})$. we can expand $\mathcal{S}(\symbf{\theta}; x_i)$

We can therefore perform a component-wise first-order Taylor series expansion of a given score contribution $\mathcal{S}(\symbf{\theta}; x_i)$ around the point $\symbf{\theta} = \symbf{\theta}_0$, wherein each component $\mathcal{S}_j(\symbf{\theta}; x_i)$ of $\mathcal{S}(\symbf{\theta}; x_i)$ ($j = 1, ..., d$) is expanded separately.

For any $\symbf{\theta} \in N_r(\symbf{\theta}_0)$, there exists $\bar{\symbf{\theta}}_j$ on the line segment connecting $\symbf{\theta}$ and $\symbf{\theta}_0$ such that 
$$
\begin{aligned}
\mathcal{S}_j(\symbf{\theta}; x_i) &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + \nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i)^T(\symbf{\theta} - \symbf{\theta}_0) + \frac{1}{2}(\symbf{\theta} - \symbf{\theta}_0)^T\nabla^2_{\symbf{\theta}} \mathcal{S}_j(\bar{\symbf{\theta}}_j; x_i)(\symbf{\theta} - \symbf{\theta}_0) \\
                 &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + [\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i) + \frac{1}{2}\nabla^2_{\symbf{\theta}} \mathcal{S}_j(\bar{\symbf{\theta}}_j; x_i)(\symbf{\theta} - \symbf{\theta}_0)]^T(\symbf{\theta} - \symbf{\theta}_0) \\
                 &= \mathcal{S}_j(\symbf{\theta}_0; x_i) + [\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)]^T(\symbf{\theta} - \symbf{\theta}_0),
\end{aligned}
$$
where the last equality follows as a result of **RC6)**. Summing over each of the individual contributions to the score function yields
$$
\begin{aligned}
\mathcal{S}_j(\symbf{\theta}; \mathbf{x}_n) &= \sum_{i=1}^n \mathcal{S}_j(\symbf{\theta}; x_i) \\
                                  &= \sum_{i=1}^n\big[\mathcal{S}_j(\symbf{\theta}_0; x_i) + [\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; x_i) + M(x_i)O(||\symbf{\theta} - \symbf{\theta}_0||)]^T(\symbf{\theta} - \symbf{\theta}_0)\big] \\
                                  &= \mathcal{S}_j(\symbf{\theta}_0; \mathbf{x}_n) + \Bigg[\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; \mathbf{x}_n) + \Bigg\{\sum_{i=1}^nM(x_i) \Bigg\}O(||\symbf{\theta} - \symbf{\theta}_0||)\Bigg]^T(\symbf{\theta} - \symbf{\theta}_0) \\
                                  &= \mathcal{S}_j(\symbf{\theta}_0; \mathbf{x}_n) + \Bigg[\frac{1}{n}\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; \mathbf{x}_n) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\symbf{\theta} - \symbf{\theta}_0||)\Bigg]^Tn(\symbf{\theta} - \symbf{\theta}_0).
\end{aligned} 
$$
If we divide through by $\sqrt n$, we arrive at $$\frac{1}{\sqrt n}\mathcal{S}_j(\symbf{\theta}; \mathbf{x}_n) = \frac{1}{\sqrt n}\mathcal{S}_j(\symbf{\theta}_0; \mathbf{x}_n) + \Bigg[\frac{1}{n}\nabla_{\symbf{\theta}} \mathcal{S}_j(\symbf{\theta}_0; \mathbf{x}_n) + \Bigg\{\frac{1}{n}\sum_{i=1}^nM(x_i) \Bigg\}O(||\symbf{\theta} - \symbf{\theta}_0||)\Bigg]^T\sqrt{n}(\symbf{\theta} - \symbf{\theta}_0).$$







