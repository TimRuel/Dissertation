---
title: "Proposal for an Adjusted Numerical Approximation to the Integrated Likelihood Function"
subtitle: "Dissertation Prospectus"
author: "Tim Ruel"
date: "Dec 12, 2023"
date-format: "MMM D, YYYY"
format: 
  revealjs:
    theme: [default]
    slide-number: true
    incremental: false
    logo: nulogo.png

include-in-header:
  - text: |
      <style>
      #title-slide .title {
        font-size: 2em;
      }
      </style>
---

## Statistical Model Assumptions

::: {style="font-size: 70%;"}

- **Data-generating mechanism**
  - There exists an unknown probability distribution $P_0$ over the population of interest that generates the data we observe from it.
- **Parameterizable**
  - The set of candidate distributions $\mathcal{P}$ is indexed by a parameter $\theta$ taking values in a space $\Theta$, so that $\mathcal{P} = \{P_{\theta} | \> \theta \in \Theta\}$. 
- **Absolutely continuous**
  - Each distribution $P$ in $\mathcal{P}$ is absolutely continuous with respect to some dominating measure (usually Lebesgue or counting measure) and therefore admits a density function $p_\theta$. This allows us to write $\mathcal{P} = \{p_{\theta} | \> \theta \in \Theta\}$.
- **Parametric**
  - The dimension of the parameter space is finite. That is, $\Theta \subseteq \mathbb{R}^d$ where $d \in \mathbb{Z}^+$.
- **Identifiable**: 
  - $P_{\theta_1} = P_{\theta_2} \implies \theta_1 = \theta_2 \> \> \forall \> \> \theta_1, \theta_2 \in \Theta.$
  
:::

## The Likelihood Function and its Transformations

::: {style="font-size: 70%;"}
- **The Likelihood Function** : $L(\theta; \mathbf{x}_n) = p(\mathbf{x}_n; \theta).$
  - The joint probability of a sample of observations $\mathbf{x}_n = (x_1, ..., x_n)$ considered as a function of $\theta$.
- **The Log-Likelihood Function**: $\ell(\theta; \mathbf{x}_n) = \log L(\theta; \mathbf{x}_n).$
  - The natural logarithm of the likelihood function. 
- **The Score Function**: $\mathcal{S}(\theta; \mathbf{x}_n) = \nabla_{\theta}\ell(\theta; \mathbf{x}_n)$
  - The gradient of the log-likelihood function with respect to $\theta$. 
- **The Observed Information**: $\mathcal{I}(\theta) = -\mathbf{H}_{\theta}\Big(\ell(\theta; \mathbf{x}_n)\Big)$
  - The negative Hessian matrix of the log-likelihood function with respect to $\theta$.
- **The Fisher Information**: $\mathscr{I}(\theta) = \text{Var}_{\theta}\big[\mathcal{S}(\theta; \mathbf{X}_n)\big]$
  - The variance of the score function. 

:::

## Maximum Likelihood Estimation

- A value $\hat{\theta} \in \Theta$ is called a **maximum likelihood estimate** of $\theta_0$ if it satisfies $$L(\hat{\theta}; \mathbf{x}_n) = \sup_{\theta \in \Theta} L(\theta; \mathbf{x}_n).$$

- $\hat{\theta}$ depends on the data $\mathbf{x}_n$ so we can write $\hat{\theta} = \hat{\theta}(\mathbf{x}_n)$. 

- The statistic $\hat{\theta}(\mathbf{X}_n)$ that produces the value $\hat{\theta}(\mathbf{x}_n)$ we observe is called the **maximum likelihood estimator** (MLE) of $\theta_0$.

- For an arbitrary model, there is no guarantee an MLE exists. When it does exist, there is no guarantee it is unique.


## Regularity Conditions 

::: {style="font-size: 65%;"}

- Any observations $x_1, ..., x_n$ belonging to a sample that has been drawn from the model's sample space are independent and identically distributed (i.i.d.) realizations of a random variable $X$ with density function $p_{\theta}(x)$.
- $P_{\theta_1} = P_{\theta_2} \implies \theta_1 = \theta_2$ for all $\theta_1, \theta_2 \in \Theta$.
- The distributions in $\mathcal{P}$ have a common support $\mathcal{X} = \{x: p_{\theta}(x) > 0 \}\subseteq \mathbb{R}$ not depending on $\theta$.
- There exists an open set $\Theta^* \subseteq \Theta$ of which $\theta_0$ is an interior point.
- $p(x; \theta)$ is twice continuously differentiable with respect to $\theta$ for all $\theta$ in a neighborhood of $\theta_0$.
- There exists a random function $M(x)$ (that does not depend on $\theta$) satisfying $\text{E}[M(X)] < \infty$ such that each third partial derivative of $\ell(\theta; x)$ is bounded in absolute value by $M(x)$ uniformly in some neighborhood of $\theta_0$.
- The integral $\int_{\mathcal{X}} p(x; \theta) dx$ can be differentiated twice under the integral sign with respect to the components of $\theta\in \Theta^*$.
- $\mathscr{I}_{X}(\theta)$ is positive definite for all $\theta \in \Theta$.
- $\Theta$ is a compact and convex subset of $\mathbb{R}^d$.

:::

## The First Bartlett Identity

::: {style="font-size: 65%;"}

- A likelihood satisfies the first Bartlett identity if  $$E_{\theta}\Big[\mathcal{S}(\theta; X)\Big] = E_{\theta}\bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\bigg] = 0.$$
- Equivalently, $$\mathscr{I}(\theta) = E_{\theta}\Big[\mathcal{S}(\theta; X)^2\Big] =  \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg].$$
- Sufficient condition: $$\frac{d}{d \theta}\int_{\mathbb{R}} p(x; \theta) dx = \int_{\mathbb{R}} \frac{\partial}{\partial \theta}p(x; \theta) dx.$$
- Likelihoods satisfying the first Bartlett identity are called **score-unbiased**.

:::

## The Second Bartlett Identity

::: {style="font-size: 60%;"}

- A likelihood satisfies the second Bartlett identity if $$\text{E}_{\theta}\Bigg[\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] + \text{E}_{\theta}\Bigg[\Bigg(\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg)^2\Bigg] = 0.$$
- Equivalently, 
- Sufficient condition: $$\frac{d^2}{d \theta^2}\int_{\mathbb{R}} p(x; \theta) dx = \int_{\mathbb{R}} \frac{\partial^2}{\partial \theta^2}p(x; \theta) dx.$$
- Likelihoods satisfying the second Bartlett identity are called **information-unbiased**.

:::

## Bartlett

$$
\rlap{
  \underbrace{
    \phantom{
      \mathscr{I}_{X}(\theta) = \text{Var}_{\theta}\Bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg]
      }
    }_{\text{Fisher information}}
  }
\mathscr{I}_{X}(\theta) = 
\overbrace{
  \text{Var}_{\theta}\Bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg]
  }^{\text{First Bartlett identity}}
$$



$$= \rlap{\overbrace{\phantom{\text{Var}_{\theta}\Bigg[\frac{\partial}{\partial \theta} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg]}}^{\text{First Bartlett identity}}} \text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg]$$



\text{E}_{\theta}\Bigg[\bigg (\frac{\partial}{\partial \theta}\ell(\theta; X) \bigg)^2\Bigg] = \text{E}_{\theta}\Bigg[-\frac{\partial^2}{\partial \theta^2} \ell(\theta; X)\Bigg] = \text{E}_{\theta}\big[\mathcal{I}_X(\theta)\big].$$

$$a+b+\rlap{\overbrace{\phantom{c+d+e+f+g}}^x}c+d
     +\underbrace{e+f+g+h+i}_y +k+l=e^2$$

## Decomposing the Model Parameter

- Consider the case in which our actual **parameter of interest** is not $\theta$ but rather a sub-parameter $\psi$ taking values in a set $\Psi$.

- Whatever remains in $\theta$ that is not a part of $\psi$ is called the **nuisance parameter** and is denoted by $\lambda$. 

- $\theta$ can therefore be decomposed as $\theta = (\psi, \lambda).$

- The set in which $\lambda$ takes its values may depend on $\psi$ so we will define its parameter space as $$\Lambda(\psi) \equiv \{\lambda \> | \> (\psi, \lambda) \in \Theta\}.$$

:::{.notes}
$\lambda$ will often obfuscate our inference regarding the true value of $\psi$, hence its name.

Note that both $\psi$ and $\lambda$ are explicitly defined here as being a subset of the full model parameter's components. 
:::

## The Generalized Parameter of Interest

- Previously we defined $\psi$ (and by extension $\lambda$) explicitly as a subset of the components of $\theta$ but this need not always be the case.

- In its most general form, the parameter of interest for a model is a function $\varphi: \Theta \to \Psi$ such that $\psi = \varphi(\theta)$.

- When $\varphi(\theta)$ happens to be equal to $\theta$, we can base our inference directly on $L(\theta)$.

- However, as its name suggests, when a nuisance parameter exists, estimating the parameter of interest can be quite difficult. 

## Pseudolikelihood Functions {.smaller}
  
- If we let $$\Theta(\psi) = \{\theta \in \Theta \> | \> \varphi(\theta) = \psi \},$$ then corresponding to $\psi \in \Psi$ is the set of likelihoods $$\mathcal{L}_{\psi} = \{L(\theta) \> | \> \theta \in \Theta(\psi)\}.$$

 - Statisticians will often attempt to find a summary of the values in $\mathcal{L}_{\psi}$ that does not depend on $\lambda$. This summary is called a **pseudolikelihood function** and can take several different forms:
    - Maximizing
    - Conditioning
    - **Integrating**

## The Integrated Likelihood Function

- The **integrated likelihood function** is defined as $$\bar{L}(\psi) = \int_{\Lambda(\psi)} L(\psi, \lambda)\pi(\lambda | \psi)d\lambda.$$

- The idea is to summarize $\mathcal{L}_{\psi}$ by its average value with respect to some weight function $\pi(\lambda | \psi)$ over $\Theta(\psi)$.

- $\pi(\lambda | \psi)$ is often referred to as a conditional prior density for $\lambda$ given $\psi$ though in reality it doesn't have to be a genuine density function.


:::{.notes}
Note that $\psi$ and $\lambda$ will often be related in some sense.  
:::

## A New Choice of Nuisance Parameter

- Severini (2007) proposed a method for reparameterizing the nuisance parameter $\lambda$ in such a way that it is "unrelated" to the parameter of interest $\psi$.

- This parameter would be constructed to have a maximum likelihood estimate that would not actually depend on $\psi$.

- Integrating the likelihood function with respect to this new nuisance parameter would grant the result some nice properties that would hold for essentially any choice of $\pi(\lambda | \psi)$ as long as it does not depend on $\psi$.

:::{.notes}
In this context, "unrelated" means the derivative of the new nuisance parameter with respect to $\psi$ is constant.
These nice properties include approximate score- and information-unbiasedness.
:::

## The Zero-Score Expectation Parameter {.smaller}

- It can be shown that for a given parameterization $\theta = (\psi, \lambda)$, a nuisance parameter $\phi$ that is unrelated to $\psi$ is given by the solution to the equation $$\mathbb{E}\big(\ell_{\lambda}(\psi, \lambda); \hat{\psi}, \phi\big) \equiv \mathbb{E}\big(\ell_{\lambda}(\psi, \lambda); \psi_0, \lambda_0\big) \Bigg| _{(\psi_0, \lambda_0) = (\hat{\psi}, \phi)} = 0,$$
where
  - $\ell_{\lambda}(\psi, \lambda)$ is the score function; that is, the derivative of the log-likelihood with respect to $\lambda$,
  - $\hat{\psi}$ is the maximum likelihood estimate for $\psi$,
  - and $\psi_0$ and $\lambda_0$ are the "true" values of the parameters.

- The parameter $\phi$ is called the **zero-score expectation parameter** (ZSE). 

:::{.notes}
Note that while $\phi$ does not depend on $\psi$, it does depend on the data through $\hat{\psi}$.
:::

## Rewriting the Integrated Likelihood

- We can rewrite our expression for the integrated likelihood function in terms of the ZSE as follows: $$\bar{L}(\psi) = \int_{\Phi} L(\psi, \lambda(\psi, \phi))\pi(\phi)d\phi.$$

- $\lambda(\psi, \phi) =\underset{\lambda}{\operatorname{argmax}} \mathbb{E}\big(\ell(\psi, \lambda); \hat{\psi}, \phi\big)$ 
- $\Phi$ is the space of possible $\phi$

## Application to a Multinomial Model {.smaller}

- Suppose that $(N_1, ..., N_m) \sim \text{Multinom}(\theta_1, ..., \theta_m)$.

- The full parameter is $\theta = (\theta_1, ..., \theta_m)$ and the parameter space $\Theta$ is the probability simplex in $\mathbb{R}^m$.

- We will take our parameter of interest $\psi$ to be the entropy of the distribution. That is, $$\psi \equiv g(\theta) =  -\sum_{j=1}^m \theta_j \log(\theta_j).$$
 - If $(n_1, ..., n_m)$ are the observed values of $(N_1, ..., N_m)$, then the likelihood and log-likelihood are, respectively, $$L(\theta) = \prod_{j=1}^m \theta_j^{n_j} \text{ and } \ell(\theta) = \log L(\theta).$$

:::{.notes}
0log(0) is taken to be 0. 

The minimum value of $\psi$ is 0 and the maximum value is log(m).
:::

## Application to a Multinomial Model

- We will denote the parameter space for $\Psi$ by $$\Psi = \{t \in \mathbb{R} \> | \> g(\theta) = t \text{ for some } \theta \in \Theta\}.$$
- The integrated likelihood then becomes $$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L(\theta(\omega; \psi))\pi(\omega)d\omega, \> \psi \in \Psi,$$
where $\Omega_{\hat{\psi}} = \{\omega \in \Theta \> | \> g(\omega) = \hat{\psi}\}$ and $\hat{\psi} = g(\hat{\theta})$ is the MLE for $\psi$.

## Application to a Multinomial Model {.smaller}

- Severini (2022) describes a method for approximating the integrated likelihood function for the entropy of this distribution based on the ZSE using basic Monte Carlo integration.

- The idea is to draw random variates $\tilde{\omega}$ from $\Omega_{\hat{\psi}}$ according to a distribution that does NOT depend on $\psi$ and then use these variates to generate values of $\theta = \theta(\tilde{\omega}, \psi)$ by solving the maximization problem $$\max \> \mathbb{E} \big(\ell(\theta); \tilde{\omega} \big) \text{ subject to } g(\theta) = \psi$$

- If $\theta(\omega_j, \psi_1)$ is the solution to the above maximization problem for $\tilde{\omega} = \omega_j$ and $\psi = \psi_1$, then the value of the integrated likelihood for $\psi$ at $\psi_1$ can be approximated by $$\bar{L}(\psi_1) = \sum_{j=1}^R L(\theta(\omega_j, \psi_1)),$$ for sufficiently large $R$.

## Result of Algorithm  {.smaller}

![](multinomial_entropy_pseudolikelihood.png)

The above graph shows the result of an implementation of this algorithm for the observed data $(n_1, ..., n_6) = (1, 1, 2, 4, 7, 10)$.

## Bayes' Theorem {.smaller}

- Instead of using simple Monte Carlo approximation, my idea has been to take advantage of the similarity between an integrated likelihood and the normalizing constant for a posterior distribution.

- Bayes' Theorem tells us that the posterior distribution for a parameter $\theta$ and data $X$ is $$p(\theta | X) = \frac{p(X|\theta)p(\theta)}{p(X)}.$$

- Since $p(X)$ is constant with respect to $\theta$ and we must have $\int p(\theta | X) d\theta = 1$, it follows that $$p(X) = \int p(X|\theta)p(\theta) d\theta$$ and therefore 
$$\int p(X|\theta)p(\theta) d\theta = \frac{p(X|\theta)p(\theta)}{p(\theta | X)}.$$

## Bayes' Theorem (contd.)

- Note the similarity in form between these two expressions $$\int_{\Phi} L(\psi, \lambda(\psi, \phi))\pi(\phi)d\phi$$ $$\int p(X|\theta)p(\theta) d\theta$$

- The problem I have been working on has been to simulate the posterior distribution $p(\theta | X)$ using a Markov Chain Monte Carlo algorithm. This will then allow me to approximate the denominator in $\frac{p(X|\theta)p(\theta)}{p(\theta | X)}$.

## References {.smaller}

Severini, Thomas A. "Integrated Likelihood Inference in Multinomial Distributions." METRON (2022).

Severini, Thomas A. “Integrated Likelihood Functions for Non-Bayesian Inference.” Biometrika 94, no. 3 (2007): 529–42.

