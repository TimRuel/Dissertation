% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%
\usepackage{bbm}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Integrated Likelihood Inference in Poisson Distributions},
  pdfauthor={Timothy Ruel},
  pdfkeywords={Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}



\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Integrated Likelihood Inference in Poisson Distributions}
\author{
Timothy Ruel\\
Department of Statistics and Data Science, Northwestern University\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:} Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!


\section{Introduction}\label{sec-intro}

Consider a vector \(\theta = (\theta_1, ..., \theta_n)\) in which each
component represents the mean of a distinct Poisson process. The purpose
of this paper is to discuss the task of conducting likelihood-based
inference for a real-valued parameter of interest
\(\psi = \tau(\theta)\). In particular, we will examine the utility of
the integrated likelihood function as a tool for obtaining interval and
point estimates for \(\psi\), using the performance of the more easily
calculated profile likelihood as a benchmark.

We may obtain a sample of values from each Poisson process through
repeated measurements of the number of events it generates over a fixed
period of time. Suppose we have done so, and let \(X_{ij}\) represent
the \(j\)th count from the \(i\)th sample, so that
\(X_{ij} \sim \text{Poisson}(\theta_i)\) for \(i = 1, ..., n\) and
\(j = 1, ..., m_i.\) The probability mass function (pmf) for a single
observation \(X_{ij} = x_{ij}\) is
\begin{equation}\phantomsection\label{eq-1-1}{
p(x_{ij}; \> \theta_i) = \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!}, \> \> x_{ij} = 0, 1, 2, ... ; \> \> \theta_i > 0.
}\end{equation}

Denote the sample of counts from the \(i\)th process by the vector
\(X_{i\bullet} = (X_{i1}, ..., X_{im_i})\), its associated mean by
\(\bar{X}_{i \bullet} = \frac{1}{m_i} \sum_{j = 1}^{m_i} X_{ij}\), and
assume that all of the counts both within and between samples are
measured independently. The likelihood function for an individual
component \(\theta_i\) based on the data \(X_{i\bullet} = x_{i\bullet}\)
is then equal to the product of the individual probabilities of the
observed counts, i.e.~\begin{equation}\phantomsection\label{eq-1-2}{
\begin{aligned}
L(\theta_i; x_{i\bullet}) &= \prod_{j=1}^{m_i} p(x_{ij}; \theta_i) \\
                          &= \prod_{j=1}^{m_i} \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!} \\
                          &= \Bigg(\prod_{j=1}^{m_i} e^{-\theta_i}\Bigg) \Bigg(\prod_{j=1}^{m_i}\theta_i^{x_{ij}}\Bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= \bigg(e^{-\sum_{j=1}^{m_i}\theta_i}\bigg) \bigg(\theta_i^{\sum_{j=1}^{m_i}x_{ij}}\bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1}.
\end{aligned}
}\end{equation}

Since \(L\) is only useful to the extent that it informs our
understanding of the value of \(\theta_i\), we are free to replace it
with any other function differing from it by just a (nonzero)
multiplicative term that is constant with respect to \(\theta_i\),
provided that the result still satisfies the necessary regularity
conditions, as this will not change any conclusions regarding
\(\theta_i\) that we draw from it. Hence, we may safely discard the term
in parentheses on the final line of Equation~\ref{eq-1-2} as it does not
depend on \(\theta_i\) and instead simply write
\begin{equation}\phantomsection\label{eq-1-3}{
L(\theta_i; x_{i\bullet}) = e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}.
}\end{equation}

It will generally be more convenient to work with the log-likelihood
function, which is given by
\begin{equation}\phantomsection\label{eq-1-4}{
\begin{aligned}
\ell(\theta_i; x_{i\bullet}) &= \log L(\theta_i; x_{i\bullet}) \\
                           &= \log \Big(e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Big) \\
                           &= -m_i\theta_i + m_i \bar{x}_{i\bullet} \log \theta_i \\
                           &=  m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation} The sum of the log-likelihood functions for each
component of \(\theta\) then forms the basis of the log-likelihood
function for \(\theta\) itself:
\begin{equation}\phantomsection\label{eq-1-5}{
\begin{aligned}
\ell(\theta; x_{1\bullet}, ..., x_{n\bullet}) &= \log L(\theta; x_{1\bullet}, ..., x_{n\bullet}) \\
                                              &= \log \Bigg(\prod_{i=1}^n L(\theta_i; x_{i\bullet})\Bigg) \\
                                              &= \sum_{i=1}^n \log L(\theta_i; x_{i\bullet}) \\
                                              &= \sum_{i=1}^n \ell(\theta_i; x_{i\bullet}) \\
                                              &= \sum_{i=1}^n m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation}

We can derive the maximum likelihood estimate (MLE) for \(\theta_i\) by
differentiating Equation~\ref{eq-1-4} with respect to \(\theta_i\),
setting the result equal to 0, and solving for \(\theta_i\). This gives
the nice result that the MLE is simply equal to the mean of the sample
of data \(X_{i\bullet}\). That is,
\begin{equation}\phantomsection\label{eq-1-6}{
\hat{\theta}_i = \bar{X}_{i\bullet}.
}\end{equation} Similarly, the MLE for the full parameter \(\theta\) is
just the vector of MLEs for its individual components:
\begin{equation}\phantomsection\label{eq-1-7}{
\hat{\theta} \equiv (\hat{\theta}_1, ..., \hat{\theta}_n) = (\bar{X}_{1\bullet}, ..., \bar{X}_{n\bullet}).
}\end{equation}

\section{Pseudolikelihoods}\label{pseudolikelihoods}

Let \(\Theta \subseteq \mathbb{R}^n_+\) represent the space of possible
values for \(\theta\) and suppose we have a real-valued \emph{parameter
of interest} \(\psi = \tau(\theta)\), where \(\tau: \Theta \to \Psi\) is
a known function with at least two continuous derivatives. Though it is
not strictly necessary, in order to align with the tendency of
researchers to focus on one-dimensional summaries of vector quantities
we will assume for our purposes that \(\psi\) is a scalar, i.e.
\(\Psi \subseteq \mathbb{R}\).

This reduced dimension of \(\Psi\) relative to \(\Theta\) implies the
existence of a \emph{nuisance parameter}
\(\lambda \in \Lambda \subseteq \mathbb{R}^{n-1}\). As its name
suggests, \(\lambda\) tends to obfuscate or outright preclude inference
regarding \(\psi\) and typically must be eliminated from the likelihood
before proceeding. The product of this elimination is called a
\emph{pseudolikelihood function}. Any function of the data and \(\psi\)
alone could theoretically be considered a pseudolikelihood, though
course in practice some are more useful than others. If we let
\(\Theta_{\psi} = \{\theta \in \Theta: \> \tau(\theta) = \psi \},\) then
associated with each \(\psi \in \Psi\) is the set of likelihood values
\(\mathcal{L}_{\psi} = \{L(\theta): \> \theta \in \Theta_{\psi}\}.\) For
a given value of \(\psi\), there may exist multiple corresponding values
of \(\lambda\).

We can construct pseudolikelihoods for \(\psi\) through clever choices
by which to summarize \(\mathcal{L}_{\psi}\) over all possible values of
\(\lambda\). Among the most popular methods of summary are profiling
(i.e. maximization), conditioning, and integration, each with respect to
the nuisance parameter. These summaries do come at a cost, however;
eliminating a model's nuisance parameter from its likelihood almost
always sacrifices some information about its parameter of interest as
well. One measure of a good pseudolikelihood, therefore, is the balance
it strikes between the amount of information it retains about \(\psi\)
and the ease with which it can be computed.

\subsection{The Profile Likelihood}\label{the-profile-likelihood}

The most straightforward method we can use to construct a
pseudolikelihood (or equivalently, a pseudo-log-likelihood) function for
\(\psi\) is usually to find the maximum of \(\ell(\theta)\) over all
possible of values of \(\theta\) for each value of \(\psi\). This yields
what is known as the \emph{profile} log-likelihood function, formally
defined as \begin{equation}\phantomsection\label{eq-2-1-1}{
\ell_p(\psi) = \sup_{\theta \in \Theta: \> \tau(\theta) = \psi} \ell(\theta), \> \> \psi \in \Psi.
}\end{equation} In the case where an explicit nuisance parameter
\(\lambda\) exists so that \(\theta\) may be written as
\(\theta = (\psi, \lambda)\), Equation~\ref{eq-2-1-1} is equivalent to
replacing \(\lambda\) with \(\hat{\lambda}_{\psi}\), its conditional MLE
given \(\psi\): \begin{equation}\phantomsection\label{eq-2-1-2}{
\ell_p(\psi) = \ell(\psi, \hat{\lambda}_{\psi}).
}\end{equation}

Historically, the efficiency with which the profile is capable of
producing accurate estimates of \(\psi\) relative to its ease of
computation has made it the method of choice for statisticians when
performing likelihood-based inference regarding a parameter of interest.
Examples of profile-based statistics are the MLE for \(\psi\), i.e.,
\begin{equation}\phantomsection\label{eq-2-1-3}{
\hat{\psi} = \underset{\psi \in \Psi}{\arg\sup} \> \ell_p(\psi),
}\end{equation} and the signed likelihood ratio statistic for \(\psi\),
given by \begin{equation}\phantomsection\label{eq-2-1-4}{
R_{\psi} = \text{sgn}(\hat{\psi} - \psi)(2(\ell_p(\hat{\psi}) - \ell_p(\psi)))^{\frac{1}{2}}.
}\end{equation}

\subsection{The Integrated Likelihood}\label{the-integrated-likelihood}

The \emph{integrated likelihood} for \(\psi\) seeks to summarize
\(\mathcal{L}_{\psi}\) by its average value with respect to some weight
function \(\pi\) over the space \(\Theta_{\psi}\). From a theoretical
standpoint, this is preferable to the maximization procedure found in
the profile likelihood as it naturally incorporates our uncertainty
regarding the nuisance parameter's true value into the resulting
pseudolikelihood. The general form of an integrated likelihood function
is given \begin{equation}\phantomsection\label{eq-2-2-1}{
\bar{L}(\psi) = \int_{\Theta_{\psi}}L(\theta)\pi(\theta; \psi)d\theta.
}\end{equation}

It is up to the researcher to choose the weight function
\(\pi(\cdot; \psi)\), which plays an important role in the properties of
the resulting integrated likelihood. Severini (2007) developed a method
for re-parameterizing \(\lambda\) that makes the integrated likelihood
relatively insensitive to the exact weight function chosen. Using this
new parameterization, we have great flexibility in choosing our weight
function; as long as it does not depend on the parameter of interest,
the integrated likelihood that is produced will enjoy many desirable
frequency properties.

\section{Application to Poisson
Models}\label{application-to-poisson-models}

We now turn our attention to the task of using the ZSE parameterization
to construct an integrated likelihood that can be used to make
inferences regarding a parameter of interest derived from the Poisson
model described in the introduction. We will

\section{Estimating the Weighted Sum of Poisson
Means}\label{estimating-the-weighted-sum-of-poisson-means}

Consider the weighted sum \begin{equation}\phantomsection\label{eq-3-1}{
Y = \sum_{i=1}^n w_iX_i,
}\end{equation} where each \(w_i\) is a known constant greater than
zero. Suppose we take for our parameter of interest the expected value
of this weighted sum, so that
\begin{equation}\phantomsection\label{eq-3-2}{
\psi \equiv \text{E}(Y) = \sum_{i=1}^n w_i\theta_i.
}\end{equation}

\subsection{Examples}\label{examples}

\section{Zero-Inflated Poisson
Regression}\label{zero-inflated-poisson-regression}

A sample of count data is called \emph{zero-inflated} when it contains
an excess amount of zero-valued observations. A common tactic to account
for this excess is to model the data using a mixture of two processes,
one that generates zeros and another that generates counts, some of
which may also be zeros. When this count-generating process follows a
Poisson distribution, we call the resulting mixture a zero-inflated
Poisson (ZIP) model.

\includegraphics{Paper_files/figure-pdf/unnamed-chunk-1-1.pdf}

Let \(U \sim \text{Bernoulli}(1 - \pi)\) and
\(V \sim \text{Poisson}(\mu)\) Suppose \(U\) and \(V\) are independent
and let \(Y = UV\). Then \(Y \sim \text{ZIP}(\mu, \pi)\). To derive its
distribution, we begin by recognizing that \(Y = 0\) when either
\(U = 0\) or \(V = 0\) so that
\begin{equation}\phantomsection\label{eq-4-1-1}{
\begin{aligned}
\mathbb{P}(Y = 0) &= \mathbb{P}(U = 0 \cup V = 0) \\
                  &= \mathbb{P}(U = 0) + \mathbb{P}(V = 0) - \mathbb{P}(U = 0 \cap V = 0) \\
                  &= \mathbb{P}(U = 0) + \mathbb{P}(V = 0) - \mathbb{P}(U = 0)\mathbb{P}(V = 0) \\
                  &= \pi + e^{-\mu} - \pi e^{-\mu} \\ 
                  &= \pi + (1 - \pi)e^{-\mu}.
\end{aligned}
}\end{equation} In order for \(Y\) to take on a value \(y > 0\), we must
have \(U = 1\) and \(V = y\). That is,
\begin{equation}\phantomsection\label{eq-4-1-2}{
\begin{aligned}
\mathbb{P}(Y = y) &= \mathbb{P}(U = 1 \cap V = y) \\
                  &= \mathbb{P}(U = 1)\mathbb{P}(V = y) \\
                  &= (1-\pi)\frac{e^{-\mu}\mu^y}{y!}, \> \> y = 1, 2, ...
\end{aligned}
}\end{equation} Thus, the full probability mass function (pmf) for a ZIP
random variable is given by
\begin{equation}\phantomsection\label{eq-4-1-3}{
\mathbb{P}(Y = y) = \begin{cases}
               \pi + (1 - \pi)e^{-\mu}, &y =0 \\
               (1-\pi)\frac{e^{-\mu}\mu^y}{y!}, & y = 1, 2, ...
            \end{cases}
}\end{equation} Alternatively, we may write
\begin{equation}\phantomsection\label{eq-4-1-4}{
\mathbb{P}(Y = y) = \Big(\pi + (1 - \pi)e^{-\mu}\Big)^{\mathbbm{1}_{y = 0}}\Bigg(   (1-\pi)\frac{e^{-\mu}\mu^y}{y!}\Bigg)^{1 - {\mathbbm{1}_{y = 0}}}, \> \> y = 0, 1, 2, ...,
}\end{equation} where \(\mathbbm{1}_{y = 0}\) denotes an indicator
function that assumes the value one when \(y = 0\) and zero otherwise.

Suppose we observe multiple counts \(Y_1, ..., Y_n\) generated
independently from \(Y\).\footnote{Recall that the parameter of a
  generic Poisson random variable is defined relative to a fixed length
  of time during which observations may be recorded and added to the
  running total. By assuming that all counts come from the same
  ZIP-distributed random variable, we are implicitly assuming that each
  count was recorded over the same period of time and thus are on equal
  footing with one another.} The likelihood function for \(\mu\) and
\(\pi\) based on an individual observation \(Y_i = y_i\) is simply equal
to the pmf for \(Y\) evaluated at \(y_i\). That is,
\begin{equation}\phantomsection\label{eq-4-1-5}{
L(\mu, \pi; y_i) = \Big(\pi + (1 - \pi)e^{-\mu}\Big)^{\mathbbm{1}_{y_i = 0}}\Bigg(   (1-\pi)\frac{e^{-\mu}\mu^{y_i}}{y_i!}\Bigg)^{1 - {\mathbbm{1}_{y_i = 0}}}, \> \> \mu > 0, \> \> \pi \in [0, 1].
}\end{equation} We can safely ignore any multiplicative constant in
\(L\) without influencing our inferences regarding \(\mu\) and \(\pi\).
In particular, we can discard the term
\(\Big(\frac{1}{y_i!}\Big)^{1 - {\mathbbm{1}_{y_i = 0}}}\) as it depends
only on the observation \(y_i\) and is constant with respect to the
model parameters. Hence, we may write
\begin{equation}\phantomsection\label{eq-4-1-6}{
L(\mu, \pi; y_i) = \Big(\pi + (1 - \pi)e^{-\mu}\Big)^{\mathbbm{1}_{y_i = 0}}\Big(   (1-\pi)e^{-\mu}\mu^{y_i}\Big)^{1 - {\mathbbm{1}_{y_i = 0}}}, \> \> \mu > 0, \> \> \pi \in [0, 1].
}\end{equation}

From this we may derive the corresponding log-likelihood function:
\begin{equation}\phantomsection\label{eq-4-1-7}{
\begin{aligned}
\ell(\mu, \pi; y_i) &= \log L(\mu, \pi; y_i) \\
                    &= \log\Bigg\{\Big(\pi + (1 - \pi)e^{-\mu}\Big)^{\mathbbm{1}_{y_i = 0}}\Big(   (1-\pi)e^{-\mu}\mu^{y_i}\Big)^{1 - {\mathbbm{1}_{y_i = 0}}} \Bigg\} \\
                    &= \log \Bigg\{\Big(\pi + (1 - \pi)e^{-\mu}\Big)^{\mathbbm{1}_{y_i = 0}} \Bigg\} + \log \Bigg\{ \Big(   (1-\pi)e^{-\mu}\mu^{y_i}\Big)^{1 - {\mathbbm{1}_{y_i = 0}}} \Bigg\} \\
                    &= \mathbbm{1}_{y_i = 0} \log\Big(\pi + (1 - \pi)e^{-\mu}\Big) + \big(1- \mathbbm{1}_{y_i = 0}\big)\log\Big((1-\pi)e^{-\mu}\mu^{y_i}\Big) \\
                    &= \mathbbm{1}_{y_i = 0} \log\Big(\pi + (1 - \pi)e^{-\mu}\Big) + \big(1- \mathbbm{1}_{y_i = 0}\big)\Big(\log(1-\pi) - \mu + y_i \log \mu\Big).
\end{aligned}
}\end{equation} The log-likelihood for the full sample
\(y_{\bullet} \equiv (y_1, ..., y_n)\) is then obtained by summing over
the index variable \(i\) as follows:
\begin{equation}\phantomsection\label{eq-4-1-8}{
\fontsize{12pt}{12pt}\selectfont
\begin{aligned}
\ell(\mu, \pi; y_{\bullet}) &= \sum_{i=1}^n \ell(\mu, \pi; y_i) \\
                            &= \sum_{i=1}^n \bigg[\mathbbm{1}_{y_i = 0} \log\Big(\pi + (1 - \pi)e^{-\mu}\Big) + \big(1- \mathbbm{1}_{y_i = 0}\big)\Big(\log(1-\pi) - \mu + y_i \log \mu\Big)\bigg] \\
                            &= \sum_{i=1}^n \mathbbm{1}_{y_i = 0} \log\Big(\pi + (1 - \pi)e^{-\mu}\Big) + \sum_{i=1}^n\big(1- \mathbbm{1}_{y_i = 0}\big)\Big(\log(1-\pi) - \mu + y_i \log \mu\Big) \\
                            &= \log\Big(\pi + (1 - \pi)e^{-\mu}\Big) \underbrace{\sum_{i=1}^n \mathbbm{1}_{y_i = 0}}_{\text{A}} + \big(\log(1-\pi) - \mu\big)\underbrace{\sum_{i=1}^n (1-\mathbbm{1}_{y_i = 0})}_{\text{B}} + \log \mu \underbrace{\sum_{i=1}^n (1-\mathbbm{1}_{y_i = 0}) y_i}_{\text{C}}.
\end{aligned}
}\end{equation}

The summation in A is counting the number of zero counts in the sample.
Let \(\bar{\pi}\) represent the proportion of these observed zero counts
in the sample so that \begin{equation}\phantomsection\label{eq-4-1-9}{
n\bar{\pi} \equiv \sum_{i=1}^n \mathbbm{1}_{y_i = 0}.
}\end{equation} Similarly, the summation in B is counting the number of
nonzero counts in the sample. Since \(1-\bar{\pi}\) represents the
proportion of the observed nonzero counts, it follows that
\begin{equation}\phantomsection\label{eq-4-1-10}{
n(1-\bar{\pi}) \equiv \sum_{i=1}^n (1-\mathbbm{1}_{y_i = 0}).
}\end{equation} Finally, consider the summation in C. Whenever
\(y_i = 0\), \((1-\mathbbm{1}_{y_i = 0})y_i = (1-1)\cdot 0 = 0 = y_i\).
Whenever \(y_i > 0\), \((1-\mathbbm{1}_{y_i = 0})y_i = (1-0)y_i = y_i.\)
It follows that \((1-\mathbbm{1}_{y_i = 0})y_i = y_i\) for all values of
\(y_i\). Hence, the summation is simply adding all the observed counts
in the sample. Let \(\bar{y}\) denote the sample mean so that
\begin{equation}\phantomsection\label{eq-4-1-11}{
n \bar{y} \equiv \sum_{i=1}^n y_i = \sum_{i=1}^n (1-\mathbbm{1}_{y_i = 0})y_i
}\end{equation} Substituting these three expressions on the left in
Equation~\ref{eq-4-1-9}, Equation~\ref{eq-4-1-10}, and
Equation~\ref{eq-4-1-11} for their corresponding summations in the final
line of Equation~\ref{eq-4-1-8}, we arrive at
\begin{equation}\phantomsection\label{eq-4-1-12}{
\ell(\mu, \pi; y_{\bullet}) = n \bar{\pi}\log\Big(\pi + (1 - \pi)e^{-\mu}\Big) +  n(1-\bar{\pi})\big(\log(1-\pi) - \mu\big) + n\bar{y}\log \mu.
}\end{equation}

Let \(\hat{\pi}_{\mu}\) denote the partial maximum likelihood estimator
of \(\pi\) for a given value of \(\mu\). Under suitable regularity
conditions, easily satisfied in this case since the Poisson distribution
belongs to the well-behaved exponential family of distributions,
\(\hat{\pi}_{\mu}\) will be the unique value of \(\pi\) (as a function
of \(\mu\) and the data) that solves the critical point equation.
\begin{equation}\phantomsection\label{eq-4-1-13}{
\frac{\partial \ell(\mu, \pi)}{\partial\pi}\Bigg|_{\pi = \hat{\pi}_{\mu}} \equiv 0.
}\end{equation} To find it, we start by differentiating both sides of
Equation~\ref{eq-4-1-12}: \[
\frac{\partial \ell(\mu, \pi)}{\partial\pi} = \frac{n \bar{\pi}}{\pi + (1-\pi)e^{-\mu}}(1 - e^{-\mu}) - \frac{n(1 - \bar{\pi})}{1 - \pi}
                                            = n \Bigg[\frac{\bar{\pi}(1 - e^{-\mu})}{\pi + (1-\pi)e^{-\mu}} - \frac{1 - \bar{\pi}}{1 - \pi} \Bigg].
\] Evaluating at \(\pi = \hat{\pi}_{\mu}\) and plugging the result into
Equation~\ref{eq-4-1-12}, we have
\begin{equation}\phantomsection\label{eq-4-1-14}{
n \Bigg[\frac{\bar{\pi}(1 - e^{-\mu})}{\hat{\pi}_{\mu} + (1-\hat{\pi}_{\mu})e^{-\mu}} - \frac{1 - \bar{\pi}}{1 - \hat{\pi}_{\mu}} \Bigg] = 0. 
}\end{equation} When we solve for \(\hat{\pi}_{\mu}\) (see the appendix
for the full derivation), we get
\begin{equation}\phantomsection\label{eq-4-1-15}{
\hat{\pi}_{\mu} = \frac{\bar{\pi} - e^{-\mu}}{1 - e^{-\mu}}.
}\end{equation} This formula for \(\hat{\pi}_{\mu}\) leads to a negative
estimate of \(\pi\) when \(\bar{\pi} < e^{-\mu}\), which is clearly
undesirable since \(\pi\) is meant to be interpreted as a probability.
To correct it, we can modify \(\hat{\pi}_{\mu}\) slightly by setting it
equal to the value in the interval \([0,1]\) that maximizes \(\ell\)
whenever \(\bar{\pi} < e^{-\mu}\). Since \(\pi\) represents the
probability of excess zero counts in the sample, and this situation can
only occur when the observed proportion of zero counts
(i.e.~\(\bar{\pi}\)) in the sample is less than what we would expect if
our Poisson variable had no zero-inflation at all\footnote{Recall that
  \(e^{-\mu}\) is the probability of observing a zero under a
  non-zero-inflated Poisson random variable with rate parameter \(\mu\).},
it is unsurprising that this value turns out simply to be zero (see the
appendix for a formal proof). Hence, our full formula for the
partial-MLE of \(\pi\) given a particular value of \(\mu\) is as
follows: \[
\hat{\pi}_{\mu} = \begin{cases}
                  \frac{\bar{\pi} - e^{-\mu}}{1 - e^{-\mu}}, & \bar{\pi} \geq e^{-\mu} \\
                  0, & \bar{\pi} < e^{-\mu}
            \end{cases}
\]

As we hinted at above, the condition that \(\bar{\pi} \geq e^{-\mu}\)
could be considered a bellwether for zero-inflation, as it indicates an
empirical excess of zero counts relative to what we would expect for a
non-zero-inflated random variable. We refer to the condition that
\(\bar{\pi} \geq e^{-\mu}\) as the \emph{zero-inflated condition}.

\section{Importance Sampling}\label{importance-sampling}

Let \(p(\theta)\) denote a prior distribution for a parameter
\(\theta \in \Theta \subseteq \mathbb{R}^d\) and \(L(\theta; X)\) the
likelihood function of our model based on data \(X\). The posterior
distribution for \(\theta\) is given by
\(\pi(\theta | X) = cL(\theta;X)p(\theta)\), where
\(c = \big(\int_{\Theta} L(\theta;X) p(\theta) d\theta\big)^{-1} < \infty.\)
Suppose we have another function \(f(\theta) >0\) for all
\(\theta \in \Theta\) and we are interested in estimating the
expectation of this function with respect to the distribution of \(p\).
Call this value \(\mu\). Then we have \[
\begin{aligned}
\mu &= \text{E}_p\big(f(\theta)\big) \\
    &= \int_{\Theta} f(\theta)p(\theta) d\theta \\
    &= \int_{\Theta} \frac{f(\theta)}{c L(\theta;X)} c L(\theta;X) p(\theta) d\theta \\
    &= \int_{\Theta} \frac{f(\theta)}{c L(\theta;X)} \pi(\theta | X) d\theta \\
    &= \text{E}_{\pi}\Bigg(\frac{f(\theta)}{c L(\theta;X)}\Bigg).
\end{aligned}
\]

The \emph{importance sampling estimator} for \(\mu\) is \[
\hat{\mu}_{\pi} = \frac{1}{R} \sum_{i=1}^R \frac{f(\theta_i)}{c L(\theta_i;X)}, \> \> \theta_i \sim \pi.
\] Note that \(\hat{\mu}_{\pi}\) is unbiased, i.e.~\[
\begin{aligned}
\text{E}_{\pi}(\hat{\mu}_{\pi}) &= \text{E}_{\pi}\Bigg(\frac{1}{R} \sum_{i=1}^R \frac{f(\theta_i)}{c L(\theta_i;X)}\Bigg) \\
                                &= \frac{1}{R} \sum_{i=1}^R\text{E}_{\pi}\Bigg(\frac{f(\theta_i)}{c L(\theta_i;X)}\Bigg) \\
                                &= \frac{1}{R} \sum_{i=1}^R \mu \\
                                &= \frac{1}{R} R\mu \\
                                &= \mu,
\end{aligned}
\]

and by the law of large numbers converges in distribution to \(\mu\),
i.e. \[
\hat{\mu}_{\pi} \to \mu \> \text{ as } \> R \to \infty.
\]

The variance of \(\hat{\mu}_{\pi}\) is given by \[
\begin{aligned}
\text{Var}_{\pi}(\hat{\mu}_{\pi}) &= \text{Var}_{\pi}\Bigg(\frac{1}{R} \sum_{i=1}^R \frac{f(\theta_i)}{c L(\theta_i;X)}\Bigg) \\
                                  &= \frac{1}{R^2} \sum_{i=1}^R \text{Var}_{\pi}\Bigg(\frac{f(\theta_i)}{c L(\theta_i;X)}\Bigg) \\
                                  &= \frac{1}{R^2} \sum_{i=1}^R \text{Var}_{\pi}\Bigg(\frac{f(\theta)}{c L(\theta;X)}\Bigg) \\
                                  &= \frac{1}{R^2} R \cdot  \text{Var}_{\pi}\Bigg(\frac{f(\theta)}{c L(\theta;X)}\Bigg) \\
                                  &= \frac{1}{R} \text{Var}_{\pi}\Bigg(\frac{f(\theta)}{c L(\theta;X)}\Bigg) \\
                                  &= \frac{1}{R} \Bigg\{ \text{E}_{\pi}\Bigg[\bigg(\frac{f(\theta)}{c L(\theta;X)}\bigg)^2\Bigg] - \Bigg[\text{E}_{\pi}\bigg(\frac{f(\theta)}{c L(\theta;X)}\bigg)\Bigg]^2\Bigg\} \\
                                  &= \frac{1}{R} \Bigg\{ \int_{\Theta} \bigg(\frac{f(\theta)}{c L(\theta;X)}\bigg)^2 \pi(\theta | X) d\theta - \mu^2\Bigg\} \\
                                  &= \frac{1}{R} \Bigg\{ \int_{\Theta} \frac{f(\theta)^2}{c^2L(\theta;X)^2} cL(\theta;X)p(\theta) d\theta - \mu^2\Bigg\} \\
                                  &= \frac{1}{R} \Bigg\{ \int_{\Theta} \frac{f(\theta)^2 p(\theta)}{cL(\theta;X)} d\theta - \mu^2\Bigg\} \\
                                  &= \frac{1}{R} \Bigg\{ \int_{\Theta} \frac{f(\theta)^2 p(\theta)^2}{cL(\theta;X)p(\theta)} d\theta - \mu^2\Bigg\} \\
                                  &= \frac{1}{R} \Bigg\{ \int_{\Theta} \frac{\big(f(\theta) p(\theta)\big)^2}{\pi(\theta | X)} d\theta - \mu^2\Bigg\} \\
                                  &= \frac{\sigma_{\pi}^2}{R},
\end{aligned}
\] where \[
\sigma_{\pi}^2 = \int_{\Theta} \frac{\big(f(\theta) p(\theta)\big)^2}{\pi(\theta | X)} d\theta - \mu^2.
\] Some clever rearranging and substituting allows us to rewrite it as
\[
\begin{aligned}
\sigma_{\pi}^2 &= \int_{\Theta} \frac{\big(f(\theta) p(\theta)\big)^2}{\pi(\theta | X)} d\theta - \mu^2 \\
               &= \int_{\Theta} \frac{\big(f(\theta) p(\theta)\big)^2}{\pi(\theta | X)} d\theta - 2\mu^2 + \mu^2 \\
               &= \int_{\Theta} \frac{\big(f(\theta) p(\theta)\big)^2}{\pi(\theta | X)} d\theta - 2\mu \int_{\Theta} f(\theta)p(\theta) d\theta  + \mu^2 \int_{\Theta} \pi(\theta | X) d\theta \\
               &= \int_{\Theta} \Bigg(\frac{\big(f(\theta) p(\theta)\big)^2}{\pi(\theta | X)} - 2\mu f(\theta)p(\theta) + \mu^2 \pi(\theta | X)\Bigg) d\theta \\
               &= \int_{\Theta} \frac{\big(f(\theta) p(\theta)\big)^2 - 2\mu f(\theta)p(\theta)\pi(\theta|X) + \mu^2 \pi(\theta | X)^2}{\pi(\theta | X)}d\theta \\
               &= \int_{\Theta} \frac{\big(f(\theta)p(\theta) - \mu \pi(\theta | X)\big)^2}{\pi(\theta | X)}d\theta.
\end{aligned}
\]

We can also write \[
\begin{aligned}
\sigma_{\pi}^2 &= \int_{\Theta} \frac{\big(f(\theta)p(\theta) - \mu \pi(\theta | X)\big)^2}{\pi(\theta | X)}d\theta \\
               &= \int_{\Theta} \Bigg(\frac{f(\theta)p(\theta) - \mu \pi(\theta | X)}{\pi(\theta | X)}\Bigg)^2 \pi(\theta | X) d\theta \\
               &= \text{E}_{\pi} \Bigg[\bigg(\frac{f(\theta)p(\theta) - \mu \pi(\theta | X)}{\pi(\theta | X)}\bigg)^2 \Bigg].
\end{aligned}
\]

Because the \(\theta_i\) are sampled from \(\pi\), the natural variance
estimate is

\[
\hat{\sigma}^2_{\pi} = \frac{1}{R} \sum_{i=1}^R \Bigg(\frac{f(\theta_i)}{c L(\theta_i;X)} - \hat{\mu}_{\pi} \Bigg)^2 = \frac{1}{R} \sum_{i=1}^R (w_if(\theta_i) - \hat{\mu}_{\pi})^2,
\]

where \(w_i = \frac{1}{cL(\theta_i; X)}\).

\[
\begin{aligned}
\sigma^2_{\pi} + \mu &= \int_{\Theta} \frac{\big(f(\theta)p(\theta)\big)^2}{\pi(\theta | X)} d\theta \\
                     &= \int_{\Theta} \frac{\big(f(\theta)p(\theta)\big)^2}{cL(\theta; X)p(\theta)} d\theta \\ 
                     &= \int_{\Theta} \frac{f(\theta)^2}{cL(\theta; X)} p(\theta) d\theta \\
                     &= \text{E}_p\Bigg(\frac{f(\theta)^2}{cL(\theta; X)} \Bigg) \\
                     &= \text{E}_{\pi}\Bigg(\frac{f(\theta)^2}{c^2L(\theta; X)^2} \Bigg).
\end{aligned}
\]

\subsection{Self-normalized importance
sampling}\label{self-normalized-importance-sampling}

\(\pi(\theta | X) = cL(\theta;X)p(\theta)\), \(c > 0\) unknown.

\(p_u(\theta) = a p(\theta)\), \(a > 0\) unknown.

\(p_u(\theta) = b p(\theta)\), \(a > 0\) unknown.

\[
\tilde{\mu}_{\pi} = \frac{}{}
\]

\section{Appendix}\label{appendix}

\textbf{Claim}: \(\pi + (1-\pi)e^{-\mu} \geq e^{-\mu}\) for all
\(\pi \in [0,1]\).

\textbf{Proof}:

\$\$

\$\$

\[
\begin{aligned}
&n \Bigg[\frac{\bar{\pi}(1 - e^{-\mu})}{\hat{\pi}_{\mu} + (1-\hat{\pi}_{\mu})e^{-\mu}} - \frac{1 - \bar{\pi}}{1 - \hat{\pi}_{\mu}} \Bigg] = 0 \\
&\implies \frac{\bar{\pi}(1 - e^{-\mu})}{\hat{\pi}_{\mu} + (1-\hat{\pi}_{\mu})e^{-\mu}} = \frac{1 - \bar{\pi}}{1 - \hat{\pi}_{\mu}}  \\
&\implies \frac{(1 - \hat{\pi}_{\mu})(1 - e^{-\mu})}{\hat{\pi}_{\mu} + (1-\hat{\pi}_{\mu})e^{-\mu}} = \frac{1 - \bar{\pi}}{\bar{\pi}} \\
&\implies \frac{\hat{\pi}_{\mu} + (1-\hat{\pi}_{\mu})e^{-\mu}}{(1 - \hat{\pi}_{\mu})(1 - e^{-\mu})} = \frac{\bar{\pi}}{1 - \bar{\pi}} \\
&\implies \frac{\hat{\pi}_{\mu}}{(1 - \hat{\pi}_{\mu})(1 - e^{-\mu})} + \frac{(1-\hat{\pi}_{\mu})e^{-\mu}}{(1 - \hat{\pi}_{\mu})(1 - e^{-\mu})} = \frac{\bar{\pi}}{1 - \bar{\pi}} \\
&\implies \frac{\hat{\pi}_{\mu}}{(1 - \hat{\pi}_{\mu})(1 - e^{-\mu})} = \frac{\bar{\pi}}{1 - \bar{\pi}} - \frac{e^{-\mu}}{1 - e^{-\mu}} \\
&\implies \frac{\hat{\pi}_{\mu}}{1 - \hat{\pi}_{\mu}} = \frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu} \\
&\implies \frac{\hat{\pi}_{\mu}}{1 - \hat{\pi}_{\mu}} = \frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu} \\
&\implies \hat{\pi}_{\mu} = \frac{\frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu}}{\frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu} + 1} \\
&\implies \hat{\pi}_{\mu} = \frac{\frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu}}{\frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) + (1 - e^{-\mu})} \\
&\implies \hat{\pi}_{\mu} = \frac{\frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu}}{(\frac{\bar{\pi}}{1 - \bar{\pi}} + 1)(1 - e^{-\mu})} \\
&\implies \hat{\pi}_{\mu} = \frac{\frac{\bar{\pi}}{1 - \bar{\pi}}(1 - e^{-\mu}) - e^{-\mu}}{\frac{1}{1 - \bar{\pi}}(1 - e^{-\mu})} \\
&\implies \hat{\pi}_{\mu} = \frac{\bar{\pi}(1 - e^{-\mu}) - (1 - \bar{\pi})e^{-\mu}}{1 - e^{-\mu}} \\
&\implies \hat{\pi}_{\mu} = \frac{\bar{\pi} - e^{-\mu}}{1 - e^{-\mu}} \\
\end{aligned}
\]


  \bibliography{bibliography.bib}



\end{document}
