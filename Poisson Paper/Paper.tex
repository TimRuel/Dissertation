% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Integrated Likelihood Inference in Poisson Distributions},
  pdfauthor={Timothy Ruel},
  pdfkeywords={Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Integrated Likelihood Inference in Poisson Distributions}
\author{
Timothy Ruel\\
Department of Statistics and Data Science, Northwestern University\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:} Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!

\section{Introduction}\label{sec-intro}

Consider a vector \(\theta = (\theta_1, ..., \theta_n)\) in which each
component represents the mean of a distinct Poisson process. The purpose
of this paper is to discuss the task of conducting likelihood-based
inference for a real-valued parameter of interest
\(\psi = \tau(\theta)\). In particular, we will examine the utility of
the integrated likelihood function as a tool for obtaining interval and
point estimates for \(\psi\), using the performance of the more easily
calculated profile likelihood as a benchmark.

We may obtain a sample of values from each Poisson process through
repeated measurements of the number of events it generates over a fixed
period of time. Suppose we have done so, and let \(X_{ij}\) represent
the \(j\)th count from the \(i\)th sample, so that
\(X_{ij} \sim \text{Poisson}(\theta_i)\) for \(i = 1, ..., n\) and
\(j = 1, ..., m_i.\) The probability mass function (pmf) for a single
observation \(X_{ij} = x_{ij}\) is
\begin{equation}\phantomsection\label{eq-1}{
p(x_{ij}; \> \theta_i) = \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!}, \> \> x_{ij} = 0, 1, 2, ... ; \> \> \theta_i > 0.
}\end{equation}

Denote the sample of counts from the \(i\)th process by the vector
\(X_{i\bullet} = (X_{i1}, ..., X_{im_i})\), its associated mean by
\(\bar{X}_{i \bullet} = \frac{1}{m_i} \sum_{j = 1}^{m_i} X_{ij}\), and
assume that all of the counts both within and between samples are
measured independently. The likelihood function for an individual
component \(\theta_i\) based on the data \(X_{i\bullet} = x_{i\bullet}\)
is then equal to the product of the individual probabilities of the
observed counts, i.e. \begin{equation}\phantomsection\label{eq-2}{
\begin{aligned}
L(\theta_i; x_{i\bullet}) &= \prod_{j=1}^{m_i} p(x_{ij}; \theta_i) \\
                          &= \prod_{j=1}^{m_i} \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!} \\
                          &= \Bigg(\prod_{j=1}^{m_i} e^{-\theta_i}\Bigg) \Bigg(\prod_{j=1}^{m_i}\theta_i^{x_{ij}}\Bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= \bigg(e^{-\sum_{j=1}^{m_i}\theta_i}\bigg) \bigg(\theta_i^{\sum_{j=1}^{m_i}x_{ij}}\bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1}.
\end{aligned}
}\end{equation}

Since \(L\) is only useful to the extent that it informs our
understanding of the value of \(\theta_i\), we are free to replace it
with any other function differing from it by just a (nonzero)
multiplicative term that is constant with respect to \(\theta_i\),
provided that the result still satisfies the necessary regularity
conditions, as this will not change any conclusions regarding
\(\theta_i\) that we draw from it. Hence, we may safely discard the term
in parentheses on the final line of Equation~\ref{eq-2} as it does not
depend on \(\theta_i\) and instead simply write
\begin{equation}\phantomsection\label{eq-3}{
L(\theta_i; x_{i\bullet}) = e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}.
}\end{equation}

It will generally be more convenient to work with the log-likelihood
function, which is given by \begin{equation}\phantomsection\label{eq-4}{
\begin{aligned}
\ell(\theta_i; x_{i\bullet}) &= \log L(\theta_i; x_{i\bullet}) \\
                           &= \log \Big(e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Big) \\
                           &= -m_i\theta_i + m_i \bar{x}_{i\bullet} \log \theta_i \\
                           &=  m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation} The sum of the log-likelihood functions for each
component of \(\theta\) then forms the basis of the log-likelihood
function for \(\theta\) itself:
\begin{equation}\phantomsection\label{eq-5}{
\begin{aligned}
\ell(\theta; x_{1\bullet}, ..., x_{n\bullet}) &= \log L(\theta; x_{1\bullet}, ..., x_{n\bullet}) \\
                                              &= \log \Bigg(\prod_{i=1}^n L(\theta_i; x_{i\bullet})\Bigg) \\
                                              &= \sum_{i=1}^n \log L(\theta_i; x_{i\bullet}) \\
                                              &= \sum_{i=1}^n \ell(\theta_i; x_{i\bullet}) \\
                                              &= \sum_{i=1}^n m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation}

We can derive the maximum likelihood estimate (MLE) for \(\theta_i\) by
differentiating Equation~\ref{eq-4} with respect to \(\theta_i\),
setting the result equal to 0, and solving for \(\theta_i\). This gives
the nice result that the MLE is simply equal to the mean of the sample
of data \(X_{i\bullet}\). That is,
\begin{equation}\phantomsection\label{eq-6}{
\hat{\theta}_i = \bar{X}_{i\bullet}.
}\end{equation} Similarly, the MLE for the full parameter \(\theta\) is
just the vector of MLEs for its individual components:
\begin{equation}\phantomsection\label{eq-7}{
\hat{\theta} \equiv (\hat{\theta}_1, ..., \hat{\theta}_n) = (\bar{X}_{1\bullet}, ..., \bar{X}_{n\bullet}).
}\end{equation}

\section{Pseudolikelihoods}\label{pseudolikelihoods}

Let \(\Theta \subseteq \mathbb{R}^n_+\) represent the space of possible
values for \(\theta\), and suppose we have a real-valued parameter of
interest \(\psi = \tau(\theta)\), where \(\tau: \Theta \to \Psi\) is a
known twice continuously differentiable function.

The natural solution to the obstacle nuisance parameters pose to making
inferences on the parameter of interest is to find a method for
eliminating them from the likelihood function altogether. The result of
this elimination is what is known as a pseudolikelihood function.

In general, a \emph{pseudolikelihood function} for \(\psi\) is a
function of the data and \(\psi\) only, having properties resembling
that of a genuine likelihood function. If we let
\(\Theta(\psi) = \{\theta \in \Theta: \> \tau(\theta) = \psi \},\) then
associated with each \(\psi \in \Psi\) is the set of likelihoods
\(\mathcal{L}_{\psi} = \{L(\theta): \> \theta \in \Theta(\psi)\}.\)

Any summary of the values in \(\mathcal{L}_{\psi}\) that does not depend
on \(\lambda\) theoretically constitutes a pseudolikehood function for
\(\psi\). There exist a variety of methods to obtain this summary but
among the most popular are profiling (maximization), conditioning, and
integration, each with respect to the nuisance parameter. None of these
summaries come without a cost though, meaning some information about
\(\psi\) is almost certainly sacrificed whenever a nuisance parameter is
eliminated from a likelihood. One measure of a good pseudolikelihood,
therefore, is how well it is able to retain information about \(\psi\)
without becoming overly complex in its computation.

For the purposes of this paper, we will limit the scope of our
discussion to just two types of pseudolikelihoods, the profile and the
integrated likelihood.

\subsection{The Profile Likelihood}\label{the-profile-likelihood}

Perhaps the most straightforward method we can use to construct a
pseudo-likelihood (or equivalently, a pseudo-log-likelihood) function
for \(\psi\) is to find the maximum of \(\ell(\theta)\) over all
possible of values of \(\theta\) for each value of \(\psi\). This yields
what is known as the \emph{profile} log-likelihood function, formally
defined as \begin{equation}\phantomsection\label{eq-8}{
\ell_p(\psi) = \sup_{\theta \in \Theta: \> \tau(\theta) = \psi} \ell(\theta), \> \> \psi \in \Psi.
}\end{equation} In the case where an explicit nuisance parameter
\(\lambda\) exists so that \(\theta\) may be written as
\(\theta = (\psi, \lambda)\), Equation~\ref{eq-8} is equivalent to
replacing \(\lambda\) with \(\hat{\lambda}_{\psi}\), its conditional MLE
given \(\psi\): \begin{equation}\phantomsection\label{eq-9}{
\ell_p(\psi) = \ell(\psi, \hat{\lambda}_{\psi}).
}\end{equation} Historically, the efficiency with which the profile is
capable of producing accurate estimates relative to its ease of
computation has made it the method of choice for statisticians when
performing likelihood-based inference regarding a parameter of interest.
Examples of profile-based statistics are the MLE for \(\psi\), i.e.,
\begin{equation}\phantomsection\label{eq-10}{
\hat{\psi} = \underset{\psi \in \Psi}{\arg\sup} \> \ell_p(\psi),
}\end{equation} and the signed likelihood ratio statistic for \(\psi\),
given by \begin{equation}\phantomsection\label{eq-10}{
R_{\psi} = \text{sgn}(\hat{\psi} - \psi)(2(\ell_p(\hat{\psi}) - \ell_p(\psi)))^{\frac{1}{2}}.
}\end{equation}

\subsection{The Integrated Likelihood}\label{the-integrated-likelihood}

The \emph{integrated likelihood} for \(\psi\) seeks to summarize
\(\mathcal{L}_{\psi}\) by its average value with respect to some weight
function \(\pi\) over \(\Theta(\psi)\). From a theoretical standpoint,
this is preferable to the maximization procedure found in the profile
likelihood as it more naturally incorporates the uncertainty we have in
the value of the nuisance parameter into the result. Formally, the
integrated likelihood function is defined as
\begin{equation}\phantomsection\label{eq-11}{
\bar{L}(\psi) = \int_{\Lambda}L(\psi, \lambda)\pi(\lambda|\psi)d\lambda,
}\end{equation} where \(\pi(\lambda|\psi)\) is a nonnegative function on
\(\Lambda\). \(\pi(\lambda|\psi)\) is sometimes called a conditional
prior density for \(\lambda\) given \(\psi\), though it need not satisfy
the requirements of a genuine density function.

Note the similarity in form between the integral in \textbf{?@eq-IL1}
and the expression for the normalizing constant of a posterior
distribution: \[\int_{\Theta} L(\theta; X)\pi(\theta) d\theta.\] This
similarity lends credence to the idea that Bayesian techniques used to
obtain empirical approximations to posterior distributions, such as
Markov Chain Monte Carlo, could also be used to approximate an
integrated likelihood function, with the result being useful for
Bayesian and frequentist inference alike.

In general, the selection of the weight function plays an important role
in the properties of the resulting integrated likelihood. In the next
chapter, we will discuss a re-parameterization of the nuisance parameter
developed by \citet{severini2007} that makes the integrated likelihood
relatively insensitive to the exact weight function chosen. Using this
new parameterization, we have great flexibility in choosing our weight
function; as long as it does not depend on the parameter of interest,
the integrated likelihood that is produced will enjoy many desirable
frequency properties.\\
\#\# Application to Poisson Models

We now turn our attention to the task of using the ZSE parameterization
to construct an integrated likelihood that can be used to make
inferences regarding a parameter of interest derived from the Poisson
model described in the introduction. We will

\section{Inference for the Weighted Sum of Poisson
Means}\label{inference-for-the-weighted-sum-of-poisson-means}

Consider the weighted sum \[Y = \sum_{i=1}^n w_iX_i,\] where each
\(w_i\) is a known constant greater than zero. Suppose we take for our
parameter of interest the expected value of this weighted sum, so that
\[\psi \equiv \text{E}(Y) = \sum_{i=1}^n w_i\theta_i.\]


\renewcommand\refname{Examples}
  \bibliography{bibliography.bib}


\end{document}
