% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Integrated Likelihood Inference in Poisson Distributions},
  pdfauthor={Timothy Ruel},
  pdfkeywords={Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Integrated Likelihood Inference in Poisson Distributions}
\author{
Timothy Ruel\\
Department of Statistics and Data Science, Northwestern University\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:} Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!

\section{Introduction}\label{sec-intro}

Consider a vector \(\theta = (\theta_1, ..., \theta_n)\) in which each
component represents the mean of a distinct Poisson process. The purpose
of this paper is to discuss the task of conducting likelihood-based
inference for a real-valued parameter of interest
\(\psi = \tau(\theta)\), where
\(\theta \in \Theta \subset \mathbb{R}^m_+\) is unknown and
\(\tau: \Theta \to \Psi\) is a known twice continuously differentiable
function. In particular, we will examine the utility of the integrated
likelihood function as a tool for estimating \(\psi\), using the
performance of the more easily calculated profile likelihood as a
benchmark.

We may obtain a sample of values from each Poisson process through
repeated measurements of the number of events it generates over a fixed
period of time. Suppose we have done so, and let \(X_{ij}\) represent
the \(j\)th count from the \(i\)th sample, so that
\(X_{ij} \sim \text{Poisson}(\theta_i)\) for \(i = 1, ..., n\) and
\(j = 1, ..., m_i.\) The probability mass function (pmf) for a single
observation \(X_{ij} = x_{ij}\) is
\begin{equation}\phantomsection\label{eq-1}{
p(x_{ij}; \> \theta_i) = \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!}, \> \> x_{ij} = 0, 1, 2, ... ; \> \> \theta_i > 0.
}\end{equation}

Denote the vector of counts from the \(i\)th process by
\(X_{i\bullet} = (X_{i1}, ..., X_{im_i})\), its associated mean by
\(\bar{X}_{i \bullet} = \frac{1}{m_i} \sum_{j = 1}^{m_i} X_{ij}\), and
assume that all of the counts both within and between samples are
measured independently. The likelihood function for an individual
component \(\theta_i\) based on the data \(X_{i\bullet} = x_{i\bullet}\)
is then equal to the product of the individual probabilities of the
observed counts. That is, \begin{equation}\phantomsection\label{eq-2}{
\begin{aligned}
L(\theta_i; x_{i\bullet}) &= \prod_{j=1}^{m_i} p(x_{ij}; \theta_i) \\
                          &= \prod_{j=1}^{m_i} \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!} \\
                          &= \Bigg(\prod_{j=1}^{m_i} e^{-\theta_i}\Bigg) \Bigg(\prod_{j=1}^{m_i}\theta_i^{x_{ij}}\Bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= \bigg(e^{-\sum_{j=1}^{m_i}\theta_i}\bigg) \bigg(\theta_i^{\sum_{j=1}^{m_i}x_{ij}}\bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1}.
\end{aligned}
}\end{equation}

We regard \(L\) as being a function of the parameter \(\theta_i\) for
fixed \(x_{i\bullet}\). Since \(L\) is only useful to the extent that it
informs our understanding of the value of \(\theta_i\), we are free to
replace it with any other function differing from it by just a (nonzero)
multiplicative term that is constant with respect to \(\theta_i\),
provided that the result still satisfies the necessary regularity
conditions, as this will not change any conclusions regarding
\(\theta_i\) that we draw from it. Hence, we may safely discard the term
in parentheses on the final line of Equation~\ref{eq-2} as it does not
depend on \(\theta_i\) and instead simply write
\begin{equation}\phantomsection\label{eq-3}{
L(\theta_i; x_{i\bullet}) = e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}.
}\end{equation}

It will generally be more convenient to work with the log-likelihood
function, which is given by \begin{equation}\phantomsection\label{eq-4}{
\begin{aligned}
\ell(\theta_i; x_{i\bullet}) &= \log L(\theta_i; x_{i\bullet}) \\
                           &= \log \Big(e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Big) \\
                           &= -m_i\theta_i + m_i \bar{x}_{i\bullet} \log \theta_i \\
                           &=  m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation} The sum of the log-likelihood functions for each
component of \(\theta\) then forms the basis of the log-likelihood
function for \(\theta\) itself:
\begin{equation}\phantomsection\label{eq-5}{
\begin{aligned}
\ell(\theta; x_{1\bullet}, ..., x_{n\bullet}) &= \log L(\theta; x_{1\bullet}, ..., x_{n\bullet}) \\
                                              &= \log \Bigg(\prod_{i=1}^n L(\theta_i; x_{i\bullet})\Bigg) \\
                                              &= \sum_{i=1}^n \log L(\theta_i; x_{i\bullet}) \\
                                              &= \sum_{i=1}^n \ell(\theta_i; x_{i\bullet}) \\
                                              &= \sum_{i=1}^n m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation}

The maximum likelihood estimate (MLE) for a parameter \(\theta\) of an
arbitrary statistical model is defined in general as the value
\(\hat{\theta}\) satisfying
\[\hat{\theta} \equiv\arg\sup_{\theta \in \Theta} \ell(\theta),\]
provided the model meets a few regularity conditions that guarantee the
MLE exists and is unique. These requirements are always satisfied for
any member of the exponential family of models, to which the Poisson
distribution belongs, and so we may speak freely of the MLEs for Poisson
parameters without fretting over the matter of their existence and
uniqueness (or lack thereof).

Returning to our working example, we can derive the MLE for \(\theta_i\)
by differentiating Equation~\ref{eq-4} with respect to \(\theta_i\),
setting the result equal to 0, and solving for \(\theta_i\). Doing so
reveals that the MLE is simply the mean of the sample of data
\(X_{i\bullet}\): \begin{equation}\phantomsection\label{eq-5}{
\hat{\theta}_i = \bar{X}_{i\bullet}.
}\end{equation} Similarly, the MLE for the full parameter \(\theta\) is
just the vector of MLEs for its individual components:
\begin{equation}\phantomsection\label{eq-6}{
\hat{\theta} \equiv (\hat{\theta}_1, ..., \hat{\theta}_n) = (\bar{X}_{1\bullet}, ..., \bar{X}_{n\bullet}).
}\end{equation}

We typically tailor our statistical models to suit the needs of the
specific questions we have in mind. In practice however, it is common to
encounter situations in which the question at hand can only be answered
through knowledge of a function of the model's parameter rather than the
parameter itself. In the case of our Poisson model, we can imagine a
scenario in which we are not interested in estimating
\(\theta \in \Theta\) but rather a new parameter
\(\psi = \tau(\theta)\), where \(\tau: \Theta \to \Psi\) is a known
function. We refer to \(\psi\) as a \emph{parameter of interest} and
\(\tau\) as its associated \emph{interest function}. Scalar parameters
of interest tend to be the norm, so we will focus our attention on
real-valued interest functions only
(i.e.~\(\Psi \subseteq \mathbb{R}\)). We will also assume \(\tau\) has
at least two continuous derivatives.

Natural choices of estimators for \(\psi\) are those that can be found
using the \emph{plug-in principle}, whereby estimates for \(\theta\) are
passed as arguments to \(\tau\), essentially ``plugging'' them into the
mapping \(\tau(\cdot)\). Performing this procedure with the MLE for
\(\theta\) yields the MLE for \(\psi\):
\begin{equation}\phantomsection\label{eq-7}{
\hat{\psi} = \tau(\hat{\theta}).
}\end{equation}

The difference in dimension between the \(n\)-dimensional \(\theta\) and
scalar \(\psi\) implies the existence of an \((n-1)\)-dimensional
\emph{nuisance parameter} \(\lambda\) in the model. As their name
suggests, nuisance parameters generally tend either to hinder or
outright preclude inference regarding \(\psi\), and typically must be
eliminated from the log-likelihood function altogether before
proceeding. This is easier said than done however.

Furthermore, \(\psi\) need not explicitly be equal to one of the
components of \(\theta\) but instead may be defined as the output of any
function \(g\) taking \(\theta\) as input and satisfying the
requirements mentioned above.

We refer to \(\psi\) and \(\lambda\) as being \emph{implicit} parameters
in such cases. In general, so it is rare in practice to encounter a
situation in which a closed form expression for a nuisance parameter
exists.

The standard procedure for eliminating \(\lambda\) from the
log-likelihood function involves choosing some method with which to
summarize \(\ell(\theta)\) over its possible values while holding
\(\psi\) fixed in place. This effectively reduces \(\ell(\theta)\) to a
simpler function depending on \(\psi\) alone, having replaced each
dimension of \(\theta\) that depends on \(\lambda\) with a static
summary of the values in its parameter space. We call this new function
a pseudo-log-likelihood function for \(\psi\) and denote its generic
form as \(\ell(\psi)\). As we encounter specific types of
pseudo-log-likelihoods, we will introduce more specialized notation as
needed. Note that while it usually has properties resembling one,
\(\ell(\psi)\) is not itself considered a genuine log-likelihood
function, and there will always be some degree of information contained
within the data lost as a result of the nuisance parameter's
elimination.

Perhaps the most straightforward method of summarization we can use to
construct \(\ell(\psi)\) is to maximize \(\ell(\theta)\) over all
possible of values of \(\theta\) for a fixed value of \(\psi\). This
yields what is known as the \emph{profile} log-likelihood function,
formally defined as \[
\ell_p(\psi) = \sup_{\theta \in \Theta: \> g(\theta) = \psi} \ell(\theta).
\]

In the case where an explicit nuisance parameter exists,
Equation~\ref{eq-2} is equivalent to replacing \(\lambda\) with its
conditional maximum likelihood estimate given \(\psi\): \[
\ell_p(\psi) = \ell(\psi, \hat{\lambda}_{\psi}).
\]

\section{Integrated Likelihood
Functions}\label{integrated-likelihood-functions}

\section{Application to Poisson
Models}\label{application-to-poisson-models}

We now turn our attention to the task of using the ZSE parameterization
to construct an integrated likelihood that can be used to make
inferences regarding a parameter of interest derived from the Poisson
model described in the introduction. We will

\section{Inference for the Weighted Sum of Poisson
Means}\label{inference-for-the-weighted-sum-of-poisson-means}

Consider the weighted sum \[Y = \sum_{i=1}^n w_iX_i,\] where each
\(w_i\) is a known constant greater than zero. Suppose we take for our
parameter of interest the expected value of this weighted sum, so that
\[\psi \equiv \text{E}(Y) = \sum_{i=1}^n w_i\theta_i.\]


\renewcommand\refname{Examples}
  \bibliography{bibliography.bib}


\end{document}
