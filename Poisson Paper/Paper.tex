% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Integrated Likelihood Inference in Poisson Distributions},
  pdfauthor={Timothy Ruel},
  pdfkeywords={Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Integrated Likelihood Inference in Poisson Distributions}
\author{
Timothy Ruel\\
Department of Statistics and Data Science, Northwestern University\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:} Directly standardized rate, Integrated likelihood ratio
statistic, Maximum integrated likelihood estimator, Profile
likelihood, Weighted sum, Zero score expectation parameter
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!

\section{Introduction}\label{sec-intro}

Consider a vector \(\theta = (\theta_1, ..., \theta_n)\) in which each
component represents the mean of a distinct Poisson process. The purpose
of this paper is to discuss the task of conducting likelihood-based
inference for a real-valued parameter of interest
\(\psi = \tau(\theta)\), where
\(\theta \in \Theta \subset \mathbb{R}^m_+\) is unknown and
\(\tau: \Theta \to \Psi\) is a known twice continuously differentiable
function. In particular, we will examine the utility of the integrated
likelihood function as a tool for estimating \(\psi\), using the
performance of the more easily calculated profile likelihood as a
benchmark.

We may obtain a sample of values from each Poisson process through
repeated measurements of the number of events it generates over a fixed
period of time. Suppose we have done so, and let \(X_{ij}\) represent
the \(j\)th count from the \(i\)th sample, so that
\(X_{ij} \sim \text{Poisson}(\theta_i)\) for \(i = 1, ..., n\) and
\(j = 1, ..., m_i.\) The probability mass function (pmf) for a single
observation \(X_{ij} = x_{ij}\) is
\begin{equation}\phantomsection\label{eq-1}{
p(x_{ij}; \> \theta_i) = \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!}, \> \> x_{ij} = 0, 1, 2, ... ; \> \> \theta_i > 0.
}\end{equation}

Denote the vector of counts from the \(i\)th process by
\(X_{i\bullet} = (X_{i1}, ..., X_{im_i})\), its associated mean by
\(\bar{X}_{i \bullet} = \frac{1}{m_i} \sum_{j = 1}^{m_i} X_{ij}\), and
assume that all of the counts both within and between samples are
measured independently. The likelihood function for an individual
component \(\theta_i\) based on the data \(X_{i\bullet} = x_{i\bullet}\)
is then equal to the product of the individual probabilities of the
observed counts. That is, \begin{equation}\phantomsection\label{eq-2}{
\begin{aligned}
L(\theta_i; x_{i\bullet}) &= \prod_{j=1}^{m_i} p(x_{ij}; \theta_i) \\
                          &= \prod_{j=1}^{m_i} \frac{e^{-\theta_i} \theta_i ^ {x_{ij}}}{x_{ij}!} \\
                          &= \Bigg(\prod_{j=1}^{m_i} e^{-\theta_i}\Bigg) \Bigg(\prod_{j=1}^{m_i}\theta_i^{x_{ij}}\Bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= \bigg(e^{-\sum_{j=1}^{m_i}\theta_i}\bigg) \bigg(\theta_i^{\sum_{j=1}^{m_i}x_{ij}}\bigg) \Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1} \\
                          &= e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}\Bigg(\prod_{j=1}^{m_i} x_{ij}!\Bigg)^{-1}.
\end{aligned}
}\end{equation}

We regard \(L\) as being a function of the parameter \(\theta_i\) for
fixed \(x_{i\bullet}\). Since \(L\) is only useful to the extent that it
informs our understanding of the value of \(\theta_i\), we are free to
replace it with any other function differing from it by just a (nonzero)
multiplicative term that is constant with respect to \(\theta_i\),
provided that the result still satisfies the necessary regularity
conditions, as this will not change any conclusions regarding
\(\theta_i\) that we draw from it. Hence, we may safely discard the term
in parentheses on the final line of Equation~\ref{eq-2} as it does not
depend on \(\theta_i\) and instead simply write
\begin{equation}\phantomsection\label{eq-3}{
L(\theta_i; x_{i\bullet}) = e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}}.
}\end{equation}

The product of the likelihood functions for each component of \(\theta\)
then forms the basis of the likelihood function for \(\theta\) itself:
\begin{equation}\phantomsection\label{eq-4}{
\begin{aligned}
L(\theta; x_{1\bullet}, ..., x_{n\bullet}) &= \prod_{i=1}^n L(\theta_i; x_{i\bullet}) \\
                                           &= \prod_{i=1}^n e^{-m_i\theta_i}\theta_i^{m_i\bar{x}_{i\bullet}} \\
                                           &= \Bigg(\prod_{i=1}^n e^{-m_i\theta_i}\Bigg) \Bigg(\prod_{i=1}^n \theta_i^{m_i\bar{x}_{i\bullet}}\Bigg) \\
                                           &= e^{-\sum_{i=1}^n m_i\theta_i}\prod_{i=1}^n \theta_i^{m_i\bar{x}_{i\bullet}}.
\end{aligned}
}\end{equation} It will be more convenient to work with the
log-likelihood function, given by
\begin{equation}\phantomsection\label{eq-4}{
\begin{aligned}
\ell(\theta) &= \log L(\theta) \\
             &= \log \Bigg(e^{-\sum_{i=1}^n m_i\theta_i}\prod_{i=1}^n \theta_i^{m_i\bar{x}_{i\bullet}} \Bigg) \\
             &= \log \Big(e^{-\sum_{i=1}^n m_i\theta_i}\Big) + \log \Bigg(\prod_{i=1}^n \theta_i^{m_i\bar{x}_{i\bullet}} \Bigg) \\
             &= -\sum_{i=1}^n m_i\theta_i + \sum_{i=1}^n m_i\bar{x}_{i\bullet} \log\theta_i \\
             &= \sum_{i=1}^n m_i\big(\bar{x}_{i\bullet} \log\theta_i - \theta_i \big).
\end{aligned}
}\end{equation}

When conducting research, it is common to encounter situations in which
the question at hand would be better answered by knowing not the value
of the full parameter of our statistical model but rather a real-valued
function of it. That is, instead of estimating \(\theta\)

scalar parameter \(\psi\) taking values in a set
\(\Psi \subset \mathbb{R}\) such that \(\psi = g(\theta)\), where
\(g: \Theta \to \Psi\) has two continuous derivatives.

Note that while \(\ell\) is a function of the \(n\)-dimensional
\(\theta\), \(\psi\) is just a scalar. This decrease in dimension
implies the existence of an \((n-1)\)-dimensional nuisance parameter
\(\lambda\) in the model that hinders or outright precludes inference
regarding \(\psi\) and consequently must be eliminated from the
log-likelihood function before proceeding. Furthermore, \(\psi\) need
not explicitly be equal to one of the components of \(\theta\) but
instead may be defined as the output of any function \(g\) taking
\(\theta\) as input and satisfying the requirements mentioned above.

We refer to \(\psi\) and \(\lambda\) as being \emph{implicit} parameters
in such cases. In general, so it is rare in practice to encounter a
situation in which a closed form expression for a nuisance parameter
exists.

The standard procedure for eliminating \(\lambda\) from the
log-likelihood function involves choosing some method with which to
summarize \(\ell(\theta)\) over its possible values while holding
\(\psi\) fixed in place. This effectively reduces \(\ell(\theta)\) to a
simpler function depending on \(\psi\) alone, having replaced each
dimension of \(\theta\) that depends on \(\lambda\) with a static
summary of the values in its parameter space. We call this new function
a pseudo-log-likelihood function for \(\psi\) and denote its generic
form as \(\ell(\psi)\). As we encounter specific types of
pseudo-log-likelihoods, we will introduce more specialized notation as
needed. Note that while it usually has properties resembling one,
\(\ell(\psi)\) is not itself considered a genuine log-likelihood
function, and there will always be some degree of information contained
within the data lost as a result of the nuisance parameter's
elimination.

Perhaps the most straightforward method of summarization we can use to
construct \(\ell(\psi)\) is to maximize \(\ell(\theta)\) over all
possible of values of \(\theta\) for a fixed value of \(\psi\). This
yields what is known as the \emph{profile} log-likelihood function,
formally defined as \[
\ell_p(\psi) = \sup_{\theta \in \Theta: \> g(\theta) = \psi} \ell(\theta).
\]

In the case where an explicit nuisance parameter exists,
Equation~\ref{eq-2} is equivalent to replacing \(\lambda\) with its
conditional maximum likelihood estimate given \(\psi\): \[
\ell_p(\psi) = \ell(\psi, \hat{\lambda}_{\psi}).
\]

\section{Integrated Likelihood
Functions}\label{integrated-likelihood-functions}

\section{Application to Poisson
Models}\label{application-to-poisson-models}

We now turn our attention to the task of using the ZSE parameterization
to construct an integrated likelihood that can be used to make
inferences regarding a parameter of interest derived from the Poisson
model described in the introduction. We will

\section{Inference for the Weighted Sum of Poisson
Means}\label{inference-for-the-weighted-sum-of-poisson-means}

Consider the weighted sum \[Y = \sum_{i=1}^n w_iX_i,\] where each
\(w_i\) is a known constant greater than zero. Suppose we take for our
parameter of interest the expected value of this weighted sum, so that
\[\psi \equiv \text{E}(Y) = \sum_{i=1}^n w_i\theta_i.\]

The maximum likelihood estimate (MLE) for \(\theta_i\) is simply the
sample mean \(\hat{\theta}_i = \frac{1}{m_i}\sum_{j=1}^{m_i}X_{ij}\).


\renewcommand\refname{Examples}
  \bibliography{bibliography.bib}


\end{document}
