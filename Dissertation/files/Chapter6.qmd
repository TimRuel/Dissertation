\chapter{Multinomial Parameters of Interest}

\section{Introduction}

This chapter concerns parameters of interest derived from a multinomial distribution using the integrated likelihood function. The parameters of interest considered are various measures of diversity, such as Shannon entropy and Simpson's diversity index, as well as a shared effect coefficient in a multinomial generalized linear model. We begin with a brief review of the multinomial distribution and its relevant properties.

A random vector $\textbf{Y} = (Y_1, ..., Y_J)$ follows a categorical distribution with parameter $$\symbf{\theta} = (\theta_1, ..., \theta_J) \in \Delta_{J-1}$$ if $\sum_{j=1}^J Y_j = 1$, $Y_j \in \{0, 1\}$, and $\mathbbm{P}(Y_j = 1) = \theta_j$ for all $j$, where $$\Delta_{J-1} \equiv \Bigg\{\symbf{\theta} \in [0, 1]^J: \sum_{j=1}^J \theta_j = 1\Bigg\}$$ denotes the $(J-1)$-dimensional probability simplex. A multinomial random vector arises as the sum of $n$ $\text{i.i.d.}$ categorical random vectors: $$\sum_{i = 1}^n \textbf{Y}_i \sim \text{Multinomial}(n, \theta), \> \> \textbf{Y}_i \overset{\text{i.i.d.}}{\sim} \text{Categorical}(\theta), \> \> i = 1, ..., n.$$ Consequently, the categorical distribution is a special case of the multinomial distribution with $n = 1$, mirroring the relationship between the Bernoulli and binomial distributions.^[The categorical distribution is also known as the *multinoulli* distribution.]

Let $(N_1, ..., N_J) \sim \text{Multinomial}(n, \symbf{\theta})$, where $N_j$ denotes the number of observations falling in category $j$ and $n = \sum_{j=1}^J N_j$. Each marginal count satisfies $N_j \sim \text{Binomial}(n, \theta_j)$, with $\mathbbm{E}_{\symbf{\theta}}(N_j) = n \theta_j$. Since $\symbf{\theta}$ represents category probabilities, the natural parameter space is the probability simplex $\Theta = \Delta_{J-1} \subset \mathbbm{R}^J.$ The likelihood and log-likelihood functions are given by $$L(\symbf{\theta}) = \prod_{j=1}^J \theta_j^{N_j}$$ and $$\ell(\symbf{\theta}) = \sum_{j=1}^J N_j\log\theta_j,$$ respectively. Maximization of $\ell(\symbf{\theta})$ subject to $\symbf{\theta} \in \Delta_{J-1}$ yields the familiar maximum likelihood estimator $$\hat{\theta}_j = \frac{N_j}{n}, \> \> j = 1, ..., J.$$

\section{Shannon Entropy}

The Shannon entropy of a discrete random variable $X$ with support $\mathcal{X}$ and probability mass function $p$ is given by $$H(X) := -\sum_{x \in \mathcal{X}} p(x) \log p(x),$$ with the convention that $0 \log 0 = 0$. In general, $H(X) \geq 0$, with equality if and only if $p(x^*) = 1$ for some $x^* \in \mathcal{X}$ and $p(x) = 0$ for all $x \neq x^*$. If $\mathcal{X}$ is finite with cardinality $J = |\mathcal{X}|$, then $H(X) \leq J$, with equality if and only if $p(x) = 1/J$ for all $x \in \mathcal{X}$. 

Thus, Shannon entropy is minimized when all probability mass is placed on a single outcome and maximized when it is distributed unifomrly across all possible outcomes. This property makes it a natural measure of the relative concentration or dispersion of probability mass over a finite set of categories. In applied settings, entropy is commonly used as an index of diversity, for example to quantify species heterogeneity in ecological communities. 

In the present setting, our parameter of interest is the entropy of a single draw from a multinomial model. Although the observed data consist of aggregated counts $(N_1, ..., N_J)$ arising from $n$ independent realizations of a categorical random vector, the entropy itself is a property of the underlying categorical distribution with probability vector $\symbf{\theta} = (\theta_1, ..., \theta_J)$. Equivalently, the multinomial model describes the distribution of the *aggregate* $(N_1, ..., N_J)$, whereas the entropy we are interested in is a property of the *single-trial* distribution. Let $X \in \{1, ..., J\}$ denote the outcome of one trial, with $\mathbbm{P}_{\theta}(X = j) = \theta_j$. Then $$H(X) = -\sum_{j=1}^J \theta_j \log \theta_j,$$ and so we may define the estimand $$\psi(\theta) := H(X) = -\sum_{j=1}^J \theta_j \log \theta_j.$$ Hence, while the data enter the model through the multinomial likelihood for $\theta$, the parameter of interest is the entropy of the corresponding categorical distribution.

\section{Simpson's Diversity Index}

\section{Shared Effect}

\section{Discussion}






