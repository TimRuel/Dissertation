\chapter{Multinomial Dyads}

For $n$ independent trials each of which leads to a success for exactly one of $d$ categories, with the $j$th category having a fixed probability of success $\theta_j$, the multinomial distribution allows us to calculate the probability of any particular combination of numbers of successes for the various categories. If $N_j$ represents the number of successes in the $j$th category, then the random vector $(N_1, ..., N_d)$ will follow a multinomial distribution with parameter $\symbf{\theta} = (\theta_1, ..., \theta_j)$ such that $\text{E}_{\symbf{\theta}}(N_j) = n\theta_j$ for $j = 1, ..., d$, where $n = \sum_{j=1}^d N_j$. Since each $\theta_j$ is meant to be interpreted as a probability, the parameter space $\Theta$ must be the probability simplex in $\mathbbm{R}^d$ so that $\theta_j \geq 0$ for all $j$ and $\sum_{j=1}^d \theta_j = 1$. 

\section{Entropy}

Suppose we are interested in the entropy of this distribution, so that the parameter of interest is $\psi = \varphi(\symbf{\theta}),$ where $$\varphi(\theta) = -\sum_{j=1}^d \theta_j \log(\theta_j).$$ Let $(n_1, ..., n_d)$ denote the observed counts of $(N_1, ..., N_d)$ so that the likelihood function is given by $$L(\symbf{\theta}) = \prod_{j=1}^d \theta_j^{n_j}.$$ The unresricted MLE of $\theta_j$ is given by $\hat{\theta}_j = \frac{n_j}{n}$. Similarly, due to the invariance property of the MLE, the unrestricted MLE of $\psi$ will simply be $\hat{\psi} = \varphi(\hat{\symbf{\theta}})$.

To obtain an integrated likelihood for $\psi$ alone, we can use the procedure described in the previous chapter to find an approximation to the integral in @eq-ZSE_IL11 where $\overset{\sim}{L}(u; \psi)$ is the likelihood function reparameterized in terms of the ZSE parameter, and $\check{L}(u)$ and $\check{\pi}(u)$ are chosen such that $\check{\pi}$ is a conjugate prior for $\check{L}$.^[This procedure is an adapted version of another algorithm developed by @severini2022 for approximating the integrated likelihood for the entropy of a multinomial distribution.] In this case, a natural choice exists due to the fact that the Dirichlet distribution is a conjugate prior for the multinomial distribution. Since our original likelihood function $L$ is based on a multinomial distribution, we can simply set $\check{L}(u) := L(u)$ and $\check{\pi}(u) \sim \text{Dir}(\symbf{\alpha})$, where $\symbf{\alpha} = (\alpha_1, ..., \alpha_d)$. Then the posterior distribution for $u$ based on data $\symbf{n} = (n_1, ..., n_d)$ is given by $L(u)\check{\pi}(u) \sim \text{Dir}(\symbf{n} + \symbf{\alpha})$. Consequently, we will take $\check{\pi}(u)$ to be the symmetric Dirichlet distribution on the probability simplex in $\mathbbm{R}^d$ with $\alpha_j = 1$ for all $j$ so that random variates of $u$ can be sampled from a $\text{Dir}(\symbf{n} + 1)$ distribution.

From @eq-ZSE_IL9, finding $\overset{\sim}{L}(u; \psi)$ is a matter of defining two functions, $Q$ and $T_\psi$ such that $\overset{\sim}{L}(u; \psi) = L(T_{\psi}(Q(u))$. $Q$ maps a random variate $u$ sampled from the above posterior to an element $\omega$ in $\Omega_{\hat{\psi}}$, and $T_{\psi}$ then maps $\omega$ to an element $\symbf{\theta}$ in $\Theta(\psi)$. Since in this situation, these quantities $u$, $\omega$, and $\symbf{\theta}$ are all members of the probability simplex, the maps $Q$ and $T_{\psi}$ can be defined as returning the elements in their respective output spaces that are closest to the input element they have each received. That is, $Q$ returns the element $\omega$ that minimizes the distance to an input $u$, subject to the constraints that the sum of the components of $\omega$ equal 1 and $\varphi(\omega) = \hat{\psi}$. Similarly, $T_{\psi}$ returns the element $\symbf{\theta}$ that minimizes the distance to an input $\omega$, subject to the constraints that the sum of the components of $\symbf{\theta}$ equal 1 and $\varphi(\symbf{\theta}) = \psi$. 

From here, we sample $R$ random variates from the appropriate Dirichlet distribution, calculate $\overset{\sim}{L}(u; \psi)$ and $L(u)$ for each one, and use @eq-ZSE_IL12 to approximate the integrated likelihood at a particular value of $\psi$. We then repeat this process over a finely spaced sequence of the possible values of $\psi$ in order to get a shape of the overall integrated likelihood. See Appendix \ref{appendix:C} for a graph comparing the plot of one such integrated likelihood to the profile likelihood, for observed data $(n_1, ..., n_6) = (1, 1, 4, 7, 10)$ and 250 samples of the appropriate Dirichlet distribution drawn for each value of $\psi$.^[The samples were obtained using the 'LaplacesDemon' R package. The distance minimizations needed for $Q$ and $T_{\psi}$ were computed numerically using the 'nloptr' R package.]

\subsection{Fixed Effects Regression}

\section{Simpson's Diversity Index}

\section{Shared Effect}








