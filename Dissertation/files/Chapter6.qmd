\chapter{Negative Binomial Dyads}

<!--
============================================================
Introduction
============================================================ 
-->

\section{Introduction}

<!--
============================================================
Models
============================================================ 
-->

\section{Models}

\subsection{Simple rates model}

\subsection{Negative binomial regression model with fixed effects}

Suppose the conditional distribution of $Y_{ij}$ given $X_{ij}$ is $Y_{ij} | X_{ij} \sim \text{NB}(\mu_{ij}, r_j)$. We may continue to interpret $\mu_{ij}$ as the expected value of $Y_{ij}$ conditional on $X_{ij}$,  while $r_j$ is best interpreted as a dispersion parameter that measures the excess variation across counts within process $j$ that a Poisson random variable would fail to capture. Then the conditional probability mass function of $Y_{ij}$ can be expressed in canonical exponential-family form as

$$
\begin{aligned}
\text{P}(Y_{ij} = y | X_{ij}) &= \frac{\Gamma(y + r_j)}{\Gamma(r_j)y!}\Bigg(\frac{r_j}{r_j + \mu_{ij}}\Bigg)^{r_j}\Bigg(\frac{\mu_{ij}}{r_j + \mu_{ij}}\Bigg)^y, \> \> y = 0, 1, 2, ...                               
\end{aligned}
$$

where $\eta_{ij} = \log(\theta_{ij})$ is the natural parameter of the model. 

As a transformation of the rate, $\eta_{ij}$ governs the mean of the distribution and thus captures how the expected count depends on the covariates. Following from the bedrock regression principle that systematic variation in the mean response should be explained linearly in terms of the predictors, we formalize this dependence by defining the *linear predictor* for observation $i$ from process $j$ as $X_{ij}^\top \beta_j,$ a linear combination of the covariates with a vector of unknown coefficients $\beta_j$. 

In a GLM, the systematic component represented by $X_{ij}^\top \beta_j$ must be connected to its random component $\mu_{ij}$, the conditional mean of the response. This is done via a *link function* $g(\cdot)$, which specifies the relationship $$g(\mu_{ij}) =X_{ij}^\top \beta_j.$$ The *canonical link function* is the special choice of $g(\cdot)$ that sets the natural parameter equal to the linear predictor, i.e., $$\eta_{ij} = X_{ij}^\top \beta_j.$${#eq-7.1.2.6} 

For the Poisson distribution, we have already shown that the natural parameter is related to the rate through $\eta_{ij} = \log(\theta_{ij})$. Because the conditional mean of each Poisson process includes the exposure time, $\mu_{ij} = t_{ij}\theta_{ij}$, the model can be equivalently written as 
$$
\begin{aligned}
\log(\mu_{ij}) &= \log(t_{ij}\theta_{ij}) \\
               &= \log(t_{ij}) + \log(\theta_{ij}) \\
               &= \log(t_{ij}) + \eta_{ij} \\
               &= \log(t_{ij}) + X_{ij}^\top \beta_j, &&(\text{using the canonical link})
\end{aligned}
$$ {#eq-7.1.2.7}
where $\log(t_{ij})$ acts as a known offset. It follows that the canonical link for a Poisson GLM is the *log link* $g(\mu_{ij}) = \log(\mu_{ij}/t_{ij})$, under which additive effects of $X_{ij}$ correspond to multiplicative effects on $\mu_{ij}$.

To simplify notation, we include a leading 1 in the process-specific covariate vector $X_{ij}^{(s)}$ that corresponds to the intercept term for process $j$. We then partition the coefficient vector $\beta_j$ as $$\beta_j = \begin{pmatrix}\beta_j^{(s)} \\ \beta^{(a)}\end{pmatrix} \in \mathbb{R}^{(p + q) \times 1},$${#eq-7.1.2.8} where $\beta_j^{(s)} \in \mathbb{R}^{p\times1}$ contains all the coefficients specific to process $j$ (including the intercept) and $\beta^{(a)} \in \mathbb{R}^{q\times1}$ contains the shared, process-agnostic slopes. The linear predictor for observation $i$ from process $j$ can then be written as $$\eta_{ij} = \big(X_{ij}^{(s)}\big)^\top \beta_j^{(s)} + \big(X_{ij}^{(a)}\big)^\top \beta^{(a)}.$${#eq-7.1.2.9}

Stacking the observations for process $j$, we can express the model in matrix form as  
$$
\eta_j = X_j^{(s)} \beta_j^{(s)} + X_j^{(a)} \beta^{(a)},
$${#eq-7.1.2.10}
where $X_j^{(s)} \in \mathbb{R}^{n_j \times p}$ and $X_j^{(a)} \in \mathbb{R}^{n_j \times q}$ are the design matrices of process-specific and process-agnostic covariates, respectively, and $\eta_j = (\eta_{1j}, \ldots, \eta_{n_j j})^\top$ is the vector of linear predictors for process $j$. If we concatenate the design matrices so that $X_j = [X_j^{(s)} \> X_j^{(a)}] \in \mathbb{R}^{n_j \times (p + q)}$, we can further simplify @eq-7.1.2.10 as
$$
\eta_j = X_j \beta_j.
$${#eq-7.1.2.11}
Collecting all coefficients across all processes gives the global parameter vector
$$
\beta = \begin{pmatrix}\beta_1^{(s)} \\[3pt] \vdots \\[3pt] \beta_J^{(s)} \\[3pt] \beta^{(a)} \end{pmatrix}
\in\mathbb{R}^{(pJ+q) \times 1},
$$
and we view $\beta_j= (\beta_j^{(s)\top}, \beta^{(a)\top})^\top$ as the subvector of $\beta$ relevant to process $j$.

The log-likelihood for an individual observation $Y_{ij}$ follows directly from the exponential-family representation of the Poisson distribution:
$$
\ell_{ij}(\beta_j; Y_{ij}) = Y_{ij} X_{ij}^\top \beta_j - t_{ij} \exp(X_{ij}^\top \beta_j)),
$${#eq-7.1.2.12}
where we have discarded any additive terms not depending on the parameters. Summing over all observations within process $j$ gives the process-level log-likelihood,
$$
\begin{aligned}
\ell_j(\beta_j; Y_j) &= \sum_{i=1}^{n_j} \ell_{ij}(\beta_j; Y_{ij}) \\
                     &= \sum_{i=1}^{n_j} \Big[ Y_{ij} X_{ij}^\top \beta_j - t_{ij} \exp(X_{ij}^\top \beta_j)) \Big] \\
                     &= Y_j^\top X_j \beta_j - t_j^\top \exp(X_j\beta_j),
\end{aligned}
$${#eq-7.1.2.13}
where $Y_j \in \mathbb{R}^{n_j\times1}$ and $t_j\in \mathbb{R}^{n_j\times1}$ collect the observed counts and exposure times, respectively, for process $j$, and $\exp(X_j\beta_j)$ is understood to mean the exponential function applied component-wise to the vector $X_j\beta_j$.  

Finally, aggregating across all processes yields the full log-likelihood,
$$
\begin{aligned}
\ell(\beta; Y) &= \sum_{j=1}^J \ell_j(\beta_j; Y_j) \\
               &= \sum_{j=1}^J \Big[ Y_j^\top X_j \beta_j - t_j^\top \exp(X_j\beta_j) \Big] \\
               &= Y^\top X \beta - t^\top \exp(X \beta),
\end{aligned}
$${#eq-7.1.2.14}
where $n = \sum_{j=1}^J n_j$, $Y = (Y_1^\top, ..., Y_J^\top)^\top \in \mathbb{R}^{n\times1}$ is the stacked response vector, $t = (t_1^\top, ..., t_J^\top)^\top \in \mathbb{R}^{n\times1}$ is the stacked vector of exposure times, and $X = \text{blockdiag}(X_1, ..., X_j) \in \mathbb{R}^{n \times (pJ+q)}$ is the block-diagonal design matrix with process-specific blocks, augmented with shared covariates $X_j^{(a)}$ repeated across processes as necessary.

Let $\hat{\beta}_j$ denote the MLE of the coefficient vector $\beta_j$. Because the exponential term in the log-likelihood in @eq-7.1.2.14 makes it a nonlinear function of each $\beta_j$, the score equations for this model cannot be solved algebraically, and thus no closed-form expressions for $\hat{\beta}_j$ exists. Instead, $\hat{\beta}_j$ can be obtained numerically using the `glm()` function from the `stats` package in R, which implements the standard iteratively reweighted least squares (IRLS) algorithm for approximating the MLEs in the GLM framework.

From @eq-7.1.2.6, the corresponding estimated linear predictors for process $j$ are given by
$$
\hat{\eta}_{ij} = X_{ij}^\top \hat{\beta}_j.
$${#eq-7.1.2.15}
Since $\eta_{ij} = \log(\theta_{ij})$, the estimated observation-level rate is 
$$
\hat{\theta}_{ij} = \exp(X_{ij}^\top \hat{\beta}_j).
$${#eq-7.1.2.16}
An estimate of the marginal rate $\bar{\theta}_j$ defined in @eq-7.1.2.4 can then be obtained by averaging these observation-level estimates over the index $i$:
$$
\hat{\theta}_{\bullet j} = \frac{1}{n_j} \sum_{i=1}^{n_j} \exp(X_{ij}^\top \hat{\beta}_j) = \frac{1}{n_j} \mathbf{1}_{n_j}^\top \exp(X_j \hat{\beta}_j).
$${#eq-7.1.2.17}
Finally, substituting these into @eq-7.1.2.5 yields the MLE for $\psi$:
$$
\hat{\psi} = \sum_{j=1}^J \alpha_j \hat{\theta}_{\bullet j} = \alpha^\top \hat{\theta}_\bullet,
$${#eq-7.1.2.18}
where 
$$
\alpha = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_J \end{pmatrix} \> \> \> \> \text{and} \> \> \> \> \hat{\theta}_\bullet = \begin{pmatrix} \hat{\theta}_{\bullet 1} \\ \vdots \\ \hat{\theta}_{\bullet J} \end{pmatrix}.
$$
If we let 
$$
\gamma = \begin{pmatrix} \frac{\alpha_1}{n_1} \mathbf{1}_{n_1} \\ \vdots \\ \frac{\alpha_J}{n_J} \mathbf{1}_{n_J} \end{pmatrix} \in \mathbb{R}^{n\times 1},
$${#eq-7.1.2.19} 
we can rewrite our expression for $\hat{\psi}$ directly in terms of the MLE for $\beta$: 
$$
\hat{\psi} = \gamma^\top \exp(X\hat{\beta}).
$${#eq-7.1.2.20}
This allows us to write our manifold of parameter vectors that produce the same estimate of $\psi$ with the compact notation
$$
\Omega_{\hat{\psi}} = \Big\{\omega \in \mathbb{R}^{(pJ + q) \times 1}: \gamma^\top \exp(X \omega) = \gamma^\top \exp(X\hat{\beta})\Big\}.
$${#eq-7.1.2.21}

To estimate the value of the ZSE-parameterized integrated likelihood at a specific value of $\psi$, say $\psi_1$, using the method in Chapter 5, we use the following procedure.

\begin{tcolorbox}[title=7.1.2.1 Integrated Likelihood Calculation Procedure, algobox, label=alg:7.1.2.1-il-estimation]

\begin{enumerate}[left=0pt, label=\arabic*.]  % outer list
\item Draw a random variate $\hat{\omega} \in \Omega_{\hat{\psi}}$ according to a distribution not depending on $\psi$. The method we have chosen for our simulations is:

    \begin{enumerate}[label=(\roman*), left=2em]  % inner list
    \item Draw a random vector $u = (u_1, \ldots, u_m)$, where $m = pJ + q$ and each $u_k \overset{\text{i.i.d.}}{\sim} \text{N}(0, 1)$, $k = 1, \ldots, m$.
    
    \item Pass $u$ as the initial guess to an \texttt{auglag()} call from the \texttt{nloptr} R package that minimizes a dummy function $f(\hat{\omega}) = 0$ subject to the constraint
       $$h(\hat{\omega}) = \gamma^\top \exp(X \hat{\omega}) - \hat{\psi} = 0.$$
    \end{enumerate}

\item Solve
   $$\tilde{\beta} = \underset{\beta \in \mathbb{R}^m}{\arg\max} \> \> \big(t \odot \exp(X\hat{\omega})\big)^\top X \beta - t^\top \exp(X \beta) \> \> \> \> \text{subject to} \> \> \gamma^\top \exp(X \hat{\omega}) = \psi.$$

\item Define the branch evaluated at $\psi_1$ as 
   $$\ell_b^{(1)}(\psi_1) = Y^\top X \tilde{\beta} - t^\top \exp(X \tilde{\beta}).$$

\item Repeat steps (1) - (3) $R-1$ times to obtain $\ell_b^{(1)}(\psi_1), \ldots, \ell_b^{(R)}(\psi_1)$, then calculate
   $$\hat{\bar{L}}(\psi_1) = \frac{1}{R} \sum_{r=1}^R \exp\Big(\ell_b^{(r)}(\psi_1)\Big).$$

\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[title=7.1.2.1 Profile Likelihood Calculation Procedure, algobox, label=alg:7.1.2.1-il-estimation]
\begin{enumerate}[left=0pt, label=\arabic*.]

\item Solve
   $$\tilde{\beta} = \underset{\beta \in \mathbb{R}^m}{\arg\max} \> \> \big(t \odot \exp(X\hat{\beta})\big)^\top X \beta - t^\top \exp(X \beta) \> \> \> \> \text{subject to} \> \> \gamma^\top \exp(X \hat{\beta}) = \psi.$$
\item Obtain the profile likelihood evaluated at $\psi_1$ by calculating
$$
L_p(\psi_1) = \exp\Big(Y^\top X \tilde{\beta} - t^\top \exp(X \tilde{\beta}) \Big).
$$

\end{enumerate}
\end{tcolorbox}

\subsection{Negative binomial regression model with mixed effects}

\subsubsection{Fixed intercepts and random slopes}

\subsubsection{Random intercepts and fixed slopes}

\subsection{Negative binomial regression model with random effects}

<!--
============================================================
Weighted Sums
============================================================ 
-->

\section{Estimating weighted sums of negative binomial rates}

\subsection{Definition}

\subsection{Simulation design}

\subsection{Simulation results}

\subsection{Discussion}

<!--
============================================================
Dispersion
============================================================ 
-->

\section{Estimating dispersion in a negative binomial model}

\subsection{Definition}

\subsection{Simulation design}

\subsection{Simulation results}

\subsection{Discussion}

<!--
============================================================
Zero-Inflation
============================================================ 
-->

\section{Detecting zero-inflation in a negative binomial model}

\subsection{Definition}

\subsection{Simulation design}

\subsection{Simulation results}

\subsection{Discussion}

<!--
============================================================
Summary and discussion
============================================================ 
-->

\section{Summary and discussion}