\chapter{Methodology}

\section{Explicit Parameters of Interest}

\section{Implicit Parameters of Interest}

The procedure described in the previous section is based on the assumption that $\lambda$ is an explicit nuisance parameter so that taking partial derivatives of $\ell$ with respect to its components is a well-defined operation. However, @severini2018 proved that reparameterizing the model in terms of the ZSE parameter yields the same nice properties in the subsequent integrated likelihood when $\psi$ and $\lambda$ are implicit as well. In this section, we consider an approach to approximating the integrated likelihood function that has been adapted from this method.

Consider a model with parameter $\theta \in \Theta$ and implicit parameter of interest $\psi = \varphi(\theta)$ for some function $\varphi: \Theta \to \mathbbm{R}$. @eq-ZSE_IL4 tells us that we can calculate the integrated likelihood for $\psi$ if we know the value of $\theta$ corresponding to a given value of the ZSE parameter, and this will be true for models with both explicit and implict parameters. Let $\hat{\psi} = \varphi(\hat{\theta})$ denote the unrestricted MLE for $\psi$ and define the set $$\Omega_{\psi} = \Big\{\omega \in \Theta: \varphi(\omega) = \psi\Big\}.$${#eq-ZSE_IL6} Then an element of $\Omega_{\hat{\psi}}$ for a model without an explicit nuisance parameter is functionally equivalent to a value $(\hat{\psi}, \phi)$ for a model with an explicit nuisance parameter.

Generalizing the result in @eq-ZSE_IL3, for a given element $\omega \in \Omega_{\hat{\psi}}$, the corresponding value of $\theta$ is that which maximizes $\text{E}_{\omega}\big[\ell(\theta; \mathbf{X}_n)\big]$ subject to the restriction that $\varphi(\theta) = \psi$. This allows us to define a function $T_{\psi}: \Omega_{\hat{\psi}} \to \Theta(\psi)$ such that $T_{\psi}(\omega) = \underset{\theta \in \Theta(\psi)}{\mathrm{argmax}} \> \text{E}_{\omega}\big[\ell(\theta; \mathbf{X}_n)\big]$. The integrated likelihood for $\psi$ is then given by $$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L\big(T_{\psi}(\omega)\big)\pi(\omega)d\omega,$${#eq-ZSE_IL7} where $\pi(\omega)$ is a density defined on $\Omega_{\hat{\psi}}$.

@eq-ZSE_IL7 can also be written as $$\bar{L}(\psi) = \text{E}_{\omega}\Big[L\big(T_{\psi}(W)\big)\Big],$${#eq-ZSE_IL8} where $W$ represents a random variable with density $\pi(\omega)$. We can further define a function $Q: \mathcal{U} \to \Omega_{\hat{\psi}}$ for some set $\mathcal{U}$ such that if $U$ is a random variable taking values in $\mathcal{U}$, then $Q(U)$ will be a random variable in $\Omega_{\hat{\psi}}$ with a distribution that is completely determined by our choice of $U$. Therefore, $$\bar{L}(\psi) = \text{E}_{\omega}\Big[L\big(T_{\psi}(Q(U))\big)\Big]$${#eq-ZSE_IL8} is an integrated likelihood for $\psi$ with a weight function corresponding to the density of $Q(U)$. 

Define $$\tilde{L}(u; \psi) = L(T_{\psi}(Q(u)), \> u \in \mathcal{U}.$${#eq-ZSE_IL9} Then we simply have $\overset{\sim}{L}(u; \psi) = L(\symbf{\theta})$, with $\symbf{\theta}$ taken to be $T_{\psi}(Q(u))$. $\overset{\sim}{L}(u; \psi)$ will be a genuine likelihood for the parameter $(u, \psi)$ provided that there always exists a value $(u, \psi)$ such that $T_{\psi}(Q(u)) = \symbf{\theta}_0$ for any possible value of $\symbf{\theta}_0$. A sufficient condition for this to occur is that for any $\psi \in \Psi$, $T_{\psi}(Q(\mathcal{U})) = \Omega_{\psi}$.

We can then write the integrated likelihood for $\psi$ in terms of $\overset{\sim}{L}(u; \psi)$ as follows: $$\bar{L}(\psi) = \int_{\mathcal{U}} \tilde{L}(u; \psi)\check{\pi}(u)du,$${#eq-ZSE_IL10} where $\check{\pi}(u)$ is a density on $\mathcal{U}$ of our choosing. If $\check{\pi}(u)$ doesn't depend on $\psi$, the integrated likelihood given by @eq-ZSE_IL10 will have the properties we desire.

We can further rewrite @eq-ZSE_IL10 as $$\bar{L}(\psi) = \int_{\Theta} \frac{\tilde{L}(u; \psi)}{\check{L}(u)} \check{L}(u)\check{\pi}(u)du,$${#eq-ZSE_IL11} where $\check{L}(u)$ represents an arbitrary likelihood function for the "parameter" $u$. From a Bayesian point of view, the quantity $\check{L}(u)\check{\pi}(u)$ can then be thought of as a posterior density for $u$ up to some proportionality constant. If $\check{\pi}(u)$ is chosen to be a conjugate prior for $\check{L}(u)$ such that the posterior density $\check{L}(u)\check{\pi}(u)$ has the same form as $\check{L}$ itself, and $\check{L}$ is chosen to be a known distribution, then random samples can be drawn directly from this posterior density using modern statistical computing packages. Alternatively, it may also be possible to obtain random samples from the posterior density through Monte Carlo methods such as importance sampling or MCMC.  

In either case, once random variates $u_1, ..., u_R$ have been sampled from $\check{L}(u)\check{\pi}(u)$, a simple empirical estimate to $\bar{L}(\psi)$ at a particular value of $\psi$ is given by $$\hat{\bar{L}}(\psi) = \frac{1}{R} \sum_{i = 1}^R \frac{\tilde{L}(u_i; \psi)}{\check{L}(u_i)}.$${#eq-ZSE_IL12} Repeating this procedure for every value of $\psi \in \Psi$ will give an overall shape for $\hat{\bar{L}}(\psi)$, which can be used to conduct inference for $\psi_0$ without interference from a nuisance parameter.

\section{Laplace's Method}

\section{Monte Carlo Methods}

\subsection{Simple Monte Carlo}

\subsection{Importance Sampling}






To obtain an integrated likelihood for $\psi$ alone, we can use the procedure described in the previous chapter to find an approximation to the integral in @eq-ZSE_IL11 where $\overset{\sim}{L}(u; \psi)$ is the likelihood function reparameterized in terms of the ZSE parameter, and $\check{L}(u)$ and $\check{\pi}(u)$ are chosen such that $\check{\pi}$ is a conjugate prior for $\check{L}$.^[This procedure is an adapted version of another algorithm developed by @severini2022 for approximating the integrated likelihood for the entropy of a multinomial distribution.] In this case, a natural choice exists due to the fact that the Dirichlet distribution is a conjugate prior for the multinomial distribution. Since our original likelihood function $L$ is based on a multinomial distribution, we can simply set $\check{L}(u) := L(u)$ and $\check{\pi}(u) \sim \text{Dir}(\symbf{\alpha})$, where $\symbf{\alpha} = (\alpha_1, ..., \alpha_d)$. Then the posterior distribution for $u$ based on data $\symbf{n} = (n_1, ..., n_d)$ is given by $L(u)\check{\pi}(u) \sim \text{Dir}(\symbf{n} + \symbf{\alpha})$. Consequently, we will take $\check{\pi}(u)$ to be the symmetric Dirichlet distribution on the probability simplex in $\mathbbm{R}^d$ with $\alpha_j = 1$ for all $j$ so that random variates of $u$ can be sampled from a $\text{Dir}(\symbf{n} + 1)$ distribution.

From @eq-ZSE_IL9, finding $\overset{\sim}{L}(u; \psi)$ is a matter of defining two functions, $Q$ and $T_\psi$ such that $\overset{\sim}{L}(u; \psi) = L(T_{\psi}(Q(u))$. $Q$ maps a random variate $u$ sampled from the above posterior to an element $\omega$ in $\Omega_{\hat{\psi}}$, and $T_{\psi}$ then maps $\omega$ to an element $\symbf{\theta}$ in $\Theta(\psi)$. Since in this situation, these quantities $u$, $\omega$, and $\symbf{\theta}$ are all members of the probability simplex, the maps $Q$ and $T_{\psi}$ can be defined as returning the elements in their respective output spaces that are closest to the input element they have each received. That is, $Q$ returns the element $\omega$ that minimizes the distance to an input $u$, subject to the constraints that the sum of the components of $\omega$ equal 1 and $\varphi(\omega) = \hat{\psi}$. Similarly, $T_{\psi}$ returns the element $\symbf{\theta}$ that minimizes the distance to an input $\omega$, subject to the constraints that the sum of the components of $\symbf{\theta}$ equal 1 and $\varphi(\symbf{\theta}) = \psi$. 

From here, we sample $R$ random variates from the appropriate Dirichlet distribution, calculate $\overset{\sim}{L}(u; \psi)$ and $L(u)$ for each one, and use @eq-ZSE_IL12 to approximate the integrated likelihood at a particular value of $\psi$. We then repeat this process over a finely spaced sequence of the possible values of $\psi$ in order to get a shape of the overall integrated likelihood. See Appendix \ref{appendix:C} for a graph comparing the plot of one such integrated likelihood to the profile likelihood, for observed data $(n_1, ..., n_6) = (1, 1, 4, 7, 10)$ and 250 samples of the appropriate Dirichlet distribution drawn for each value of $\psi$.^[The samples were obtained using the 'LaplacesDemon' R package. The distance minimizations needed for $Q$ and $T_{\psi}$ were computed numerically using the 'nloptr' R package.]








