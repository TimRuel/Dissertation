\chapter{Chapter 3}\label{appendix:B}
\index{Appendix@\emph{Appendix B}}

\section{Monte Carlo Methods}

\subsection{Simple Monte Carlo}

\subsection{Importance Sampling}

To obtain an integrated likelihood for $\psi$ alone, we can use the procedure described in the previous chapter to find an approximation to the integral in @eq-ZSE_IL11 where $\overset{\sim}{L}(u; \psi)$ is the likelihood function reparameterized in terms of the ZSE parameter, and $\check{L}(u)$ and $\check{\pi}(u)$ are chosen such that $\check{\pi}$ is a conjugate prior for $\check{L}$.^[This procedure is an adapted version of another algorithm developed by @severini2022 for approximating the integrated likelihood for the entropy of a multinomial distribution.] In this case, a natural choice exists due to the fact that the Dirichlet distribution is a conjugate prior for the multinomial distribution. Since our original likelihood function $L$ is based on a multinomial distribution, we can simply set $\check{L}(u) = L(u)$ and $\check{\pi}(u) \sim \text{Dir}(\symbf{\alpha})$, where $\symbf{\alpha} = (\alpha_1, ..., \alpha_d)$. Then the posterior distribution for $u$ based on data $\symbf{n} = (n_1, ..., n_d)$ is given by $L(u)\check{\pi}(u) \sim \text{Dir}(\symbf{n} + \symbf{\alpha})$. Consequently, we will take $\check{\pi}(u)$ to be the symmetric Dirichlet distribution on the probability simplex in $\mathbbm{R}^d$ with $\alpha_j = 1$ for all $j$ so that random variates of $u$ can be sampled from a $\text{Dir}(\symbf{n} + 1)$ distribution.

From @eq-ZSE_IL9, finding $\overset{\sim}{L}(u; \psi)$ is a matter of defining two functions, $Q$ and $T_\psi$ such that $\overset{\sim}{L}(u; \psi) = L(T_{\psi}(Q(u))$. $Q$ maps a random variate $u$ sampled from the above posterior to an element $\omega$ in $\Omega_{\hat{\psi}}$, and $T_{\psi}$ then maps $\omega$ to an element $\symbf{\theta}$ in $\Theta(\psi)$. Since in this situation, these quantities $u$, $\omega$, and $\symbf{\theta}$ are all members of the probability simplex, the maps $Q$ and $T_{\psi}$ can be defined as returning the elements in their respective output spaces that are closest to the input element they have each received. That is, $Q$ returns the element $\omega$ that minimizes the distance to an input $u$, subject to the constraints that the sum of the components of $\omega$ equal 1 and $\varphi(\omega) = \hat{\psi}$. Similarly, $T_{\psi}$ returns the element $\symbf{\theta}$ that minimizes the distance to an input $\omega$, subject to the constraints that the sum of the components of $\symbf{\theta}$ equal 1 and $\varphi(\symbf{\theta}) = \psi$. 

From here, we sample $R$ random variates from the appropriate Dirichlet distribution, calculate $\overset{\sim}{L}(u; \psi)$ and $L(u)$ for each one, and use @eq-ZSE_IL12 to approximate the integrated likelihood at a particular value of $\psi$. We then repeat this process over a finely spaced sequence of the possible values of $\psi$ in order to get a shape of the overall integrated likelihood. See Appendix \ref{appendix:C} for a graph comparing the plot of one such integrated likelihood to the profile likelihood, for observed data $(n_1, ..., n_6) = (1, 1, 4, 7, 10)$ and 250 samples of the appropriate Dirichlet distribution drawn for each value of $\psi$.^[The samples were obtained using the 'LaplacesDemon' R package. The distance minimizations needed for $Q$ and $T_{\psi}$ were computed numerically using the 'nloptr' R package.]