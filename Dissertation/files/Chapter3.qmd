\chapter{Methodology}

\section{The Zero-Score Expectation Parameter}

@severini2007 considered the problem of selecting a weight function $\pi(\lambda|\psi)$ such that when the likelihood function is integrated with respect to this density over the nuisance parameter space, the result is useful for non-Bayesian inference. To do this, he outlined four properties (see Appendix \ref{appendix:B}) that an integrated likelihood must satisfy if it is to be of any use and went on to show that such a function could be obtained by doing the following:
\begin{enumerate}[label = \arabic*)]
  \item Find a reparameterization $(\psi, \lambda) \mapsto (\psi, \phi)$ of the model such that the new nuisance parameter $\phi$ is unrelated to $\psi$ in the sense that $\hat{\phi}_{\psi} = \hat{\phi}$; that is, the conditional maximum likelihood estimate of $\phi$ given $\psi$ is simply equal to the unrestricted maximum likelihood estimate of $\phi$.
  \item Select a prior density $\pi(\lambda|\psi)$ such that when the model undergoes the above reparameterization, the resulting prior density $\pi(\phi)$ does not depend on $\psi$.
\end{enumerate}
An integrated likelihood function for $\psi$ that possesses the desired properties will then be given by $$\bar{L}(\psi) = \int_{\Phi} \tilde{L}(\psi, \phi) \pi(\phi) d\phi,$${#eq-ZSE_IL1} where $\tilde{L}(\psi, \phi)$ is the likelihood function for the model after it has been reparameterized in terms of $\phi$. The exact choice of prior density for $\phi$ is not particularly important; the only restriction placed upon it is that it must not depend on $\psi$. Hence, the crux of the matter really lies in completing the first step. The approach taken by @severini2007 is to define a new nuisance parameter $\phi$ as the solution to the equation $$\text{E}_{(\psi_0, \lambda_0)}\Big[\nabla_{\lambda}\ell(\psi, \lambda; \mathbf{X}_n)\Big]\Bigg|_{(\psi_0, \lambda_0) = (\hat{\psi}, \phi)} = \mathbf{0},$${#eq-ZSE_IL2} where $\psi_0$ and $\lambda_0$ denote the true values of $\psi$ and $\lambda$, and $\hat{\psi}$ is the unrestricted MLE for $\psi_0$. The expectation here is being taken with respect to the data $\mathbf{X}_n = \mathbf{x}_n$ and not the parameters themselves. @eq-ZSE_IL2 can thus be rewritten as $$I(\psi, \lambda, \hat{\psi}, \phi) = \mathbf{0},$$ where $$I(\psi, \lambda, \psi_0, \lambda_0) = \int_{\mathbbm{R}^n} \big[\nabla_{\lambda}\ell(\psi, \lambda; \mathbf{x}_n)\big] p(\mathbf{x}_n; \psi_0, \lambda_0))d\mathbf{x}_n.$$

Assuming $I$ is invertible, for a particular value of $(\psi, \lambda, \hat{\psi})$, there will be a unique value of $\phi$ that solves @eq-ZSE_IL2. $\phi$ is called the *zero-score expectation* (ZSE) parameter because it is defined as the value that makes the expectation of the score function (in terms of $\lambda$, not the full parameter) with respect to $p(\mathbf{x}_n; \psi_0, \lambda_0)$ evaluated at the point $(\psi_0, \lambda_0) = (\hat{\psi}, \phi)$ equal to zero. This means that $\phi$ is really a function of $(\psi, \lambda, \hat{\psi})$, i.e., $\phi = \phi(\psi, \lambda, \hat{\psi})$. This in turn implies that $\phi$ is a function of the data through $\hat{\psi}$. Normally we try to avoid creating such dependencies in our parameters as it renders them useless for the purpose of parameterizing a statistical model. However, from the perspective of the likelihood function, once the data have been collected they are considered fixed in place and there is no issue with using a quantity such as $\phi$ that depends on the data to parameterize it.

For a given value of $\phi$, the corresponding value of $\lambda$ can be found by $$\lambda(\psi, \phi) = \underset{\lambda \in \Lambda}{\mathrm{argmax}} \> \text{E}_{(\hat{\psi}, \phi)}\big[\ell(\psi, \lambda; \mathbf{X}_n)\big].$${#eq-ZSE_IL3} For a certain choice of prior density $\pi(\phi)$, this allows us to write @eq-ZSE_IL1 in terms of $L(\psi, \lambda)$: $$\bar{L}(\psi) = \int_{\Phi} L(\psi, \lambda(\psi, \phi)) \pi(\phi) d\phi.$${#eq-ZSE_IL4} 

\section{Explicit Parameters of Interest}

\section{Implicit Parameters of Interest}

The procedure described in the previous section is based on the assumption that $\lambda$ is an explicit nuisance parameter so that taking partial derivatives of $\ell$ with respect to its components is a well-defined operation. However, @severini2018 proved that reparameterizing the model in terms of the ZSE parameter yields the same nice properties in the subsequent integrated likelihood when $\psi$ and $\lambda$ are implicit as well. In this section, we consider an approach to approximating the integrated likelihood function that has been adapted from this method.

Consider a model with parameter $\theta \in \Theta$ and implicit parameter of interest $\psi = \varphi(\theta)$ for some function $\varphi: \Theta \to \mathbbm{R}$. @eq-ZSE_IL4 tells us that we can calculate the integrated likelihood for $\psi$ if we know the value of $\theta$ corresponding to a given value of the ZSE parameter, and this will be true for models with both explicit and implict parameters. Let $\hat{\psi} = \varphi(\hat{\theta})$ denote the unrestricted MLE for $\psi$ and define the set $$\Omega_{\psi} = \Big\{\omega \in \Theta: \varphi(\omega) = \psi\Big\}.$${#eq-ZSE_IL6} Then an element of $\Omega_{\hat{\psi}}$ for a model without an explicit nuisance parameter is functionally equivalent to a value $(\hat{\psi}, \phi)$ for a model with an explicit nuisance parameter.

Generalizing the result in @eq-ZSE_IL3, for a given element $\omega \in \Omega_{\hat{\psi}}$, the corresponding value of $\theta$ is that which maximizes $\text{E}_{\omega}\big[\ell(\theta; \mathbf{X}_n)\big]$ subject to the restriction that $\varphi(\theta) = \psi$. This allows us to define a function $T_{\psi}: \Omega_{\hat{\psi}} \to \Theta(\psi)$ such that $T_{\psi}(\omega) = \underset{\theta \in \Theta(\psi)}{\mathrm{argmax}} \> \text{E}_{\omega}\big[\ell(\theta; \mathbf{X}_n)\big]$. The integrated likelihood for $\psi$ is then given by $$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L\big(T_{\psi}(\omega)\big)\pi(\omega)d\omega,$${#eq-ZSE_IL7} where $\pi(\omega)$ is a density defined on $\Omega_{\hat{\psi}}$.

@eq-ZSE_IL7 can also be written as $$\bar{L}(\psi) = \text{E}_{\omega}\Big[L\big(T_{\psi}(W)\big)\Big],$${#eq-ZSE_IL8} where $W$ represents a random variable with density $\pi(\omega)$. We can further define a function $Q: \mathcal{U} \to \Omega_{\hat{\psi}}$ for some set $\mathcal{U}$ such that if $U$ is a random variable taking values in $\mathcal{U}$, then $Q(U)$ will be a random variable in $\Omega_{\hat{\psi}}$ with a distribution that is completely determined by our choice of $U$. Therefore, $$\bar{L}(\psi) = \text{E}_{\omega}\Big[L\big(T_{\psi}(Q(U))\big)\Big]$${#eq-ZSE_IL8} is an integrated likelihood for $\psi$ with a weight function corresponding to the density of $Q(U)$. 

Define $$\tilde{L}(u; \psi) = L(T_{\psi}(Q(u)), \> u \in \mathcal{U}.$${#eq-ZSE_IL9} Then we simply have $\overset{\sim}{L}(u; \psi) = L(\symbf{\theta})$, with $\symbf{\theta}$ taken to be $T_{\psi}(Q(u))$. $\overset{\sim}{L}(u; \psi)$ will be a genuine likelihood for the parameter $(u, \psi)$ provided that there always exists a value $(u, \psi)$ such that $T_{\psi}(Q(u)) = \symbf{\theta}_0$ for any possible value of $\symbf{\theta}_0$. A sufficient condition for this to occur is that for any $\psi \in \Psi$, $T_{\psi}(Q(\mathcal{U})) = \Omega_{\psi}$.

We can then write the integrated likelihood for $\psi$ in terms of $\overset{\sim}{L}(u; \psi)$ as follows: $$\bar{L}(\psi) = \int_{\mathcal{U}} \tilde{L}(u; \psi)\check{\pi}(u)du,$${#eq-ZSE_IL10} where $\check{\pi}(u)$ is a density on $\mathcal{U}$ of our choosing. If $\check{\pi}(u)$ doesn't depend on $\psi$, the integrated likelihood given by @eq-ZSE_IL10 will have the properties we desire.

We can further rewrite @eq-ZSE_IL10 as $$\bar{L}(\psi) = \int_{\Theta} \frac{\tilde{L}(u; \psi)}{\check{L}(u)} \check{L}(u)\check{\pi}(u)du,$${#eq-ZSE_IL11} where $\check{L}(u)$ represents an arbitrary likelihood function for the "parameter" $u$. From a Bayesian point of view, the quantity $\check{L}(u)\check{\pi}(u)$ can then be thought of as a posterior density for $u$ up to some proportionality constant. If $\check{\pi}(u)$ is chosen to be a conjugate prior for $\check{L}(u)$ such that the posterior density $\check{L}(u)\check{\pi}(u)$ has the same form as $\check{L}$ itself, and $\check{L}$ is chosen to be a known distribution, then random samples can be drawn directly from this posterior density using modern statistical computing packages. Alternatively, it may also be possible to obtain random samples from the posterior density through Monte Carlo methods such as importance sampling or MCMC.  

In either case, once random variates $u_1, ..., u_R$ have been sampled from $\check{L}(u)\check{\pi}(u)$, a simple empirical estimate to $\bar{L}(\psi)$ at a particular value of $\psi$ is given by $$\hat{\bar{L}}(\psi) = \frac{1}{R} \sum_{i = 1}^R \frac{\tilde{L}(u_i; \psi)}{\check{L}(u_i)}.$${#eq-ZSE_IL12} Repeating this procedure for every value of $\psi \in \Psi$ will give an overall shape for $\hat{\bar{L}}(\psi)$, which can be used to conduct inference for $\psi_0$ without interference from a nuisance parameter.










