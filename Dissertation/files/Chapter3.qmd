\chapter{Pseudolikelihood Functions}

\section{Model Parameter Decomposition}

It is often the case that we are not interested in estimating the full parameter $\symbf{\theta} \in \symbf{\theta} \subseteq \mathbbm{R}^d$, but rather a different parameter $\psi$ taking values in a set $\Psi \subseteq \mathbbm{R}^p$, where $p < d$. In such an event, we refer to $\psi$ as the *parameter of interest*. Crucially, as we will see, $\psi$ can always be expressed as a function of $\symbf{\theta}$.

Since $\psi$ is of lower dimension than $\symbf{\theta}$, it necessarily follows that there is another parameter $\lambda$, taking values in a set $\Lambda\subseteq{\mathbbm{R}^q}$, where $p + q = d$, that is made up of whatever is "left over" from the full parameter $\symbf{\theta}$. We refer to $\lambda$ as the *nuisance parameter*, so named for its ability to complicate inference regarding the parameter of interest. Despite not being the object of study themselves, nuisance parameters are nevertheless capable of modifying the distributions of our observations and therefore must be accounted for when conducting inference or estimation regarding the parameter of interest.^[Nuisance parameters are not always uniquely defined. In fact, depending on the choice of parameter of interest, there may be multiple or even infinite ways to define one.] The process by which this is accomplished is nontrivial and often represents a serious obstacle that must be overcome. 

While not required, we will assume the parameter of interest $\psi$ is always one-dimensional. That is, $\Psi \subseteq \mathbbm{R}$ and consequently $\Lambda \subseteq \mathbbm{R}^{d-1}$. This restriction reflects the common habit of researchers to focus on scalar-valued summaries of vector quantities. For example, suppose we observe data $Y = (y_1, ..., y_n)$, where each $y_i$ is the outcome of some random variable $Y_i \sim N(\mu_i, \sigma^2_i)$, and we are interested in estimating the average of the population means, $\frac{1}{n}\sum_{i = 1}^n \mu_i$. Rather than defining $\psi = (\mu_1, ..., \mu_n)$, we can instead define $\psi = \frac{1}{n}\sum_{i = 1}^n \mu_i$ directly, bypassing the need to estimate each $\mu_i$ individually before taking their average. This does carry the trade-off of increasing the dimension of the nuisance parameter, which must be dealt with before conducting inference or estimation on $\psi_0$, the true value of $\psi$. We will examine some of the issues posed by high-dimensional nuisance parameters in greater detail in the next chapter.

\subsection{Explicit Parameters}

Parameters of interest and nuisance parameters can be broadly classified into two categories, explicit or implicit. For a given statistical model, both types of parameter must occupy the same category - it is not possible for $\psi$ to be explicit and $\lambda$ to be implicit, or vice versa. 

Let us first consider the case in which $\psi$ and $\lambda$ are *explicit* parameters. This means that $\psi$ is a sub-vector of $\symbf{\theta}$, so that all the components of $\psi$ are also components of $\symbf{\theta}$. Then there exists a set $I = \{I_1, ..., I_p\} \subsetneq \{1, ..., d\}$ such that $$\psi = (\theta_{I_1}, ..., \theta_{I_p}).$${#eq-expl_param1} It immediately follows that $\lambda$ is the sub-vector of all components of $\symbf{\theta}$ that are not part of $\psi$. More precisely, if we let $J = \{J_1, ..., J_{q}\} \subsetneq \{1, ..., d\}$ such that $I \cup J = \{1, ..., d\}$ and $I \cap J = \emptyset$, then $$\lambda = (\theta_{J_1}, ..., \theta_{J_q}).$${#eq-expl_param2} $\symbf{\theta}$ can therefore be decomposed as $\symbf{\theta} = (\psi, \lambda)$ when $\psi$ and $\lambda$ are explicit, provided we shuffle the indices appropriately.

\subsection{Implicit Parameters}

Now let us consider the case in which $\psi$ and $\lambda$ are *implicit* parameters. This means there exists some function $\varphi: \symbf{\theta} \to \Psi$ for which the parameter of interest can be written as $$\psi = \varphi(\symbf{\theta}).$${#eq-impl_param} As before, $\Psi$ is still assumed to be a subset of $\mathbbm{R}^p$ where $p$ is less than $d$. This reduction in dimension again implies the existence of a nuisance parameter $\lambda \in \Lambda \subseteq{\mathbbm{R}}^{k-m}$. However, unlike in the explicit case, a closed form expression for $\lambda$ in terms of the original components of $\symbf{\theta}$ need not exist. For this reason, implicit nuisance parameters are in general more difficult to eliminate compared to their explicit counterparts.

When the parameter of interest and nuisance parameter are explicit, it is always possible to define a function $\varphi$ such that $$\varphi(\symbf{\theta}) = (\theta_{I_1}, ..., \theta_{I_p}) = \psi ,$${#eq-expl_param3} where $\{I_1, ..., I_p\}$ is defined as above. Hence, the first case is really just a special example of this more general one in which $\psi = \varphi(\symbf{\theta})$. With this understanding in mind, we will use the notation $\psi = \varphi(\symbf{\theta})$ to refer to the parameter of interest in general, only making the distinction between implicitness and explicitness when the difference is relevant to the situation.

\subsection{Nuisance Parameter Elimination}

The natural solution to the hindrance nuisance parameters pose to making inferences on the parameter of interest is to find a method for eliminating them from the likelihood function altogether. The result of this elimination is what is known as a pseudolikelihood function.

In general, a *pseudolikelihood function* for $\psi$ is a function of the data and $\psi$ only, having properties resembling that of a genuine likelihood function. Suppose $\psi = \varphi(\symbf{\theta})$ for some function $\varphi$ and parameter $\symbf{\theta} \in \Theta$. If we let $\Theta(\psi) = \{\symbf{\theta} \in \Theta: \> \varphi(\symbf{\theta}) = \psi \},$ then associated with each $\psi \in \Psi$ is the set of likelihoods $\mathcal{L}_{\psi} = \{L(\symbf{\theta}): \> \symbf{\theta} \in \Theta(\psi)\}.$ 

Any summary of the values in $\mathcal{L}_{\psi}$ that does not depend on $\lambda$ theoretically constitutes a pseudolikehood function for $\psi$. There exist a variety of methods to obtain this summary but among the most popular are profiling (maximization), conditioning, and integration, each with respect to the nuisance parameter. None of these summaries come without a cost though, meaning some information about $\psi_0$ is almost certainly sacrificed whenever a nuisance parameter is eliminated from a likelihood. One measure of a good pseudolikelihood, therefore, is how well it is able to retain information about $\psi_0$ without becoming overly complex in its computation. 

In the previous chapter, we introduced the Bartlett identities as being a set of equations relating the derivatives of the log-likelihodd to one another. They can also be used to understand the difference between likelihood and pseudolikelihood functions. A genuine likelihood function of $\symbf{\theta}$ can be characterized as any nonnegative random function of $\symbf{\theta}$ for which all of the Bartlett identities hold. Similarly, we can think of a pseudolikelihood function of $\symbf{\theta}$ as being any nonnegative random function of $\symbf{\theta}$ for which at least one of the Bartlett identities does not hold. Hence, the identities act as a litmus test of sorts for determining the validity of a pseudolikelihood as an approximation to the genuine likelihood from which it originated - the more identities it does satisfy, the better the approximation. A pseudolikelihood that satisfies the first Bartlett identity is called *score-unbiased*; one that satisfies the second is called *information-unbiased*. Historically, more attention has been given to constructing pseudo-likelihoods that are score-unbiased, at least asymptotically (Cf. Kalbfleisch and Sprott, 1970; De Bin et al., 2015; Schumann et al., 2021, 2023).

\section{The Marginal Likelihood}

Suppose we observe some data $\mathbf{X} = \mathbf{x}$ such that the pair of statistics $(\mathbf{T}(\mathbf{X}), \mathbf{S}(\mathbf{X}))$ is sufficient for $\symbf{\theta} = (\psi, \lambda)$. Then, abusing notation slightly, we can write $$p_{\symbf{\theta}}(\mathbf{X}) = p_{\symbf{\theta}}(\mathbf{T}, \mathbf{S}).$${#eq-MC1} Since the right-hand side is a joint density for $\mathbf{T}$ and  $\mathbf{S}$, it can be factored into a product of a marginal and a conditional density as follows: $$p_{\symbf{\theta}}(\mathbf{T}, \mathbf{S}) = p_{\symbf{\theta}}(\mathbf{T}| \mathbf{S})p_{\symbf{\theta}}(\mathbf{S}).$${#eq-MC2} 

If the right-hand term in this product doesn't depend on $\lambda$, i.e., $p_{\symbf{\theta}}(\mathbf{S}) = p_{\psi}(\mathbf{S})$, then we have $$p_{\symbf{\theta}}(\mathbf{T}, \mathbf{S}) = p_{\symbf{\theta}}(\mathbf{T}| \mathbf{S})p_{\psi}(\mathbf{S}).$${#eq-MC3} In this case, a pseudolikelihood for $\psi$ is simply $$L_m(\psi) = p_{\psi}(\mathbf{S}).$${#eq-MC4} $L_m(\psi)$ is called a *marginal likelihood* for $\psi$ as it is based on the marginal distribution of $\mathbf{S} = \mathbf{S}(\mathbf{X})$. The conditional part of the density, $p_{\symbf{\theta}}(\mathbf{T}| \mathbf{S})$, does still depend on $\psi$, however, and so by choosing to base our inferences regarding $\psi_0$ solely on $L_m(\psi)$, we have ignored some relevant information for $\psi_0$ that was contained in the data. 

\section{The Conditional Likelihood}

If instead it is the left-hand term in @eq-MC2 that doesn't depend on $\lambda$, i.e., $p_{\symbf{\theta}}(\mathbf{T}| \mathbf{S}) = p_{\psi}(\mathbf{T}| \mathbf{S})$, then we have $$p_{\symbf{\theta}}(\mathbf{T}, \mathbf{S}) = p_{\psi}(\mathbf{T}| \mathbf{S})p_{\theta}(\mathbf{S}).$${#eq-MC5} Here, a pseudolikelihood for $\psi$ may be given by $$L_C(\psi) = p_{\psi}(\mathbf{T}| \mathbf{S}).$${#eq-MC6} $L_C(\psi)$ is called a *conditional likelihood* for $\psi$ as it is based on the conditional distribution of $\mathbf{T}$ given $\mathbf{S}$. As with the marginal likelihood, some information has been disregarded through our omission of $p_{\theta}(\mathbf{S})$ in our inference for $\psi_0$.

Thus, the use of either a marginal or a conditional likelihood as a pseudolikelihood for a parameter of interest is theoretically only justified when the benefits of eliminating the nuisance parameter through marginalization or conditioning outweigh the corresponding loss in information. In practice, however, the real limiting constraint of using a marginal or conditional likelihood tends not to be the lack of clarity they confer to our inferences, but rather their lack of existence in the first place. The ability to factor a density as in @eq-MC3 or @eq-MC5 is a rather strong condition, and there are plenty of models for which it is impossible to construct a marginal or conditional likelihood for its parameter of interest. When they do exist, it is typically worthwhile to use them.

\section{The Profile Likelihood}

Profile likelihoods are one of the most straightforward methods for eliminating a nuisance parameter $\lambda$ from a likelihood function. The idea is to summarize the set $\mathcal{L}_{\psi}$ by its maximum value. Formally, the profile likelihood is defined as $$L_p(\psi) = \sup_{\symbf{\theta} \in \Theta(\psi)} L(\symbf{\theta}) = L(\hat{\symbf{\theta}}_\psi),$$ where $\hat{\symbf{\theta}}_\psi$ is the MLE of $\symbf{\symbf{\theta}_0}$ for fixed $\psi$. If we define $\hat{\lambda}_{\psi}$ to be the conditional MLE for $\lambda$ given $\psi$, meaning the value of $\lambda$ that maximizes the likelihood for a particular value of $\psi$, then we must have $\hat{\symbf{\theta}}_\psi = (\psi, \hat{\lambda}_\psi)$.

Much of the allure of a profile likelihood can be traced to its ease of computation. In the event of a regular model, finding the value of $\hat{\lambda}_\psi$ corresponding to a particular $\psi$ is equivalent to solving a convex optimization problem. Either an analytical solution to this problem will exist or a numerical one can be obtained using modern computational tools. In both cases, $\hat{\lambda}_\psi$ can be found without much trouble, and from there it's just a matter of setting $\lambda = \hat{\lambda}_\psi$ inside $L(\psi, \lambda)$ to obtain $L_p(\psi)$. 

The method is not without its drawbacks however. As noted by @kalbfleisch1970, a major disadvantage to profile likelihoods are their inability to take into account the dimensionality of the nuisance parameter. By effectively always assuming that $\lambda = \hat{\lambda}_{\psi}$, they fail to incorporate any uncertainty we might have in its value into the resulting pseudolikelihood, leading to overly confident estimates of $\psi$. This effect is especially pronounced when the dimension of $\Lambda$ is large. @berger1999 also bring up the scenario in which the likelihood has a sharp ridge running in one direction as being one in which the profile likelihood will perform poorly. In such a situation, the profile of the likelihood along the ridge would not be representative of the shape of the likelihood elsewhere, and yet it is exactly that profile which will be obtained through maximization.

Nevertheless, a profile likelihood can still be a useful tool for conducting inference regarding $\psi_0$. At worst, it offers a baseline of sorts to assess the quality of other pseudolikelihoods. There is no point in using another pseudolikelihood that is harder to compute if it offers the same amount of information about $\psi_0$ - it must justify this increase in complexity with a corresponding increase in features that lend themselves to a greater degree of accuracy in our inference, such as higher level of peakedness around the value of $\psi_0$.

\section{The Integrated Likelihood}

The *integrated likelihood* for $\psi$ seeks to summarize $\mathcal{L}_{\psi}$ by its average value with respect to some weight function $\pi$ over $\Theta(\psi)$. From a theoretical standpoint, this is preferable to maximization as it incorporates (or at least is capable of incorporating) the uncertainty we have in the value of the nuisance parameter into the resulting pseudolikelihood in a very natural way. Formally, the integrated likelihood function is defined as $$\bar{L}(\psi) = \int_{\Lambda}L(\psi, \lambda)\pi(\lambda|\psi)d\lambda,$${#eq-IL1} where $\pi(\lambda|\psi)$ is a nonnegative function on $\Lambda$. $\pi(\lambda|\psi)$ is sometimes called a conditional prior density for $\lambda$ given $\psi$, though it need not satisfy the requirements of a genuine density function.

Note the similarity in form between the integral in @eq-IL1 and the expression for the normalizing constant of a posterior distribution: $$\int_{\Theta} L(\theta; X)\pi(\theta) d\theta.$$ This similarity lends weight to the idea that Bayesian techniques used to obtain empirical approximations to posterior distributions, such as Markov Chain Monte Carlo, could also be used to approximate an integrated likelihood function, with the result being useful for Bayesian and frequentist inference alike.

In general, the selection of the weight function plays an important role in the properties of the resulting integrated likelihood. The obvious first choice to make for it, and therefore the one that could be considered the "default", is simply $\pi(\lambda|\psi) \propto 1$, i.e., a uniform density. This yields what is known as the uniform-integrated likelihood: $$\bar{L}^U(\psi) = \int_{\Lambda}L(\psi, \lambda)d\lambda.$${#eq-IL2} In the next chapter, we will discuss a re-parameterization of the nuisance parameter developed by @severini2007 that makes the integrated likelihood relatively insensitive to the exact weight function chosen. Using this new parameterization, we have great flexibility in choosing our weight function; as long as it does not depend on the parameter of interest, the integrated likelihood that is produced will enjoy many desirable frequency properties.   




