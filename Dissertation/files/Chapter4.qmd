\chapter{Multinomial Dyads}

\section{Introduction}

This chapter concerns parameters of interest derived from a multinomial distribution under various model assumptions. The parameters of interest considered are Shannon entropy, Simpson's diversity index, and a shared effect coefficient in a multinomial logistic regression. We begin with a brief review of the multinomial distribution and its relevant properties.

Let $\symbf{Y}$ denote the outcome of a single categorical trial with parameter $\symbf{\theta} = (\theta_1, ..., \theta_J) \in \Delta^{J-1},$
where
$$
\Delta^{J-1} = \Bigg\{\symbf{\theta} \in [0, 1]^J: \sum_{j=1}^J \theta_j = 1\Bigg\}
$${#eq-simplex}
is the $(J-1)$-dimensional probability simplex. We may represent $\symbf{Y}$ as a random vector taking values in the canonical basis of $\mathbbm{R}^J$, i.e.,
$$
\symbf{Y} \in \{\symbf{e}_1, ..., \symbf{e}_J\},
$$
where $\symbf{e}_j$ denotes the $j$th standard basis vector. The distribution of $\symbf{Y}$ is specified by 
$$
\mathbbm{P}(\symbf{Y} = \symbf{e}_j) = \theta_j, \quad j = 1, \dots, J,
$${#eq-categorical-distribution}
and we write $\symbf{Y} \sim \text{Categorical}(\symbf{\theta})$.

A multinomial random vector arises as the sum of $n$ $\text{i.i.d.}$ categorical random vectors:
$$
\sum_{i = 1}^n \textbf{Y}_i \sim \text{Multinomial}(n, \symbf{\theta}),
\quad
\textbf{Y}_i \overset{\text{i.i.d.}}{\sim} \text{Categorical}(\symbf{\theta}),
\quad
i = 1, ..., n.
$${#eq-multinomial-construction}
In particular, the categorical distribution is a special case of the multinomial distribution with $n = 1$, mirroring the relationship between the Bernoulli and binomial distributions.^[The categorical distribution is also known as the *multinoulli* distribution.]

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{files/Images/simplex.png}
  \caption{%
    \textsc{The Probability Simplex for a Multinomial Model With Three Categories.}
    The simplex $\Delta^2$ is embedded in $\mathbbm{R}^3$ with vertices
    $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$.
    Each point on the surface corresponds to a probability vector $\symbf{\theta}$
    satisfying $\sum_j \theta_j = 1$.
  }
  \label{fig:multinomial-simplex}
\end{figure}

Let $(N_1, ..., N_J) \sim \text{Multinomial}(n, \symbf{\theta})$, where $N_j$ denotes the number of observations falling in category $j$ and $n = \sum_{j=1}^J N_j$. Each marginal count satisfies $N_j \sim \text{Binomial}(n, \theta_j)$, with $\mathbbm{E}_{\symbf{\theta}}(N_j) = n \theta_j$. Since $\symbf{\theta}$ represents category probabilities, the parameter space is $\Theta = \Delta^{J-1} \subset \mathbbm{R}^J.$ The likelihood and log-likelihood functions are given by 
$$
L(\symbf{\theta}) = \prod_{j=1}^J \theta_j^{N_j}
$${#eq-multinomial-likelihood} 
and 
$$
\ell(\symbf{\theta}) = \sum_{j=1}^J N_j \log \theta_j,
$${#eq-multinomial-log-likelihood} 
respectively. Maximization of @eq-multinomial-log-likelihood subject to $\symbf{\theta} \in \Delta^{J-1}$ yields the familiar maximum likelihood estimator
$$
\hat{\theta}_j = \frac{N_j}{n},
\quad
j = 1, ..., J.
$${#eq-multinomial-mle}

The multinomial likelihood is invariant under relabeling of the category indices: permuting the components of $\symbf{\theta}$ leaves the likelihood unchanged. This symmetry implies that, absent a sample size large enough to detect fractional differences between cell probabilities, multiple configurations of the probability vector are observationally equivalent.^[This invariance does not imply non-identifiability of the multinomial parameter, but rather reflects weak identification in directions that are immaterial for the likelihood yet consequential for certain parameters of interest.] This equivalence is most pronounced near the boundary of the probability simplex. When one or more categories are rare or unobserved, the likelihood provides little information about which specific components of $\symbf{\theta}$ are effectively zero, resulting in flat or weakly identified directions associated with different boundary faces.

\section{Models}

\subsection{Simple logit model}

A natural starting point for entropy inference under a multinomial model is to treat the category probabilities as free parameters estimated directly from count data. This was the framework considered by @severini2022, who found that an integrated likelihood based on the ZSE nuisance parameterization yields point and interval estimators for the entropy of a multinomial distribution that have improved frequency properties (e.g. reduced bias, lower root mean square error, closer-to-nominal empirical coverage rates, etc.) relative to their profile likelihood counterparts, provided that the sample size is small relative to the number of categories. We adopt this setting here as a benchmark but employ a parameterization that is more amenable to numerical optimization and to extension in more complex models.

Suppose we observe $\textbf{Y}_i \overset{\text{i.i.d.}}{\sim} \text{Categorical}(\symbf{\theta})$ for $i = 1, ..., n$ and $\symbf{\theta} \in \Delta^{J-1}$. Fixing category $J$ as a reference, we define the logit parameters
$$
\eta_j = \log\!\left(\frac{\theta_j}{\theta_J}\right), \quad j = 1, \dots, J-1,
$${#eq-logit-parameterization}
so that each $\eta_j$ represents the log--odds of category $j$ relative to the reference category $J$. Such a transformation maps the interior of $\Delta^{J-1}$ bijectively onto $\mathbbm{R}^{J-1}$, allowing optimization of an objective function with respect to the model parameter to be carried out in unconstrained Euclidean space. The simplex constraints therefore no longer need to be enforced explicitly, and we avoid any boundary-related degeneracies that arise when one or more category probabilities are close to zero. In particular, rare or unobserved categories may be represented by negative logit values that, while very large, are ultimately still finite. The likelihood surfaces expressed in the logit parameterization tend to be smoother as a result, with no sharp corners to interfere with differentiability near the boundaries of the parameter space. This leads to improved numerical stability when approximating both the integrated and profile likelihoods for $\psi$, especially in settings where the interest function depends on logarithms of the category probabilities.

To recover the category probabilities from the logits, we invert @eq-logit-parameterization. Letting $\symbf{\eta} = (\eta_1, \dots, \eta_{J-1}) \in \mathbbm{R}^{J-1}$, we have
$$
\theta_j(\symbf{\eta}) =
\frac{\exp(\eta_j)}{1 + \sum_{k=1}^{J-1} \exp(\eta_k)},
\quad j = 1, \dots, J-1,
\qquad
\theta_J(\symbf{\eta}) =
\frac{1}{1 + \sum_{k=1}^{J-1} \exp(\eta_k)}.
$${#eq-softmax-parameterization}
This normalized exponential mapping is closely related to the softmax function, with the use of a reference category in place to ensure the model identifiable. We refer to the multinomial model equipped with this parameterization as the simple logit model, reflecting the absence of covariate effects or additional regression structure.

The log-likelihood for this model is given by
$$
\ell(\symbf{\eta}) = \sum_{j=1}^J N_j \log \theta_j(\symbf{\eta}),
$$
where $\theta_j(\symbf{\eta})$ is defined as in @eq-softmax-parameterization. 

\subsection{Multinomial logistic regression model}

The multinomial logistic regression model extends the simple logit model by allowing the category probabilities to depend on observed covariates. Inference is based on independent observations $\{(\symbf{X}_i, \symbf{Y}_i)\}_{i=1}^n$, where $\symbf{X}_i \in \mathbbm{R}^p$ denotes the covariate vector associated with observation $i$ and $\symbf{Y}_i \in \{\symbf{e}_1, \dots, \symbf{e}_J\}$ denotes the corresponding categorical response. The logits defined in @eq-logit-parameterization are now modeled as a linear function of the covariates. Specifically, fixing category $J$ as a reference, the logit for the $i$-th observation of the $j$-th category is defined as the linear predictor
$$
\eta_{ij} = \symbf{X}_i^\top \symbf{\beta}_j, \quad j = 1, \dots, J-1,
$${#eq-logit-parameterization-with-covariates}
where $\symbf{\beta}_j \in \mathbbm{R}^p$ is a category-specific vector of regression coefficients.

Let 
$$
\theta_j(\symbf{x}) = \mathbbm{P}(\symbf{Y} = \symbf{e}_j \mid \symbf{X} = \symbf{x}), \quad j = 1, ..., J,
$${#eq-generic-conditional-category-probabilities}
denote the conditional probability that a categorical response falls in category $j$ given an observed covariate value $\symbf{x} \in \mathbbm{R}^p$. For observation $i$, the corresponding cell probabilities are obtained by evaluation at the observed covariates,
$$
\theta_{ij} = \mathbbm{P}(\symbf{Y}_i = \symbf{e}_j \mid \symbf{X}_i), \quad j = 1, ..., J,
$${#eq-indexed-conditional-category-probabilities}
that is, the conditional probability that the value of the $i$th response falls in category $j$ given the covariate vector $\symbf{X}_i$. Under the multinomial logistic regression model, the conditional probability function $\theta_j(\symbf{x})$ is given by the mapping
$$
\theta_j(\symbf{x}) =
\frac{\exp(\symbf{x}^\top \symbf{\beta}_j)}{1 + \sum_{k=1}^{J-1} \exp(\symbf{x}^\top \symbf{\beta}_k)},
\quad j = 1, \dots, J-1,
\qquad
\theta_{J}(\symbf{x}) =
\frac{1}{1 + \sum_{k=1}^{J-1} \exp(\symbf{x}^\top \symbf{\beta}_k)}.
$${#eq-softmax-parameterization-with-covariates}
which generalizes the softmax transformation in @eq-softmax-parameterization to incorporate covariate effects.

Note that parameters of interest derived from the category probabilities must be defined with respect to the marginal distribution of the response, rather than the observation-specific conditional probabilities. In particular, we consider the marginal category probabilities 
$$
\begin{aligned}
\bar{\theta}_j
&= \mathbbm{P}(\symbf{Y} = \symbf{e}_j) \\
&= \mathbbm{E}\left[\mathbbm{P}(\symbf{Y} = \symbf{e}_j \mid \symbf{X}) \right] \\
&= \mathbbm{E}\left[ \theta_{j}(\symbf{X}) \right] \\
&= \int_{\mathbbm{R}^p} \theta_{j}(\symbf{x})dF_{\symbf{X}}(\symbf{x}),
\quad j = 1, \dots, J,
\end{aligned}
$${#eq-marginal-category-probabilities}
where $F_{\symbf{X}}$ denotes the joint distribution of the covariates.

We may estimate each $\bar{\theta}_j$ by replacing the unknown distribution $F_{\symbf{X}}$ with its empirical counterpart, yielding the plug-in estimator 
$$
\hat{\bar{\theta}}_j = \frac{1}{n} \sum_{i=1}^n \theta_{ij}.
$${#eq-marginal-probability-estimator}
This allows us to direct our inference toward a population-level parameter of interest, rather than a value conditional on the realized covariate sample.

<!--
============================================================
Shannon Entropy
============================================================ 
-->

\section{Inference for Shannon entropy}

\subsection{Definition}

The Shannon entropy of a discrete random variable $X$ with support $\mathcal{X}$ and probability mass function $p$ is given by
$$
\text{H}(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x),
$${#eq-shannon-def}
with the convention that $0 \log 0 = 0$. In general, $\text{H}(X) \geq 0$, with equality if and only if $p(x^*) = 1$ for some $x^* \in \mathcal{X}$ and $p(x) = 0$ for all $x \neq x^*$. If $\mathcal{X}$ is finite with cardinality $J = |\mathcal{X}|$, then $\text{H}(X) \leq J$, with equality if and only if $p(x) = 1/J$ for all $x \in \mathcal{X}$. Thus, the distribution's entropy is minimized when all its probability mass is concentrated on a single outcome and maximized when it is distributed uniformly across all possible outcomes. This property makes entropy a natural measure of the concentration or dispersion of probability mass over a finite set of categories. In applied settings, entropy is commonly used as an index of diversity, for example to quantify species heterogeneity in ecological communities. In the present setting, we are interested in the entropy associated with a multinomial model, or more precisely, the entropy of a single categorical trial underlying the multinomial construction in @eq-multinomial-construction.

Let $\symbf{Y} \sim \text{Categorical}(\symbf{\theta})$ denote the outcome of one trial, with its distribution being as in @eq-categorical-distribution. Then the entropy of $\symbf{Y}$ is given by
$$
\begin{aligned} 
\text{H}(\symbf{Y}) &= -\sum_{\symbf{y} \in \{e_1,\dots,e_J\}} \mathbbm{P}(\symbf{Y} = \symbf{y})\, \log \Big[\mathbbm{P}(\symbf{Y} = \symbf{y})\Big] \\ 
                    &= - \sum_{j=1}^J \mathbbm{P}(\symbf{Y} = \symbf{e}_j)\, \log \Big[\mathbbm{P}(\symbf{Y} = \symbf{e}_j)\Big] \\
                    &= - \sum_{j=1}^J \theta_j \log \theta_j. 
\end{aligned}
$${#eq-entropy-categorical}
It follows that our interest function for the entropy of a multinomial distribution with parameter $\symbf{\theta}$ should be
$$
\varphi(\symbf{\theta}) = -\sum_{j=1}^J \theta_j \log \theta_j,
$${#eq-entropy-multinomial}
and so we may take our parameter of interest to be
$$
\psi = \varphi(\symbf{\theta}).
$${#eq-entropy-psi}
Note that while we may view the data as entering the model through the multinomial likelihood for $\symbf{\theta}$, it is more appropriate to conceive of the entropy as being a property of the corresponding categorical distribution.

\subsection{Simple logit model}

\subsubsection{Simulation design}

\subsubsection{Simulation results}

\subsubsection{Discussion}

\subsection{Multinomial logistic regression model}

\subsubsection{Simulation design}

\subsubsection{Simulation results}

\subsubsection{Discussion}

<!--
============================================================
Simpson's Diversity Index
============================================================ 
-->

\section{Inference for Simpson's diversity index}

\subsection{Definition}

The index of diversity introduced by @simpson1949 provides an alternative measure of the concentration of mass in a probability vector. For a vector $\symbf{\theta} \in \Delta^{J-1}$, Simpson's index is defined as 
$$
D(\symbf{\theta}) = \sum_{j=1}^J \theta_j^2.
$${#eq-simpson-index-sum}
Equivalently, $D(\symbf{\theta})$ is the squared Euclidean norm of $\symbf{\theta}$,
$$
D(\symbf{\theta}) = ||\symbf{\theta}||_2^2.
$${#eq-simpson-index-norm}
Note that $1/J \leq D(\symbf{\theta}) \leq 1.$ The minimum $D(\symbf{\theta}) = 1/J$ is attained when $\symbf{\theta}$ is uniform, so $\theta_j = 1/J$ for all $j$, corresponding to maximal dispersion of probability mass across categories. The maximum $D(\symbf{\theta})$ occurs when all mass is concentrated in a single component, meaning $\symbf{\theta}$ coincides with a vertex of the simplex. Thus, larger values of $D(\symbf{\theta})$ indicate greater concentration of probability mass, while smaller values reflect greater diversity.

Unlike Shannon entropy, which depends logarithmically on the cell probabilities, Simpson's index is a quadratic function of $\symbf{\theta}$. As a result, it places greater relative weight on large components of the probability vector and is less sensitive to the presence of categories with very small probability. This property makes Simpson's index particularly stable in settings where rare categories are difficult to estimate reliably.

From a statistical perspective, $D(\symbf{\theta})$ has a simple probabilistic interpretation: if two observations are drawn independently from a categorical distribution with probability vector $\symbf{\theta}$, then $D(\symbf{\theta})$ is the probability that the two observations fall in the same category, while $1 - D(\symbf{\theta})$ is the probability that they fall in different categories. Accordingly, we may take both 
$$
\psi = D(\symbf{\theta})
$${#eq-simpson-index-PoI-1}
and
$$
\tilde{\psi} = 1 - D(\symbf{\theta})
$${#eq-simpson-index-PoI-2}
as implicit scalar parameters of interest in a multinomial distribution.

\subsection{Simple logit model}

\subsubsection{Simulation design}

\subsubsection{Simulation results}

\subsubsection{Discussion}

\subsection{Multinomial logistic regression model}

\subsubsection{Simulation design}

\subsubsection{Simulation results}

\subsubsection{Discussion}

<!--
============================================================
Shared Effect
============================================================ 
-->

\section{Inference for a shared effect in a multinomial logistic regression}

\subsection{Definition}

\subsection{Simulation design}

\subsection{Simulation results}

\subsection{Discussion}

<!--
============================================================
Summary and discussion
============================================================ 
-->

\section{Summary and discussion}





