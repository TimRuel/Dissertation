\chapter{Poisson Dyads}

This chapter aims to examine the utility of the integrated likelihood function as a tool for estimating parameters of interest in dyads with models of observations that follow a Poisson distribution. In each dyad we investigate, we will compare and contrast the performances of the point and interval estimators produced by the integrated likelihood (or more commonly, our approximations to the integrated likelihood) for the dyad's parameter of interest to the analogous estimators produced by the profile likelihood function.^[The profile likelihood has a conceptual simplicity and an ease of computation that makes it a convenient choice for an estimation benchmark.]

\section{Weighted Sums of Poisson Rates}

Consider a group of $G$ independent Poisson processes with distinct rates per unit time of $\theta_1, ..., \theta_G$. Suppose we are interested in estimating a weighted sum of these rates, i.e. $$\psi \equiv \sum_{g=1}^G \alpha_g \theta_g = \alpha^\top \theta,$$ where $\alpha_1, ..., \alpha_G$ are known positive weights. We will assess the ability of the integrated likelihood to produce accurate estimates for $\psi$ under three separate model frameworks. The models we will consider are (i) a basic one in which we estimate the Poisson rate in each group using only the group's observed count and exposure time, (ii) a fixed effects regression model in which we assume there is an underlying covariate structure influencing the observed counts in each group, and (iii) a hierarchical mixed effects regression model in which we assume some or all of the intercepts and slopes from the previous model are random effects instead of fixed. We will also take into account the robustness of the pseudolikelihood-derived estimators to model misspecification when evaluating their performance.

\subsection{Basic Group Rates}

Let $Y_g$ denote the running tally of observations that we record from the $g$-th process during an interval of time $t_g$ (the exposure), so that $$Y_g \sim \text{Poisson}(t_g\theta_g).$$ The log-likelihood for $\theta$  is $$\ell(\theta; Y) = \sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big),$$ where we have discarded any additive terms not depending on $\theta$. The unconstrained MLE for $\theta$ is $\hat{\theta} = (Y_1 / t_1, ..., Y_G / t_G)^{\top}$, and the corresponding unconstrained MLE for $\psi$ is $\hat{\psi} = \alpha^\top \hat{\theta}$.

Since $\psi$ is a function of the full model parameter, we can employ the ZSE parameterization method for an implicit nuisance parameter introduced in @severini2018 to approximate the integrated likelihood. Recall that elements of the subspace $\Omega_{\hat{\psi}} = \{\omega \in \Theta: \alpha^\top \omega = \hat{\psi}\}$ play the role of the ZSE parameter in models for which the nuisance parameter is implicit. Let $Q(\theta; \omega)$ represent the expected value of $\ell(\theta; Y)$ under the probability distribution indexed by an element $\omega \in \Omega_{\hat{\psi}}$. That is,
$$
\begin{aligned}
Q(\theta; \omega) &:= \text{E}_{\omega}[\ell(\theta; Y)] \\
                  &= \text{E}_{\omega}\Bigg[\sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big)\Bigg] \\
                  &= \sum_{g=1}^G \big(\text{E}_{\omega_g}[Y_g] \log \theta_g - t_g \theta_g\big) \\
                  &= \sum_{g=1}^G \big(t_g\omega_g \log \theta_g - t_g \theta_g\big).
\end{aligned}
$$

For a given value of $\psi$, the ZSE-constrained optimizer is 
$$
\tilde{\theta}(\psi; \omega) := \arg \max_{\substack{\theta}} Q(\theta; \omega) \> \text{ s.t. } \> \alpha^{\top} \theta = \psi.
$$
Evaluating $L(\theta; Y) := \exp(\ell(\theta; Y))$ at $\theta = \tilde{\theta}(\psi; \omega)$ gives us a mapping from $\psi$ to its associated likelihood value under the distribution indexed by $\omega$. If we integrate this mapping over $\Omega_{\hat{\psi}}$ with respect to some weight function $\pi(\omega)$ that doesn't depend on $\psi$, we can obtain the integrated likelihood function for $\psi$ based on the ZSE parameterization, i.e.

$$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L(\tilde{\theta}(\psi; \omega)) \pi(\omega) d\omega.$$

Unsurprisingly, no closed-form solution exists for this integral. Instead, we can use Monte Carlo integration to approximate it with the following estimator:

$$\hat{\bar{L}}(\psi) = \frac{1}{R} \sum_{j=1}^R L(\tilde{\theta}(\omega^{(j)}; \psi); Y),$$
where the choice of $\pi(\omega)$ is now understood to be defined implicitly as the density function admitted by the probability measure $d\Pi(\omega)$ that governs how each $\omega^{(j)}$ is drawn from $\Omega_{\hat{\psi}}$. As long as the selection does not depend on the value of $\psi$, we are essentially free to choose any (non-degenerate) probability distribution we wish, and the form of $\pi(\omega)$ will be adjusted accordingly.

Hence, under a simple MC approximation scheme, the functions $L(\tilde{\theta}(\omega^{(j)}; \psi); Y)$ for $j = 1, ..., R$ are individual "branches" of the integrated likelihood that we average to obtain an estimate of its true underlying value at a particular value of $\psi$. Let $$b_{\omega}(\psi) := \log L(\tilde{\theta}(\psi; \omega^{(j)}); Y)$$ denote the branch curve generated by the value $\omega \in \Omega_{\hat{\psi}}$, and let its mode be given by $$\psi^*_{\omega} := \arg \max_{\substack{\psi}} b_{\omega}(\psi).$$ 

For an arbitrary dyad, two distinct values $\omega^{(1)}, \omega^{(2)} \in \Omega_{\hat{\psi}}$ will in general produce branch curves with modes $\psi^*_{\omega^{(1)}}$ and $\psi^*_{\omega^{(2)}}$, respectively, that are also distinct. In other words, there is no reason to expect that branch curves corresponding to different values drawn from $\Omega_{\hat{\psi}}$ will be maximized at the same location. However, if our dyad is such that the model belongs to the exponential family and our parameter of interest $\psi$ is a linear function of the full model parameter, then this is no longer case. Indeed, it can be shown that under these two conditions, the branch curves of the integrated likelihood function for $\psi$ will all have the same mode regardless of the values of $\omega$ that generated them, and furthermore that mode will simply be the unconstrained MLE for $\psi$, given by $\hat{\psi} = \alpha^{\top} \hat{\theta}$ (see appendix:A for a proof). Therefore our MC approximation to the integrated likelihood for $\psi$, being nothing more a simple average of these branches at each value of $\psi$, must also be maximized at $\hat{\psi}$.

Of course, by definition $\hat{\psi}$ is also the maximizer of the profile likelihood, meaning that the integrated likelihood and profile likelihood will produce identical maximum likelihood estimates for $\psi$ in any dyad satisfying these criteria. We can further use a quadratic expansion of an arbitrary branch curve to show that the branch's curvature around its peak at $\hat{\psi}$ will be equal to that of the profile likelihood up to a second-order approximation (again, see appendix:A for a proof). It follows that the behavior of our MC approximation to the integrated likelihood for $\psi$, itself being nothing more than a simple average of these branches, will be virtually indistinguishable from that of the profile likelihood near $\hat{\psi}$ since each branch contributes a quadratic with the same peak and very similar curvature. Thus it is clear that for any dyad satisfying these criteria, it does not matter whether we use the integrated or the profile likelihood to conduct inference regarding its parameter of interest - our conclusions will be identical. However, the profile likelihood is in general both conceptually and computationally easier to implement, making it the better choice in such cases.

\subsection{Fixed Effects Regression}

Suppose that we still have $G$ independent Poisson processes with distinct rates per unit time $\theta_1, ..., \theta_G$, but we now choose to model the recorded counts from each of these processes as the observed values of a response variable in a linear regression with fixed effects. To understand the sampling procedure germane to this model, first consider an arbitrary process with index $g$, where $g \in \{1, ..., G\}$. From this process we make $n_g$ independent observations, where an "observation" here refers to the count of recorded events generated by the process over a measured interval of time. Let $Y_{ig}$ represent the $i$-th observation made from the $g$-th process during an exposure time $t_{ig}$, so that $$Y_{ig} \sim \text{Poisson}(\mu_{ig}),$$ where $\mu_{ig} = t_{ig} \theta_g$ and $i \in \{1, ..., n_g\}$.

Let us further assume that each observed $Y_{ig}$ has associated with it two vectors of observed covariates $X_{ig} \in \mathbb{R}^{p\times1}$ and $Z_{ig} \in \mathbb{R}^{q\times1}$, where $p$ and $q$ are known non-negative integers such that $p + q \geq 1$. That is, for the $i$-th count of instances we record from the $g$-th Poisson process, we also observe values from $p + q$ independent covariates. A covariate is assigned to a component of the vector $X_{ig}$ if its effect on the response $Y_{ig}$ is assumed to vary across processes (i.e. its slope coefficient depends on the value of $g$) or to a component of the vector $Z_{ig}$ if its effect on $Y_{ig}$ is assumed to be constant regardless of the process with which it is associated (i.e. it has a single slope coefficient for all values of $g$). ^[We still assume the effects associated with the covariates belonging to $X_{ig}$ are *fixed* and not *random*, in the sense that we do not consider the effects themselves (the corresponding slope coefficients) to be realizations of an underlying random variable.] 



Let $\alpha_g$ represent the intercept term in group $g$, 

and $Z_{ig}$ on the basis of whether the covariate

As the Poisson distribution belongs to the exponential family of distributions, this is an example of a generalized linear model (GLM). Every GLM needs a link function relating the conditional mean of its response variable given its set of predictors to the linear predictor itself; we will choose for ours the canonical link for a Poisson GLM, the natural logarithm. 

To estimate the true group rate $\theta_g$ in the previous model, it sufficed to count the overall number of observations $Y_g$ over some total exposure time $t_g$. In theory, this time could have been broken up into a series of smaller non-overlapping intervals with chunks of observations being recorded in each. However, 

\subsection{Mixed Effects Regression}

\subsubsection{Fixed Intercepts and Random Slopes}

\subsubsection{Random Intercepts and Fixed Slopes}

\subsubsection{Random Intercepts and Random Slopes}

\section{Overdispersion}

\section{Zero-Inflation}