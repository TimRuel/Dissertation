\chapter{Poisson Dyads}

This chapter aims to examine the utility of the integrated likelihood function as a tool for estimating parameters of interest in dyads with models of observations that follow a Poisson distribution. In each dyad we investigate, we will compare and contrast the performances of the point and interval estimators produced by the integrated likelihood (or more commonly, our approximations to the integrated likelihood) for the dyad's parameter of interest to the analogous estimators produced by the profile likelihood function.^[The profile likelihood has a conceptual simplicity and an ease of computation that makes it a convenient choice for an estimation benchmark.]

\section{Weighted Sums of Poisson Rates}

In this section we consider the task of estimating the weighted sum of a group of independent and distinct Poisson rates. The models we will consider are (i) a basic model in which we estimate the Poisson rate in each group using only the group's observed count and exposure time, (ii) a fixed effects regression model in which we assume there is an underlying covariate structure influencing the observed counts in each group, and (iii) a hierarchical mixed effects regression model in which we assume some or all of the intercepts and slopes from the previous model are random effects instead of fixed. We will also take into account the robustness of the pseudolikelihood-derived estimators to model misspecification when evaluating their performance.

\subsection{Basic Group Rates}

Consider a group of $G$ independent Poisson processes having distinct rates per unit time of $\theta_1, ..., \theta_G$. Let $Y_g$ denote the running tally of observations that we record from the $g$-th process during an interval of time $t_g$, so that $$Y_g \sim \text{Poisson}(t_g\theta_g).$$ Suppose we are interested in estimating a weighted sum of these Poisson rates, i.e. our parameter of interest is $$\psi = \sum_{g=1}^G \alpha_g \theta_g,$$ where $\alpha_1, ..., \alpha_G$ are known positive weights. Letting $\alpha$ and $\theta$ denote the column vectors of their constituent elements, we can also write this using the more compact notation $$\psi = \alpha^\top \theta.$$ 

Let $Y = (Y_1, ..., Y_G)^\top$ represent the vector of recorded counts from each group. The log-likelihood for $\theta$  is $$\ell(\theta; Y) = \sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big),$$ where we have discarded any additive terms not depending on $\theta$. The unconstrained MLE for $\theta$ is $\hat{\theta} = \Big(\frac{Y_1}{t_1}, ..., \frac{Y_G}{t_G}\Big)^{\top}$, and the corresponding unconstrained MLE for $\psi$ is $\hat{\psi} = \alpha^\top \hat{\theta}$.

Because $\psi$ is a function of the full model parameter, there is no explicit nuisance parameter over which can integrate to obtain an integrated likelihood for it. Instead, we must use the ZSE parameterization method for an implicit nuisance parameter introduced in @severini2018 to approximate the integral. See also chapters 4 and 5 for a more general discussion of the ZSE parameter and the role it plays in constructing integrated likelihood functions with desirable properties.

Recall that elements of the subspace $\Omega_{\hat{\psi}} = \{\omega \in \Theta: \alpha^\top \omega = \hat{\psi}\}$ play the role of the ZSE parameter in models for which the nuisance parameter is implicit. Let $Q(\theta; \omega)$ represent the expected value of $\ell(\theta; Y)$ under the probability distribution indexed by an element $\omega \in \Omega_{\hat{\psi}}$. That is,
$$
\begin{aligned}
Q(\theta; \omega) &:= \text{E}_{\omega}[\ell(\theta; Y)] \\
                  &= \text{E}_{\omega}\Bigg[\sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big)\Bigg] \\
                  &= \sum_{g=1}^G \big(\text{E}_{\omega_g}[Y_g] \log \theta_g - t_g \theta_g\big) \\
                  &= \sum_{g=1}^G \big(t_g\omega_g \log \theta_g - t_g \theta_g\big).
\end{aligned}
$$

For a given value of $\psi$, the ZSE-constrained optimizer is 
$$
\tilde{\theta}(\psi; \omega) := \arg \max_{\substack{\theta}} Q(\theta; \omega) \> \text{ s.t. } \> \alpha^{\top} \theta = \psi.
$$
Evaluating $L(\theta; Y) := \exp(\ell(\theta; Y))$ at $\theta = \tilde{\theta}(\psi; \omega)$ gives us a mapping from $\psi$ to its associated likelihood value under the distribution indexed by $\omega$. If we integrate this mapping over $\Omega_{\hat{\psi}}$ with respect to some weight function $\pi(\omega)$ that doesn't depend on $\psi$, we can obtain the integrated likelihood function for $\psi$ based on the ZSE parameterization, i.e.

$$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L(\tilde{\theta}(\psi; \omega)) \pi(\omega) d\omega.$$

Unsurprisingly, no closed-form solution exists for this integral. Instead, we can use Monte Carlo integration to approximate it with the following estimator:

$$\hat{\bar{L}}(\psi) = \frac{1}{R} \sum_{j=1}^R L(\tilde{\theta}(\omega^{(j)}; \psi); Y),$$
where the choice of $\pi(\omega)$ is now understood to be defined implicitly as the density function admitted by the probability measure $d\Pi(\omega)$ that governs how each $\omega^{(j)}$ is drawn from $\Omega_{\hat{\psi}}$. As long as the selection does not depend on the value of $\psi$, we are essentially free to choose any (non-degenerate) probability distribution we wish, and the form of $\pi(\omega)$ will be adjusted accordingly.

Hence, under a simple MC approximation scheme, the functions $L(\tilde{\theta}(\omega^{(j)}; \psi); Y)$ for $j = 1, ..., R$ are individual "branches" of the integrated likelihood that we average to obtain an estimate of its true underlying value at a particular value of $\psi$. Let $$b_{\omega}(\psi) := \log L(\tilde{\theta}(\psi; \omega^{(j)}); Y)$$ denote the branch curve generated by the value $\omega \in \Omega_{\hat{\psi}}$, and let its mode be given by $$\psi^*_{\omega} := \arg \max_{\substack{\psi}} b_{\omega}(\psi).$$ 

For an arbitrary dyad, two distinct values $\omega^{(1)}, \omega^{(2)} \in \Omega_{\hat{\psi}}$ will in general produce branch curves with modes $\psi^*_{\omega^{(1)}}$ and $\psi^*_{\omega^{(2)}}$, respectively, that are also distinct. In other words, there is no reason to expect that branch curves corresponding to different values drawn from $\Omega_{\hat{\psi}}$ will be maximized at the same location. However, if our dyad is such that the model belongs to the exponential family and our parameter of interest $\psi$ is a linear function of the full model parameter, then this is no longer case. Indeed, it can be shown that under these two conditions, the branch curves of the integrated likelihood function for $\psi$ will all have the same mode regardless of the values of $\omega$ that generated them, and furthermore that mode will simply be the unconstrained MLE for $\psi$, given by $\hat{\psi} = \alpha^{\top} \hat{\theta}$ (see appendix:A for a proof). Thus, our MC approximation to the integrated likelihood for $\psi$, being nothing more a simple average of these branches at each value of $\psi$, must also be maximized at $\hat{\psi}$.

This suggests that the maximum likelihood estimate for $\psi$ based on the simple MC approximation to the ZSE-parameterized integrated likelihood under the dyad criteria will be the same as the one produced by the profile likelihood. Furthermore, we can use a quadratic expansion of an arbitrary branch curve to show that the branch's curvature around its peak at $\hat{\psi}$ will be equal to that of the profile likelihood up to a second-order approximation (again, see appendix:A for a proof). It follows that the behavior of our MC approximation to the integrated likelihood for $\psi$, itself being nothing more than a simple average of these branches, will be virtually indistinguishable from that of the profile likelihood near $\hat{\psi}$ since each branch contributes a quadratic with the same peak and very similar curvature. Thus it is clear that for any dyad satisfying these criteria, it does not matter whether one uses the integrated or the profile likelihood to conduct inference regarding its parameter of interest - the conclusions will be identical. However, the profile likelihood is in general both conceptually and computationally easier to implement, making it the better choice in such cases.

\subsection{Fixed Effects Regression}

\subsection{Mixed Effects Regression}

\subsubsection{Fixed Intercepts and Random Slopes}

\subsubsection{Random Intercepts and Fixed Slopes}

\subsubsection{Random Intercepts and Random Slopes}

\section{Overdispersion}

\section{Zero-Inflation}