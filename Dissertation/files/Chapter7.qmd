\chapter{Poisson Dyads}

This chapter aims to examine the utility of the integrated likelihood function as a tool for estimating parameters of interest in dyads with models of observations that follow a Poisson distribution. In each dyad we investigate, we will compare and contrast the performances of the point and interval estimators produced by the integrated likelihood (or more commonly, our approximations to the integrated likelihood) for the dyad's parameter of interest to the analogous estimators produced by the profile likelihood function.^[The profile likelihood has a conceptual simplicity and an ease of computation that makes it a convenient choice for an estimation benchmark.]

\section{Weighted Sums of Poisson Rates}

To motivate our discussion, let us imagine a hypothetical scenario to which we will return at various points throughout this section for context. Suppose there has been an outbreak of disease among a group of hospitals, and we have been tasked with assessing its severity. A basic question to ask is, "What has been the overall rate of infection across all affected hospitals since the start of the outbreak?" A seemingly straightforward answer is the simple ratio of total infections to total patient-days reported across all the hospitals since the start of the outbreak.^[A patient-day is a single day spent by a patient in a hospital.] We must tread with some caution here, however. The number of infections is clearly random, but what about the patient-days? The properties of our estimator hinge upon the answer so it would be prudent to give it some consideration.

To interpret the number of patient-days as a random variable is to declare that what really matters is not the rate of infection among hospitals in this outbreak in particular but rather the more abstract notion of the disease's "true" rate of infection in general. This would mean that both the numerator and denominator in our ratio are random variables, transforming it from an "estimator which happens to be a ratio" into an actual "ratio estimator". Conversely, to treat them as fixed parameters is to condition on their exact values reported by each hospital in the group, thereby restricting our attention to the rate of infection among these hospitals only. Neither choice is inherently right or wrong - it is up to us to decide where our interest lies - but once we have decided, we must honor the consequences of that choice in our analysis. Failure to do so would lead to bias in our estimates. We will begin with the assumption that the point of our analysis is to gauge the risk involved with this outbreak specifically, so until such time as it is explicitly noted otherwise, it is safe to regard the patient-days that each hospital reports as being fixed parameters.

The overall rate of infection among only the hospitals in our group is therefore what constitutes our parameter of interest. As usual we will call this value $\psi$ and its estimator $\hat{\psi}$. In practice, especially for an example such as this, the line between $\psi$ and $\hat{\psi}$ often gets blurred, with many treating the observed rate as though it is the same as the true underlying rate. From the perspective of assessing impact, this is completely reasonable as the observed rate is by definition what the hospitals actually experience. However, for the sake of avoiding conceptual ambiguity in our language and intent, we will continue to treat them as two distinct entities. If nothing else, it serves as a friendly reminder of the importance of maintaining proper separation between estimand and estimator.

Assume there are $J$ hospitals in the group, and let $Y_j$ and $t_j$ represent the number of infections and patient-days, respectively, reported by hospital $j$ since the start of the outbreak. Then our estimator may be written as 
$$
\begin{aligned}
\hat{\psi} &= \frac{\sum_{j=1}^J Y_j}{\sum_{j=1}^J t_j} \\
           &= \frac{\sum_{j=1}^J t_j (Y_j / t_j)}{\sum_{j=1}^J t_j} \\
           &= \frac{\sum_{j=1}^J t_j \hat{\theta}_j}{\sum_{j=1}^J t_j} \\
           &= \sum_{j=1}^J \alpha_j \hat{\theta}_j,
\end{aligned}
$$
where $\alpha_j \equiv \frac{t_j}{\sum_{j=1}^J t_j}$ and $\hat{\theta}_j \equiv Y_j / t_j$ is the observed rate of infection at hospital $j$. If $\theta_j$ represents its associated estimand, we can similarly rewrite our expression for $\psi$ in a similar form as $$\psi = \sum_{j=1}^J \alpha_j \theta_j.$$

Thus, the overall rate of infection can be decomposed as the weighted average of the individual hospitals' rates of infection, where the weights for each hospital are proportional to the amount each contributed to the total number of patient-days across all the hospitals since the outbreak began. This is noteworthy because often we have more data regarding factors that influence the infection totals at the hospital level than we do at the group level. These factors could act as covariates in a regression model that would potentially enable more precise estimates of the individual rates, which in turn would lead to an estimator of $\theta$ with lower variance than one that attempts to estimate it through the aggregated totals alone.

Estimating the individual hospital rates first also means that our model parameter is now the $J$-dimensional vector $\theta = (\theta_1, ..., \theta_J)^\top$ while our parameter of interest $\psi$ remains a scalar. Hence, regardless of the distribution that we use to model the infection counts, the resulting likelihood function will contain an implicit $(J-1)$-dimensional nuisance parameter for which the techniques discussed in Chapter 3 now apply.

We can further abstract our conception of the parameter of interest as being a weighted average of individual rates by relaxing the restriction that the weights $\alpha_j$ must depend on the patient-days at all. In its full generality, we can now conceive of our parameter of interest $\psi$ as being a weighted *sum* of individual-level rates, where the weights are only required to be positive constants (not necessarily summing to 1) and their precise meaning can depend on the question being asked.

Consider a group of $J$ independent Poisson processes with distinct rates per unit time of $\theta_1, ..., \theta_J$. Suppose we are interested in estimating a weighted sum of these rates, i.e. $$\psi \equiv \sum_{j=1}^J \alpha_j \theta_j = \alpha^\top \theta,$$ where $\alpha_1, ..., \alpha_J$ are known positive weights. We will assess the ability of the integrated likelihood to produce accurate estimates for $\psi$ under three separate model frameworks. The models we will consider are (i) a basic one in which we estimate the Poisson rate in each group using only the group's observed count and exposure time, (ii) a fixed effects regression model in which we assume there is an underlying covariate structure influencing the observed counts in each group, and (iii) a hierarchical mixed effects regression model in which we assume some or all of the intercepts and slopes from the previous model are random effects instead of fixed. We will also take into account the robustness of the pseudolikelihood-derived estimators to model misspecification when evaluating their performance.

\subsection{Basic Group Rates}

Let $Y_j$ denote the count of observations that we record from the $j$-th process during an interval of time $t_j$ (the exposure), so that $$Y_j \sim \text{Poisson}(t_j\theta_j).$$ The log-likelihood for $\theta$ is $$\ell(\theta; Y) = \sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big),$$ where we have discarded any additive terms not depending on $\theta$. The unconstrained MLE for $\theta$ is $\hat{\theta} = (Y_1 / t_1, ..., Y_G / t_G)^{\top}$, and the corresponding unconstrained MLE for $\psi$ is $\hat{\psi} = \alpha^\top \hat{\theta}$.

Since $\psi$ is a function of the full model parameter, we can employ the ZSE parameterization method for an implicit nuisance parameter introduced in @severini2018 to approximate the integrated likelihood. Recall that elements of the subspace $\Omega_{\hat{\psi}} = \{\omega \in \Theta: \alpha^\top \omega = \hat{\psi}\}$ play the role of the ZSE parameter in models for which the nuisance parameter is implicit. Let $Q(\theta; \omega)$ represent the expected value of $\ell(\theta; Y)$ under the probability distribution indexed by an element $\omega \in \Omega_{\hat{\psi}}$. That is,
$$
\begin{aligned}
Q(\theta; \omega) &:= \text{E}_{\omega}[\ell(\theta; Y)] \\
                  &= \text{E}_{\omega}\Bigg[\sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big)\Bigg] \\
                  &= \sum_{g=1}^G \big(\text{E}_{\omega_g}[Y_g] \log \theta_g - t_g \theta_g\big) \\
                  &= \sum_{g=1}^G \big(t_g\omega_g \log \theta_g - t_g \theta_g\big).
\end{aligned}
$$

For a given value of $\psi$, the ZSE-constrained optimizer is 
$$
\tilde{\theta}(\psi; \omega) := \arg \max_{\substack{\theta}} Q(\theta; \omega) \> \text{ s.t. } \> \alpha^{\top} \theta = \psi.
$$
Evaluating $L(\theta; Y) := \exp(\ell(\theta; Y))$ at $\theta = \tilde{\theta}(\psi; \omega)$ gives us a mapping from $\psi$ to its associated likelihood value under the distribution indexed by $\omega$. If we integrate this mapping over $\Omega_{\hat{\psi}}$ with respect to some weight function $\pi(\omega)$ that doesn't depend on $\psi$, we can obtain the integrated likelihood function for $\psi$ based on the ZSE parameterization, i.e.

$$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L(\tilde{\theta}(\psi; \omega)) \pi(\omega) d\omega.$$

Unsurprisingly, no closed-form solution exists for this integral. Instead, we can use Monte Carlo integration to approximate it with the following estimator:

$$\hat{\bar{L}}(\psi) = \frac{1}{R} \sum_{j=1}^R L(\tilde{\theta}(\omega^{(j)}; \psi); Y),$$
where the choice of $\pi(\omega)$ is now understood to be defined implicitly as the density function admitted by the probability measure $d\Pi(\omega)$ that governs how each $\omega^{(j)}$ is drawn from $\Omega_{\hat{\psi}}$. As long as the selection does not depend on the value of $\psi$, we are essentially free to choose any (non-degenerate) probability distribution we wish, and the form of $\pi(\omega)$ will be adjusted accordingly.

Hence, under a simple MC approximation scheme, the functions $L(\tilde{\theta}(\omega^{(j)}; \psi); Y)$ for $j = 1, ..., R$ are individual "branches" of the integrated likelihood that we average to obtain an estimate of its true underlying value at a particular value of $\psi$. Let $$b_{\omega}(\psi) := \log L(\tilde{\theta}(\psi; \omega^{(j)}); Y)$$ denote the branch curve generated by the value $\omega \in \Omega_{\hat{\psi}}$, and let its mode be given by $$\psi^*_{\omega} := \arg \max_{\substack{\psi}} b_{\omega}(\psi).$$ 

For an arbitrary dyad, two distinct values $\omega^{(1)}, \omega^{(2)} \in \Omega_{\hat{\psi}}$ will in general produce branch curves with modes $\psi^*_{\omega^{(1)}}$ and $\psi^*_{\omega^{(2)}}$, respectively, that are also distinct. In other words, there is no reason to expect that branch curves corresponding to different values drawn from $\Omega_{\hat{\psi}}$ will be maximized at the same location. However, if our dyad is such that the model belongs to the exponential family and our parameter of interest $\psi$ is a linear function of the full model parameter, then this is no longer case. Indeed, it can be shown that under these two conditions, the branch curves of the integrated likelihood function for $\psi$ will all have the same mode regardless of the values of $\omega$ that generated them, and furthermore that mode will simply be the unconstrained MLE for $\psi$, given by $\hat{\psi} = \alpha^{\top} \hat{\theta}$ (see appendix:A for a proof). Therefore our MC approximation to the integrated likelihood for $\psi$, being nothing more a simple average of these branches at each value of $\psi$, must also be maximized at $\hat{\psi}$.

Of course, by definition $\hat{\psi}$ is also the maximizer of the profile likelihood, meaning that the integrated likelihood and profile likelihood will produce identical maximum likelihood estimates for $\psi$ in any dyad satisfying these criteria. We can further use a quadratic expansion of an arbitrary branch curve to show that the branch's curvature around its peak at $\hat{\psi}$ will be equal to that of the profile likelihood up to a second-order approximation (again, see appendix:A for a proof). It follows that the behavior of our MC approximation to the integrated likelihood for $\psi$, itself being nothing more than a simple average of these branches, will be virtually indistinguishable from that of the profile likelihood near $\hat{\psi}$ since each branch contributes a quadratic with the same peak and very similar curvature. Thus it is clear that for any dyad satisfying these criteria, it does not matter whether we use the integrated or the profile likelihood to conduct inference regarding its parameter of interest - our conclusions will be identical. However, the profile likelihood is in general both conceptually and computationally easier to implement, making it the better choice in such cases.

\subsection{Fixed Effects Regression}

Now suppose there is a set of covariates that we have reason to believe is influencing the counts we record from the Poisson process in each group. For example, 




that at the same time we record Poisson counts from each group, we also make observations from a set of covariates we have reason to believe are influencing the 


regress the Poisson counts we observe from each group against a corresponding set of covariates that we observe as well. The Poisson distribution's membership in the exponential family of distributions makes this an example of a generalized linear model (GLM). As the structure of any regression model depends upon the manner in which its data was collected, it would be wise to begin by establishing the sampling procedure germane to this GLM. As before, assume there are $J$ groups  

For each of the $J$ groups, we assume that there is a distinct Poisson process with underlying 



We will assume that every count we record from group $j$ is generated by a Poisson process with an underlying rate per unit exposure of $\theta_j$ that is modulated by the presence of covariates

Let $Y_{ij}$ represent the $i$-th observation made from the process in the $j$-th group during an exposure time $t_{ij}$, so that $$Y_{ij} \sim \text{Poisson}(\mu_{ij}),$$ where $\mu_{ij} = t_{ij} \theta_j$ and $i \in \{1, ..., n_j\}$. If we were to disregard the covariates at this point and sum all quantities over the index $i$, we would arrive at the setup of the previous section. Instead, we impose a regression structure on the data by assuming that the mean of $Y_{ij}$ has a conditional dependence on its associated vector of covariates $X_{ij}$ through a linear combination (or some function thereof) of their observed values with a vector of unknown parameters $\beta$. We will assume in this section that all of the coefficients in $\beta$ are constant parameters with a fixed effect on the Poisson counts we observe, as opposed to themselves being realizations of some group of latent random variables. These coefficients are the parameters that interact most directly with the data we observe, so it is sensible and justified to take $\beta$ for our full model parameter.

 Every GLM needs a link function relating the conditional mean of its response variable given its set of predictors to the linear predictor itself; we will choose for ours the canonical link for a Poisson GLM, the natural logarithm. If we let $\text{E}(Y_{ij} | X_{ij}) = \mu_{ij}$, then we have $$\log(\mu_{ij}) = X_{ij}^\top\beta.$$ Substituting in our earlier expression for $\mu_{ij}$ in terms of $t_{ij}$ and $\theta_j$ and doing some rearranging, we arrive at $$\log(\theta_j) = X_{ij}^\top \beta - \log(t_{ij})$$ for $i = 1, ..., n_j$ and $j = 1, ..., J$.

For the $i$-th count from the $g$-th process, we observe values from $p + q$ independent covariates, where $p$ and $q$ are known non-negative integers such that $p + q \geq 1$. $p$ represents the number of covariates that have a process-dependent effect on their associated response $Y_{ig}$ (i.e. their slope coefficient depends on the value of $g$). 

Let us further assume that each observed $Y_{ig}$ has associated with it two vectors of observed covariates $X_{ig} \in \mathbb{R}^{p\times1}$ and $Z_{ig} \in \mathbb{R}^{q\times1}$,  That is,  A covariate is assigned to a component of the vector $X_{ig}$ if its effect on the response $Y_{ig}$ is assumed to vary across processes  or to a component of the vector $Z_{ig}$ if its effect on $Y_{ig}$ is assumed to be constant regardless of the process with which it is associated (i.e. it has a single slope coefficient for all values of $g$). ^[We still assume the effects associated with the covariates belonging to $X_{ig}$ are *fixed* and not *random*, in the sense that we do not consider the effects themselves (the corresponding slope coefficients) to be realizations of an underlying random variable.] 



Let $\alpha_g$ represent the intercept term in group $g$, 

and $Z_{ig}$ on the basis of whether the covariate

\subsection{Mixed Effects Regression}

\subsubsection{Fixed Intercepts and Random Slopes}

\subsubsection{Random Intercepts and Fixed Slopes}

\subsubsection{Random Intercepts and Random Slopes}

\section{Overdispersion}

\section{Zero-Inflation}