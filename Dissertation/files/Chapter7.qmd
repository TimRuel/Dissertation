\chapter{Count Dyads}

This chapter aims to examine the utility of the integrated likelihood function as a tool for estimating parameters of interest in dyads with models of data generated from count distributions. In each dyad we investigate, we will compare and contrast the performances of the point and interval estimators produced by the integrated likelihood (or more commonly, our approximations to the integrated likelihood) for the dyad's parameter of interest to the analogous estimators produced by the profile likelihood function. Our use of the profile likelihood stems from its conceptual simplicity and ease of computation, making it a convenient choice for an estimation benchmark.

\section{Weighted Sums of Rate Parameters}

To motivate our discussion, let us imagine a hypothetical scenario to which we will return at various points throughout this section for context. Suppose there has been an outbreak of disease among a group of hospitals, and we have been tasked with assessing its severity. A basic question to ask is, "What has been the overall rate of infection across all affected hospitals since the start of the outbreak?" A seemingly straightforward answer is the simple ratio of total infections to total patient-days reported across all the hospitals since the start of the outbreak.^[A patient-day is a single day spent by a patient in a hospital.] We must tread with some caution here, however. The number of infections is clearly random, but what about the patient-days? The properties of our estimator hinge upon the answer so it would be prudent to give it some consideration.

To interpret the number of patient-days as a random variable is to declare that what really matters is not the rate of infection among hospitals in this outbreak in particular but rather the more abstract notion of the disease's "true" rate of infection in general. This would mean that both the numerator and denominator in our ratio are random variables, transforming it from an "estimator which happens to be a ratio" into an actual "ratio estimator". Conversely, to treat them as fixed parameters is to condition on their exact values reported by each hospital in the group, thereby restricting our attention to the rate of infection among these hospitals only. Neither choice is inherently right or wrong - it is up to us to decide where our interest lies - but once we have decided, we must honor the consequences of that choice in our analysis. Failure to do so would lead to bias in our estimates. We will begin with the assumption that the point of our analysis is to gauge the risk involved with this outbreak specifically, so until such time as it is explicitly noted otherwise, it is safe to regard the patient-days that each hospital reports as being fixed parameters.

The overall rate of infection among only the hospitals in our group is therefore what constitutes our parameter of interest. As usual we will call this value $\psi$ and its estimator $\hat{\psi}$. In practice, especially for an example such as this, the line between $\psi$ and $\hat{\psi}$ often gets blurred, with many treating the observed rate as though it is the same as the true underlying rate. From the perspective of assessing impact, this is understandable as the observed rate is by definition what the hospitals actually experience. However, for the sake of avoiding conceptual ambiguity in our language and intent, we will continue to treat them as two distinct entities. If nothing else, it serves as a friendly reminder of the importance of maintaining proper separation between estimand and estimator.

Assume there are $J$ hospitals in the group, and let $Y_j$ and $t_j$ represent the number of infections and patient-days, respectively, reported by hospital $j$ since the start of the outbreak. Then our estimator may be written as 
$$
\begin{aligned}
\hat{\psi} &= \frac{\sum_{j=1}^J Y_j}{\sum_{j=1}^J t_j} \\
           &= \frac{\sum_{j=1}^J t_j (Y_j / t_j)}{\sum_{j=1}^J t_j} \\
           &= \frac{\sum_{j=1}^J t_j \hat{\theta}_j}{\sum_{j=1}^J t_j} \\
           &= \sum_{j=1}^J t_j' \hat{\theta}_j,
\end{aligned}
$${#eq-1}
where $t_j' \equiv \frac{t_j}{\sum_{j=1}^J t_j}$ and $\hat{\theta}_j \equiv Y_j / t_j$ is the observed rate of infection at hospital $j$. If $\theta_j$ represents its associated estimand, we can similarly rewrite our expression for $\psi$ in a similar form as $$\psi = \sum_{j=1}^J t_j' \theta_j.$${#eq-2}

Thus, the overall rate of infection can be decomposed as the weighted *average* of the individual hospitals' rates of infection. Note that the term "average" is proper here since $\sum_{j=1}^J t_j' = 1$. This follows directly from our requirement that each weight $t_j'$ be proportional to the number of patient-days contributed by hospital $j$ to the total number of patient-days across all the hospitals since the outbreak began. We made this restriction as a consequence of the question that motivated our analysis, but it was entirely arbitrary. There is nothing actually requiring the weights to sum to one, or indeed that they depend on the patient-days at all. We could just as easily have asked about an aggregated infection rate relative to some other hospital-specific metric (e.g. number of available beds, population of surrounding region, etc.) that would in turn inform the precise values of the weights we choose for our parameter of interest. 

And of course the utility of a weighted sum as a parameter of interest is not unique to our example of evaluating the severity of a disease outbreak in a group of hospitals either. In general, we may consider any set of $J$ independent count-generating processes with distinct rates (countable events per unit time) of $\theta_1, ..., \theta_J$ and corresponding weights $\alpha_1, ..., \alpha_J$ as inducing a parameter of interest of the form $$\psi = \sum_{j=1}^J \alpha_j \theta_j.$${#eq-3} It is not necessary to assume that the weights are normalized, though we will still require that $\alpha_j > 0$ for all $j$ in order to retain the interpretation of $\psi$ as being a weighted *sum* of individual rates. 

In the following sections, we will assess the ability of the integrated likelihood to produce accurate estimates for $\psi$ under three separate model frameworks. The models we will consider are (i) a naive one in which we estimate the rate in each group using only the group's observed count and exposure time, (ii) a fixed effects regression model in which we assume there is an underlying covariate structure influencing the observed counts in each group, and (iii) a hierarchical mixed effects regression model in which we assume some or all of the intercepts and slopes from the previous model are random effects instead of fixed. We will also take into account the robustness of the pseudolikelihood-derived estimators to model misspecification when evaluating their performance.

\subsection{Naive Rates}

We begin with a model characterized by its sole use of a process's observed count of events over some recorded interval of time to estimate its associated rate. This is perhaps the simplest nontrivial model one can employ for our parameter of interest, so we will refer to it as the *naive rates model*. 

Let $Y_j$ denote the count of events that we record from the $j$-th process during an exposure time $t_j$ (the patient-days in our outbreak example). We will proceed for the moment under the assumption that the mean of the random variable underlying each count-generating process is roughly equal to its variance, meaning there is no risk of overdispersion. It is therefore safe to assume that the counts follow a Poisson distribution, i.e. $$Y_j \sim \text{Poisson}(t_j\theta_j).$$ The log-likelihood for $\theta$ then becomes $$\ell(\theta; Y) = \sum_{j=1}^J \big(Y_j \log \theta_j - t_j \theta_j\big),$$ where we have discarded any additive terms not depending on $\theta$. The unconstrained MLE for $\theta_j$ is given by $\hat{\theta}_j = Y_j / t_j$, with the corresponding MLE for $\psi$ being obtained by plugging these estimates into @eq-3, yielding $$\hat{\psi} = \sum_{j=1}^J \alpha_j \hat{\theta}_j.$${#eq-4}

Since $\psi$ is a function of the full model parameter, we can employ the ZSE parameterization method for an implicit nuisance parameter introduced in @severini2018 to approximate the integrated likelihood. Recall that elements of the subspace $\Omega_{\hat{\psi}} = \{\omega \in \Theta: \alpha^\top \omega = \hat{\psi}\}$ play the role of the ZSE parameter in models for which the nuisance parameter is implicit. Let $Q(\theta; \omega)$ represent the expected value of $\ell(\theta; Y)$ under the probability distribution indexed by an element $\omega \in \Omega_{\hat{\psi}}$. That is,
$$
\begin{aligned}
Q(\theta; \omega) &:= \text{E}_{\omega}[\ell(\theta; Y)] \\
                  &= \text{E}_{\omega}\Bigg[\sum_{g=1}^G \big(Y_g \log \theta_g - t_g \theta_g\big)\Bigg] \\
                  &= \sum_{g=1}^G \big(\text{E}_{\omega_g}[Y_g] \log \theta_g - t_g \theta_g\big) \\
                  &= \sum_{g=1}^G \big(t_g\omega_g \log \theta_g - t_g \theta_g\big).
\end{aligned}
$$

For a given value of $\psi$, the ZSE-constrained optimizer is 
$$
\tilde{\theta}(\psi; \omega) := \arg \max_{\substack{\theta}} Q(\theta; \omega) \> \text{ s.t. } \> \alpha^{\top} \theta = \psi.
$$
Evaluating $L(\theta; Y) := \exp(\ell(\theta; Y))$ at $\theta = \tilde{\theta}(\psi; \omega)$ gives us a mapping from $\psi$ to its associated likelihood value under the distribution indexed by $\omega$. If we integrate this mapping over $\Omega_{\hat{\psi}}$ with respect to some weight function $\pi(\omega)$ that doesn't depend on $\psi$, we can obtain the integrated likelihood function for $\psi$ based on the ZSE parameterization, i.e.

$$\bar{L}(\psi) = \int_{\Omega_{\hat{\psi}}} L(\tilde{\theta}(\psi; \omega)) \pi(\omega) d\omega.$$

Unsurprisingly, no closed-form solution exists for this integral. Instead, we can use Monte Carlo integration to approximate it with the following estimator:

$$\hat{\bar{L}}(\psi) = \frac{1}{R} \sum_{j=1}^R L(\tilde{\theta}(\omega^{(j)}; \psi); Y),$$
where the choice of $\pi(\omega)$ is now understood to be defined implicitly as the density function admitted by the probability measure $d\Pi(\omega)$ that governs how each $\omega^{(j)}$ is drawn from $\Omega_{\hat{\psi}}$. As long as the selection does not depend on the value of $\psi$, we are essentially free to choose any (non-degenerate) probability distribution we wish, and the form of $\pi(\omega)$ will be adjusted accordingly.

Hence, under a simple MC approximation scheme, the functions $L(\tilde{\theta}(\omega^{(j)}; \psi); Y)$ for $j = 1, ..., R$ are individual "branches" of the integrated likelihood that we average to obtain an estimate of its true underlying value at a particular value of $\psi$. Let $$b_{\omega}(\psi) := \log L(\tilde{\theta}(\psi; \omega^{(j)}); Y)$$ denote the branch curve generated by the value $\omega \in \Omega_{\hat{\psi}}$, and let its mode be given by $$\psi^*_{\omega} := \arg \max_{\substack{\psi}} b_{\omega}(\psi).$$ 

For an arbitrary dyad, two distinct values $\omega^{(1)}, \omega^{(2)} \in \Omega_{\hat{\psi}}$ will in general produce branch curves with modes $\psi^*_{\omega^{(1)}}$ and $\psi^*_{\omega^{(2)}}$, respectively, that are also distinct. In other words, there is no reason to expect that branch curves corresponding to different values drawn from $\Omega_{\hat{\psi}}$ will be maximized at the same location. However, if our dyad is such that the model belongs to the exponential family and our parameter of interest $\psi$ is a linear function of the full model parameter, then this is no longer case. Indeed, it can be shown that under these two conditions, the branch curves of the integrated likelihood function for $\psi$ will all have the same mode regardless of the values of $\omega$ that generated them, and furthermore that mode will simply be the unconstrained MLE for $\psi$, given by $\hat{\psi} = \alpha^{\top} \hat{\theta}$ (see appendix:A for a proof). Therefore our MC approximation to the integrated likelihood for $\psi$, being nothing more a simple average of these branches at each value of $\psi$, must also be maximized at $\hat{\psi}$.

Of course, by definition $\hat{\psi}$ is also the maximizer of the profile likelihood, meaning that the integrated likelihood and profile likelihood will produce identical maximum likelihood estimates for $\psi$ in any dyad satisfying these criteria. We can further use a quadratic expansion of an arbitrary branch curve to show that the branch's curvature around its peak at $\hat{\psi}$ will be equal to that of the profile likelihood up to a second-order approximation (again, see appendix:A for a proof). It follows that the behavior of our MC approximation to the integrated likelihood for $\psi$, itself being nothing more than a simple average of these branches, will be virtually indistinguishable from that of the profile likelihood near $\hat{\psi}$ since each branch contributes a quadratic with the same peak and very similar curvature. Thus it is clear that for any dyad satisfying these criteria, it does not matter whether we use the integrated or the profile likelihood to conduct inference regarding its parameter of interest - our conclusions will be identical. However, the profile likelihood is in general both conceptually and computationally easier to implement, making it the better choice in such cases.

\subsection{Fixed Effects Regression}

In the previous section, we relied solely on the observed counts and exposure times to estimate $\psi$. In practice, however, additional external factors often influence the counts we observe from each process. For instance, in the disease-outbreak example, variations in patient age distributions or other comorbidities across hospitals could affect the number of reported infections. To account for such influences, we turn to the standard framework of linear regression, which formally relates covariates to systematic variation in the observed counts.

Let $Y_{ij}$ denote the $i$-th count from process $j$ observed over an exposure time $t_{ij}$. Unlike the naive rates model in which only the total count per process was relevant, the presence of covariates that vary by observation requires us to retain each individual $Y_{ij}$ and $t_{ij}$. If we were to ignore the covariates and sum over the index $i$, we would recover the structure of the naive model. Suppose that for each count we also observe a vector of covariates $X_{ij}$ that may affect the rate at which process $j$ generates events. We model these effects using a *fixed effects regression* framework, in which the coefficients associated with each covariate are treated as constant but unknown parameters to be estimated from the data. Some covariates may have *process-specific* effects (their impact varies across $j$), while others may have *process-agnostic* effects (affecting $Y_{ij}$ uniformly across all processes $j = 1, ..., J$). We partition the covariate vector as $$X_{ij} = \begin{pmatrix} X_{ij}^{(s)} \\ X_{ij}^{(a)}\end{pmatrix}$${#eq-7.1.2.1} where $X_{ij}^{(s)} \in \mathbb{R}^{p\times1}$ and $X_{ij}^{(a)} \in \mathbb{R}^{q\times1}$ collect the process-specific and process-agnostic covariates, respectively. 

Because covariates can influence the number of events recorded in each observation, the underlying rate $$\theta_{ij} \equiv \theta_j(X_{ij})$${#eq-7.1.2.2} may vary with $i$. Each observed count $Y_{ij}$ then has a conditional mean $$\mu_{ij} \equiv \text{E}(Y_{ij} | X_{ij}) = t_{ij} \theta_{ij}.$${#eq-7.1.2.3} We define the *marginal rate* of process $j$ as the expected rate averaged over the distribution of covariates, i.e., $$\bar{\theta}_{j} \equiv \text{E}_{X_j}[\theta_j(X_{j})] = \int\theta_j(X_{j})dF_{X_j}(X_{j}),$${#eq-7.1.2.4} where $X_j$ represents a generic covariate vector associated with process $j$, drawn according to the joint distribution $F_{X_{j}}$. These marginal rates generalize the naive rates from the previous model by averaging over covariates rather than relying on raw totals. Our parameter of interest for this model may then be written as 
$$
\psi = \sum_{j=1}^J \alpha_j \bar{\theta}_j,
$${#eq-7.1.2.5}
for some positive weights $\alpha_1, ..., \alpha_J$.

Up to this point, we have refrained from placing any distributional assumption on the observed counts. In this chapter, we focus on two commonly used distributions for count data: the Poisson and the negative binomial. Since both distributions belong to the exponential family, our model fits naturally within the framework of generalized linear models (GLMs), which provide a formal connection between the covariate-dependent rates $\theta_j(X_{ij})$ and the observed counts. In the following subsections, we examine each distribution in turn and assess how well an integrated likelihood function can produce estimators of weighted sums of the marginal rates $\bar{\theta}_j$ under these standard count models.

\subsubsection{The Poisson GLM}

Suppose the conditional distribution of $Y_{ij}$ given $X_{ij}$ is $Y_{ij} | X_{ij} \sim \text{Poisson}(\mu_{ij})$. Then the conditional probability mass function of $Y_{ij}$ can be expressed in canonical exponential-family form as

$$
\begin{aligned}
\text{P}(Y_{ij} = y | X_{ij}) &= \frac{e^{-\mu_{ij}}\mu_{ij}^y}{y!} \\
              &= \exp\big[y \log(\mu_{ij}) - \mu_{ij} - \log(y!)\big] \\
              &= \exp\big[y \log(t_{ij} \theta_{ij}) - t_{ij} \theta_{ij} - \log(y!)\big] \\
              &= \exp\big[y \log(\theta_{ij}) - t_{ij} \theta_{ij} + y\log(t_{ij}) - \log(y!)\big] \\
              &= \exp\big[y \eta_{ij} - t_{ij} \exp(\eta_{ij}) + y\log(t_{ij}) - \log(y!)\big],
\end{aligned}
$$
where $\eta_{ij} = \log(\theta_{ij})$ is the natural parameter of the model. 

As a transformation of the rate, $\eta_{ij}$ governs the mean of the distribution and thus captures how the expected count depends on the covariates. Following from the bedrock regression principle that systematic variation in the mean response should be explained linearly in terms of the predictors, we formalize this dependence by defining the *linear predictor* for observation $i$ from process $j$ as $X_{ij}^\top \beta_j,$ a linear combination of the covariates with a vector of unknown coefficients $\beta_j$. 

In a GLM, the systematic component represented by $X_{ij}^\top \beta_j$ must be connected to its random component $\mu_{ij}$, the conditional mean of the response. This is done via a *link function* $g(\cdot)$, which specifies the relationship $$g(\mu_{ij}) =X_{ij}^\top \beta_j.$$ The *canonical link function* is the special choice of $g(\cdot)$ that sets the natural parameter equal to the linear predictor, i.e., $$\eta_{ij} = X_{ij}^\top \beta_j.$${#eq-7.1.2.6} 

For the Poisson distribution, we have already shown that the natural parameter is related to the rate through $\eta_{ij} = \log(\theta_{ij})$. Because the conditional mean of each Poisson process includes the exposure time, $\mu_{ij} = t_{ij}\theta_{ij}$, the model can be equivalently written as 
$$
\begin{aligned}
\log(\mu_{ij}) &= \log(t_{ij}\theta_{ij}) \\
               &= \log(t_{ij}) + \log(\theta_{ij}) \\
               &= \log(t_{ij}) + \eta_{ij} \\
               &= \log(t_{ij}) + X_{ij}^\top \beta_j, &&(\text{using the canonical link})
\end{aligned}
$$ {#eq-7.1.2.7}
where $\log(t_{ij})$ acts as a known offset. It follows that the canonical link for a Poisson GLM is the *log link* $g(\mu_{ij}) = \log(\mu_{ij}/t_{ij})$, under which additive effects of $X_{ij}$ correspond to multiplicative effects on $\mu_{ij}$.

To simplify notation, we include a leading 1 in the process-specific covariate vector $X_{ij}^{(s)}$ that corresponds to the intercept term for process $j$. We then partition the coefficient vector $\beta_j$ as $$\beta_j = \begin{pmatrix}\beta_j^{(s)} \\ \beta^{(a)}\end{pmatrix} \in \mathbb{R}^{(p + q) \times 1},$${#eq-7.1.2.8} where $\beta_j^{(s)} \in \mathbb{R}^{p\times1}$ contains all the coefficients specific to process $j$ (including the intercept) and $\beta^{(a)} \in \mathbb{R}^{q\times1}$ contains the shared, process-agnostic slopes. The linear predictor for observation $i$ from process $j$ can then be written as $$\eta_{ij} = \big(X_{ij}^{(s)}\big)^\top \beta_j^{(s)} + \big(X_{ij}^{(a)}\big)^\top \beta^{(a)}.$${#eq-7.1.2.9}

Stacking the observations for process $j$, we can express the model in matrix form as  
$$
\eta_j = X_j^{(s)} \beta_j^{(s)} + X_j^{(a)} \beta^{(a)},
$${#eq-7.1.2.10}
where $X_j^{(s)} \in \mathbb{R}^{n_j \times p}$ and $X_j^{(a)} \in \mathbb{R}^{n_j \times q}$ are the design matrices of process-specific and process-agnostic covariates, respectively, and $\eta_j = (\eta_{1j}, \ldots, \eta_{n_j j})^\top$ is the vector of linear predictors for process $j$. If we concatenate the design matrices so that $X_j = [X_j^{(s)} \> X_j^{(a)}] \in \mathbb{R}^{n_j \times (p + q)}$, we can further simplify @eq-7.1.2.10 as
$$
\eta_j = X_j \beta_j.
$${#eq-7.1.2.11}
Collecting all coefficients across all processes gives the global parameter vector
$$
\beta = \begin{pmatrix}\beta_1^{(s)} \\[3pt] \vdots \\[3pt] \beta_J^{(s)} \\[3pt] \beta^{(a)} \end{pmatrix}
\in\mathbb{R}^{(pJ+q) \times 1},
$$
and we view $\beta_j= (\beta_j^{(s)\top}, \beta^{(a)\top})^\top$ as the subvector of $\beta$ relevant to process $j$.

The log-likelihood for an individual observation $Y_{ij}$ follows directly from the exponential-family representation of the Poisson distribution:
$$
\ell_{ij}(\beta_j; Y_{ij}) = Y_{ij} X_{ij}^\top \beta_j - t_{ij} \exp(X_{ij}^\top \beta_j)),
$${#eq-7.1.2.12}
where we have discarded any additive terms not depending on the parameters. Summing over all observations within process $j$ gives the process-level log-likelihood,
$$
\begin{aligned}
\ell_j(\beta_j; Y_j) &= \sum_{i=1}^{n_j} \ell_{ij}(\beta_j; Y_{ij}) \\
                     &= \sum_{i=1}^{n_j} \Big[ Y_{ij} X_{ij}^\top \beta_j - t_{ij} \exp(X_{ij}^\top \beta_j)) \Big] \\
                     &= Y_j^\top X_j \beta_j - t_j^\top \exp(X_j\beta_j),
\end{aligned}
$${#eq-7.1.2.13}
where $Y_j \in \mathbb{R}^{n_j\times1}$ and $t_j\in \mathbb{R}^{n_j\times1}$ collect the observed counts and exposure times, respectively, for process $j$, and $\exp(X_j\beta_j)$ is understood to mean the exponential function applied component-wise to the vector $X_j\beta_j$.  

Finally, aggregating across all processes yields the full log-likelihood,
$$
\begin{aligned}
\ell(\beta; Y) &= \sum_{j=1}^J \ell_j(\beta_j; Y_j) \\
               &= \sum_{j=1}^J \Big[ Y_j^\top X_j \beta_j - t_j^\top \exp(X_j\beta_j) \Big] \\
               &= Y^\top X \beta - t^\top \exp(X \beta),
\end{aligned}
$${#eq-7.1.2.14}
where $n = \sum_{j=1}^J n_j$, $Y = (Y_1^\top, ..., Y_J^\top)^\top \in \mathbb{R}^{n\times1}$ is the stacked response vector, $t = (t_1^\top, ..., t_J^\top)^\top \in \mathbb{R}^{n\times1}$ is the stacked vector of exposure times, and $X = \text{blockdiag}(X_1, ..., X_j) \in \mathbb{R}^{n \times (pJ+q)}$ is the block-diagonal design matrix with process-specific blocks, augmented with shared covariates $X_j^{(a)}$ repeated across processes as necessary.

Let $\hat{\beta}_j$ denote the MLE of the coefficient vector $\beta_j$. Because the exponential term in the log-likelihood in @eq-7.1.2.14 makes it a nonlinear function of each $\beta_j$, the score equations for this model cannot be solved algebraically, and thus no closed-form expressions for $\hat{\beta}_j$ exists. Instead, $\hat{\beta}_j$ can be obtained numerically using the `glm()` function from the `stats` package in R, which implements the standard iteratively reweighted least squares (IRLS) algorithm for approximating the MLEs in the GLM framework.

From @eq-7.1.2.6, the corresponding estimated linear predictors for process $j$ are given by
$$
\hat{\eta}_{ij} = X_{ij}^\top \hat{\beta}_j.
$${#eq-7.1.2.15}
Since $\eta_{ij} = \log(\theta_{ij})$, the estimated observation-level rate is 
$$
\hat{\theta}_{ij} = \exp(X_{ij}^\top \hat{\beta}_j).
$${#eq-7.1.2.16}
An estimate of the marginal rate $\bar{\theta}_j$ defined in @eq-7.1.2.4 can then be obtained by averaging these observation-level estimates over the index $i$:
$$
\hat{\theta}_{\bullet j} = \frac{1}{n_j} \sum_{i=1}^{n_j} \exp(X_{ij}^\top \hat{\beta}_j) = \frac{1}{n_j} \mathbf{1}_{n_j}^\top \exp(X_j \hat{\beta}_j).
$${#eq-7.1.2.17}
Finally, substituting these into @eq-7.1.2.5 yields the MLE for $\psi$:
$$
\hat{\psi} = \sum_{j=1}^J \alpha_j \hat{\theta}_{\bullet j} = \alpha^\top \hat{\theta}_\bullet,
$${#eq-7.1.2.18}
where 
$$
\alpha = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_J \end{pmatrix} \> \> \> \> \text{and} \> \> \> \> \hat{\theta}_\bullet = \begin{pmatrix} \hat{\theta}_{\bullet 1} \\ \vdots \\ \hat{\theta}_{\bullet J} \end{pmatrix}.
$$
If we let 
$$
\gamma = \begin{pmatrix} \frac{\alpha_1}{n_1} \mathbf{1}_{n_1} \\ \vdots \\ \frac{\alpha_J}{n_J} \mathbf{1}_{n_J} \end{pmatrix} \in \mathbb{R}^{n\times 1},
$${#eq-7.1.2.19} 
we can rewrite our expression for $\hat{\psi}$ directly in terms of the MLE for $\beta$: 
$$
\hat{\psi} = \gamma^\top \exp(X\hat{\beta}).
$${#eq-7.1.2.20}
This allows us to write our manifold of parameter vectors that produce the same estimate of $\psi$ with the compact notation
$$
\Omega_{\hat{\psi}} = \Big\{\omega \in \mathbb{R}^{(pJ + q) \times 1}: \gamma^\top \exp(X \omega) = \gamma^\top \exp(X\hat{\beta})\Big\}.
$${#eq-7.1.2.21}

To estimate the value of the ZSE-parameterized integrated likelihood at a specific value of $\psi$, say $\psi_1$, using the method in Chapter 5, we use the following procedure.

\begin{tcolorbox}[title=7.1.2.1 Integrated Likelihood Calculation Procedure, algobox, label=alg:7.1.2.1-il-estimation]

\begin{enumerate}[left=0pt, label=\arabic*.]  % outer list
\item Draw a random variate $\hat{\omega} \in \Omega_{\hat{\psi}}$ according to a distribution not depending on $\psi$. The method we have chosen for our simulations is:

    \begin{enumerate}[label=(\roman*), left=2em]  % inner list
    \item Draw a random vector $u = (u_1, \ldots, u_m)$, where $m = pJ + q$ and each $u_k \overset{\text{i.i.d.}}{\sim} \text{N}(0, 1)$, $k = 1, \ldots, m$.
    
    \item Pass $u$ as the initial guess to an \texttt{auglag()} call from the \texttt{nloptr} R package that minimizes a dummy function $f(\hat{\omega}) \equiv 0$ subject to the constraint
       $$h(\hat{\omega}) \equiv \gamma^\top \exp(X \hat{\omega}) - \hat{\psi} = 0.$$
    \end{enumerate}

\item Solve
   $$\tilde{\beta} = \underset{\beta \in \mathbb{R}^m}{\arg\max} \> \> \big(t \odot \exp(X\hat{\omega})\big)^\top X \beta - t^\top \exp(X \beta) \> \> \> \> \text{subject to} \> \> \gamma^\top \exp(X \hat{\omega}) = \psi.$$

\item Define the branch evaluated at $\psi_1$ as 
   $$\ell_b^{(1)}(\psi_1) = Y^\top X \tilde{\beta} - t^\top \exp(X \tilde{\beta}).$$

\item Repeat steps (1) - (3) $R-1$ times to obtain $\ell_b^{(1)}(\psi_1), \ldots, \ell_b^{(R)}(\psi_1)$, then calculate
   $$\hat{\bar{L}}(\psi_1) = \frac{1}{R} \sum_{r=1}^R \exp\Big(\ell_b^{(r)}(\psi_1)\Big).$$

\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[title=7.1.2.1 Profile Likelihood Calculation Procedure, algobox, label=alg:7.1.2.1-il-estimation]
\begin{enumerate}[left=0pt, label=\arabic*.]

\item Solve
   $$\tilde{\beta} = \underset{\beta \in \mathbb{R}^m}{\arg\max} \> \> \big(t \odot \exp(X\hat{\beta})\big)^\top X \beta - t^\top \exp(X \beta) \> \> \> \> \text{subject to} \> \> \gamma^\top \exp(X \hat{\beta}) = \psi.$$
\item Obtain the profile likelihood evaluated at $\psi_1$ by calculating
$$
L_p(\psi_1) = \exp\Big(Y^\top X \tilde{\beta} - t^\top \exp(X \tilde{\beta}) \Big).
$$

\end{enumerate}
\end{tcolorbox}

\subsubsection{The Negative Binomial GLM}

Suppose the conditional distribution of $Y_{ij}$ given $X_{ij}$ is $Y_{ij} | X_{ij} \sim \text{NB}(\mu_{ij}, r_j)$. We may continue to interpret $\mu_{ij}$ as the expected value of $Y_{ij}$ conditional on $X_{ij}$,  while $r_j$ is best interpreted as a dispersion parameter that measures the excess variation across counts within process $j$ that a Poisson random variable would fail to capture. Then the conditional probability mass function of $Y_{ij}$ can be expressed in canonical exponential-family form as

$$
\begin{aligned}
\text{P}(Y_{ij} = y | X_{ij}) &= \frac{\Gamma(y + r_j)}{\Gamma(r_j)y!}\Bigg(\frac{r_j}{r_j + \mu_{ij}}\Bigg)^{r_j}\Bigg(\frac{\mu_{ij}}{r_j + \mu_{ij}}\Bigg)^y, \> \> y = 0, 1, 2, ... \\
                              
\end{aligned}
$$
where $\eta_{ij} = \log(\theta_{ij})$ is the natural parameter of the model. 

As a transformation of the rate, $\eta_{ij}$ governs the mean of the distribution and thus captures how the expected count depends on the covariates. Following from the bedrock regression principle that systematic variation in the mean response should be explained linearly in terms of the predictors, we formalize this dependence by defining the *linear predictor* for observation $i$ from process $j$ as $X_{ij}^\top \beta_j,$ a linear combination of the covariates with a vector of unknown coefficients $\beta_j$. 

In a GLM, the systematic component represented by $X_{ij}^\top \beta_j$ must be connected to its random component $\mu_{ij}$, the conditional mean of the response. This is done via a *link function* $g(\cdot)$, which specifies the relationship $$g(\mu_{ij}) =X_{ij}^\top \beta_j.$$ The *canonical link function* is the special choice of $g(\cdot)$ that sets the natural parameter equal to the linear predictor, i.e., $$\eta_{ij} = X_{ij}^\top \beta_j.$${#eq-7.1.2.6} 

For the Poisson distribution, we have already shown that the natural parameter is related to the rate through $\eta_{ij} = \log(\theta_{ij})$. Because the conditional mean of each Poisson process includes the exposure time, $\mu_{ij} = t_{ij}\theta_{ij}$, the model can be equivalently written as 
$$
\begin{aligned}
\log(\mu_{ij}) &= \log(t_{ij}\theta_{ij}) \\
               &= \log(t_{ij}) + \log(\theta_{ij}) \\
               &= \log(t_{ij}) + \eta_{ij} \\
               &= \log(t_{ij}) + X_{ij}^\top \beta_j, &&(\text{using the canonical link})
\end{aligned}
$$ {#eq-7.1.2.7}
where $\log(t_{ij})$ acts as a known offset. It follows that the canonical link for a Poisson GLM is the *log link* $g(\mu_{ij}) = \log(\mu_{ij}/t_{ij})$, under which additive effects of $X_{ij}$ correspond to multiplicative effects on $\mu_{ij}$.

To simplify notation, we include a leading 1 in the process-specific covariate vector $X_{ij}^{(s)}$ that corresponds to the intercept term for process $j$. We then partition the coefficient vector $\beta_j$ as $$\beta_j = \begin{pmatrix}\beta_j^{(s)} \\ \beta^{(a)}\end{pmatrix} \in \mathbb{R}^{(p + q) \times 1},$${#eq-7.1.2.8} where $\beta_j^{(s)} \in \mathbb{R}^{p\times1}$ contains all the coefficients specific to process $j$ (including the intercept) and $\beta^{(a)} \in \mathbb{R}^{q\times1}$ contains the shared, process-agnostic slopes. The linear predictor for observation $i$ from process $j$ can then be written as $$\eta_{ij} = \big(X_{ij}^{(s)}\big)^\top \beta_j^{(s)} + \big(X_{ij}^{(a)}\big)^\top \beta^{(a)}.$${#eq-7.1.2.9}

Stacking the observations for process $j$, we can express the model in matrix form as  
$$
\eta_j = X_j^{(s)} \beta_j^{(s)} + X_j^{(a)} \beta^{(a)},
$${#eq-7.1.2.10}
where $X_j^{(s)} \in \mathbb{R}^{n_j \times p}$ and $X_j^{(a)} \in \mathbb{R}^{n_j \times q}$ are the design matrices of process-specific and process-agnostic covariates, respectively, and $\eta_j = (\eta_{1j}, \ldots, \eta_{n_j j})^\top$ is the vector of linear predictors for process $j$. If we concatenate the design matrices so that $X_j = [X_j^{(s)} \> X_j^{(a)}] \in \mathbb{R}^{n_j \times (p + q)}$, we can further simplify @eq-7.1.2.10 as
$$
\eta_j = X_j \beta_j.
$${#eq-7.1.2.11}
Collecting all coefficients across all processes gives the global parameter vector
$$
\beta = \begin{pmatrix}\beta_1^{(s)} \\[3pt] \vdots \\[3pt] \beta_J^{(s)} \\[3pt] \beta^{(a)} \end{pmatrix}
\in\mathbb{R}^{(pJ+q) \times 1},
$$
and we view $\beta_j= (\beta_j^{(s)\top}, \beta^{(a)\top})^\top$ as the subvector of $\beta$ relevant to process $j$.

The log-likelihood for an individual observation $Y_{ij}$ follows directly from the exponential-family representation of the Poisson distribution:
$$
\ell_{ij}(\beta_j; Y_{ij}) = Y_{ij} X_{ij}^\top \beta_j - t_{ij} \exp(X_{ij}^\top \beta_j)),
$${#eq-7.1.2.12}
where we have discarded any additive terms not depending on the parameters. Summing over all observations within process $j$ gives the process-level log-likelihood,
$$
\begin{aligned}
\ell_j(\beta_j; Y_j) &= \sum_{i=1}^{n_j} \ell_{ij}(\beta_j; Y_{ij}) \\
                     &= \sum_{i=1}^{n_j} \Big[ Y_{ij} X_{ij}^\top \beta_j - t_{ij} \exp(X_{ij}^\top \beta_j)) \Big] \\
                     &= Y_j^\top X_j \beta_j - t_j^\top \exp(X_j\beta_j),
\end{aligned}
$${#eq-7.1.2.13}
where $Y_j \in \mathbb{R}^{n_j\times1}$ and $t_j\in \mathbb{R}^{n_j\times1}$ collect the observed counts and exposure times, respectively, for process $j$, and $\exp(X_j\beta_j)$ is understood to mean the exponential function applied component-wise to the vector $X_j\beta_j$.  

Finally, aggregating across all processes yields the full log-likelihood,
$$
\begin{aligned}
\ell(\beta; Y) &= \sum_{j=1}^J \ell_j(\beta_j; Y_j) \\
               &= \sum_{j=1}^J \Big[ Y_j^\top X_j \beta_j - t_j^\top \exp(X_j\beta_j) \Big] \\
               &= Y^\top X \beta - t^\top \exp(X \beta),
\end{aligned}
$${#eq-7.1.2.14}
where $n = \sum_{j=1}^J n_j$, $Y = (Y_1^\top, ..., Y_J^\top)^\top \in \mathbb{R}^{n\times1}$ is the stacked response vector, $t = (t_1^\top, ..., t_J^\top)^\top \in \mathbb{R}^{n\times1}$ is the stacked vector of exposure times, and $X = \text{blockdiag}(X_1, ..., X_j) \in \mathbb{R}^{n \times (pJ+q)}$ is the block-diagonal design matrix with process-specific blocks, augmented with shared covariates $X_j^{(a)}$ repeated across processes as necessary.

Let $\hat{\beta}_j$ denote the MLE of the coefficient vector $\beta_j$. Because the exponential term in the log-likelihood in @eq-7.1.2.14 makes it a nonlinear function of each $\beta_j$, the score equations for this model cannot be solved algebraically, and thus no closed-form expressions for $\hat{\beta}_j$ exists. Instead, $\hat{\beta}_j$ can be obtained numerically using the `glm()` function from the `stats` package in R, which implements the standard iteratively reweighted least squares (IRLS) algorithm for approximating the MLEs in the GLM framework.

From @eq-7.1.2.6, the corresponding estimated linear predictors for process $j$ are given by
$$
\hat{\eta}_{ij} = X_{ij}^\top \hat{\beta}_j.
$${#eq-7.1.2.15}
Since $\eta_{ij} = \log(\theta_{ij})$, the estimated observation-level rate is 
$$
\hat{\theta}_{ij} = \exp(X_{ij}^\top \hat{\beta}_j).
$${#eq-7.1.2.16}
An estimate of the marginal rate $\bar{\theta}_j$ defined in @eq-7.1.2.4 can then be obtained by averaging these observation-level estimates over the index $i$:
$$
\hat{\theta}_{\bullet j} = \frac{1}{n_j} \sum_{i=1}^{n_j} \exp(X_{ij}^\top \hat{\beta}_j) = \frac{1}{n_j} \mathbf{1}_{n_j}^\top \exp(X_j \hat{\beta}_j).
$${#eq-7.1.2.17}
Finally, substituting these into @eq-7.1.2.5 yields the MLE for $\psi$:
$$
\hat{\psi} = \sum_{j=1}^J \alpha_j \hat{\theta}_{\bullet j} = \alpha^\top \hat{\theta}_\bullet,
$${#eq-7.1.2.18}
where 
$$
\alpha = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_J \end{pmatrix} \> \> \> \> \text{and} \> \> \> \> \hat{\theta}_\bullet = \begin{pmatrix} \hat{\theta}_{\bullet 1} \\ \vdots \\ \hat{\theta}_{\bullet J} \end{pmatrix}.
$$
If we let 
$$
\gamma = \begin{pmatrix} \frac{\alpha_1}{n_1} \mathbf{1}_{n_1} \\ \vdots \\ \frac{\alpha_J}{n_J} \mathbf{1}_{n_J} \end{pmatrix} \in \mathbb{R}^{n\times 1},
$${#eq-7.1.2.19} 
we can rewrite our expression for $\hat{\psi}$ directly in terms of the MLE for $\beta$: 
$$
\hat{\psi} = \gamma^\top \exp(X\hat{\beta}).
$${#eq-7.1.2.20}
This allows us to write our manifold of parameter vectors that produce the same estimate of $\psi$ with the compact notation
$$
\Omega_{\hat{\psi}} = \Big\{\omega \in \mathbb{R}^{(pJ + q) \times 1}: \gamma^\top \exp(X \omega) = \gamma^\top \exp(X\hat{\beta})\Big\}.
$${#eq-7.1.2.21}

To estimate the value of the ZSE-parameterized integrated likelihood at a specific value of $\psi$, say $\psi_1$, using the method in Chapter 5, we use the following procedure.

\begin{tcolorbox}[title=7.1.2.1 Integrated Likelihood Calculation Procedure, algobox, label=alg:7.1.2.1-il-estimation]

\begin{enumerate}[left=0pt, label=\arabic*.]  % outer list
\item Draw a random variate $\hat{\omega} \in \Omega_{\hat{\psi}}$ according to a distribution not depending on $\psi$. The method we have chosen for our simulations is:

    \begin{enumerate}[label=(\roman*), left=2em]  % inner list
    \item Draw a random vector $u = (u_1, \ldots, u_m)$, where $m = pJ + q$ and each $u_k \overset{\text{i.i.d.}}{\sim} \text{N}(0, 1)$, $k = 1, \ldots, m$.
    
    \item Pass $u$ as the initial guess to an \texttt{auglag()} call from the \texttt{nloptr} R package that minimizes a dummy function $f(\hat{\omega}) \equiv 0$ subject to the constraint
       $$h(\hat{\omega}) \equiv \gamma^\top \exp(X \hat{\omega}) - \hat{\psi} = 0.$$
    \end{enumerate}

\item Solve
   $$\tilde{\beta} = \underset{\beta \in \mathbb{R}^m}{\arg\max} \> \> \big(t \odot \exp(X\hat{\omega})\big)^\top X \beta - t^\top \exp(X \beta) \> \> \> \> \text{subject to} \> \> \gamma^\top \exp(X \hat{\omega}) = \psi.$$

\item Define the branch evaluated at $\psi_1$ as 
   $$\ell_b^{(1)}(\psi_1) = Y^\top X \tilde{\beta} - t^\top \exp(X \tilde{\beta}).$$

\item Repeat steps (1) - (3) $R-1$ times to obtain $\ell_b^{(1)}(\psi_1), \ldots, \ell_b^{(R)}(\psi_1)$, then calculate
   $$\hat{\bar{L}}(\psi_1) = \frac{1}{R} \sum_{r=1}^R \exp\Big(\ell_b^{(r)}(\psi_1)\Big).$$

\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[title=7.1.2.1 Profile Likelihood Calculation Procedure, algobox, label=alg:7.1.2.1-il-estimation]
\begin{enumerate}[left=0pt, label=\arabic*.]

\item Solve
   $$\tilde{\beta} = \underset{\beta \in \mathbb{R}^m}{\arg\max} \> \> \big(t \odot \exp(X\hat{\beta})\big)^\top X \beta - t^\top \exp(X \beta) \> \> \> \> \text{subject to} \> \> \gamma^\top \exp(X \hat{\beta}) = \psi.$$
\item Obtain the profile likelihood evaluated at $\psi_1$ by calculating
$$
L_p(\psi_1) = \exp\Big(Y^\top X \tilde{\beta} - t^\top \exp(X \tilde{\beta}) \Big).
$$

\end{enumerate}
\end{tcolorbox}

\subsection{Mixed Effects Regression}

\subsubsection{Fixed Intercepts and Random Slopes}

\subsubsection{Random Intercepts and Fixed Slopes}

\subsubsection{Random Intercepts and Random Slopes}

\section{Overdispersion}

\section{Zero-Inflation}